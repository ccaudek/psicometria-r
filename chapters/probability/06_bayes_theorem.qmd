# Il teorema di Bayes {#sec-prob-bayes-theorem}

::: callout-important
## In questo capitolo imparerai a

- capire in profondità il teorema di Bayes e la sua importanza;
- utilizzare il teorema di Bayes per analizzare e interpretare i test diagnostici, tenendo in considerazione la prevalenza della malattia in questione;
- affrontare e risolvere problemi di probabilità discreta che necessitano dell'applicazione del teorema di Bayes.
:::

::: callout-tip
## Prerequisiti

- Leggere *Everything is Predictable: How Bayesian Statistics Explain Our World* [@chivers2024everything]. Questo libro offre una descrizione chiara e accessibile dell'impatto che il teorema di Bayes ha avuto sulla vita moderna.
- Leggere [Bayesian Models of Cognition](https://oecs.mit.edu/pub/lwxmte1p/release/2) di Thomas L. Griffiths, una voce della [Open Encyclopedia of Cognitive Science](https://oecs.mit.edu).
- Leggere il capitolo *Conditional Probability* [@schervish2014probability].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

> "It is, without exaggeration, perhaps the most important single equation in history."  
> — **Tom Chivers (2024)**

## Introduzione

Il teorema di Bayes fornisce una soluzione ottimale ai problemi induttivi, ovvero quelle situazioni in cui cerchiamo di inferire caratteristiche generali (come la struttura tridimensionale di un ambiente o i pensieri di un’altra persona) sulla base di dati parziali [@ma2023bayesian; @domini20033; @caudek2024fenomeni; @baker2011bayesian]. Questa regola si rivela particolarmente utile quando le evidenze disponibili non permettono di distinguere in modo definitivo tra diverse ipotesi.

Di fatto, tutte le previsioni e le inferenze ottenute con questo approccio conservano un margine di incertezza. Perfino in un universo interamente deterministico, la nostra conoscenza non potrebbe mai essere totale: non è possibile determinare la posizione e lo stato di ogni particella che compone la realtà. Disponiamo di dati parziali e sensi limitati, motivo per cui l’incertezza diventa il presupposto ineludibile di ogni inferenza.

In tal senso, la vita reale è più simile a una partita di poker che a una di scacchi: non possediamo mai informazioni perfette e complete, ma prendiamo decisioni basandoci sulle conoscenze (o ipotesi) disponibili [@chivers2024everything]. Il teorema di Bayes descrive come aggiornare le nostre convinzioni su una determinata ipotesi man mano che raccogliamo nuovi dati, in un contesto di incertezza che non si può eliminare ma soltanto gestire.

Questo processo prende il nome di **inferenza induttiva**, ossia la capacità di formulare congetture generali (come la posizione di un oggetto o lo stato mentale di un individuo) a partire da osservazioni limitate. Il teorema di Bayes fornisce un quadro matematico solido per integrare gradualmente nuove informazioni e, quindi, prendere decisioni o formulare previsioni in modo razionale anche in condizioni di evidenze incomplete.


## Una Rivoluzione nel Pensiero Probabilistico

Nel XVIII secolo, Thomas Bayes (1701-1761), un ecclesiastico presbiteriano, gettò le basi di una vera e propria rivoluzione nel calcolo delle probabilità e nella statistica. Il suo contributo è passato alla storia come **teorema di Bayes** e, nel corso dei secoli, ha segnato profondamente la scienza e la tecnologia moderne, compresa l’intelligenza artificiale [@chivers2024everything].

### La figura di Thomas Bayes

Bayes proveniva da una famiglia benestante e studiò teologia a Edimburgo, preparandosi al ministero religioso. Come ricorda il biografo David Bellhouse, Bayes non era un accademico nel senso moderno del termine, ma un erudito libero, interessato alla conoscenza per passione personale [@bellhouse2004].

Durante la sua vita, Bayes pubblicò due testi:

1. **Un trattato di teologia**: *Divine Benevolence: Or, an Attempt to Prove that the Principal End of the Divine Providence and Government is the Happiness of His Creatures* (1731), una teodicea che cerca di spiegare come la legge naturale possa ottimizzare il benessere universale.  
2. **Una difesa del calcolo infinitesimale**: *An Introduction to the Doctrine of Fluxions* (1736), in risposta alle critiche di George Berkeley sugli infinitesimi e i concetti fondamentali del calcolo newtoniano [@jesseph1993berkeley].

Il lavoro che segnò la svolta nella teoria della probabilità fu però pubblicato postumo, nel 1763, sulle *Philosophical Transactions of the Royal Society*: *An Essay towards Solving a Problem in the Doctrine of Chances*. Per la prima volta, si formalizzava un metodo per aggiornare le ipotesi probabilistiche alla luce di nuove evidenze, ponendo le fondamenta dell’inferenza bayesiana [@stigler1990history].

### Bayes e il ruolo culturale della scienza

Come sottolinea ancora Bellhouse, nel XVIII secolo era comune, tra le élite colte, dedicarsi allo studio di discipline scientifiche per prestigio sociale. Per Bayes, la matematica era dunque una passione coltivata con spirito libero. Il suo merito straordinario fu di spingere l’interpretazione della probabilità verso una prospettiva **epistemologica** innovativa, dove la probabilità diventa espressione quantitativa della nostra ignoranza sul mondo. 

In contrapposizione alla visione “classica”, che vedeva la probabilità come frequenza osservabile in eventi ripetuti, Bayes propose che essa potesse rappresentare il **grado di fiducia** di un osservatore, inevitabilmente influenzato da conoscenze pregresse e da pregiudizi individuali. In questo senso, la probabilità assume un carattere dinamico e soggettivo, configurandosi come uno **strumento di conoscenza** che si aggiorna di continuo al variare dei dati [@spiegelhalter2019art].

#### Un esperimento mentale illuminante

Per illustrare la sua idea, Bayes propose un semplice esempio: immagina di lanciare alcune palline su un tavolo da biliardo. Dopo aver segnato con una linea il punto in cui si ferma una pallina bianca (e averla poi rimossa), si lanciano altre palline rosse e si conta quante cadono a destra e quante a sinistra di quella linea. Sulla base di queste osservazioni, come si può “indovinare” la posizione della linea? E con quale probabilità la prossima pallina rossa cadrà a sinistra di essa?

La soluzione di Bayes combina i **dati osservati** (numero di palline cadute a sinistra o a destra) con le **convinzioni iniziali** dell’osservatore (il cosiddetto “prior”), delineando un processo di apprendimento graduale che guida la revisione critica delle ipotesi.

### Il ruolo di Richard Price

Dopo la morte di Bayes, fu un altro ecclesiastico, **Richard Price (1723-1791)**, a dare impulso alla diffusione del saggio bayesiano. Price aveva un’ottima reputazione negli ambienti intellettuali dell’epoca, grazie anche alle sue relazioni con figure di spicco come **Benjamin Franklin**, **Thomas Jefferson** e **John Adams**.

Price prese in carico il manoscritto di Bayes, lo sottopose al fisico **John Canton** e ne curò la pubblicazione postuma, operando modifiche significative. Rispetto alla versione originale di Bayes, concentrata quasi esclusivamente sugli aspetti teorici, Price aggiunse una parte dedicata alle applicazioni pratiche, rendendo il testo più fruibile a un pubblico più ampio. Per questo motivo, lo storico Stephen Stigler lo definisce «il primo bayesiano della storia».

### Dal silenzio alla riscoperta

Per oltre cinquant’anni, il lavoro di Bayes rimase in ombra, oscurato dall’opera pionieristica di **Pierre-Simon Laplace**. Già nel 1774, Laplace pervenne indipendentemente a principi analoghi, e successivamente li sistematizzò nella monumentale *Théorie analytique des probabilités* (1812). Solo in tempi più recenti, con l’avvento dei metodi di calcolo moderno e dell’informatica, la statura del teorema di Bayes è emersa in tutta la sua importanza.

Oggi, il teorema di Bayes è considerato un cardine della statistica moderna: formalizza il modo in cui aggiorniamo le nostre credenze alla luce di nuovi dati. Questo schema è cruciale in ogni disciplina scientifica e tecnologica che debba fare i conti con incertezza e dati incompleti. Dalla genomica all’econometria, dalla fisica delle particelle alle scienze cognitive, il paradigma bayesiano risulta prezioso per gestire e interpretare informazioni in continuo aggiornamento.

### L’eredità di Bayes nell’era digitale

Nell’intelligenza artificiale, le idee bayesiane sono alla base di sistemi di apprendimento automatico e modelli probabilistici complessi. Strumenti come i moderni modelli linguistici (ad esempio ChatGPT e Claude) sfruttano strategie di inferenza bayesiana – anche se in forme estremamente avanzate – per generare risposte, fare previsioni e adattarsi costantemente agli input degli utenti.

La parabola storica di questo teorema, nato dalle speculazioni di un pastore presbiteriano del Settecento, mostra chiaramente il potenziale trasformativo delle idee matematiche. Come sottolinea Tom Chivers nel suo *Everything Is Predictable: How Bayesian Statistics Explain Our World*, la statistica bayesiana è diventata una sorta di **“grammatica universale”** per interpretare la realtà, permettendoci di affrontare con metodo situazioni complesse, modellare l’incertezza e formulare previsioni in contesti dove l’informazione è inevitabilmente limitata [@chivers2024everything].

**In sintesi**, la forza del teorema di Bayes non risiede soltanto nella sua eleganza formale, ma soprattutto nella sua **portata epistemologica**: esso traduce in termini matematici la nostra naturale tendenza ad apprendere da ciò che osserviamo e a rivedere continuamente ciò che crediamo. Per questo rimane, ancora oggi, un punto di riferimento fondamentale in qualunque disciplina che affronti il problema della conoscenza in condizioni di incertezza.

## La Regola di Bayes

L'inferenza bayesiana si basa su un principio fondamentale della teoria delle probabilità: la **regola di Bayes**. Questa formula consente di aggiornare le credenze alla luce di nuove evidenze, combinando informazioni a priori con dati osservati.

Per comprenderla, consideriamo due variabili casuali, $A$ e $B$. La probabilità congiunta di questi due eventi, ovvero la probabilità che entrambi accadano simultaneamente, può essere espressa in due modi diversi.

**1. Applicazione della regola della catena.**  
La **regola della catena** ci permette di esprimere la probabilità congiunta come il prodotto tra una probabilità condizionata e una probabilità marginale. In particolare, possiamo scrivere:

$$
P(A, B) = P(A \mid B) P(B).
$$

Questa espressione indica che la probabilità congiunta $P(A, B)$ si ottiene moltiplicando la probabilità condizionata $P(A \mid B)$ (cioè la probabilità che $A$ si verifichi dato che $B$ è accaduto) per la probabilità marginale $P(B)$ (cioè la probabilità che $B$ si verifichi indipendentemente da $A$).

**2. Simmetria della probabilità congiunta.**  
Poiché la probabilità congiunta è simmetrica, possiamo anche esprimerla invertendo l’ordine delle variabili:

$$
P(A, B) = P(B \mid A) P(A).
$$

Questa formula è ottenuta applicando la stessa regola della catena, ma considerando prima $B$ condizionato a $A$.

**3. Eguaglianza delle due espressioni.**  
Poiché entrambi i modi di scrivere la probabilità congiunta rappresentano la stessa quantità, possiamo eguagliarli:

$$
P(A \mid B) P(B) = P(B \mid A) P(A).
$$

**4. Derivazione della regola di Bayes.**  
Ora possiamo risolvere questa equazione per $P(B \mid A)$. Dividendo entrambi i lati per $P(A)$, otteniamo:

$$
P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
$$

Questa è la **regola di Bayes**. Essa ci permette di calcolare la probabilità che $B$ si verifichi dato che sappiamo che $A$ è accaduto, utilizzando:

- $P(A \mid B)$, cioè la probabilità di osservare $A$ se $B$ fosse vero;
- $P(B)$, la probabilità a priori di $B$ (cioè prima di considerare l'evidenza di $A$);
- $P(A)$, la probabilità marginale di $A$, che funge da termine di normalizzazione per assicurare che le probabilità condizionate si sommino a 1.

Questa formula è il cuore dell'inferenza bayesiana, poiché consente di aggiornare la nostra conoscenza su un evento alla luce di nuove osservazioni.

Nel contesto dell'inferenza bayesiana:

- **$P(B)$** rappresenta la nostra conoscenza iniziale (**prior**),
- **$P(A \mid B)$** è la **verosimiglianza**, ovvero la probabilità di osservare i dati supponendo che l'ipotesi sia vera,
- **$P(B \mid A)$** è la probabilità aggiornata (**posterior**), che incorpora l'evidenza fornita dai dati.


## Applicazioni della Regola di Bayes

La **regola di Bayes** è alla base dell’inferenza probabilistica: fornisce il metodo formale per aggiornare le nostre credenze (o ipotesi) alla luce di nuovi dati. Immaginiamo di disporre di un insieme di dati osservati, denotato con $D$. L’insieme di tutte le possibili spiegazioni o **ipotesi** che potrebbero aver generato questi dati è detto **spazio delle ipotesi** $\mathcal{H}$. Se indichiamo con $H$ una specifica ipotesi in $\mathcal{H}$ e con $H'$ le possibili ipotesi alternative, ciò che vogliamo stabilire è: *con quale probabilità i dati $D$ sono stati generati proprio da $H$, anziché da un’altra ipotesi $H' \in \mathcal{H}$?*

### Probabilità a priori e a posteriori

Prima di raccogliere i dati $D$, l’agente (o il ricercatore) assegna a ogni ipotesi $H$ una **probabilità a priori** $P(H)$, che rappresenta il grado di fiducia iniziale in $H$, basato su conoscenze pregresse, supposizioni o esperienze passate.

Dopo aver osservato i dati, questa stima va modificata: i dati raccolti possono confermare (o mettere in dubbio) l’ipotesi. La **probabilità a posteriori** $P(H \mid D)$ indica dunque la probabilità che $H$ sia vera alla luce dell’evidenza empirica fornita da $D$.

### La formula della regola di Bayes

La **regola di Bayes** formalizza questo processo di aggiornamento:

$$
P(H \mid D) = \frac{P(D \mid H) \, P(H)}{P(D)},
$$ {#eq-bayes-rule-2}

dove ciascun termine ha un preciso significato:

1. $\mathbf{P(D \mid H)}$ (**verosimiglianza** o **likelihood**) – Rappresenta la probabilità di osservare i dati $D$, supponendo che l’ipotesi $H$ sia vera. In pratica, è una misura di quanto bene $H$ “spiega” i dati osservati.
2. $\mathbf{P(H)}$ (**probabilità a priori**) – Indica quanto ritenevamo probabile $H$ prima di guardare i dati.
3. $\mathbf{P(D)}$ (**probabilità marginale** o **evidenza**) – È la probabilità complessiva di osservare $D$, considerando tutte le ipotesi possibili nello spazio $\mathcal{H}$. Matematicamente, si calcola come:

$$
P(D) = \sum_{H' \in \mathcal{H}} P(D \mid H') \, P(H').
$$

Questo termine, chiamato anche **fattore di normalizzazione**, assicura che le probabilità a posteriori di tutte le ipotesi in $\mathcal{H}$ sommate insieme diano 1 (come richiede la coerenza probabilistica).

**Significato operativo della formula:**

- il **rapporto** $\frac{P(D \mid H)}{P(D)}$ ci dice *quanto* i dati favoriscano l’ipotesi $H$ rispetto alle altre; 
- moltiplicando questo valore per la **probabilità a priori** $P(H)$, otteniamo la **probabilità a posteriori** $P(H \mid D)$, cioè il grado di credenza in $H$ dopo aver preso in esame l’evidenza empirica.

In altri termini, la regola di Bayes evidenzia come le ipotesi con più elevata verosimiglianza (ossia che spiegano meglio i dati) e/o con un priore più alto siano quelle che la mente (o il modello statistico) propende a ritenere più plausibili, dopo aver osservato i dati.

### Nota sulle ipotesi continue

Nel caso in cui $\mathcal{H}$ contenga un’infinità di ipotesi (ad esempio, ipotesi parametriche con valori reali), la somma $\sum_{H' \in \mathcal{H}}$ si trasforma in un **integrale** sull’insieme continuo di ipotesi. La logica rimane comunque la stessa: i dati modulano e “pesano” ogni ipotesi in base alla sua compatibilità con l’evidenza sperimentale.

**In sintesi**, la regola di Bayes è il cuore di molti metodi di inferenza in statistica, machine learning e scienze naturali. Essa ci consente di integrare informazioni nuove all’interno del nostro bagaglio di conoscenze pregresse, aggiornando in modo razionale le ipotesi sul mondo che ci circonda.

### La Marginalizzazione

Il calcolo della probabilità marginale $P(D)$ è fondamentale per normalizzare la distribuzione a posteriori. Questo avviene considerando tutte le ipotesi possibili. 

- **Caso discreto:** la probabilità marginale si ottiene sommando su tutte le possibili ipotesi $H' \in \mathcal{H}$:

  $$
  P(D) = \sum_{H' \in \mathcal{H}} P(D \mid H') P(H').
  $$

  Sostituendo questa espressione nella regola di Bayes, otteniamo la formula esplicita della distribuzione a posteriori:

  $$
  P(H \mid D) = \frac{P(D \mid H) P(H)}{\sum_{H' \in \mathcal{H}} P(D \mid H') P(H')}.
  $$ {#eq-marginal-prob}

- **Caso continuo:** se lo spazio delle ipotesi è continuo, la somma viene sostituita da un **integrale**:

  $$
  P(D) = \int_{\mathcal{H}} P(D \mid H') P(H') \, dH'.
  $$

  La distribuzione a posteriori si esprime quindi come:

  $$
  P(H \mid D) = \frac{P(D \mid H) P(H)}{\int_{\mathcal{H}} P(D \mid H') P(H') \, dH'}.
  $$ {#eq-bayes-rule-cont}

L'evidenza $P(D)$ può essere computazionalmente onerosa da calcolare, specialmente in spazi ad alta dimensionalità. Per questo motivo, si utilizzano tecniche di approssimazione come il **campionamento Monte Carlo** o i **metodi variazionali**.

### Componenti Chiave della Formula di Bayes

La regola di Bayes si basa su tre elementi essenziali:

1. **probabilità a priori $P(H)$**: la credenza iniziale sull'ipotesi $H$ prima di osservare i dati;
2. **verosimiglianza $P(D \mid H)$**: la probabilità dei dati osservati, dato che $H$ sia vera. Misura quanto l'ipotesi è compatibile con l'evidenza;
3. **probabilità a posteriori $P(H \mid D)$**: la credenza aggiornata in $H$ dopo aver osservato i dati.

L'aggiornamento delle credenze attraverso la **regola di Bayes** è un processo iterativo: ogni volta che vengono raccolti nuovi dati, la distribuzione a posteriori diventa la nuova distribuzione a priori per il successivo aggiornamento. Questo meccanismo consente di adattare continuamente le proprie credenze alla luce di nuove informazioni, rendendo l'approccio bayesiano estremamente utile per modellare il ragionamento umano in condizioni di incertezza.

### Applicazioni in Psicologia

Negli ultimi anni, i **modelli bayesiani** hanno acquisito un ruolo centrale nello studio della cognizione umana, fornendo una struttura formale per comprendere come il cervello costruisca rappresentazioni del mondo e prenda decisioni sulla base di dati incerti. Come discusso da @griffiths2024bayesian, questi modelli sono stati applicati a una vasta gamma di processi cognitivi, tra cui:

- **Apprendimento e generalizzazione**: i modelli bayesiani descrivono come gli individui apprendano nuove categorie e concetti sulla base di dati limitati e rumorosi (Tenenbaum, Griffiths, & Kemp, 2006).
- **Percezione e interpretazione sensoriale**: la percezione visiva e il riconoscimento di oggetti possono essere spiegati come un'inferenza bayesiana sulla base di segnali sensoriali ambigui [@yuille2006vision; @domini20033].
- **Controllo motorio**: il sistema motorio umano sembra ottimizzare i movimenti attraverso una combinazione di modelli interni e aggiornamenti bayesiani (Kording & Wolpert, 2006).
- **Memoria e recupero delle informazioni**: i processi mnemonici, come il richiamo della memoria semantica, possono essere modellati come inferenze bayesiane basate su conoscenze pregresse (Steyvers, Griffiths, & Dennis, 2006).
- **Acquisizione del linguaggio**: l'apprendimento del linguaggio nei bambini può essere descritto attraverso processi probabilistici che permettono di inferire le strutture grammaticali sulla base di dati linguistici limitati (Chater & Manning, 2006; Xu & Tenenbaum, in press).
- **Apprendimento causale**: la capacità di inferire relazioni causali dagli eventi osservati è coerente con un modello bayesiano, in cui la mente valuta la probabilità di una relazione causale sulla base dell'evidenza disponibile (Griffiths & Tenenbaum, 2005, 2007).
- **Ragionamento e decisione**: il ragionamento simbolico e il processo decisionale possono essere formalizzati come un aggiornamento bayesiano delle credenze sulla base di nuove informazioni (Oaksford & Chater, 2001).
- **Cognizione sociale**: le inferenze sulle intenzioni e credenze altrui possono essere modellate attraverso processi bayesiani, permettendo di spiegare come le persone comprendano il comportamento altrui (Baker, Tenenbaum, & Saxe, 2007).

### L’Inferenza Bayesiana nella Cognizione Umana

Un tema centrale che emerge da questi programmi di ricerca è la seguente domanda: **come fa la mente umana ad andare oltre i dati dell'esperienza?** In altre parole, come riesce il cervello a costruire modelli complessi del mondo a partire da informazioni limitate e spesso ambigue?

L’approccio bayesiano propone che il cervello utilizzi un processo di **inferenza probabilistica** per aggiornare continuamente le proprie credenze, combinando informazioni pregresse con nuove osservazioni per affinare le proprie rappresentazioni mentali. Questo meccanismo consente di spiegare molte delle capacità cognitive umane, dall’apprendimento rapido di nuove categorie alla capacità di adattarsi a un ambiente mutevole, fino alla formulazione di inferenze sociali e alla presa di decisioni in condizioni di incertezza.

L’adozione dei modelli bayesiani nella psicologia cognitiva ha portato a una nuova comprensione della mente come **sistema predittivo**, in grado di formulare ipotesi probabilistiche sugli eventi futuri e di correggerle dinamicamente sulla base dell’esperienza. Questo approccio ha profonde implicazioni per lo studio del comportamento umano e per lo sviluppo di nuove tecniche di modellizzazione nei campi della psicologia, delle neuroscienze e dell’intelligenza artificiale.


## Test medici

Uno degli esempi più comuni per comprendere il teorema di Bayes riguarda i test diagnostici.

::: {#exm-}
Consideriamo un test di mammografia utilizzato per diagnosticare il cancro al seno che abbiamo già discusso nel @sec-prob-conditional-prob. Definiamo le seguenti ipotesi:

- **$M^+$**: la persona ha il cancro al seno;
- **$M^-$**: la persona non ha il cancro al seno.

L'evidenza è il risultato positivo del test, indicato con $T^+$. Il nostro obiettivo è calcolare la probabilità che una persona abbia il cancro al seno, dato un risultato positivo al test, ovvero $P(M^+ \mid T^+)$.

**Definizione dei termini nella regola di Bayes.**  
Il teorema di Bayes afferma che:

$$
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) P(M^+)}{P(T^+)} ,
$$

dove:

- **$P(T^+ \mid M^+)$** è la **sensibilità** del test, cioè la probabilità che il test risulti positivo se la persona ha effettivamente il cancro. Nel nostro caso, $P(T^+ \mid M^+) = 0.90$.
- **$P(M^+)$** è la **probabilità a priori** di avere il cancro al seno, ovvero la prevalenza della malattia nella popolazione. Supponiamo che sia $P(M^+) = 0.01$ (1%).
- **$P(T^+ \mid M^-)$** è la **probabilità di un falso positivo**, cioè la probabilità che il test risulti positivo anche in assenza di malattia. Questa è complementare alla specificità del test:

 $$
  P(T^+ \mid M^-) = 1 - \text{Specificità} = 1 - 0.90 = 0.10.
 $$

- **$P(M^-)$** è la probabilità a priori che una persona non abbia il cancro, ovvero:

 $$
  P(M^-) = 1 - P(M^+) = 1 - 0.01 = 0.99.
 $$

- **$P(T^+)$** è la probabilità marginale che il test risulti positivo, calcolata considerando entrambe le possibilità (cioè che la persona abbia o non abbia il cancro):

 $$
  P(T^+) = P(T^+ \mid M^+) P(M^+) + P(T^+ \mid M^-) P(M^-).
 $$

  Sostituendo i valori numerici:

 $$
  P(T^+) = (0.90 \cdot 0.01) + (0.10 \cdot 0.99) = 0.009 + 0.099 = 0.108.
 $$

**Applicazione della Regola di Bayes.**  
Ora possiamo calcolare la probabilità a posteriori $P(M^+ \mid T^+)$:

$$
P(M^+ \mid T^+) = \frac{0.90 \cdot 0.01}{0.108} = \frac{0.009}{0.108} = 0.0833.
$$

**Interpretazione del Risultato.**  
Questo risultato indica che, nonostante il test abbia una sensibilità e una specificità del 90%, la probabilità che una persona con un test positivo abbia effettivamente il cancro è solo dell’**8.3%**. Questo effetto è dovuto alla bassa prevalenza della malattia: anche se il test è relativamente accurato, il numero di falsi positivi è ancora alto rispetto ai veri positivi. Tale risultato conferma quanto precedentemente ottenuto nel @sec-prob-conditional-prob, attraverso un metodo di calcolo alternativo.

Questa formulazione mostra come la regola di Bayes permetta di aggiornare la probabilità di avere la malattia dopo aver osservato il risultato del test, combinando la sensibilità, la specificità e la prevalenza della malattia nella popolazione.
:::

### Affidabilità di un Test HIV e Aggiornamento Bayesiano

Vogliamo valutare l’affidabilità di un test per l’HIV e capire come la nostra stima di infezione **cambia dopo due test consecutivi positivi**. Utilizzeremo la **regola di Bayes** per aggiornare la probabilità di avere l’HIV man mano che otteniamo nuovi risultati.

:::{#exm-hiv} 

**Probabilità di HIV dopo *due* test consecutivi positivi**

**Notazione:**

- $\mathbf{M^+}$: la persona ha l’HIV;
- $\mathbf{M^-}$: la persona non ha l’HIV;
- $\mathbf{T^+}$: il test risulta positivo;
- $\mathbf{T^-}$: il test risulta negativo.

**Dati noti:**

- **prevalenza dell’HIV** (probabilità a priori di avere l’HIV):  
  $$
  P(M^+) = 0.003 \quad (\text{ossia } 0.3\%).
  $$

- **sensibilità** del test (probabilità di un test positivo se la persona è malata):  
  $$
  P(T^+ \mid M^+) = 0.95.
  $$

- **specificità** del test (probabilità di un test negativo se la persona NON è malata):  
  $$
  P(T^- \mid M^-) = 0.9928
  \quad \Longrightarrow \quad 
  P(T^+ \mid M^-) = 1 - 0.9928 = 0.0072.
  $$

**Passo 1: Probabilità di $M^+$ dopo un test positivo.**

Applichiamo la regola di Bayes per calcolare la probabilità di essere malati ($M^+$) dato un primo test positivo ($T^+$):

$$
P(M^+ \mid T^+) 
= \frac{P(T^+ \mid M^+) \, P(M^+)}{P(T^+)}.
$$

La **probabilità marginale** $P(T^+)$ di ottenere un test positivo (considerando sia i malati sia i non malati) si ottiene con:

$$
P(T^+) = P(T^+ \mid M^+) \, P(M^+) \;+\; P(T^+ \mid M^-) \, P(M^-).
$$

Tenendo conto che $P(M^-) = 1 - P(M^+) = 0.997$, calcoliamo:

$$
P(T^+) 
= (0.95 \times 0.003) + (0.0072 \times 0.997)
= 0.00285 + 0.00718
= 0.01003.
$$

A questo punto, la probabilità a posteriori $P(M^+ \mid T^+)$ è:

$$
P(M^+ \mid T^+) 
= \frac{0.95 \times 0.003}{0.01003}
= \frac{0.00285}{0.01003}
\approx 0.2844 
\quad (\text{cioè } 28.44\%).
$$

**Interpretazione**: dopo un **primo test positivo**, la probabilità di essere HIV-positivi sale dallo **0.3%** al **28.44%**, un aumento notevole, ma non ancora vicino al 100%.

**Passo 2: Probabilità di $M^+$ dopo due test positivi.**

Dopo il **primo test** risultato positivo, abbiamo già aggiornato la probabilità di malattia in $P(M^+ \mid T_1^+)$. Ora ci interessa calcolare la probabilità che il **secondo test** (indichiamolo con $T_2^+$) risulti positivo, **dato** che il primo test è stato positivo. Formalmente, cerchiamo:

$$
P(T_2^+ \mid T_1^+).
$$

Applicando il teorema delle probabilità totali sullo stato di malattia $M^+$ o non malattia $M^-$:

$$
P(T_2^+ \mid T_1^+) 
= P(T_2^+ \mid M^+, T_1^+) \, P(M^+ \mid T_1^+)
+ P(T_2^+ \mid M^-, T_1^+) \, P(M^- \mid T_1^+).
$$

- **Assunzione cruciale**: l’esito del secondo test, dati lo stato di salute $M^+$ o $M^-$, è indipendente dall’esito del primo test. In altre parole, se una persona è malata, la probabilità che **ciascun test** risulti positivo è sempre la sensibilità ($0.95$), a prescindere dai risultati precedenti. Se la persona **non** è malata, la probabilità di un test positivo rimane il tasso di falso positivo ($0.0072$).  
  Grazie a questa assunzione di indipendenza condizionata, possiamo scrivere:
  $$
  P(T_2^+ \mid M^+, T_1^+) = P(T_2^+ \mid M^+) = 0.95 
  $$
  e
  $$
  P(T_2^+ \mid M^-, T_1^+) = P(T_2^+ \mid M^-) = 0.0072.
  $$

- $P(M^+ \mid T_1^+)$ e $P(M^- \mid T_1^+)$ sono invece le probabilità a posteriori di essere (o non essere) malati **dopo** il primo test positivo, che avevamo già calcolato. In particolare:  
  $$
  P(M^+ \mid T_1^+) \approx 0.2844,
  $$
  $$
  P(M^- \mid T_1^+) = 1 - P(M^+ \mid T_1^+) = 0.7156.
  $$

Sostituiamo i valori numerici:

$$
\begin{aligned}
P(T_2^+ \mid T_1^+) 
&= (0.95 \times 0.2844) + \bigl(0.0072 \times 0.7156\bigr) \\
&= 0.2702 \;+\; 0.00515 \\
&= 0.27535 \;\approx\; 0.2753.
\end{aligned}
$$


**Passo 3: Calcolo della probabilità di *essere malati* dopo due test positivi.**

Ora che conosciamo la probabilità di un secondo test positivo, possiamo sfruttare nuovamente la **regola di Bayes** per trovare la probabilità a posteriori di essere malati **dopo** che **entrambi** i test hanno dato esito positivo, cioè $P(M^+ \mid T_1^+, T_2^+)$. La formula è:

$$
P(M^+ \mid T_1^+, T_2^+) 
= \frac{P(T_2^+ \mid M^+, T_1^+) \, P(M^+ \mid T_1^+)}{P(T_2^+ \mid T_1^+)}.
$$

- Al **numeratore** compaiono la sensibilità del test, $P(T_2^+ \mid M^+) = 0.95$, e la probabilità a posteriori dopo il primo test positivo, $P(M^+ \mid T_1^+) = 0.2844$.  
- Al **denominatore** troviamo la probabilità che il secondo test sia positivo, data la positività del primo, calcolata nel passo precedente: $P(T_2^+ \mid T_1^+) \approx 0.2753$.

Sostituiamo i valori:

$$
P(M^+ \mid T_1^+, T_2^+) 
= \frac{0.95 \times 0.2844}{0.2753}
\approx 0.981 
\quad (\text{cioè } 98.1\%).
$$

**Interpretazione:**

1. **dopo il primo test positivo** ($T_1^+$), la probabilità di avere l’HIV passa dal **0.3%** (prevalenza iniziale) a circa **28.44%**;  
2. **dopo il secondo test positivo** ($T_2^+$), la probabilità sale ulteriormente a **98.1%**, rendendo la diagnosi quasi certa.

Questo **salto di accuratezza** mostra l’importanza dell’aggiornamento bayesiano in caso di bassa prevalenza: un singolo risultato positivo può significare “qualcosa” (aumenta la probabilità), ma **non è sufficiente** a raggiungere la certezza diagnostica. Ripetere il test e ottenere un secondo esito positivo conferma molto più solidamente l’ipotesi di malattia.

Questo esempio fornisce un'altra illustrazione dell’utilità della **regola di Bayes** nell’interpretazione dei risultati diagnostici, specialmente in situazioni in cui la prevalenza della malattia è limitata.
:::

<!-- ::: {.callout-important title="Il Valore Predittivo di un Test di Laboratorio" collapse="true"} -->

<!-- Per semplicità, possiamo riscrivere il teorema di Bayes in due modi distinti per calcolare ciò che viene chiamato valore predittivo del test positivo e valore predittivo del test negativo. -->

<!-- - **Valore Predittivo Positivo (VPP)**: la probabilità che una persona sia malata dato un test positivo.   -->
<!-- - **Valore Predittivo Negativo (VPN)**: la probabilità che una persona sia sana dato un test negativo.   -->

<!-- Per stimare questi valori, è essenziale comprendere tre concetti fondamentali:   -->

<!-- 1. **Prevalenza** $(P(M^+))$: rappresenta la proporzione di individui malati nella popolazione. Per esempio, una prevalenza dello 0,5% implica che su 1000 persone, 5 hanno la malattia.   -->

<!-- 2. **Sensibilità** $(P(T^+ \mid M^+))$: misura la capacità del test di identificare correttamente i malati. È data dal rapporto tra il numero di veri positivi e il totale dei malati:   -->

<!--    $$ -->
<!--    \text{Sensibilità} = \frac{TP}{TP + FN}, -->
<!--    $$ -->

<!--    dove: -->

<!--    - $TP$ sono i **veri positivi** (malati correttamente identificati dal test). -->
<!--    - $FN$ sono i **falsi negativi** (malati che il test non ha rilevato).   -->

<!-- 3. **Specificità** $(P(T^- \mid M^-))$: misura la capacità del test di identificare correttamente i sani. È il rapporto tra il numero di veri negativi e il totale delle persone sane:   -->

<!--    $$ -->
<!--    \text{Specificità} = \frac{TN}{TN + FP} , -->
<!--    $$ -->

<!--    dove: -->

<!--    - $TN$ sono i **veri negativi** (persone sane correttamente escluse dal test). -->
<!--    - $FP$ sono i **falsi positivi** (persone sane erroneamente diagnosticate come malate).   -->

<!-- **Rappresentazione in tabella** -->

<!-- |       | Test Positivo $(T^+)$  | Test Negativo $(T^-)$  | Totale  | -->
<!-- | :---: | :----------------------: | :----------------------: | :------: | -->
<!-- | **Malato** $(M^+)$ | $P(T^+ \cap M^+)$ (**Sensibilità**)  | $P(T^- \cap M^+)$ ($1 -$ Sensibilità)  | $P(M^+)$  | -->
<!-- | **Sano** $(M^-)$   | $P(T^+ \cap M^-)$ ($1 -$ Specificità)  | $P(T^- \cap M^-)$ (**Specificità**)  | $P(M^-)$  | -->
<!-- | **Totale** | $P(T^+)$  | $P(T^-)$  | 1  | -->

<!-- Questa tabella descrive il legame tra i risultati del test e la condizione effettiva dell'individuo. -->

<!-- #### Calcolo del Valore Predittivo Positivo (VPP)  -->

<!-- Il VPP esprime la probabilità che una persona con un test positivo sia effettivamente malata. Usando la regola di Bayes, otteniamo:   -->

<!-- $$ -->
<!-- P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) \cdot P(M^+)}{P(T^+)} -->
<!-- $$ -->

<!-- Sostituendo la formula di $P(T^+)$ dalla tabella: -->

<!-- $$ -->
<!-- P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) \cdot P(M^+)}{P(T^+ \mid M^+) \cdot P(M^+) + P(T^+ \mid M^-) \cdot P(M^-)} -->
<!-- $$ -->

<!-- Scritto in termini di sensibilità, specificità e prevalenza: -->

<!-- $$ -->
<!-- VPP = \frac{(\text{Sensibilità} \times \text{Prevalenza})}{(\text{Sensibilità} \times \text{Prevalenza}) + (1 - \text{Specificità}) \times (1 - \text{Prevalenza})}. -->
<!-- $$ -->

<!-- #### Calcolo del Valore Predittivo Negativo (VPN)   -->

<!-- Il VPN esprime la probabilità che una persona con un test negativo sia effettivamente sana. Applicando il teorema di Bayes: -->

<!-- $$ -->
<!-- P(M^- \mid T^-) = \frac{P(T^- \mid M^-) \cdot P(M^-)}{P(T^-)} -->
<!-- $$ -->

<!-- Espandendo $P(T^-)$: -->

<!-- $$ -->
<!-- P(M^- \mid T^-) = \frac{P(T^- \mid M^-) \cdot (1 - P(M^+))}{P(T^- \mid M^-) \cdot (1 - P(M^+)) + P(T^- \mid M^+) \cdot P(M^+)} -->
<!-- $$ -->

<!-- Sostituendo sensibilità, specificità e prevalenza: -->

<!-- $$ -->
<!-- VPN = \frac{\text{Specificità} \times (1 - \text{Prevalenza})}{\text{Specificità} \times (1 - \text{Prevalenza}) + (1 - \text{Sensibilità}) \times \text{Prevalenza}}. -->
<!-- $$ -->

<!-- #### Interpretazione e Implicazioni -->

<!-- 1. **Un test con alta sensibilità e specificità non garantisce un VPP elevato** se la prevalenza della malattia è bassa.   -->
<!--    - Anche con un test altamente accurato, un numero elevato di falsi positivi può abbassare la probabilità effettiva di essere malati dopo un test positivo. -->

<!-- 2. **Il VPN è generalmente più alto quando la prevalenza è bassa**, il che significa che un test negativo è molto affidabile nell’escludere la malattia quando questa è rara. -->

<!-- 3. **Ripetere il test** riduce l'incertezza: se un primo test positivo porta a un VPP basso, un secondo test positivo consecutivo può aumentare notevolmente la probabilità di essere effettivamente malati. -->

<!-- In conclusione, il valore predittivo di un test non dipende solo dalle sue caratteristiche intrinseche, ma anche dalla prevalenza della malattia. Il **teorema di Bayes** ci fornisce uno strumento essenziale per aggiornare la probabilità di malattia dopo aver osservato il risultato di un test, migliorando la nostra capacità di interpretare i risultati in modo più accurato. -->
<!-- ::: -->

<!-- ::: {.callout-important title="Lo screening per il cancro al seno mediante mammografia" collapse="true"} -->

<!-- Implementiamo in **R** le formule per calcolare il **valore predittivo positivo (VPP)** e il **valore predittivo negativo (VPN)** di un test diagnostico, utilizzando gli stessi dati dell'esercizio precedente sulla mammografia. -->

<!-- ```{r} -->
<!-- positive_predictive_value_of_diagnostic_test <- function(sens, spec, prev) { -->
<!--   (sens * prev) / (sens * prev + (1 - spec) * (1 - prev)) -->
<!-- } -->

<!-- negative_predictive_value_of_diagnostic_test <- function(sens, spec, prev) { -->
<!--   (spec * (1 - prev)) / (spec * (1 - prev) + (1 - sens) * prev) -->
<!-- } -->
<!-- ``` -->

<!-- Inseriamo i dati del problema. -->

<!-- ```{r} -->
<!-- sens = 0.9  # sensibilità -->
<!-- spec = 0.9  # specificità -->
<!-- prev = 0.01  # prevalenza -->
<!-- ``` -->

<!-- Il valore predittivo del test positivo è: -->

<!-- ```{r} -->
<!-- res_pos <- positive_predictive_value_of_diagnostic_test(sens, spec, prev) -->
<!-- cat(sprintf("P(M+ | T+) = %.3f\n", res_pos)) -->
<!-- ``` -->

<!-- Il valore predittivo del test negativo è: -->

<!-- ```{r} -->
<!-- res_neg <- negative_predictive_value_of_diagnostic_test(sens, spec, prev) -->
<!-- cat(sprintf("P(M- | T-) = %.3f\n", res_neg)) -->
<!-- ``` -->
<!-- ::: -->


## La Fallacia del Procuratore e il Teorema di Bayes

Il **teorema di Bayes** non trova applicazione solo in campo medico, ma è essenziale anche nei procedimenti giudiziari. Infatti, fraintendimenti nell’interpretazione di probabilità e statistiche possono portare a **gravi errori di giudizio**. Uno degli errori più comuni in questo contesto è la **fallacia del procuratore**.


### Che cos’è la fallacia del procuratore?

La fallacia del procuratore consiste nel confondere la probabilità di osservare una certa evidenza se una persona è innocente, **$P(T^+ \mid I)$**, con la probabilità che una persona sia innocente dopo aver osservato quella evidenza, **$P(I \mid T^+)$**.

- In termini giudiziari, questo equivale a dire: "Poiché è estremamente improbabile ottenere un certo riscontro (ad es. un test positivo) se la persona è innocente, allora è estremamente improbabile che la persona sia innocente se si è ottenuto un esito positivo".
- In realtà, per stabilire se la persona è innocente o colpevole **dopo** aver visto il risultato, occorre considerare sia la bassa frequenza delle persone effettivamente colpevoli nella popolazione (**prevalenza**) sia la possibilità di **falsi positivi**. Il **teorema di Bayes** fornisce lo strumento formale per integrare questi elementi.


### Esempio di test del DNA

Supponiamo di utilizzare un test del DNA per identificare un sospetto tra **65 milioni** di persone. Il test ha:

- **Sensibilità** ($P(T^+ \mid C)$) = 99%  
  $\rightarrow$ Se la persona è effettivamente colpevole, il test risulta positivo il 99% delle volte.
- **Specificità** ($P(T^- \mid I)$) = 99.99997%  
  $\rightarrow$ Se la persona è innocente, il test risulta negativo il 99.99997% delle volte.  
  Da cui segue che il tasso di falso positivo è $1 - 0.9999997 = 0.0000003 = 0.00003\%$.
- **Prevalenza** ($P(C)$) = $1/65{,}000{,}000 \approx 1.54 \times 10^{-8}$  
  $\rightarrow$ Un individuo scelto a caso ha una probabilità di circa $1.54 \times 10^{-8}$ (cioè 1 su 65 milioni) di essere il vero colpevole.

Un campione di DNA coincide con quello di una persona trovata nel database e il test dà **risultato positivo**. Qual è la probabilità che costui sia davvero colpevole? Formalmente, vogliamo $P(C \mid T^+)$.


#### Passo 1: Calcolare $P(T^+)$, la probabilità di un test positivo

La probabilità complessiva di un esito positivo deriva da due scenari alternativi:

1. **La persona è colpevole** e il test è positivo:  
   $P(T^+ \mid C) \times P(C)$.
2. **La persona è innocente** e il test è positivo per errore (falso positivo):  
   $P(T^+ \mid I) \times P(I)$.

Perciò, usando la regola della probabilità totale:

$$
P(T^+) 
= P(T^+ \mid C) \, P(C) \;+\; P(T^+ \mid I) \, P(I).
$$

Assegniamo i valori numerici:

- $P(T^+ \mid C) = 0.99$ (sensibilità).
- $P(C) = 1.54 \times 10^{-8}$.
- $P(T^+ \mid I) = 1 - P(T^- \mid I) = 1 - 0.9999997 = 0.0000003$.
- $P(I) = 1 - P(C) \approx 0.99999998$.

Eseguiamo il calcolo:

$$
\begin{aligned}
P(T^+) 
&= (0.99 \times 1.54 \times 10^{-8}) + (0.0000003 \times 0.99999998)\\
&= 1.5231 \times 10^{-8} + 2.9999994 \times 10^{-7}\\
&= 3.1523 \times 10^{-7}.
\end{aligned}
$$


#### Passo 2: Applicare la regola di Bayes per $P(C \mid T^+)$

Ora possiamo calcolare la probabilità di essere colpevoli dato che il test è positivo:

$$
P(C \mid T^+) 
= \frac{P(T^+ \mid C)\,P(C)}{P(T^+)}.
$$

Inseriamo i valori:

$$
\begin{aligned}
P(C \mid T^+) 
&= \frac{(0.99 \times 1.54 \times 10^{-8})}{3.1523 \times 10^{-7}}\\
&= \frac{1.5231 \times 10^{-8}}{3.1523 \times 10^{-7}}\\
&\approx 0.0483 \quad (\text{cioè } 4.83\%).
\end{aligned}
$$


### Interpretazione: perché è “solo” il 4.83%?

Sebbene **sensibilità e specificità** del test siano entrambe molto alte, la **prevalenza estremamente bassa** del colpevole (1 su 65 milioni) riduce notevolmente la probabilità a posteriori $P(C \mid T^+)$. In una popolazione di 65 milioni di individui, anche un esiguo tasso di falsi positivi **($0.0000003$)** genera un numero assoluto di risultati positivi fra gli innocenti molto più grande del numero di colpevoli reali.

In pratica, pur avendo un test positivo, la probabilità che la persona sia davvero colpevole **resta modesta** (circa 4.83%), perché i “falsi allarmi” nella massa di individui innocenti superano di gran lunga i (pochi) veri positivi.


### Evitare la fallacia del procuratore

La fallacia del procuratore consiste nel confondere:

- **$P(T^+ \mid I)$**: la probabilità che un innocente risulti positivo (falso positivo),
- **$P(I \mid T^+)$**: la probabilità di essere innocenti dopo un test positivo.

Questa confusione porta a sovrastimare la colpevolezza di un individuo basandosi su una singola evidenza statistica. Applicando il **teorema di Bayes**, invece, si comprende che un test positivo non implica automaticamente colpevolezza, soprattutto quando la malattia (o il reato, in questo caso) è molto raro. Nei processi giudiziari, ciò significa che **un dato probabilistico deve sempre essere contestualizzato** alla popolazione di riferimento: la corretta interpretazione delle prove è fondamentale per evitare errori giudiziari.

#### Conclusione epistemologica  

L’impiego di test probabilistici in ambito giudiziario richiede un'applicazione rigorosa del **teorema di Bayes** per evitare distorsioni interpretative. Solo un corretto aggiornamento delle credenze, integrando:  

- **la probabilità pre-test** ($P(C)$, prevalenza del colpevole nella popolazione investigata),  
- **la potenza diagnostica del test** (sensibilità e specificità),  
- **il tasso di errore strumentale** (falsi positivi e falsi negativi),  

consente di ridurre il rischio di **errori giudiziari sistematici**. In assenza di questa integrazione, anche test estremamente precisi possono condurre a **ingiuste condanne**, trasformando strumenti scientifici affidabili in fonti di distorsione probatoria.


## Probabilità Inversa: Dal Problema Classico all’Inferenza Bayesiana

Gli esempi precedenti mostrano due tipi di domande probabilistiche fondamentali:

1. **Probabilità diretta**  
   - *“Qual è la probabilità di osservare un certo risultato, supponendo che l’ipotesi sia vera?”*

2. **Probabilità inversa**  
   - *“Qual è la probabilità che un’ipotesi sia vera, dati i risultati osservati?”*

Questa distinzione è cruciale per comprendere il **teorema di Bayes** e le differenze tra l’approccio **frequentista** e quello **bayesiano** alla probabilità.


### Esempi di Probabilità Diretta e Inversa

Prendiamo come esempio il lancio di una moneta:

- **Probabilità diretta**:  
  Se riteniamo che la moneta sia equa (cioè $P(\text{Testa}) = 0{.}5$), qual è la probabilità di osservare **zero teste** in cinque lanci? In questo caso, stiamo calcolando
  $$
  P(D \mid H) = (0.5)^5 = 0.03125,
  $$
  dove $D$ rappresenta il dato (“zero teste in cinque lanci”) e $H$ l’ipotesi (“la moneta è equa”).  

- **Probabilità inversa**:  
  Ora poniamo la domanda opposta. Abbiamo lanciato una moneta cinque volte e osservato **zero teste**. Quanto è probabile che la moneta sia davvero equa?  
  Qui vogliamo conoscere $\displaystyle P(H \mid D)$ (l’ipotesi “la moneta è equa” dopo aver visto il risultato) anziché $P(D \mid H)$. Per rispondere correttamente, ci occorre il **teorema di Bayes**, che combina la probabilità dei dati ($P(D \mid H)$) con una stima iniziale (il **prior**) su quanto riteniamo probabile l’ipotesi prima dell’osservazione.


### Dalla Probabilità Diretta alla Probabilità Inversa: Il Contributo di Bayes

Per lungo tempo, la teoria della probabilità si è occupata quasi esclusivamente di **probabilità diretta**: “se l’ipotesi è vera, qual è la probabilità di osservare un certo esito?”.  
Nel XVIII secolo, Thomas Bayes capovolse la prospettiva, concentrandosi su come determinare la **probabilità dell’ipotesi** a partire dalle evidenze disponibili, introdusse cioè l’idea di **probabilità inversa**. Questa svolta ha aperto la strada a ciò che oggi chiamiamo **inferenza bayesiana**, permettendo di aggiornare in modo sistematico e rigoroso la credibilità di un’ipotesi dopo aver osservato nuovi dati.

### L’Impatto della Probabilità Inversa

La possibilità di stimare $\displaystyle P(H \mid D)$, cioè la **probabilità di un’ipotesi data l’evidenza osservata**, si è rivelata fondamentale in molti ambiti:

- **Scienza e sperimentazione**: quanto è probabile che un’ipotesi sia vera dopo aver raccolto i dati di un esperimento?  
- **Medicina**: quanto è probabile che un paziente abbia una certa malattia, se il test diagnostico è positivo?  
- **Giustizia**: quanto è probabile che una persona sia colpevole, se il DNA trovato sulla scena del crimine combacia col suo?

In tutti questi casi non basta calcolare la probabilità dei dati “dato un’ipotesi” $\bigl(P(D \mid H)\bigr)$; occorre invece aggiornare la stima della probabilità dell’ipotesi alla luce dei dati $\bigl(P(H \mid D)\bigr)$.

**In sintesi**, l’**inferenza bayesiana** risponde appunto a questa seconda domanda, passando dalla **probabilità diretta** alla **probabilità inversa** in modo rigoroso. Grazie al **teorema di Bayes**, possiamo combinare in modo coerente le nostre conoscenze pregresse (il cosiddetto *prior*) con le evidenze raccolte, ottenendo una **probabilità a posteriori** che rappresenta la nostra nuova convinzione. Senza questa prospettiva, gran parte dei problemi scientifici e delle decisioni pratiche resterebbe priva di un metodo per collegare razionalmente le evidenze empiriche alle ipotesi da verificare.


::: {.callout-tip title="Il problema di Monty Hall" collapse="true"}
Abbiamo già discusso in precedenza il problema di Monty Hall. Vediamo ora come possa essere risolto utilizzando il Teorema di Bayes. 

Ci sono tre porte:

- dietro una porta c'è un'automobile (il premio);
- dietro le altre due porte ci sono delle capre.

Il giocatore sceglie una porta (ad esempio, la porta 1). Successivamente, il conduttore (che sa cosa c'è dietro ogni porta) apre una delle due porte rimanenti, rivelando una capra (ad esempio, apre la porta 3). A questo punto, il giocatore può decidere se mantenere la scelta iniziale o cambiare porta.

Vogliamo calcolare la probabilità che l'automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3.

Definizione degli eventi.

- A1, A2, A3: l'automobile si trova dietro la porta 1, 2 o 3, rispettivamente.
- P3: il conduttore apre la porta 3, rivelando una capra.

Vogliamo calcolare la probabilità condizionata $P(A2 \mid P3)$, ovvero la probabilità che l'automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3. La formula di Bayes è:

$$
P(A2 \mid P3) = \frac{P(P3 \mid A2) \cdot P(A2)}{P(P3)}
$$

1. **$P(A2)$**: probabilità iniziale che l'automobile sia dietro la porta 2.
   - Poiché ci sono tre porte e solo un'automobile, $P(A2) = \frac{1}{3}$.

2. **$P(P3 \mid A2)$**: probabilità che il conduttore apra la porta 3, sapendo che l'automobile è dietro la porta 2.
   - Se l'automobile è dietro la porta 2, il conduttore deve aprire la porta 3 (non può aprire la porta 1, scelta dal giocatore, né la porta 2, dove c'è l'automobile). Quindi, $P(P3 \mid A2) = 1$.

3. **$P(P3)$**: probabilità che il conduttore apra la porta 3.
   - Il conduttore può aprire la porta 3 in due casi:
     - Se l'automobile è dietro la porta 2 (con probabilità $\frac{1}{3}$).
     - Se l'automobile è dietro la porta 1 (con probabilità $\frac{1}{3}$), e il conduttore sceglie casualmente tra la porta 2 e la porta 3 (con probabilità $\frac{1}{2}$).
   - Quindi:
     $$
     \begin{align}
     P(P3) &= P(P3 \mid A2) \cdot P(A2) + P(P3 \mid A1) \cdot P(A1) \notag\\
     &= 1 \cdot \frac{1}{3} + \frac{1}{2} \cdot \frac{1}{3} = \frac{1}{3} + \frac{1}{6} = \frac{1}{2}.\notag
     \end{align}
     $$

Sostituendo i valori calcolati:

$$
P(A2 \mid P3) = \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}} = \frac{\frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}.
$$

In conclusione, la probabilità che l'automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3, è $\frac{2}{3}$ (circa 66.7%). Questo significa che cambiando porta, il giocatore ha una probabilità di vincita del 66.7%, mentre mantenendo la scelta iniziale la probabilità è solo del 33.3%.

Questo esempio mostra come il Teorema di Bayes permetta di aggiornare le probabilità in base a nuove informazioni. Nel contesto del problema di Monty Hall, cambiare porta raddoppia le possibilità di vincere l'automobile.
:::

## Riflessioni Conclusive

In questo capitolo abbiamo esplorato vari esempi, principalmente nel campo medico e forense, per illustrare come il teorema di Bayes permetta di combinare le informazioni derivate dalle osservazioni con le conoscenze precedenti (priori), aggiornando così il nostro grado di convinzione rispetto a un'ipotesi. Il teorema di Bayes fornisce un meccanismo razionale, noto come "aggiornamento bayesiano", che ci consente di ricalibrare le nostre convinzioni iniziali alla luce di nuove evidenze.

Una lezione fondamentale che il teorema di Bayes ci insegna, sia nella ricerca scientifica che nella vita quotidiana, è che spesso non ci interessa tanto conoscere la probabilità che qualcosa accada assumendo vera un'ipotesi, quanto piuttosto la probabilità che un'ipotesi sia vera, dato che abbiamo osservato una certa evidenza. In altre parole, la forza del teorema di Bayes sta nella sua capacità di affrontare direttamente il problema inverso, cioè come dedurre la verità di un'ipotesi a partire dalle osservazioni.

Il framework bayesiano per l'inferenza probabilistica offre un approccio generale per comprendere come i problemi di induzione possano essere risolti in linea di principio e, forse, anche come possano essere affrontati dalla mente umana.

In questo capitolo ci siamo concentrati sull'applicazione del teorema di Bayes utilizzando probabilità puntuali. Tuttavia, il teorema esprime pienamente il suo potenziale quando sia l'evidenza che i gradi di certezza a priori delle ipotesi sono rappresentati attraverso distribuzioni di probabilità continue. Questo sarà l'argomento centrale nella prossima sezione della dispensa, dove approfondiremo il flusso di lavoro bayesiano e l'uso di distribuzioni continue nell'aggiornamento bayesiano.

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}


