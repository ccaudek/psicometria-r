---
execute:
  freeze: auto
---

# Le fasi del progetto di analisi dei dati {#sec-eda-proj-structure}

::: callout-important 
## In questo capitolo imparerai a

- gestire il ciclo di vita di un progetto di analisi: dalla definizione della domanda di ricerca alla comunicazione dei risultati
- organizzare i file del progetto per garantirne portabilità e condivisione
:::

::: callout-tip 
## Prerequisiti

- Leggere [Veridical Data Science](https://vdsbook.com) [@yu2024veridical] focalizzandoti sul primo capitolo, che introduce le problematiche della data science, e sul quarto capitolo, che fornisce le linee guida dettagliate sull'organizzazione di un progetto di analisi dei dati.
:::

::: callout-caution 
## Preparazione del Notebook

```{r}
# Carica il file _common.R per impostazioni di pacchetti e opzioni
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione

Una gestione accurata ed efficace dei dati è fondamentale, soprattutto in discipline come la psicologia, dove l'analisi di dataset complessi è una componente centrale della ricerca. Assicurare che i dati siano raccolti con precisione, organizzati in modo chiaro e facilmente accessibili per analisi e verifiche è essenziale per preservare l'integrità del lavoro scientifico e promuovere la sua riproducibilità. Una gestione rigorosa dei dati garantisce qualità e affidabilità durante tutte le fasi del progetto, dalla raccolta alla documentazione dei processi di elaborazione e delle eventuali modifiche apportate.

Dati ben organizzati e documentati non solo semplificano e rendono più efficiente il processo di analisi, ma riducono anche il rischio di errori, migliorando l'utilizzabilità e l'interpretazione delle informazioni. Questo aspetto è particolarmente rilevante quando si lavora con dataset provenienti da fonti eterogenee o con strutture complesse. Inoltre, la trasparenza e la completezza nella gestione dei dati rappresentano una condizione imprescindibile per garantire la riproducibilità della ricerca, pilastro fondamentale della scienza. 

La possibilità per altri ricercatori di replicare i risultati utilizzando gli stessi dati e metodi rafforza la credibilità delle conclusioni e contribuisce a costruire un progresso scientifico condiviso e solido. Una gestione dei dati responsabile, dunque, non è solo una buona pratica, ma una necessità per la produzione di conoscenze affidabili e sostenibili.

## Capacità di Gestione dei Dati in R

R è uno strumento potente e versatile, progettato per supportare ogni fase del ciclo di vita dei dati, dalla raccolta alla documentazione, rendendolo indispensabile per chiunque lavori con dati complessi.

- **Importazione ed esportazione dei dati**: Con pacchetti come `readr` e `rio`, R facilita l'importazione di dati da diverse fonti, tra cui file CSV, database e API web, e consente l'esportazione in formati adatti a diversi utilizzi.
- **Pulizia e preparazione dei dati**: Pacchetti come `dplyr`, `tidyr` e `stringr` offrono strumenti intuitivi e potenti per manipolare, trasformare e preparare i dati in modo efficiente, rendendoli pronti per l'analisi.
- **Esplorazione e sintesi**: R, attraverso pacchetti come `dplyr` e `ggplot2`, permette di calcolare statistiche descrittive, individuare pattern significativi e visualizzare distribuzioni e relazioni in modo chiaro e informativo.
- **Documentazione dinamica**: Grazie a strumenti come R Markdown e Quarto, è possibile creare documenti interattivi che integrano codice, analisi, testo esplicativo e risultati, promuovendo la riproducibilità e la trasparenza del lavoro.
- **Controllo delle versioni**: L'integrazione di Git in RStudio offre un sistema di gestione delle versioni che consente di monitorare modifiche, collaborare con altri e garantire la tracciabilità del processo analitico.

Queste funzionalità rendono R uno strumento essenziale per gestire i dati in modo organizzato, trasparente e ottimale, facilitando analisi rigorose e riproducibili.

## Configurare l’Ambiente R

Per sfruttare al meglio le potenzialità di R, è essenziale configurare correttamente RStudio e integrare i pacchetti fondamentali per la gestione dei dati. Una configurazione adeguata favorisce la riproducibilità e l’organizzazione del lavoro.

### Workspace e Cronologia

Accedi a **Tools > Global Options > General** e modifica le seguenti impostazioni:  

- Disabilita l’opzione *Restore .RData into workspace at startup*.  
- Imposta *Save workspace to .RData on exit* su **Never**.  

Queste configurazioni incoraggiano una gestione basata sugli script, rendendo il lavoro più trasparente e riducendo conflitti tra sessioni diverse. Ogni analisi sarà così chiaramente documentata nello script, evitando dipendenze da file temporanei o precedenti sessioni.

### Pacchetti Essenziali

R è composto da un modulo base che fornisce le funzionalità fondamentali del linguaggio, ma la sua potenza deriva dall’enorme ecosistema di pacchetti aggiuntivi. Questi pacchetti possono essere caricati secondo necessità. Per questo corso, utilizzeremo regolarmente i seguenti pacchetti:

- **`here`**: per una gestione ordinata dei percorsi relativi, evitando problemi legati ai percorsi assoluti.  
- **`tidyverse`**: una raccolta di pacchetti per manipolazione, analisi e visualizzazione dei dati, che include `dplyr`, `tidyr`, `ggplot2` e altri strumenti indispensabili.

Assicurati di installarli e caricarli all’inizio di ogni script con i comandi:  

```r
# install.packages(c("here", "tidyverse")) 
# è solo necessario installarli una volta
library(here)
library(tidyverse)
```

### Gestione dei Progetti

I progetti in RStudio rappresentano uno strumento chiave per mantenere il lavoro organizzato. Utilizzare i progetti consente di lavorare in ambienti separati, dove ogni progetto ha la propria directory dedicata.  

Per creare un nuovo progetto:  

1. Vai su **File > New Project**.  
2. Seleziona una directory specifica per il progetto.  
3. Salva tutti i file correlati (script, dati, risultati) all’interno della directory del progetto.  

Questo approccio facilita la navigazione e previene errori derivanti dall’uso di file non correlati. Ogni progetto diventa così un’unità autonoma, ideale per mantenere ordine e coerenza nel lavoro.

## Il Ciclo di Vita di un Progetto di Data Science

I progetti di analisi dei dati seguono solitamente queste fasi [@yu2024veridical]:

1. **Formulazione del problema e raccolta dei dati**: Definizione delle domande di ricerca e acquisizione dei dataset.
2. **Pulizia, preprocessing e analisi esplorativa**: Preparazione dei dati per l'analisi attraverso trasformazioni e sintesi.
3. **Analisi predittiva e/o inferenziale**: (Opzionale) Modelli statistici o predittivi per rispondere alle domande di ricerca.
4. **Valutazione dei risultati**: Interpretazione e verifica delle conclusioni tratte.
5. **Comunicazione dei risultati**: Presentazione dei risultati in forma visiva e narrativa.

Non tutti i progetti includono la fase 3, ma quasi tutti attraversano le altre fasi, rendendo essenziale un approccio organizzato e ben strutturato.


### Fase 1: Formulazione del Problema e Raccolta dei Dati

La formulazione di una domanda di ricerca chiara e precisa rappresenta il punto di partenza fondamentale per qualsiasi progetto di data science. È essenziale definire con attenzione l’ambito in cui si intende condurre l’analisi dei dati. Questo ambito può assumere due principali direzioni:  

1. **Ambito Applicativo**   
   In questo contesto, l’obiettivo è spesso quello di produrre un report descrittivo che sintetizzi i risultati di un intervento specifico. Un esempio potrebbe essere la valutazione dell’efficacia di un programma di educazione sessuale rivolto agli adolescenti nelle scuole. In questo caso, l’analisi si concentra sulla descrizione dei risultati ottenuti, senza necessariamente approfondire le basi teoriche sottostanti.  

2. **Ambito della Ricerca Psicologica**   
   In questo secondo caso, l’approccio è più rigoroso e complesso. Non è sufficiente descrivere i risultati, ma è necessario giustificare le scelte metodologiche, come la selezione degli strumenti di misurazione e la formulazione delle domande di ricerca, basandosi su teorie consolidate e sulla letteratura scientifica esistente. Questo aspetto è stato approfondito nel @sec-measurement del materiale didattico.  

**Caratteristiche di una Buona Domanda di Ricerca**  
La domanda di ricerca deve essere formulata in modo da poter essere risolta attraverso l’analisi dei dati disponibili. Spesso, la domanda iniziale può risultare troppo vaga o ambiziosa, rendendo necessario un processo di revisione per adattarla alle informazioni effettivamente accessibili. Questo approccio garantisce che i dati raccolti o utilizzati siano appropriati e sufficienti per rispondere in modo efficace al problema di ricerca.  

**Raccolta dei Dati**  
La fase di raccolta dei dati è altrettanto cruciale per il successo del progetto. Esistono due principali scenari:  

1. **Utilizzo di Dati Esistenti**:  
   In alcuni casi, è possibile sfruttare dataset già disponibili, provenienti da repository pubblici, database interni o esperimenti precedenti.  

2. **Nuova Raccolta di Dati**:  
   In altri casi, è necessario procedere con una raccolta dati ex novo. In entrambe le situazioni, è fondamentale pianificare con cura le analisi statistiche prima di avviare la raccolta. Una mancata pianificazione può portare a dataset incompleti, privi di informazioni essenziali o non conformi alle assunzioni richieste dai modelli statistici previsti.  

**Aspetti Chiave della Raccolta Dati**  
Per garantire l’affidabilità dei risultati, è essenziale comprendere a fondo i processi e le metodologie utilizzate per acquisire i dati. Questo include:  

- **Documentazione delle Tecniche e Procedure**:  
  Descrivere in dettaglio le tecniche di raccolta, le procedure seguite e i potenziali bias che potrebbero influenzare i risultati.  
- **Valutazione delle Limitazioni**:  
  Identificare e considerare eventuali limitazioni dei dati, in modo da interpretare i risultati in modo critico e consapevole.  

Questo approccio metodologico contribuisce a garantire che le misure ottenute siano affidabili e che le analisi siano condotte in modo rigoroso e trasparente.

### Fase 2: Pulizia dei Dati e Analisi Esplorativa

#### Importazione ed Esportazione dei dati in R

**Importazione dei Dati**

In R, i dati vengono solitamente caricati in un **data frame**. Per facilitare questo processo, il pacchetto `rio` fornisce la funzione universale `import()`, che consente di caricare dati da formati diversi (come `.csv`, `.xlsx`, `.json`, ecc.) senza dover usare funzioni specifiche per ogni tipo di file. 

Per garantire un'organizzazione ottimale dei file, è utile:  

1. Creare una struttura di progetto chiara, con cartelle dedicate (es. `data/raw` per i dati grezzi e `data/processed` per quelli elaborati).  
2. Usare percorsi relativi, che rendono i progetti portabili e più facilmente replicabili.  

Il pacchetto **here** semplifica la gestione dei percorsi relativi, assicurando che il codice funzioni correttamente indipendentemente dalla posizione del progetto.

Supponiamo di avere un progetto chiamato `my_project` con la seguente struttura:  

```
my_project/
├── my_project.Rproj
├── data/
│   ├── raw/
│   │   └── my_data.csv
│   ├── processed/
```

Per importare il file `my_data.csv` nella memoria di lavoro di R, possiamo usare:  

```r
library(here)
library(rio)

# Importazione del file
df <- import(here("data", "raw", "my_data.csv"))
```

**Esportazione dei Dati**

Dopo aver elaborato i dati, è importante salvarli in modo organizzato. Con il pacchetto `rio`, l’esportazione si effettua tramite `export()`. Ad esempio, per salvare un file elaborato nella cartella `data/processed`:

```r
# Esportazione del file elaborato
export(df, here("data", "processed", "my_data_processed.csv"))
```

**Vantaggi di un Workflow Organizzato**

1. **Portabilità**: Usando percorsi relativi con `here`, il progetto può essere spostato su diversi computer o condiviso senza necessità di modificare il codice.  
2. **Semplificazione**: `rio` elimina la necessità di ricordare funzioni specifiche per diversi tipi di file.  
3. **Riproducibilità**: Una struttura chiara e un codice ben organizzato favoriscono la replicabilità dei risultati.  

#### Pulizia dei Dati

Dopo aver definito la domanda della ricerca e avere raccolto i dati rilevanti, è il momento di pulire i dati. Un dataset pulito è ordinato, formattato in modo appropriato e ha voci non ambigue. La fase iniziale di pulizia dei dati consiste nell'identificare problemi con i dati (come formattazioni anomale e valori non validi) e modificarli in modo che i valori siano validi e formattati in modo comprensibile sia per il computer che per noi. La pulizia dei dati è una fase estremamente importante di un progetto di data science perché non solo aiuta a garantire che i dati siano interpretati correttamente dal computer, ma aiuta anche a sviluppare una comprensione dettagliata delle informazioni contenute nei dati e delle loro limitazioni.

L'obiettivo della pulizia dei dati è creare una versione dei dati che rifletta nella maniera più fedele possibile la realtà e che sia interpretata correttamente dal computer. Per garantire che il computer utilizzi fedelmente le informazioni contenute nei dati, è necessario modificare i dati (scrivendo codice, non modificando il file dati grezzo stesso) in modo che siano in linea con ciò che il computer "si aspetta". Tuttavia, il processo di pulizia dei dati è necessariamente soggettivo e comporta fare assunzioni sulle quantità reali sottostanti misurate e decisioni su quali modifiche siano le più sensate.

#### Preprocessing

Il preprocessing si riferisce al processo di modifica dei dati puliti per soddisfare i requisiti di un algoritmo specifico che si desidera applicare. Ad esempio, se si utilizza un algoritmo che richiede che le variabili siano sulla stessa scala, potrebbe essere necessario trasformarle, oppure, se si utilizza un algoritmo che non consente valori mancanti, potrebbe essere necessario imputarli o rimuoverli. Durante il preprocessing, potrebbe essere utile anche definire nuove caratteristiche/variabili utilizzando le informazioni esistenti nei dati, se si ritiene che queste possano essere utili per l'analisi.

Come per la pulizia dei dati, non esiste un unico modo corretto per pre-elaborare un dataset, e la procedura finale comporta tipicamente una serie di decisioni che dovrebbero essere documentate nel codice e nei file di documentazione.

#### Analisi Esplorativa dei Dati

Dopo l'acquisizione dei dati, si procede con un'Analisi Esplorativa dei Dati (EDA - Exploratory Data Analysis). Questa fase iniziale mira a far familiarizzare il ricercatore con il dataset e a scoprire pattern nascosti. Si realizza attraverso:

- La costruzione di tabelle di frequenza e contingenza.
- Il calcolo di statistiche descrittive (come indici di posizione, dispersione e forma della distribuzione).
- La creazione di rappresentazioni grafiche preliminari.

L'EDA permette di generare ipotesi sui dati e di guidare le successive analisi statistiche.


### Fase 3: Analisi Predittiva e Inferenziale

Molte domande nella data science si presentano come problemi di inferenza e/o previsione, in cui l’obiettivo principale è utilizzare dati osservati, passati o presenti, per descrivere le caratteristiche di una popolazione più ampia o per fare previsioni su dati futuri non ancora disponibili. Questo tipo di analisi è spesso orientato a supportare decisioni nel mondo reale.

### Fase 4: Valutazione dei Risultati

In questa fase, i risultati ottenuti vengono analizzati alla luce della domanda di ricerca iniziale. Si procede a una valutazione sia quantitativa, attraverso l'applicazione di tecniche statistiche appropriate, sia qualitativa, attraverso un'attenta riflessione critica.

### Fase 5: Comunicazione dei Risultati 

L'ultima fase di un progetto di analisi dei dati consiste nel condividere i risultati con un pubblico più ampio, il che richiede la preparazione di materiali comunicativi chiari e concisi. L'obiettivo è trasformare i risultati dell'analisi in informazioni utili per supportare il processo decisionale. Questo può includere la stesura di un articolo scientifico, la creazione di un report per un team di lavoro, o la preparazione di una presentazione con diapositive.

La comunicazione deve essere adattata al pubblico di riferimento. Non si deve dare per scontato che il pubblico abbia familiarità con il progetto: è fondamentale spiegare l'analisi e le visualizzazioni in modo chiaro e dettagliato. Anche se per il ricercatore il messaggio principale di una figura o diapositiva può sembrare ovvio, è sempre una buona pratica guidare il pubblico nella sua interpretazione, evitando l'uso di gergo tecnico complesso.

## Organizzazione del Progetto

Un requisito fondamentale per un progetto di analisi dei dati è organizzare in modo efficiente i file sul proprio computer. Questo include i file dei dati, il codice e la documentazione del progetto. Tutti questi elementi dovrebbero essere raccolti all'interno di una singola cartella dedicata al progetto.

### Home Directory

In RStudio, è possibile creare un file chiamato `nome_del_progetto.Rproj`, che consente di configurare automaticamente la *home directory* del progetto, ovvero la cartella principale da cui R avvia il lavoro relativo al progetto. Per utilizzare questa funzionalità, è sufficiente aprire RStudio cliccando direttamente sul file `nome_del_progetto.Rproj`.

La *home directory* rappresenta il punto di riferimento principale per tutte le operazioni del progetto, come il caricamento di file, il salvataggio degli output e la gestione delle risorse. 

Grazie a questa configurazione, è possibile utilizzare percorsi relativi per accedere ai file all'interno del progetto. I percorsi relativi si basano sempre sulla cartella principale del progetto, il che rende il codice più portabile e adattabile. In pratica, chiunque scarichi il tuo progetto sarà in grado di eseguirlo senza dover modificare manualmente i percorsi dei file. Questo approccio migliora la condivisione e garantisce una maggiore riproducibilità del tuo lavoro.

### Struttura di un Progetto

Un’adeguata organizzazione della struttura di un progetto è fondamentale per garantire chiarezza, riproducibilità e portabilità. @yu2024veridical propone il seguente template per la gestione di un progetto:

![](../../figures/project_structure.png){width="27.5%"}

#### **Cartelle Principali**

1. **`data/`:**  
   Questa cartella raccoglie i dataset utilizzati nel progetto, suddivisi in:  
   - **`raw/`**: per i dati grezzi, che non devono mai essere modificati. Esempio: `data/raw/data.csv`.  
   - **`processed/`**: per i dati elaborati, cioè quelli puliti e trasformati per le analisi successive.  

   È indispensabile includere un **codebook** nella sottocartella `raw` per documentare variabili, unità di misura e ogni altra informazione necessaria per comprendere i dati. Questo migliora la trasparenza e facilita la condivisione del progetto.  

2. **`dslc_documentation/`:**  
   Contiene tutti i file necessari per documentare e condurre le analisi.  
   - File principali: possono essere in formato `.qmd` (Quarto per R) o `.ipynb` (Jupyter Notebook per Python), ordinati cronologicamente tramite un prefisso numerico (ad esempio, `01_data_cleaning.qmd`, `02_analysis.qmd`).  
   - **`functions/`:** una sottocartella dedicata a script con funzioni personalizzate, utili per diverse fasi del progetto. Ad esempio, `.R` per R o `.py` per Python.  

---

#### **File Supplementari**

- **`README.md`:**  
  Un file fondamentale che descrive la struttura del progetto e offre una breve panoramica del contenuto di ogni cartella e file. Deve includere:  
  - Lo scopo del progetto.  
  - Una spiegazione della struttura delle cartelle.  
  - I passaggi necessari per replicare le analisi.

---

#### **Vantaggi della Struttura Proposta**

1. **Organizzazione Chiara:**  
   Separare i dati grezzi da quelli elaborati riduce il rischio di modificare accidentalmente le versioni originali.  
   
2. **Riproducibilità:**  
   La documentazione completa (codebook, file analitici e README) facilita la comprensione e la riproduzione del progetto anche a distanza di tempo o da parte di altri utenti.  

3. **Portabilità:**  
   Utilizzando percorsi relativi (ad esempio, con il pacchetto **here** in R), l’intero progetto può essere facilmente trasferito tra utenti o computer senza modificare il codice.

4. **Efficienza:**  
   La sottocartella `functions/` consente di riutilizzare codice in più parti del progetto, migliorando la modularità e riducendo la duplicazione di script.

In conclusione, l’organizzazione del progetto secondo questo template garantisce non solo un flusso di lavoro ordinato e ben documentato, ma anche la possibilità di condividere facilmente il progetto con colleghi o team. Adottare questa struttura è un primo passo essenziale verso un'analisi dati rigorosa e riproducibile.

## Riflessioni Conclusive

La forza e la bellezza del codice risiedono nella sua riusabilità: una volta scritto, può essere applicato infinite volte per ottenere risultati coerenti. Se configurato correttamente, lo stesso codice applicato agli stessi dati produrrà sempre gli stessi risultati. Questo principio, noto come *riproducibilità computazionale*, è fondamentale per garantire la trasparenza e l’affidabilità del lavoro scientifico.

La riproducibilità offre numerosi vantaggi:

- **Monitorare le modifiche del progetto**  
  La possibilità di riprodurre il lavoro semplifica il monitoraggio delle evoluzioni e dei cambiamenti nel progetto. Questo consente di comprendere come il progetto si è sviluppato nel tempo, facilitando il confronto tra diverse versioni o approcci.

- **Riprodurre il proprio lavoro**  
  Il primo beneficiario della riproducibilità sei tu stesso. Essere in grado di replicare i propri risultati è essenziale, soprattutto se in futuro sarà necessario rivedere o approfondire il lavoro. La riproducibilità garantisce che le analisi possano essere riprese con facilità e senza ambiguità.

- **Costruire su basi solide**  
  Altri ricercatori possono utilizzare il tuo lavoro come punto di partenza, espandendolo o applicandolo a nuovi contesti. La condivisione di codice riproducibile non solo favorisce la collaborazione, ma contribuisce a creare un corpo di conoscenze più robusto e condiviso.

Tuttavia, rendere il codice riproducibile non è sempre semplice. Richiede attenzione nella documentazione, un’organizzazione chiara del progetto e strumenti adeguati per garantire che l’ambiente di lavoro sia stabile e replicabile. In questo capitolo, abbiamo esplorato alcune strategie e tecniche per raggiungere questi obiettivi.

::: {.callout-note}
Un problema cruciale nella psicologia contemporanea è la *crisi di replicabilità*, che evidenzia come molti risultati di ricerca non siano replicabili [@open2015estimating]. La *riproducibilità computazionale*, pur avendo un obiettivo più ristretto, si concentra sulla possibilità di ottenere gli stessi risultati applicando lo stesso codice agli stessi dati. Questo approccio, sebbene non risolva interamente la crisi, rappresenta un passo fondamentale verso una scienza più trasparente e rigorosa.
:::

## Esercizi

::: {.callout-tip title="Esercizio" collapse="true"}
L'obiettivo di questo esercizio è comprendere il ciclo di vita di un progetto di analisi dei dati, l'organizzazione del progetto e la gestione della riproducibilità.

1. **Gestione del progetto di analisi**  

   - Quali sono le fasi principali di un progetto di analisi dei dati secondo Yu (2024)?
   - Spiega il ruolo della fase di formulazione del problema e raccolta dei dati.

2. **Organizzazione del workspace in R**  

   - Quali impostazioni devono essere modificate in RStudio per favorire la riproducibilità?
   - Perché è importante usare percorsi relativi nei progetti in RStudio?
   - Descrivi il ruolo del pacchetto `here` nella gestione dei percorsi dei file.

3. **Struttura dei progetti in R**  

   - Quali sono i vantaggi dell'utilizzo dei progetti in RStudio?
   - Quali sono le cartelle principali in una struttura organizzata di un progetto?
   - Perché è utile separare i dati grezzi dai dati processati?

4. **Importazione ed esportazione dei dati**  

   - Quali pacchetti di R possono essere utilizzati per importare ed esportare dati?  
   - Scrivi un esempio di codice per importare un file CSV usando `rio` e il pacchetto `here`.  
   - Come puoi esportare un dataset modificato in una cartella dedicata ai dati processati?

5. **Pulizia e preprocessing dei dati**  

   - Qual è la differenza tra pulizia e preprocessing dei dati?  
   - Quali strumenti di `dplyr` sono comunemente usati per pulire e trasformare i dati?

6. **Analisi esplorativa dei dati (EDA)**  

   - Quali sono alcuni strumenti utilizzati in R per effettuare un'analisi esplorativa dei dati?  
   - Scrivi un breve esempio di codice in R per calcolare statistiche descrittive di base su un dataset.  

7. **Riproducibilità e comunicazione dei risultati**  

   - Perché la riproducibilità è un elemento chiave nella scienza dei dati?  
   - Quali strumenti offre Quarto per la documentazione e la condivisione dei risultati di un’analisi?
:::

::: {.callout-tip title="Soluzione" collapse="true"}
**1. Gestione del progetto di analisi**

- **Fasi principali del progetto di analisi dei dati (Yu, 2024):**  
  1. Formulazione del problema e raccolta dei dati  
  2. Pulizia, preprocessing e analisi esplorativa  
  3. Analisi predittiva e/o inferenziale (se applicabile)  
  4. Valutazione dei risultati  
  5. Comunicazione dei risultati  

- **Ruolo della fase di formulazione del problema:**  
  Aiuta a definire gli obiettivi dell'analisi e a selezionare le fonti di dati adeguate. Una domanda di ricerca ben definita garantisce che i dati siano pertinenti e che le analisi siano mirate.

**2. Organizzazione del workspace in R**

- **Impostazioni da modificare in RStudio:**  
  - Disabilitare *Restore .RData into workspace at startup*  
  - Impostare *Save workspace to .RData on exit* su "Never"  

- **Importanza dei percorsi relativi:**  
  Permettono di rendere il progetto portabile e riproducibile, evitando problemi di percorsi assoluti specifici per un computer.

- **Ruolo del pacchetto `here`**  
  Aiuta a gestire i percorsi relativi all'interno del progetto senza dover specificare percorsi assoluti.

**3. Struttura dei progetti in R**

- **Vantaggi dell'uso dei progetti in RStudio:**  
  Mantengono ambienti separati, organizzano i file e facilitano la riproducibilità.

- **Cartelle principali in una struttura organizzata:**  
  - `data/raw/` → Dati grezzi  
  - `data/processed/` → Dati elaborati  
  - `dslc_documentation/` → Documentazione e script  
  - `functions/` → Funzioni personalizzate  

- **Separare dati grezzi da dati processati:**  
  Evita di modificare accidentalmente i dati originali, garantendo riproducibilità.

**4. Importazione ed esportazione dei dati**

- **Pacchetti per importazione/esportazione:**  
  - `rio`: unifica funzioni di import/export  
  - `readr`: specifico per CSV e altri formati di testo  
  - `here`: gestisce percorsi relativi  

- **Esempio di codice per importare dati CSV:**  

  ```r
  library(here)
  library(rio)
  df <- rio::import(here("data", "raw", "my_data.csv"))
  ```

- **Esportare dati modificati:**  

  ```r
  rio::export(df, here("data", "processed", "my_data_processed.csv"))
  ```

**5. Pulizia e preprocessing dei dati**

- **Differenza tra pulizia e preprocessing:**  
  - Pulizia: rimozione di errori, gestione dei dati mancanti, formattazione  
  - Preprocessing: trasformazione dei dati per adattarli a modelli specifici  

- **Strumenti di `dplyr` per pulizia e trasformazione:**  
  - `mutate()`, `filter()`, `select()`, `rename()`, `relocate()`

**6. Analisi esplorativa dei dati (EDA)**

- **Strumenti comuni:**  
  - `summary()`, `str()`, `glimpse()`, `ggplot2` per visualizzazione  

- **Esempio di codice per statistiche descrittive:**  
  ```r
  summary(df)
  ```

**7. Riproducibilità e comunicazione dei risultati**

- **Importanza della riproducibilità:**  
  - Facilita la verifica e il miglioramento degli studi  
  - Previene errori accidentali  
  - Consente a terzi di replicare e costruire su ricerche precedenti  

- **Strumenti di Quarto:**  
  - Permette di combinare testo, codice e output in documenti riproducibili  
  - Supporta citazioni automatiche e gestione delle bibliografie  
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}
