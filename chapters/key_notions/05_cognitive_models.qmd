# Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia {#sec-key-notions-cogn-models}

::: callout-important
## In questo capitolo imparerai a

* comprendere il ruolo dei modelli computazionali nella psicologia scientifica, riconoscendo come essi permettano di formalizzare, simulare e spiegare i processi mentali in modo più rigoroso rispetto ai modelli descrittivi;
* conoscere la struttura e la logica di due modelli fondamentali — il modello di Rescorla-Wagner (per l’apprendimento associativo) e il Drift Diffusion Model (per le decisioni sotto incertezza) — e comprenderne il significato dei parametri principali attraverso esempi e simulazioni.
:::

::: callout-tip
## Prerequisiti

- Consulta *Why is the Rescorla-Wagner model so influential?* [@soto2023rescorla].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(rtdists)
```
:::


## Introduzione 

Per comprendere e spiegare i processi mentali in modo più rigoroso, è necessario adottare modelli che vadano oltre la semplice descrizione. Questo capitolo introduce i modelli meccanicistici e computazionali, mostrando come possano rafforzare la spiegazione psicologica. Negli ultimi anni, la psicologia ha attraversato una crisi profonda legata alla *riproducibilità* dei risultati sperimentali. Molti effetti classici non riescono a replicarsi in studi successivi, sollevando interrogativi sulla solidità delle teorie psicologiche. In questo contesto, è sempre più chiaro che il tipo di modelli utilizzati per spiegare i fenomeni psicologici ha un impatto cruciale sulla credibilità e robustezza della ricerca scientifica. Una distinzione centrale a questo riguardo è quella tra *modelli fenomenologici* e *modelli meccanicistici*. 


### Modelli fenomenologici: descrivere senza spiegare

I modelli fenomenologici si limitano a descrivere relazioni osservabili tra variabili psicologiche, spesso attraverso formule matematiche o rappresentazioni statistiche. Un esempio classico è una legge psicofisica che descrive la relazione tra stimolazione sensoriale e risposta percepita. Sebbene questi modelli possano essere estremamente predittivi, non forniscono informazioni sul "come" e "perché" un certo fenomeno si verifica. Non specificano, cioè, *le entità e le attività organizzate che lo generano* (es. meccanismi cognitivi, neuroni, moduli funzionali).

Come sottolineato da @povich_mechanistic_2025, modelli fenomenologici come questi possono essere *accurati*, *compatti*, persino *predittivi* — ma non necessariamente *esplicativi*. In effetti, possono mancare della capacità di rispondere a domande controfattuali del tipo “che cosa succederebbe se…?” e non permettono un controllo diretto sul fenomeno. Questa limitazione si rivela particolarmente problematica in un’epoca in cui la replicabilità richiede non solo constatare un effetto, ma anche comprenderne le condizioni causali e contestuali.


### Dai modelli meccanicistici alla modellazione computazionale

Un *modello meccanicistico* cerca di rappresentare le componenti causali di un fenomeno. Secondo una definizione ampiamente condivisa, un meccanismo è “una collezione organizzata di entità e attività che produce o mantiene un certo fenomeno” [@bechtel2009looking]. I modelli meccanicistici hanno l’obiettivo di descrivere questi meccanismi, specificando in che modo le componenti interagiscono per generare il comportamento osservato.

Nel contesto della psicologia, i modelli meccanicistici vanno oltre la descrizione di correlazioni osservabili. Cercano di identificare strutture cognitive, processi neurali o dinamiche corpo-ambiente o interazioni tra livelli (funzionale, computazionale, implementazionale). Un esempio ben noto è il modello della *long-term potentiation* (LTP) nella memoria, che spiega come variazioni nei recettori NMDA e AMPA e nella concentrazione di ioni calcio e magnesio determinano il rafforzamento sinaptico — un chiaro caso di spiegazione meccanicistica.

Oggi, molti modelli meccanicistici in psicologia sono implementati come modelli computazionali, ovvero rappresentazioni formali che simulano i processi interni ipotizzati. Attraverso la simulazione e la stima dei parametri, questi modelli permettono di inferire il funzionamento dei meccanismi sottostanti a partire dal comportamento osservabile. I modelli computazionali soddisfano i criteri della spiegazione meccanicistica quando forniscono informazioni su entità ipotetiche (come credenze, soglie decisionali, accumulo di evidenza) e sulle loro interazioni causali.


### La differenza epistemica: come distinguere spiegazione da predizione

Un punto chiave nella distinzione tra spiegazioni fenomenologiche e meccanicistiche è che solo le seconde soddisfano i criteri di *potere esplicativo* propriamente detto. Come chiarisce @povich_mechanistic_2025, un modello esplicativo deve permettere di:

* rispondere a domande controfattuali (*"che cosa succederebbe se una componente fosse diversa?"*);
* fornire la base per manipolare o controllare il fenomeno.

Questi criteri sono cruciali per superare la crisi della replicabilità: sapere che un effetto si verifica in certe condizioni è poco utile se non si capisce *perché* avviene e *quali sono i meccanismi* sottostanti che lo rendono stabile o instabile rispetto a cambiamenti contestuali.


### Oltre le metafore meccaniche: che cosa rende *meccanicistico* un modello?

Una fonte comune di confusione riguarda l’idea che un modello, per essere *meccanicistico*, debba avere necessariamente la forma di una macchina, con entità concrete (es. neuroni, aree cerebrali) e connessioni visibili tra di esse. Ma questa è una semplificazione fuorviante. Ciò che rende un modello *meccanicistico* non è la sua forma visiva o metaforica, ma la sua capacità di rappresentare *l’organizzazione causale del processo* che genera un certo comportamento.

Un modello può essere espresso con equazioni matematiche, algoritmi, reti neurali, simulazioni, eppure contribuire in modo decisivo a una spiegazione meccanicistica se *specifica in che modo le componenti del sistema interagiscono per produrre l’effetto osservato*.

Per chiarire questa idea, possiamo richiamare i tre livelli di spiegazione proposti da David Marr (1982), uno dei riferimenti fondamentali nella psicologia cognitiva computazionale:

* *Livello computazionale*: *Cosa* fa il sistema e *perché* (qual è il problema che risolve?).
* *Livello algoritmico*: *Come* lo fa? Quali rappresentazioni interne e quali trasformazioni (regole di calcolo) sono coinvolte?
* *Livello implementativo*: *Con quali mezzi fisici* è realizzato (es. circuiti neurali)?

Questa distinzione aiuta a chiarire che un modello può essere meccanicistico anche se non rappresenta direttamente substrati biologici, purché descriva in modo formale come un sistema risolve un problema e quali regole seguono le sue componenti.

Nel contesto della psicologia, i modelli computazionali che operano al *livello algoritmico o computazionale* — come il modello di Rescorla-Wagner o il Drift Diffusion Model — sono perfettamente coerenti con un approccio meccanicistico, anche se non rappresentano esplicitamente l’implementazione biologica.

Questi modelli sono “meccanicistici” nel senso che:

* *descrivono entità funzionali* (es. valore atteso, evidenza accumulata),
* *specificano regole di interazione* tra queste entità (es. aggiornamento, accumulo, soglie),
* *producono il comportamento osservabile* come risultato di queste interazioni.

Dunque, ciò che conta non è la *forma* del modello, ma la *funzione esplicativa* che svolge all’interno della teoria psicologica. Modelli formulati come sistemi dinamici, modelli bayesiani gerarchici, modelli di reti neurali artificiali o modelli simbolici possono tutti contribuire a spiegazioni meccanicistiche, se mostrano come un certo comportamento emerga da un’organizzazione di componenti in interazione.


### Perché i modelli meccanicistici rafforzano la riproducibilità

La crisi della riproducibilità può essere vista come il sintomo di un’eccessiva fiducia in modelli fenomenologici che mancano di profondità esplicativa. I modelli meccanicistici, al contrario:

* esplicitano gli assunti causali e strutturali;
* permettono verifiche controfattuali e manipolazioni;
* chiariscono quando e perché un effetto dovrebbe ripetersi o variare;
* sono meno vulnerabili al *cherry picking* e agli *effetti di contesto* non dichiarati;
* i modelli computazionali meccanicistici, come il DDM e il modello di Rescorla-Wagner, consentono di simulare e verificare quantitativamente ipotesi sui meccanismi interni, rendendo più trasparente e replicabile l’inferenza psicologica.

In breve, un modello meccanicistico non si limita a dire che “A predice B”, ma mostra come *A produce B*, e in quali condizioni questo accade.


## Esempi di modelli computazionali meccanicistici

Nel contesto della psicologia cognitiva e della neuroscienza computazionale, molti modelli meccanicistici assumono la forma di modelli computazionali. Questi modelli non si limitano a correlare input e output, ma simulano i processi interni che trasformano gli input (es. stimoli) in output (es. risposte comportamentali), ipotizzando una dinamica causale tra variabili latenti e osservabili.


### Il modello di Rescorla-Wagner: apprendimento associativo come aggiornamento predittivo

Uno dei modelli più influenti nello studio dell'apprendimento è il modello di Rescorla-Wagner. Questo modello descrive come gli individui apprendano le associazioni tra stimoli e risposte sulla base dell'errore di previsione. L'apprendimento avviene aggiornando le aspettative di ricompensa in base alle esperienze passate, utilizzando due parametri fondamentali:

- *$\alpha$ (tasso di apprendimento)*: determina quanto l'errore di previsione influisce sull'aggiornamento dell'aspettativa.
- *$\beta$ (temperatura della scelta)*: regola la probabilità di selezionare l'opzione con il valore atteso più alto rispetto a esplorare alternative.

#### L'Apprendimento Associativo

L'apprendimento per rinforzo studia come le persone imparano a massimizzare le ricompense in ambienti in cui la scelta ottimale è inizialmente sconosciuta. Immaginiamo un partecipante che deve scegliere ripetutamente tra due slot machine, ricevendo ricompense con probabilità diverse per ogni macchina. L'obiettivo è massimizzare le vincite nel tempo.

Per illustrare il modello, si usa spesso la metafora delle slot machine. Nel caso più semplice, si immagina un agente che svolge il compito con $n$ tentativi, due slot machine e probabilità di ricompensa fisse $\mu = [0.2, 0.8]$.

#### Regola di Apprendimento per Rinforzo ($\delta$-rule)

Il modello di Rescorla-Wagner descrive l'apprendimento come un processo basato sull'errore di previsione. L'aggiornamento del valore di uno stimolo avviene secondo la seguente equazione:

$$
V_{s,t} = V_{s,t-1} + \alpha (r_{t-1} - V_{s,t-1})
$$

Dove:

- $V_{s,t}$ è il valore atteso dello stimolo $s$ al tempo $t$.
- $r_{t-1}$ è la ricompensa ottenuta alla prova precedente.
- $\alpha$ (tra 0 e 1) è il tasso di apprendimento, che determina la velocità con cui l'agente aggiorna le proprie aspettative.

Se il valore di $\alpha$ è alto, l'apprendimento sarà rapido, mentre se è basso, l'agente si baserà maggiormente sulle esperienze passate.

#### Modello di Scelta: Softmax

Dopo aver aggiornato i valori attesi delle opzioni, il partecipante deve scegliere tra esse.

Due strategie possibili sono:

- *sfruttamento*: selezionare sempre l'opzione con il valore più alto.
- *esplorazione*: scegliere occasionalmente un'opzione con un valore più basso per verificare se potrebbe essere migliore.

Per modellare questo comportamento si usa la funzione Softmax:

$$
p(s) = \frac{\exp(\beta \cdot V_{s})}{\sum_i \exp(\beta \cdot V_{i})}
$$

Dove $\beta$ è un parametro che determina il grado di esplorazione:

- $\beta = 0$: scelta completamente casuale.
- $\beta \to \infty$: scelta deterministica dell'opzione con il valore più alto.

Un individuo con $\beta$ alto sceglierà quasi sempre l'opzione con il valore atteso più elevato, mentre con un $\beta$ basso esplorerà più frequentemente.

#### Simulazione dell'Apprendimento con il Modello di Rescorla-Wagner

Possiamo ora simulare questo processo in R. La funzione seguente implementa la regola di aggiornamento dell’aspettativa sulla base dell’errore di previsione:

```{r}
update_rw <- function(value, alpha=0.15, lambda=1) {
  value + alpha * (lambda - value)
}
```

Simuliamo ora l'apprendimento per 40 prove, assumendo che il partecipante riceva sempre una ricompensa:

```{r}
n_trials <- 40
strength <- numeric(n_trials)
for(trial in 2:n_trials) {
  strength[trial] <- update_rw(strength[trial-1])
}
plot(1:n_trials, strength, type = 'l', ylim = c(0,1), xlab = "Prove", ylab = "Aspettativa di ricompensa")
```

L'aspettativa di ricompensa aumenta progressivamente fino a stabilizzarsi.


#### Estinzione dell'Associazione

Modificando il valore del rinforzo, possiamo anche simulare l’estinzione dell’apprendimento. Se dopo 25 prove la ricompensa non viene più fornita, il valore associato allo stimolo diminuisce gradualmente:

```{r}
n_trials <- 50                
strength <- numeric(n_trials)
lambda <- 1

for(trial in 2:n_trials) {
  if(trial > 25) {
    lambda <- 0
  }
  strength[trial] <- update_rw(value = strength[trial-1], lambda = lambda)
}

plot(1:n_trials, strength, type = 'l', ylim = c(0,1), xlab = "Prove", ylab = "Aspettativa di ricompensa")
```

L'associazione si estingue gradualmente quando il rinforzo viene rimosso.

#### Implementazione della Regola Softmax

Per simulare le scelte di un partecipante utilizziamo la funzione Softmax:

```{r}
softmax <- function(beta, x) {
  1 / (1 + exp(-beta * x))
}

beta <- 5
x <- seq(-1, 1, length.out = 100)
y <- softmax(beta, x)
plot(x, y, type = 'l', xlab = "Valore (A) - valore (B)", ylab = "p(scelta = A)")
```

La funzione mostra che:

1. La probabilità di scegliere un'opzione aumenta con il suo valore atteso.
2. Con $\beta$ elevato, il partecipante sceglie quasi sempre l'opzione migliore.
3. Con $\beta$ basso, le scelte sono più casuali.


#### Verifica e Applicazioni del Modello

Quello descritto è il meccanismo generatore dei dati ipotizzato dal modello di Rescorla-Wagner. Per testare il modello, è necessario stimare i parametri $\alpha$ e $\beta$, e confrontare le previsioni del modello con i dati osservati. Tuttavia, in questo corso non affronteremo il problema della stima dei parametri del modello di Rescorla-Wagner. L'obiettivo principale è comprendere cosa significhi formalizzare quantitativamente un modello psicologico e in che modo questo approccio si differenzi da una semplice analisi delle associazioni tra variabili.

In sintesi, il modello di Rescorla-Wagner rappresenta uno strumento essenziale per lo studio dell'apprendimento associativo. Attraverso la simulazione dell'aggiornamento delle aspettative e delle strategie decisionali, possiamo descrivere il comportamento di individui che apprendono in contesti di rinforzo. Questo modello ha trovato applicazione in numerosi ambiti della psicologia cognitiva e delle neuroscienze, contribuendo alla comprensione dei processi di apprendimento e decisione.


### Il Drift Diffusion Model (DDM): decisione come accumulo stocastico di evidenza

Il processo decisionale è uno dei temi centrali della psicologia cognitiva e delle neuroscienze. Ogni giorno prendiamo decisioni, dalle più semplici alle più complesse, influenzate da fattori come la percezione, la memoria, l’attenzione e il contesto in cui ci troviamo. Una domanda fondamentale è: *Come prendiamo decisioni in condizioni di incertezza?*  

Uno dei modelli più utilizzati per rispondere a questa domanda è il *Drift Diffusion Model (DDM)*, un modello matematico che descrive il processo di accumulo delle informazioni fino alla presa di una decisione. Questo modello consente di quantificare e comprendere i meccanismi alla base delle scelte umane.

#### Cos’è il Drift Diffusion Model?

Il DDM descrive come le persone raccolgono informazioni nel tempo per prendere una decisione tra due alternative. Immagina di dover stabilire se un punto si sta muovendo verso destra o verso sinistra. Non hai una risposta immediata, ma accumuli informazioni (o "evidenza") nel tempo fino a quando non sei abbastanza sicuro per scegliere.  

Questo processo è influenzato da vari fattori, come la chiarezza delle prove disponibili e l’incertezza associata alla decisione.

#### Come Funziona il Processo di Accumulo dell’Evidenza?

Il processo decisionale può essere paragonato a un *accumulo graduale di informazioni* a favore di una delle due opzioni disponibili. Ecco come funziona:

1. *Raccolta delle informazioni*  
   Ogni nuova informazione che ricevi si accumula a favore di una delle due alternative. Ad esempio, se stai cercando di determinare la direzione del movimento di un punto, ogni piccolo dettaglio visivo ti aiuta ad avvicinarti a una decisione.

2. *Velocità di accumulo (Drift rate)*  
   La velocità con cui raccogli le informazioni dipende dalla qualità del segnale. Se le prove sono chiare e forti, l’accumulo sarà veloce. Se invece sono ambigue, il processo sarà più lento.

3. *Rumore e incertezza*  
   Durante l’accumulo, c’è sempre una componente casuale o "rumore", che può causare fluttuazioni nel processo. Questo significa che l’informazione non si accumula in modo perfettamente lineare, ma può oscillare a causa di fattori casuali.

4. *Soglie decisionali*  
   Prima di iniziare il compito, ci sono due "soglie" che rappresentano i punti di decisione. Quando l’evidenza accumulata raggiunge una di queste soglie, si prende la decisione corrispondente.

5. *Tempo di reazione*  
   Il tempo impiegato per raggiungere una delle soglie è il *tempo di reazione*. Se le informazioni sono chiare, la decisione sarà rapida; se sono ambigue, il tempo sarà più lungo.

Una metafora utile per comprendere questo processo è quella di *un secchio che si riempie goccia dopo goccia*: ogni informazione raccolta corrisponde a una goccia. Quando il livello d’acqua supera una soglia, si prende la decisione.


#### I Parametri del DDM

Il DDM è caratterizzato da quattro parametri principali che descrivono diversi aspetti del processo decisionale:

1. *Tasso di drift ($v$)*  
   Rappresenta la velocità con cui l’evidenza si accumula a favore di una decisione. Valori più alti indicano un processo decisionale più efficiente, mentre valori più bassi suggeriscono un’accumulazione lenta e incerta.

2. *Separazione delle soglie ($a$)*  
   Indica la distanza tra le due soglie decisionali. Valori più alti corrispondono a decisioni più caute (tempi di reazione più lunghi ma minore probabilità di errore), mentre valori più bassi indicano decisioni più rapide ma potenzialmente meno accurate.

3. *Tempo di non-decisione ($t_0$)*  
   Corrisponde al tempo necessario per processi che precedono e seguono l’accumulo di evidenza, come la percezione dello stimolo e l’esecuzione della risposta. Questo tempo è indipendente dall’accumulo delle informazioni.

4. *Bias iniziale ($z$)*  
   Definisce il punto di partenza del processo di accumulo. Se è equidistante tra le due soglie, la decisione è imparziale. Se invece è spostato verso una delle due soglie, significa che la persona ha una predisposizione a scegliere una delle due alternative.

#### Il Compromesso tra Velocità e Accuratezza

Uno degli aspetti più interessanti del DDM è il *compromesso tra velocità e accuratezza*.  

- Se una persona desidera rispondere rapidamente, può *abbassare le soglie decisionali*, ma questo aumenta la probabilità di errore.  
- Se invece punta a una maggiore accuratezza, può *aumentare la distanza tra le soglie*, rendendo il processo più lento ma più affidabile.

Questo compromesso è evidente in compiti sperimentali come:  

- Il *compito di Stroop*, dove bisogna ignorare un’informazione interferente (es. leggere il colore di una parola e non il significato della parola stessa).  
- Il *compito di decisione lessicale*, in cui si deve determinare se una stringa di lettere è una parola esistente o meno.  

Il DDM permette di capire se le differenze nei tempi di reazione tra gruppi dipendono da una strategia più cauta (maggiore $a$) o da una difficoltà nell’accumulare evidenza (minore $v$).

#### Perché è Importante il DDM?

Il DDM è uno strumento potente perché permette di *quantificare* aspetti del processo decisionale che altrimenti sarebbero difficili da misurare, come la velocità di accumulo dell’evidenza o l’effetto del rumore sulla decisione.  

È stato applicato in numerosi ambiti, tra cui:

- *Compiti percettivi e decisionali*: studi sulla discriminazione di stimoli visivi e uditivi.
- *Processi di controllo cognitivo*: analisi delle differenze individuali nella regolazione dell’impulsività.
- *Psicopatologia*: esplorazione delle alterazioni nel processo decisionale in condizioni come depressione, ansia e schizofrenia.

Il *Drift Diffusion Model* offre dunque una rappresentazione chiara e quantitativa del processo decisionale in condizioni di incertezza. Descrivendo l’accumulo graduale delle informazioni e il raggiungimento delle soglie decisionali, il modello ci aiuta a comprendere il compromesso tra velocità e accuratezza e i fattori che influenzano le scelte.  

L’applicazione del DDM in psicologia cognitiva e neuroscienze permette di studiare non solo il comportamento umano, ma anche i meccanismi neurali che regolano il processo decisionale.

#### Simulazione del DDM

Una delle potenzialità del DDM è la possibilità di simulare dati sintetici per confrontare le predizioni del modello con dati empirici. In R, possiamo generare una simulazione semplificata del modello utilizzando pacchetti dedicati come `rtdists` o `brms`. 

Un esempio di codice per simulare dati con parametri definiti:

```{r}
# Nuova configurazione dei parametri
a <- 1.2   # Separazione delle soglie (aumentato)
v <- 0.3   # Tasso di drift
t0 <- 0.2  # Tempo di non-decisione
z <- 0.5   # Bias iniziale (deve essere tra 0 e 1)

# Generazione dei dati
sim_data <- rdiffusion(n = 1000, a = a, v = v, t0 = t0, z = z)
```

Questa funzione genera 1000 decisioni simulate, ciascuna con un tempo di reazione (rt) e una risposta (response) che dipendono dai parametri impostati.

```{r}
# Visualizzazione dei tempi di reazione
hist(
  sim_data$rt, 
  breaks = 30, 
  main = "Distribuzione dei tempi di reazione", 
  xlab = "RT (s)"
)
```

Questo codice genera una distribuzione di tempi di reazione e scelte coerenti con le ipotesi del DDM, permettendo di esplorare l'effetto delle variazioni dei parametri sul comportamento del modello.

In sintesi, il Drift Diffusion Model fornisce un quadro teorico potente per l'analisi del processo decisionale in psicologia cognitiva. Modellando il tempo di reazione e la probabilità di risposta in termini di parametri interpretabili, il DDM permette di distinguere tra strategie decisionali e difficoltà cognitive, superando i limiti di un'analisi puramente descrittiva. Grazie alla sua capacità di catturare la dinamica dei processi decisionali, il DDM è oggi uno degli strumenti più utilizzati per studiare il comportamento umano in contesti sperimentali e applicativi.


## Riflessioni Conclusive

La crisi di replicabilità che sta attraversando la psicologia contemporanea richiede un cambiamento di prospettiva: è necessario abbandonare i modelli puramente descrittivi a favore di approcci meccanicistici e computazionali. Questi ultimi, infatti, non si limitano a prevedere il comportamento, ma identificano i processi algoritmici che lo generano, trasformando domande vaghe in ipotesi formalizzate e verificabili.

In questo capitolo abbiamo introdotto due modelli fondamentali: il modello Rescorla-Wagner e il modello Drift Diffusion model. 

Il modello di Rescorla-Wagner illustra come l'apprendimento associativo derivi da un aggiornamento incrementale delle aspettative, guidato dall'errore di previsione. La sua forza risiede nella capacità di quantificare parametri psicologicamente significativi, come il tasso di apprendimento (α), che riflette la sensibilità individuale alle discrepanze tra aspettative e realtà. L'aggiunta della funzione softmax permette di modellare il conflitto tra esplorazione e sfruttamento, catturando la variabilità adattiva del comportamento.

Parallelamente, il Drift Diffusion Model (DDM) scompone la decisione in un processo dinamico di accumulo di evidenza, in cui parametri come la velocità di elaborazione (drift rate), la cautela decisionale (threshold separation) e i tempi non decisionali (non-decision time) permettono di distinguere componenti cognitive diverse. Questo approccio supera le descrizioni statiche, rivelando come diverse strategie emergano dall'interazione tra vincoli computazionali e contesto.

Entrambi i modelli evidenziano il valore della formalizzazione matematica nello studio dei processi cognitivi. Il modello di Rescorla-Wagner è particolarmente utile per comprendere come gli individui apprendano e aggiornino le proprie credenze sulla base dell'esperienza, mentre il DDM fornisce una rappresentazione più dettagliata delle dinamiche della presa di decisione e del compromesso tra velocità e accuratezza.

In conclusione, l’approccio computazionale alla psicologia cognitiva permette di superare i limiti di un’analisi puramente descrittiva, fornendo strumenti matematici per testare le ipotesi sui processi cognitivi. L'uso combinato di modelli di apprendimento e decisionali consente di ottenere una visione più completa dei meccanismi che guidano il comportamento umano, con implicazioni sia per la ricerca di base che per le applicazioni cliniche.

Questo approccio si inserisce anche nel contesto più ampio della psichiatria computazionale e della modellazione bayesiana dei processi mentali, in cui l'obiettivo non è solo descrivere ciò che le persone fanno, ma inferire quali processi latenti sono più plausibili alla luce dei dati. In questo senso, la formalizzazione dei meccanismi non solo migliora la spiegazione, ma anche l'inferenza e la generalizzazione, che sono due ingredienti essenziali per una scienza psicologica cumulativa e replicabile.


## Esercizi {.unnumbered}

::: {.callout-important title="Problemi" collapse="true"}

1. Che cosa descrive il modello di Rescorla-Wagner?
2. Qual è il ruolo del parametro α nel modello di Rescorla-Wagner? 
3. Quale funzione matematica viene utilizzata per modellare il bilanciamento tra esplorazione ed esploitazione nel modello di Rescorla-Wagner?
4. Quali sono i principali parametri del Drift Diffusion Model (DDM)? 
5. In che modo il DDM spiega il compromesso tra velocità e accuratezza nelle decisioni? 

:::

::: {.callout-tip title="Soluzioni" collapse="true"}

1. Il modello di Rescorla-Wagner descrive come gli individui apprendano le associazioni tra stimoli e risposte in base all’errore di previsione. L’aspettativa di ricompensa viene aggiornata attraverso l’esperienza, con un processo regolato dal tasso di apprendimento ($\alpha$). 

2. Il parametro $\alpha$ (tasso di apprendimento) determina quanto velocemente un individuo aggiorna le proprie aspettative in base all’errore di previsione. Se $\alpha$ è alto, l’apprendimento è rapido; se è basso, l’individuo si basa maggiormente sulle esperienze passate.

3. La funzione Softmax viene utilizzata per modellare il bilanciamento tra esplorazione ed esploitazione. Essa regola la probabilità di scegliere un’opzione in base al valore atteso e alla temperatura della scelta ($\beta$).  

4. I principali parametri del DDM sono:

   - *Tasso di drift ($v$)*: velocità con cui viene accumulata l’evidenza.  
   - *Separazione delle soglie ($a$)*: distanza tra le soglie decisionali.  
   - *Tempo di non-decisione ($t_0$)*: tempo impiegato per processi indipendenti dall’accumulo dell’evidenza.  
   - *Bias iniziale ($z$)*: punto di partenza dell’accumulo dell’evidenza.  

5. Il DDM spiega il compromesso tra velocità e accuratezza attraverso la separazione delle soglie decisionali ($a$). Se le soglie sono più vicine, le decisioni sono più rapide ma meno accurate; se sono più distanti, le decisioni sono più lente ma più precise.  
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}
