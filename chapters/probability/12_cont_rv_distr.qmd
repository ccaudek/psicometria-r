---
execute:
  freeze: auto
---

# Distribuzioni di v.c. continue {#sec-prob-cont-prob-distr}

::: callout-important
## In questo capitolo imparerai a

- interpretare e utilizzare le principali distribuzioni di densità delle variabili casuali continue.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Continuous random variables* del testo di @blitzstein2019introduction.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione

Analogamente a quanto avviene per le variabili casuali discrete, anche per le variabili casuali continue possiamo rappresentare la variabilità all'interno di una popolazione attraverso un modello statistico, ma in questo caso utilizziamo le densità di probabilità -- si veda il @sec-prob-distr. Mentre le distribuzioni di probabilità discrete si applicano a fenomeni con un numero finito o numerabile di esiti, le densità di probabilità descrivono variabili casuali che possono assumere un continuum di valori.

La funzione di densità di probabilità $f(x)$ associata a una variabile casuale continua $X$ rappresenta la distribuzione della probabilità all'interno della popolazione. Questa funzione non fornisce la probabilità esatta di un singolo valore, ma piuttosto la probabilità di osservare valori di $X$ all'interno di un intervallo specifico. Così come per le distribuzioni discrete, anche le densità di probabilità costituiscono un modello della popolazione, una rappresentazione matematica che ci consente di fare previsioni e di comprendere meglio i fenomeni aleatori continui.

Iniziamo con la distribuzione continua uniforme.

## La Distribuzione Uniforme

La **distribuzione uniforme** è una delle distribuzioni di probabilità più semplici e intuitive. Si utilizza quando ogni valore in un determinato intervallo ha la stessa probabilità di verificarsi. Un esempio comune è un esperimento in cui si fa ruotare uno spinner circolare e si registra l'angolo ottenuto: poiché ogni angolo ha la stessa probabilità, i valori sono distribuiti uniformemente.

### Simulazione della distribuzione uniforme

Per comprendere meglio questa distribuzione, consideriamo una simulazione. Generiamo 20 valori casuali che rappresentano gli angoli ottenuti dallo spinner e li visualizziamo con un istogramma:

```{r}
set.seed(123)
spinner_results <- runif(20, min = 0, max = 360)
print(spinner_results)

ggplot(data.frame(Valori = spinner_results), aes(x = Valori)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.5) +
  labs(x = "Risultato dello spinner", y = "Frequenza relativa",
       title = "Istogramma dei risultati (20 simulazioni)")
```

Con sole 20 osservazioni, l'istogramma potrebbe non apparire uniforme a causa della variabilità naturale nei piccoli campioni. Aumentiamo il numero di simulazioni a 100.000 per ottenere un'immagine più chiara:

```{r}
spinner_results_large <- runif(100000, min = 0, max = 360)

ggplot(data.frame(Valori = spinner_results_large), aes(x = Valori)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.5) +
  labs(x = "Risultato dello spinner", y = "Frequenza relativa",
       title = "Istogramma dei risultati (100.000 simulazioni)")
```

Con 100.000 valori, l'istogramma appare visivamente uniforme, confermando la distribuzione uniforme dei dati nell'intervallo [0, 360].

### Funzione di densità uniforme

La distribuzione uniforme può essere descritta da una **funzione di densità di probabilità** (PDF). Per una distribuzione uniforme continua su un intervallo $[a, b]$, la densità è costante e definita come:

$$
f(x) = \frac{1}{b - a} \quad \text{per } x \in [a, b].
$$

Nel caso dello spinner, con $a = 0$ e $b = 360$, la densità è:

$$
f(x) = \frac{1}{360}.
$$

Possiamo visualizzare questa funzione in R:

```{r}
x <- seq(0, 360, length.out = 100)
density_uniform <- dunif(x, min = 0, max = 360)

ggplot(data.frame(x = x, y = density_uniform), aes(x = x, y = y)) +
  geom_line(linewidth = 1, color = "blue") +
  labs(x = "x", y = "p(x)", title = "Funzione di densità uniforme")
```

### Calcolo della probabilità in un intervallo

La probabilità di ottenere un valore tra $150$ e $250$ in una distribuzione uniforme è data dall'area sotto la funzione di densità in quell'intervallo:

$$
P(150 < X < 250) = (250 - 150) \cdot \frac{1}{360}.
$$

In R, possiamo calcolare questa probabilità manualmente:

```{r}
prob_intervallo <- (250 - 150) * (1 / 360)
prob_intervallo
```

Oppure utilizzando la funzione cumulativa `punif`:

```{r}
prob_intervallo_cdf <- 
  punif(250, min = 0, max = 360) - punif(150, min = 0, max = 360)
prob_intervallo_cdf
```

La probabilità è circa **0.2778**. Visualizziamo questa area nell'intervallo [150, 250]:

```{r}
x <- seq(0, 360, length.out = 1000)
fx <- dunif(x, min = 0, max = 360)

ggplot(data.frame(x = x, fx = fx), aes(x = x, y = fx)) +
  geom_line(linewidth = 1, color = "blue") +
  geom_area(data = subset(data.frame(x = x, fx = fx), x >= 150 & x <= 250),
            aes(x = x, y = fx), fill = "gray", alpha = 0.5) +
  labs(x = "x", y = "p(x)", title = "Probabilità per l'intervallo [150, 250]")
```

### Proprietà della distribuzione uniforme

Per una variabile casuale $X \sim \mathcal{U}(a, b)$, le proprietà principali sono:

1. **Valore atteso (media)**:
   $$
   E(X) = \frac{a + b}{2}.
   $$

2. **Varianza**:
   $$
   V(X) = \frac{(b - a)^2}{12}.
   $$

Ad esempio, per $\mathcal{U}(0, 360)$:

```{r}
E_X <- (0 + 360) / 2
V_X <- (360 - 0)^2 / 12
E_X
V_X
```

### Funzioni in R per la distribuzione uniforme

R offre diverse funzioni per manipolare la distribuzione uniforme:

- **`runif`**: genera numeri casuali.
- **`dunif`**: calcola la densità per un valore specifico.
- **`punif`**: calcola la probabilità cumulativa fino a un valore.
- **`qunif`**: restituisce il quantile corrispondente a una probabilità specifica.

Esempi pratici:

```{r}
# Densità per valori specifici
dunif(c(0.5, 0.8, 1.2), min = 0, max = 1)

# Probabilità cumulativa
punif(c(0.5, 0.8), min = 0, max = 1)

# Quantili
qunif(c(0.5, 0.8), min = 0, max = 1)

# Generazione di numeri casuali
set.seed(123)
runif(5, min = 0, max = 1)
```

In conclusione, la distribuzione uniforme fornisce una base fondamentale per comprendere i concetti di probabilità, densità e variabilità, ed è supportata da una gamma completa di strumenti in R per l'analisi e la simulazione.

## Distribuzione Esponenziale

La **distribuzione esponenziale** è una distribuzione di probabilità continua ampiamente utilizzata per modellare il **tempo di attesa** fino al verificarsi di un evento. Questa distribuzione è caratterizzata dalla **proprietà di assenza di memoria**, che la rende unica. 

### Proprietà di assenza di memoria

L'assenza di memoria implica che la probabilità di un evento futuro è indipendente dal tempo già trascorso. Ad esempio, supponiamo che il tempo di rottura di un bicchiere da vino dopo il primo utilizzo segua una distribuzione esponenziale. Se un bicchiere non si è rotto dopo 3 anni, la probabilità che si rompa nel prossimo anno è uguale alla probabilità che un bicchiere nuovo si rompa nel primo anno di utilizzo.

### Funzione di densità

La funzione di densità di probabilità (PDF) per una variabile casuale esponenziale $X$ è definita come:

$$
f(x) = \lambda e^{-\lambda x}, \quad \lambda > 0,\, x \geq 0,
$$

dove $\lambda$ è il **tasso** di eventi per unità di tempo, pari all'inverso del tempo medio di attesa $\mu$:

$$
\lambda = \frac{1}{\mu}.
$$

Pertanto, possiamo esprimere la densità anche in funzione del tempo medio di attesa $\mu$:

$$
f(x) = \frac{1}{\mu} e^{-x / \mu}.
$$

### Proprietà principali

1. **Media**:
   $$
   E(X) = \frac{1}{\lambda}.
   $$

2. **Varianza**:
   $$
   V(X) = \frac{1}{\lambda^2}.
   $$

3. **Deviazione standard**:
   $$
   \sigma_X = \frac{1}{\lambda} = \mu.
   $$

### Esempio pratico

Supponiamo che il tempo medio per la pubblicazione dei voti di un esame sia $\mu = 4$ giorni. La funzione di densità diventa:

$$
f(x) = \frac{1}{4} e^{-x / 4}.
$$

### Visualizzazione della distribuzione

In R, possiamo rappresentare la densità della distribuzione esponenziale con:

```{r}
# Parametri della distribuzione
mu <- 4
lambda <- 1 / mu

# Generare valori per x
x <- seq(0, 20, by = 0.01)

# Calcolare la densità
pdf <- dexp(x, rate = lambda)

# Grafico della densità
ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(linewidth = 1, color = "blue") +
  labs(
    x = "x", y = "f(x)", 
    title = "Funzione di densità della distribuzione esponenziale"
  )
```

### Calcolo delle probabilità

#### Probabilità che $X \leq 1.5$

La probabilità che il tempo di attesa sia minore o uguale a 1.5 giorni può essere calcolata utilizzando la funzione di ripartizione cumulativa (CDF):

$$
P(X \leq 1.5) = F(1.5) = 1 - e^{-\lambda \cdot 1.5}.
$$

In R:

```{r}
pexp(1.5, rate = lambda)
```

Visualizziamo questa probabilità come l'area sotto la curva fino a $x = 1.5$:

```{r}
ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(linewidth = 1, color = "blue") +
  geom_area(
    data = subset(data.frame(x, pdf), x <= 1.5), 
    aes(x = x, y = pdf), fill = "gray", alpha = 0.5
  ) +
  labs(
    x = "x", y = "f(x)", 
    title = "Probabilità P(X <= 1.5)"
  )
```

#### Probabilità che $1 \leq X \leq 6$

Questa probabilità si calcola come differenza tra le funzioni di ripartizione:

$$
P(1 \leq X \leq 6) = F(6) - F(1).
$$

In R:

```{r}
pexp(6, rate = lambda) - pexp(1, rate = lambda)
```

Visualizziamo l'area corrispondente:

```{r}
ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(linewidth = 1, color = "blue") +
  geom_area(
    data = subset(data.frame(x, pdf), x >= 1 & x <= 6), 
    aes(x = x, y = pdf), fill = "gray", alpha = 0.5
  ) +
  labs(
    x = "x", y = "f(x)", 
    title = "Probabilità P(1 <= X <= 6)"
  )
```

#### Probabilità che $X \geq 5.5$

La probabilità può essere calcolata come evento complementare:

$$
P(X \geq 5.5) = 1 - F(5.5).
$$

In R:

```{r}
1 - pexp(5.5, rate = lambda)
# Oppure
pexp(5.5, rate = lambda, lower.tail = FALSE)
```

Visualizziamo l'area sotto la curva per $x \geq 5.5$:

```{r}
ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(linewidth = 1, color = "blue") +
  geom_area(
    data = subset(data.frame(x, pdf), x >= 5.5), 
    aes(x = x, y = pdf), fill = "gray", alpha = 0.5
  ) +
  labs(
    x = "x", y = "f(x)", 
    title = "Probabilità P(X >= 5.5)"
  )
```

### Istogramma di valori simulati

Simuliamo 1.000.000 di valori casuali da una distribuzione esponenziale con $\lambda = 1/4$ e costruiamo un istogramma confrontato con la densità teorica:

```{r}
# Simulazione di valori casuali
set.seed(123)
samples <- rexp(1000000, rate = lambda)

# Istogramma con densità sovrapposta
ggplot(data.frame(samples), aes(x = samples)) +
  geom_histogram(
    aes(y = after_stat(density)), bins = 100, 
    fill = "skyblue", color = "black", alpha = 0.5
  ) +
  geom_line(
    data = data.frame(x, pdf), aes(x = x, y = pdf), 
    color = "red", linewidth = 1
  ) +
  xlim(0, 20) +
  labs(
    x = "Tempo di attesa", y = "Frequenza relativa",
    title = "Istogramma dei valori simulati con densità teorica"
  )
```

In conclusione, la distribuzione esponenziale è uno strumento essenziale per modellare eventi che dipendono dal tempo di attesa. Grazie alla sua semplicità e alla proprietà di assenza di memoria, è particolarmente utile in molti contesti applicativi. Con le funzioni di R (`dexp`, `pexp`, `qexp`, `rexp`), è possibile analizzare e visualizzare facilmente i dati che seguono questa distribuzione.

## Distribuzione Normale

La **distribuzione normale**, anche nota come **distribuzione gaussiana**, è una delle distribuzioni di probabilità più importanti in statistica. Deve il suo nome a Carl Friedrich Gauss, che ne scoprì l'utilità per modellare gli errori di misurazione. Adolphe Quetelet, uno dei padri delle scienze sociali quantitative, fu il primo a utilizzare questa distribuzione per studiare le misurazioni dell'uomo. Karl Pearson introdusse il termine "normale", pur riconoscendo che ciò potesse erroneamente suggerire l'anormalità di altre distribuzioni.

### Una famiglia di distribuzioni

La distribuzione normale non è unica, ma rappresenta una famiglia di distribuzioni definite da due parametri:

- **$\mu$ (media):** il valore attorno al quale i dati sono simmetricamente distribuiti.
- **$\sigma$ (deviazione standard):** misura la dispersione dei dati attorno alla media.

La densità di probabilità per una variabile casuale continua $Y$ che segue una distribuzione normale è data da:

$$
f(y; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right),
$$

dove $\mu \in \mathbb{R}$ e $\sigma > 0$.

Questa curva ha una caratteristica **forma a campana**, simmetrica rispetto alla media $\mu$. La media, la mediana e la moda coincidono.


### Origini storiche: dal binomiale alla normale

La connessione tra distribuzioni binomiali e normali fu notata da Abraham de Moivre nel 1733. Egli osservò che, aumentando il numero di prove di una distribuzione binomiale, la forma della distribuzione si avvicina a quella di una curva normale. Questo fenomeno può essere illustrato confrontando distribuzioni binomiali con pochi e molti eventi.

### Distribuzione binomiale con poche prove

Con un numero di prove $n = 10$ e una probabilità di successo $p = 0.9$, la distribuzione binomiale è chiaramente asimmetrica:

```{r}
# Parametri
n <- 10
p <- 0.9

# Calcolo della distribuzione binomiale
r_values <- 0:n
dist <- dbinom(r_values, size = n, prob = p)

# Grafico
ggplot(data.frame(Successi = r_values, Probabilità = dist), aes(x = Successi, y = Probabilità)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(
    title = "Distribuzione Binomiale: n = 10, p = 0.9", 
    x = "Numero di Successi", y = "Probabilità"
  )
```

### Distribuzione binomiale con molte prove

Aumentando il numero di prove a $n = 1000$ con lo stesso $p$, la distribuzione assume una forma più simmetrica, simile a quella della normale:

```{r}
# Parametri aggiornati
n <- 1000

# Calcolo della distribuzione binomiale
r_values <- 850:950  # Intervallo per una migliore visualizzazione
dist <- dbinom(r_values, size = n, prob = p)

# Grafico
ggplot(data.frame(Successi = r_values, Probabilità = dist), aes(x = Successi, y = Probabilità)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(
    title = "Distribuzione Binomiale: n = 1000, p = 0.9", 
    x = "Numero di Successi", y = "Probabilità"
  )
```

### La normale generata da simulazioni

Un modo intuitivo per comprendere la distribuzione normale è attraverso la **simulazione di passeggiate casuali**. Immaginiamo che 1000 persone partano da una posizione iniziale e facciano 16 passi, ciascuno determinato da un lancio di moneta che stabilisce se avanzare o arretrare. Dopo 16 passi, la distribuzione delle loro posizioni segue una curva normale.

```{r}
# Parametri
numero_passi <- 16
ripetizioni <- 1000

# Generazione di passeggiate casuali
set.seed(123)
x <- matrix(0, nrow = numero_passi + 1, ncol = ripetizioni)

for (i in 1:ripetizioni) {
  passi <- runif(numero_passi, min = -1, max = 1)
  x[-1, i] <- cumsum(passi)
}

# Grafico delle passeggiate casuali
df <- data.frame(
  Passo = rep(0:numero_passi, times = ripetizioni), 
  Distanza = as.vector(x)
)

ggplot(
  df, 
  aes(
    x = Passo, 
    y = Distanza, 
    group = rep(1:ripetizioni, each = numero_passi + 1))
  ) +
  geom_line(color = "blue", alpha = 0.05) +
  labs(
    title = "Passeggiate Casuali", 
    x = "Numero di Passi", y = "Distanza dall'Origine"
  )
```

## Proprietà della distribuzione normale

### Media e varianza

- **Media**: $\mathbb{E}(Y) = \mu$  
- **Varianza**: $\mathbb{V}(Y) = \sigma^2$  

### Regola empirica

Nella normale, una proporzione nota dei dati cade entro 1, 2 o 3 deviazioni standard dalla media:

- **68.3%** tra $[\mu - \sigma, \mu + \sigma]$  
- **95.6%** tra $[\mu - 2\sigma, \mu + 2\sigma]$  
- **99.7%** tra $[\mu - 3\sigma, \mu + 3\sigma]$  

## Distribuzione normale standard

La **normale standard** è un caso speciale della normale con $\mu = 0$ e $\sigma = 1$. Ogni normale può essere trasformata in una normale standard attraverso la formula:

$$
Z = \frac{Y - \mu}{\sigma}.
$$

## Funzioni principali in R

R offre funzioni integrate per la distribuzione normale:

1. **`dnorm()`**: calcola la densità di probabilità.  
2. **`pnorm()`**: calcola la funzione cumulativa (CDF).  
3. **`qnorm()`**: restituisce i quantili della distribuzione.  
4. **`rnorm()`**: genera valori casuali.

### Esempi

1. **Densità**:
   ```{r}
   dnorm(115, mean = 100, sd = 15)
   ```

2. **Probabilità cumulativa**:
   ```{r}
   pnorm(115, mean = 100, sd = 15)
   ```

3. **Quantili**:
   ```{r}
   qnorm(0.975, mean = 100, sd = 15)
   ```

4. **Valori casuali**:
   ```{r}
   set.seed(123)
   rnorm(10, mean = 100, sd = 15)
   ```

## Visualizzazione

Esempio di confronto tra PDF, CDF e quantili:

```{r}
# Importare le librerie necessarie
library(gridExtra)

# Definire i parametri
mu <- 100
sigma <- 15

# Generare l'intervallo di valori per x
x <- seq(mu - 3 * sigma, mu + 3 * sigma, length.out = 1000)

# PDF
pdf_plot <- ggplot(
  data.frame(x = x, pdf = dnorm(x, mean = mu, sd = sigma)), 
  aes(x = x, y = pdf)
  ) +
  geom_line(color = "blue") +
  labs(title = "PDF", x = "Valori", y = "Densità") 

# CDF
cdf_plot <- ggplot(
  data.frame(x = x, cdf = pnorm(x, mean = mu, sd = sigma)), 
  aes(x = x, y = cdf)
  ) +
  geom_line(color = "orange") +
  labs(title = "CDF", x = "Valori", y = "Cumulativa")

# PPF
quantiles <- seq(0.01, 0.99, length.out = 100)
ppf_plot <- ggplot(
  data.frame(
    Quantile = quantiles, 
    Valori = qnorm(quantiles, mean = mu, sd = sigma)
  ), 
  aes(x = Quantile, y = Valori)
  ) +
  geom_line(color = "green") +
  labs(title = "PPF", x = "Quantile", y = "Valori") 

# Mostrare i grafici
grid.arrange(pdf_plot, cdf_plot, ppf_plot, ncol = 3)

```

La distribuzione normale è alla base di numerosi metodi statistici ed è fondamentale per modellare dati reali e comprendere fenomeni naturali.

## Distribuzione Chi-Quadrato

La distribuzione **$\chi^2$** deriva dalla distribuzione normale e descrive la somma dei quadrati di $k$ variabili casuali indipendenti e identicamente distribuite (i.i.d.) che seguono la distribuzione normale standard $\mathcal{N}(0, 1)$. Una variabile casuale $\chi^2_{~k}$ con $k$ gradi di libertà è definita come:

$$
Z_1^2 + Z_2^2 + \dots + Z_k^2,
$$

dove $Z_1, Z_2, \dots, Z_k \sim \mathcal{N}(0, 1)$. Il parametro $k$, detto **gradi di libertà** ($\nu$), determina la forma della distribuzione.

### Funzione di densità

La densità di probabilità della distribuzione $\chi^2_{~\nu}$ è data da:

$$
f(x) = C_{\nu} x^{\nu/2 - 1} \exp(-x/2), \quad \text{per } x > 0,
$$

dove $C_{\nu}$ è una costante di normalizzazione. 

### Simulazione della Distribuzione Chi-Quadrato

Utilizziamo la definizione per simulare la distribuzione $\chi^2$ con 3 gradi di libertà.

```{r}
# Impostare il seed per la riproducibilità
set.seed(1234)

# Generare 1000 valori casuali per 3 variabili gaussiane standard
n <- 1000
var1 <- rnorm(n, mean = 0, sd = 1)
var2 <- rnorm(n, mean = 0, sd = 1)
var3 <- rnorm(n, mean = 0, sd = 1)

# Calcolare la somma dei quadrati
chi_sq_values <- var1^2 + var2^2 + var3^2

# Creare un dataframe per il grafico
data <- data.frame(chi_sq_values = chi_sq_values)

# Istogramma e densità teorica
ggplot(data, aes(x = chi_sq_values)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 30, fill = "lightblue", color = "black", alpha = 0.7
    ) +
  stat_function(fun = dchisq, args = list(df = 3), color = "red", size = 1) +
  labs(
    title = "Distribuzione Chi-Quadrato (df = 3)",
    x = "Valore",
    y = "Densità"
  )
```

**Descrizione:**

- L'istogramma rappresenta i valori empirici simulati.
- La curva rossa rappresenta la densità teorica della distribuzione $\chi^2_{~3}$.

### Media e Varianza Empiriche

Calcoliamo la media e la varianza dei valori simulati:

```{r}
# Media empirica
mean(chi_sq_values)

# Varianza empirica
var(chi_sq_values)
```

Questi valori possono essere confrontati con le proprietà teoriche della distribuzione $\chi^2$:

- **Media**: $\nu = 3$.
- **Varianza**: $2\nu = 6$.

### Grafico per Diversi Gradi di Libertà

Confrontiamo le distribuzioni $\chi^2$ per diversi valori di $\nu$.

```{r}
# Intervallo di x
x <- seq(0, 40, by = 0.1)

# Gradi di libertà
nus <- c(2, 4, 8, 16)

# Creare un dataframe
data <- do.call(rbind, lapply(nus, function(nu) {
  data.frame(x = x, f_x = dchisq(x, df = nu), nu = as.factor(nu))
}))

# Grafico
ggplot(data, aes(x = x, y = f_x, color = nu)) +
  geom_line(size = 1) +
  labs(
    title = "Distribuzioni Chi-Quadrato per Diversi Valori di \u03bd",
    x = "x",
    y = "f(x)",
    color = expression(nu)
  ) 
```

### Proprietà della Distribuzione Chi-Quadrato

1. **Asimmetria**: La distribuzione $\chi^2_{\nu}$ è asimmetrica, ma diventa più simmetrica al crescere di $\nu$.
2. **Media**: $\mathbb{E}[\chi^2_{\nu}] = \nu$.
3. **Varianza**: $\mathbb{V}[\chi^2_{\nu}] = 2\nu$.
4. **Convergenza**: Per $\nu \to \infty$, $\chi^2_{\nu} \to \mathcal{N}(\nu, 2\nu)$.
5. **Somma**: La somma di variabili $\chi^2$ indipendenti con gradi di libertà $\nu_1, \nu_2, \dots, \nu_k$ segue una distribuzione $\chi^2$ con $\nu = \sum_{i=1}^k \nu_i$.

### Applicazioni

La distribuzione $\chi^2$ è utilizzata in molteplici ambiti statistici, tra cui:

- **Test di indipendenza**: Per verificare se due variabili categoriche sono indipendenti.
- **Test di adattamento**: Per confrontare una distribuzione empirica con una teorica.
- **ANOVA**: Per valutare la variabilità nei dati.

Questa distribuzione è particolarmente importante in contesti di inferenza statistica e si trova alla base di numerosi test parametrici.

## Distribuzione $t$ di Student

La **distribuzione $t$ di Student** è una delle distribuzioni fondamentali della statistica inferenziale. Deriva dalle distribuzioni Normale e Chi-quadrato ed è particolarmente utile per analizzare campioni di piccole dimensioni o situazioni in cui la varianza della popolazione è sconosciuta.

### Definizione Formale

Se:

- $Z \sim \mathcal{N}(0, 1)$ (distribuzione Normale standard),
- $W \sim \chi^2_{\nu}$ (distribuzione Chi-quadrato con $\nu$ gradi di libertà),

e $Z$ e $W$ sono indipendenti, allora la variabile casuale

$$
T = \frac{Z}{\sqrt{\frac{W}{\nu}}}
$$

segue una **distribuzione $t$ di Student** con $\nu$ gradi di libertà. Si indica come $T \sim t_{\nu}$.

### Proprietà della Distribuzione $t$ di Student

1. **Forma della distribuzione**:

   - La distribuzione $t$ è simmetrica rispetto a zero, come la Normale standard ($\mathcal{N}(0, 1)$).
   - Presenta **code più pesanti** rispetto alla Normale, riflettendo una maggiore probabilità di osservare valori estremi.

2. **Code pesanti e gradi di libertà**:

   - La pesantezza delle code diminuisce con l'aumentare dei gradi di libertà ($\nu$).
   - Per $\nu \to \infty$, la distribuzione $t$ converge alla distribuzione Normale standard.

3. **Media e varianza**:

   - La **media** è $0$ per $\nu > 1$.
   - La **varianza** è:
   
     $$
     \text{Var}(T) = \frac{\nu}{\nu - 2}, \quad \text{per } \nu > 2.
     $$
     
     Per $\nu \leq 2$, la varianza non è definita.

4. **Applicazioni principali**:

   - **Test t di Student**: Confronto delle medie di due gruppi o test per una singola media.
   - **Intervalli di confidenza**: Stima dell'intervallo per la media quando la varianza è sconosciuta.

### Differenze tra la Distribuzione $t$ e la Normale

| **Caratteristica**            | **Distribuzione Normale**     | **Distribuzione $t$ di Student**  |
|-------------------------------|-------------------------------|--------------------------------------|
| Forma                         | Simmetrica, a campana         | Simmetrica, a campana               |
| Code                          | Sottili                       | Pesanti                             |
| Dipendenza dai gradi di libertà | No                            | Sì                                  |
| Convergenza                   | Non varia                     | Con $\nu \to \infty$, converge alla Normale |

### Visualizzazione della Distribuzione $t$

Confrontiamo graficamente la distribuzione $t$ con diversi gradi di libertà e la distribuzione Normale standard:

```{r}
# Creazione dei dati
x <- seq(-4, 4, length.out = 1000)
df <- c(1, 2, 5, 10)  # Gradi di libertà

# Dataframe con curve di densità
data <- data.frame(
  x = rep(x, length(df) + 1),
  density = c(
    dnorm(x),
    dt(x, df[1]),
    dt(x, df[2]),
    dt(x, df[3]),
    dt(x, df[4])
  ),
  distribution = rep(c("Normale", paste("t (df =", df, ")")), each = length(x))
)

# Plot
ggplot(data, aes(x = x, y = density, color = distribution)) +
  geom_line(size = 1) +
  labs(
    title = "Distribuzione Normale e distribuzioni $t$ di Student",
    x = "Valore",
    y = "Densità",
    color = "Distribuzione"
  ) 
```

### Simulazione della Distribuzione $t$

Simuliamo una distribuzione $t$ con 10 gradi di libertà e confrontiamola con la densità teorica.

```{r}
# Impostare il seed per la riproducibilità
set.seed(123)

# Simulare 1000 valori da una distribuzione t
n <- 1000
df <- 10  # Gradi di libertà
t_values <- rt(n, df = df)

# Creare un dataframe per il grafico
data <- data.frame(t_values = t_values)

# Istogramma con densità teorica
ggplot(data, aes(x = t_values)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    bins = 30, fill = "lightblue", color = "black", alpha = 0.7
  ) +
  stat_function(fun = dt, args = list(df = df), color = "red", size = 1) +
  labs(
    title = paste("Distribuzione $t$ di Student (df =", df, ")"),
    x = "Valore",
    y = "Densità"
  ) 
```

### Proprietà Teoriche della Distribuzione $t$

1. **Media**:
   $$
   \mathbb{E}[T] = 0, \quad \text{per } \nu > 1.
   $$

2. **Varianza**:
   $$
   \mathbb{V}[T] = \frac{\nu}{\nu - 2}, \quad \text{per } \nu > 2.
   $$

3. **Simmetria**:
   - La distribuzione è simmetrica rispetto a zero, come la Normale.

4. **Code**:
   - Le code sono più pesanti rispetto alla Normale, riflettendo una maggiore incertezza per piccoli campioni.

In conclusione, la distribuzione $t$ di Student è uno strumento versatile nell'inferenza statistica, trovando applicazione in contesti sia frequentisti che bayesiani. È particolarmente utile in situazioni in cui la conoscenza della varianza è limitata o i campioni sono di dimensioni ridotte. Grazie alla sua forma simmetrica e alle code più pesanti rispetto alla distribuzione Normale, la distribuzione $t$ può modellare meglio l'incertezza, includendo una maggiore probabilità per valori estremi.

Nel contesto bayesiano, la distribuzione $t$ viene utilizzata come:

- **Prior informativo robusto**, per modellare parametri con valori plausibili lontani dalla media ma con una penalizzazione graduale per valori estremi.
- **Distribuzione predittiva** per sintetizzare l'incertezza derivante da campioni piccoli o con variabilità elevata.

In entrambi i paradigmi, la distribuzione $t$ rappresenta una scelta robusta, capace di riflettere in modo flessibile la natura dei dati. Inoltre, per valori elevati dei gradi di libertà, la distribuzione $t$ converge alla distribuzione Normale, un caso limite che ne estende ulteriormente l’utilità in vari contesti analitici.

## Funzione Beta di Eulero

La **funzione Beta di Eulero** è una funzione matematica, non una densità di probabilità, ma è strettamente collegata alla distribuzione Beta, poiché appare nella sua definizione. Indicata comunemente con il simbolo $\mathcal{B}(\alpha, \beta)$, la funzione Beta può essere espressa in vari modi. Per i nostri scopi, utilizziamo la seguente definizione:

$$
\mathcal{B}(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\,,
$$

dove $\Gamma(x)$ rappresenta la funzione Gamma, una generalizzazione del fattoriale definita per numeri reali positivi. Quando $x$ è un numero intero, la funzione Gamma si riduce al fattoriale traslato:

$$
\Gamma(x) = (x-1)!.
$$

### Esempio di Calcolo

Supponiamo di voler calcolare $\mathcal{B}(3, 9)$. Utilizzando la definizione, abbiamo:

$$
\mathcal{B}(3, 9) = \frac{\Gamma(3) \cdot \Gamma(9)}{\Gamma(3 + 9)}.
$$

In R, possiamo calcolarla in tre modi diversi.

#### Utilizzando la definizione con la funzione `gamma()`:

```{r}
alpha <- 3
beta <- 9

beta_function <- gamma(alpha) * gamma(beta) / gamma(alpha + beta)
beta_function
```

#### Utilizzando direttamente la funzione `beta()` di R:

```{r}
beta(alpha, beta)
```

#### Calcolo manuale con fattoriali:

```{r}
(factorial(alpha - 1) * factorial(beta - 1)) / factorial(alpha + beta - 1)
```

Tutti e tre i metodi restituiscono lo stesso risultato, confermando la correttezza della definizione.

### Rilevanza della Funzione Beta

La funzione Beta è centrale in probabilità e statistica, in particolare nella definizione della **densità di probabilità Beta**. Essa serve a normalizzare la densità, garantendo che l'area sotto la curva sia pari a $1$. La connessione con la funzione Gamma ne amplia ulteriormente le applicazioni, rendendola uno strumento essenziale per descrivere relazioni fra distribuzioni e modelli.

## Distribuzione Beta

La **distribuzione Beta**, indicata come $\mathcal{Beta}(\alpha, \beta)$, è una distribuzione di probabilità continua definita sull'intervallo $(0, 1)$. È particolarmente utile per modellare proporzioni, probabilità o qualsiasi fenomeno limitato tra 0 e 1. La sua flessibilità la rende adatta a molte applicazioni, sia in ambito frequentista che bayesiano.

### Definizione Formale

Se una variabile casuale $\theta$ segue una distribuzione Beta con parametri $\alpha > 0$ e $\beta > 0$, indicata come $\theta \sim \mathcal{Beta}(\alpha, \beta)$, la sua funzione di densità è data da:

$$
\mathcal{Beta}(\theta \mid \alpha, \beta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}, \quad \theta \in (0, 1),
$$

dove:

- $B(\alpha, \beta)$ è la **funzione Beta di Eulero**, definita come:

$$
B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)},
$$

- $\Gamma(x)$ è la funzione Gamma.

Un'altra rappresentazione equivalente è:

$$
\mathcal{Beta}(\theta \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}.
$$

### Ruolo dei Parametri $\alpha$ e $\beta$

I parametri $\alpha$ e $\beta$ determinano la forma della distribuzione:

- **$\alpha > 1$**: favorisce valori di $\theta$ vicini a 1.
- **$\beta > 1$**: favorisce valori di $\theta$ vicini a 0.
- **$\alpha = \beta = 1$**: corrisponde alla distribuzione uniforme sull'intervallo $[0, 1]$.
- **$\alpha, \beta < 1$**: la distribuzione è bimodale, concentrandosi agli estremi (vicino a 0 e 1).

### Proprietà della Distribuzione Beta

1. **Valore atteso**:
   $$
   \mathbb{E}(\theta) = \frac{\alpha}{\alpha + \beta}.
   $$

2. **Varianza**:
   $$
   \mathbb{V}(\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
   $$

3. **Moda** (se $\alpha, \beta > 1$):
   $$
   \text{Moda}(\theta) = \frac{\alpha - 1}{\alpha + \beta - 2}.
   $$

Queste proprietà evidenziano come $\alpha$ e $\beta$ possano essere interpretati come "successi" e "fallimenti" in una serie di prove, fornendo un collegamento intuitivo con la distribuzione binomiale.

### Relazione con la Distribuzione Binomiale

La distribuzione Beta può essere vista come una generalizzazione continua della distribuzione binomiale. In particolare, mentre la distribuzione binomiale modella il **numero di successi** in una serie di prove, la distribuzione Beta modella la **probabilità di successo** come una variabile casuale.

Nell'inferenza bayesiana, la distribuzione Beta è spesso utilizzata come **prior coniugato** per la distribuzione binomiale. Ad esempio:

- Se $\theta \sim \mathcal{Beta}(\alpha, \beta)$ è il prior,
- E osserviamo $x$ successi in $n$ prove, allora la distribuzione a posteriori è:

$$
\theta \mid \text{dati} \sim \mathcal{Beta}(\alpha + x, \beta + n - x).
$$

Questa proprietà consente un aggiornamento semplice delle credenze basato sui dati osservati.

### Visualizzazione della Distribuzione Beta

Di seguito mostriamo come la forma della distribuzione Beta varia al variare dei parametri $\alpha$ e $\beta$:

```{r}
# Parametri
x <- seq(0, 1, length.out = 200)
alphas <- c(0.5, 5.0, 1.0, 2.0, 2.0)
betas <- c(0.5, 1.0, 3.0, 2.0, 5.0)

# Creare un dataframe
df <- do.call(rbind, lapply(1:length(alphas), function(i) {
  data.frame(
    x = x,
    density = dbeta(x, alphas[i], betas[i]),
    label = paste0("α = ", alphas[i], ", β = ", betas[i])
  )
}))

# Plot
ggplot(df, aes(x = x, y = density, color = label)) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)",
    title = "Distribuzioni Beta"
  ) +
  theme(legend.title = element_blank(), legend.position = "top")
```

### Costante di Normalizzazione

La costante di normalizzazione della distribuzione Beta è il reciproco della funzione Beta di Eulero, $B(\alpha, \beta)$. Questa garantisce che:

$$
\int_0^1 \mathcal{Beta}(\theta \mid \alpha, \beta) \, d\theta = 1.
$$

Ad esempio, calcoliamo $B(3, 9)$ in R utilizzando tre approcci:

**Usando la funzione Gamma:**

```{r}
alpha <- 3
beta <- 9
beta_function <- gamma(alpha) * gamma(beta) / gamma(alpha + beta)
beta_function
```

**Usando la funzione `beta()`:**

```{r}
beta(alpha, beta)
```

**Calcolo manuale:**

```{r}
factorial(alpha - 1) * factorial(beta - 1) / factorial(alpha + beta - 1)
```


### Applicazioni della Distribuzione Beta

1. **Modellazione di proporzioni**:

   - Percentuali di successo in esperimenti.
   - Percentuali di risposte in sondaggi.

2. **Inferenza Bayesiana**:

   - La Beta è prior coniugato per modelli basati su prove di Bernoulli o distribuzioni binomiali.

3. **Stime robuste**:

   - La flessibilità della Beta permette di rappresentare credenze iniziali con diverse incertezze.

In conclusione, la distribuzione Beta è un potente strumento matematico e statistico. La sua flessibilità nel modellare proporzioni e probabilità, insieme alla semplicità di aggiornamento nell'inferenza bayesiana, la rende essenziale per molteplici applicazioni, dall'analisi dei dati al machine learning. La sua relazione con la funzione Beta di Eulero e con la distribuzione binomiale la collega a fondamenta matematiche solide e ampiamente utilizzate.


## Distribuzione di Cauchy

La **distribuzione di Cauchy** è un caso speciale della distribuzione $t$ di Student con un solo grado di libertà ($t_1$). Questa distribuzione è caratterizzata da code molto pesanti e da una media e varianza non definite, rendendola particolarmente utile in contesti dove valori estremi possono avere un'influenza importante.

### Funzione di Densità

La funzione di densità di probabilità della distribuzione di Cauchy è definita da due parametri:

- **$\alpha$**: posizione (location), che determina il centro della distribuzione.
- **$\beta > 0$**: scala (scale), che controlla la larghezza della distribuzione.

La densità è data da:

$$
f(x \mid \alpha, \beta) = \frac{1}{\pi \beta \left[1 + \left( \frac{x - \alpha}{\beta} \right)^2\right]} ,
$$

dove:

- $x \in \mathbb{R}$,
- $\alpha \in \mathbb{R}$,
- $\beta > 0$.

Questa funzione descrive una distribuzione simmetrica attorno a $\alpha$, con code più pesanti rispetto alla distribuzione Normale.

### Proprietà della Distribuzione di Cauchy

1. **Simmetria**: La distribuzione è simmetrica rispetto a $\alpha$.
2. **Code Pesanti**: Le code sono significativamente più pesanti rispetto alla distribuzione Normale, con una decrescita più lenta ($\propto x^{-2}$).
3. **Media e Varianza**: La distribuzione non ha una media né una varianza definita.
4. **Relazione con $t_1$**: La distribuzione di Cauchy è equivalente a una distribuzione $t$ di Student con 1 grado di libertà.
5. **Caratteristiche Estreme**: I valori estremi hanno una probabilità più alta rispetto ad altre distribuzioni comuni, rendendola utile per modellare fenomeni con outlier significativi.

### Visualizzazione della Distribuzione di Cauchy

Per comprendere l'effetto dei parametri $\alpha$ e $\beta$ sulla forma della distribuzione, consideriamo alcuni esempi con:

- $\alpha = 0.0, 0.0, 0.0, -2.0$,
- $\beta = 0.5, 1.0, 2.0, 1.0$.

```{r}
# Definire i parametri
x <- seq(-5, 5, length.out = 500)
alphas <- c(0.0, 0.0, 0.0, -2.0)
betas <- c(0.5, 1.0, 2.0, 1.0)

# Creare un data frame per i risultati
df <- do.call(rbind, lapply(1:length(alphas), function(i) {
  data.frame(
    x = x,
    density = dcauchy(x, location = alphas[i], scale = betas[i]),
    label = paste0("α = ", alphas[i], ", β = ", betas[i])
  )
}))

# Grafico
ggplot(df, aes(x = x, y = density, color = label)) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)",
    title = "Distribuzioni di Cauchy con diversi parametri"
  ) +
  theme(
    legend.title = element_blank(),
    legend.position = "top"
  )
```

### Applicazioni della Distribuzione di Cauchy

1. **Inferenza Bayesiana**:

   - Utilizzata come prior **robusto** in modelli bayesiani, particolarmente quando si vuole attribuire una probabilità maggiore a valori estremi rispetto a una distribuzione Normale.

2. **Modellazione di Fenomeni con Outlier**:

   - La distribuzione di Cauchy è adatta per descrivere dati con valori estremi significativi che possono influenzare fortemente altre distribuzioni.

3. **Analisi Teorica**:

   - Essendo una distribuzione con varianza non definita, viene spesso usata per dimostrare i limiti delle inferenze basate su media e varianza.

In conclusione, la distribuzione di Cauchy, con le sue proprietà uniche come code pesanti e l'assenza di media e varianza definite, è uno strumento fondamentale per modellare fenomeni in cui i valori estremi giocano un ruolo importante. La sua relazione con la distribuzione $t$ di Student e la sua utilità nei modelli bayesiani ne ampliano ulteriormente le applicazioni in contesti statistici e probabilistici avanzati.

## Distribuzione Gamma

La **distribuzione Gamma** è una distribuzione di probabilità continua utilizzata principalmente per modellare variabili strettamente positive, come tassi, varianze, o tempi di attesa. È fondamentale nella statistica bayesiana come distribuzione a priori per parametri positivi e trova applicazione in ambiti come la modellazione di eventi rari o processi stocastici.

### Definizione Generale

La distribuzione Gamma è caratterizzata da due parametri principali:

- **Parametro di forma** ($\alpha$): determina la forma generale della distribuzione.
- **Parametro di scala** ($\theta$) o, alternativamente, il **parametro di tasso** ($\beta = 1/\theta$): regola la larghezza o la dispersione della distribuzione.

La funzione di densità di probabilità (PDF) è data da:

$$
f(x \mid \alpha, \theta) = \frac{x^{\alpha-1} e^{-x/\theta}}{\theta^\alpha \Gamma(\alpha)}, \quad x > 0,
$$

dove:

- $x$ è la variabile casuale continua,
- $\Gamma(\alpha)$ è la funzione Gamma di Eulero, definita come:

$$
\Gamma(\alpha) = \int_0^\infty t^{\alpha-1} e^{-t} dt.
$$

Se utilizziamo il parametro di tasso $\beta = 1/\theta$, la PDF può essere scritta come:

$$
f(x \mid \alpha, \beta) = \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}, \quad x > 0.
$$

### Relazioni con Altre Distribuzioni

1. **Distribuzione Esponenziale**:  
   La distribuzione Gamma generalizza la distribuzione esponenziale. Quando $\alpha = 1$, la distribuzione Gamma coincide con una distribuzione esponenziale con parametro di tasso $\beta$.

   $$
   \text{Gamma}(1, \beta) = \text{Esponenziale}(\beta).
   $$

2. **Teorema del Limite Centrale**:  
   Quando il parametro di forma $\alpha$ è grande ($\alpha \to \infty$), la distribuzione Gamma può essere approssimata da una distribuzione Normale con:
   - Media $\mu = \alpha \cdot \theta$,
   - Varianza $\sigma^2 = \alpha \cdot \theta^2$.

3. **Somma di Variabili Esponenziali**:  
   La distribuzione Gamma può essere vista come la somma di $\alpha$ variabili casuali indipendenti, ciascuna con una distribuzione esponenziale con lo stesso parametro di tasso $\beta$:

   $$
   \text{Gamma}(\alpha, \beta) = \sum_{i=1}^\alpha \text{Esponenziale}(\beta).
   $$

### Proprietà della Distribuzione Gamma

1. **Media**:
   $$
   \mathbb{E}[X] = \alpha \cdot \theta = \frac{\alpha}{\beta}.
   $$

2. **Varianza**:
   $$
   \text{Var}(X) = \alpha \cdot \theta^2 = \frac{\alpha}{\beta^2}.
   $$

3. **Moda** (per $\alpha > 1$):
   $$
   \text{Moda}(X) = (\alpha - 1) \cdot \theta.
   $$

### Parametri e Forma della Distribuzione

- **Parametro di Forma ($\alpha$)**:  
   Controlla la distribuzione:
   - $\alpha < 1$: la distribuzione ha una coda lunga e si inclina verso valori bassi.
   - $\alpha = 1$: la distribuzione è esponenziale.
   - $\alpha > 1$: la distribuzione ha una modalità (picco) in $(\alpha - 1) \cdot \theta$.

- **Parametro di Scala ($\theta$) o Tasso ($\beta$)**:  
   - Un $\theta$ grande ($\beta$ piccolo) indica maggiore variabilità e una distribuzione più ampia.
   - Un $\theta$ piccolo ($\beta$ grande) indica minore variabilità e una distribuzione più stretta.


### Visualizzazione della Distribuzione Gamma

Di seguito, mostriamo un esempio per $\alpha = 3$ e $\beta = 5/3$, calcolando e rappresentando graficamente la distribuzione.

#### Codice R

1. **Calcolo della Media e della Deviazione Standard**:
   ```{r}
   # Parametri
   alpha <- 3
   beta <- 5 / 3

   # Calcolo
   mean <- alpha / beta
   sigma <- sqrt(alpha / beta^2)

   cat("Media:", mean, "\n")
   cat("Deviazione Standard:", sigma, "\n")
   ```

2. **Generazione e Plot dei Dati**:
   ```{r}
   # Generazione di dati
   set.seed(123)
   data <- rgamma(100000, shape = alpha, rate = beta)

   # Data frame per ggplot
   df <- data.frame(values = data)

   # Plot
   ggplot(df, aes(x = values)) +
     geom_histogram(aes(y = ..density..), bins = 30, fill = "green", alpha = 0.6) +
     stat_function(fun = function(x) dgamma(x, shape = alpha, rate = beta),
                   color = "red", size = 1) +
     labs(
       x = "Valore",
       y = "Densità di probabilità",
       title = "Distribuzione Gamma con α=3 e β=5/3"
     ) 
   ```

### Applicazioni della Distribuzione Gamma

1. **Modellazione del Tempo di Attesa**:  
   La distribuzione Gamma è ideale per modellare tempi di attesa, ad esempio, il tempo necessario affinché si verifichino $n$ eventi in un processo di Poisson.

2. **Inferenza Bayesiana**:  
   - Utilizzata come prior per parametri positivi, come tassi ($\lambda$) o varianze ($\sigma^2$).
   - Ad esempio, nella modellazione bayesiana dei processi di Poisson, una distribuzione Gamma è una scelta naturale per il prior su $\lambda$.

3. **Biologia**:  
   - In biologia, trova applicazione nella modellazione di tassi di mutazione o sopravvivenza.

In conclusione, la distribuzione Gamma è una distribuzione versatile, adatta per modellare una varietà di fenomeni strettamente positivi. Grazie alla sua flessibilità e alla relazione con altre distribuzioni, come l'esponenziale e la normale, rappresenta uno strumento essenziale sia nella statistica frequentista che in quella bayesiana. La parametrizzazione tramite $\alpha$ e $\theta$ consente un controllo preciso sulla forma e la dispersione, rendendola applicabile a molteplici ambiti, dalla scienza ai modelli finanziari.

## Riflessioni conclusive

Le distribuzioni di probabilità costituiscono il cuore dell'inferenza statistica, sia bayesiana che frequentista. In questo capitolo, abbiamo esplorato come R offre un insieme completo di strumenti per lavorare con diverse distribuzioni, permettendo di modellare e analizzare una vasta gamma di fenomeni.

### Principali Applicazioni

1. **Inferenza Bayesiana**:  
   Le distribuzioni di probabilità, come la Beta, la Gamma e la Normale, sono essenziali per definire priors, calcolare posteriori e quantificare l'incertezza nei modelli bayesiani. Ad esempio:
   - La distribuzione Beta è ideale per modellare credenze a priori su proporzioni o probabilità.
   - La distribuzione Gamma è ampiamente usata per modellare parametri positivi come tassi o varianze.

2. **Analisi Statistica e Modellazione**:  
   Le distribuzioni, come la \(t\) di Student, sono fondamentali per il confronto tra campioni, mentre la Normale è indispensabile per modellare fenomeni che seguono la legge del limite centrale.

3. **Generazione e Simulazione di Dati**:  
   R permette di generare campioni casuali da distribuzioni comuni, utili per simulazioni, bootstrap e validazione di modelli.

### Funzionalità di R

Con poche funzioni, R consente di:

- **Generare campioni casuali**: con funzioni come `rnorm`, `rgamma`, `rbeta`, possiamo simulare dati da distribuzioni specifiche.
- **Calcolare densità**: ad esempio, con `dnorm`, `dgamma`, `dbeta`, possiamo visualizzare le funzioni di densità.
- **Calcolare probabilità cumulate**: con funzioni come `pnorm`, `pbeta`, possiamo determinare probabilità su intervalli specifici.
- **Determinare quantili**: con funzioni come `qnorm`, `qgamma`, possiamo calcolare i punti corrispondenti a specifici livelli di probabilità.

### Versatilità delle Distribuzioni

Le distribuzioni esplorate non solo descrivono fenomeni naturali, ma sono anche i "mattoncini" per costruire modelli statistici complessi. Le loro proprietà, come la media, la varianza, la simmetria o le code pesanti, consentono di adattare il modello al fenomeno studiato.

In conclusione, il linguaggio R, con la sua flessibilità e ricchezza di strumenti, permette di padroneggiare le distribuzioni di probabilità, non solo come oggetti matematici, ma anche come strumenti pratici per rispondere a domande complesse. La comprensione e l'uso delle distribuzioni presentate costituiscono le fondamenta per avanzare verso tecniche più sofisticate, come l'inferenza bayesiana avanzata o la modellazione gerarchica. 


## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

