# Controlli predittivi bayesiani (a priori e a posteriori) con `cmdstanr` {#sec-mcmc-prediction}



## Introduzione {.unnumbered .unlisted}


Quando costruiamo un modello, possiamo chiederci due cose diverse: 

1. *che cosa sappiamo dei parametri* dati i dati osservati (la *posterior*), e  
2. *che cosa ci aspettiamo di osservare* in nuove rilevazioni (la *distribuzione predittiva a posteriori*).

Nel lavoro psicologico applicato, la seconda domanda è spesso quella decisiva: vogliamo usare il modello per *prevedere* come si comporteranno nuovi dati e, soprattutto, per capire se il modello *riproduce in modo plausibile* le caratteristiche dei dati reali. Da qui nascono due strumenti pratici:

- *Controlli predittivi a priori* (*prior predictive checks*): generiamo dati sintetici *prima* di vedere i dati reali, solo dalle ipotesi iniziali (prior). Se questi scenari “immaginati” risultano totalmente irrealistici, il problema è nelle nostre assunzioni di partenza.  
- *Controlli predittivi a posteriori* (*posterior predictive checks*): generiamo dati sintetici *dopo* aver visto i dati, cioè dalla distribuzione dei parametri aggiornata (posteriori). Questo ci dice se il modello, una volta “istruito” dai dati, è in grado di produrre esiti simili a quelli osservati.

In questo capitolo *mettiamo in pratica* queste idee: vedremo come simulare dati dal modello, visualizzarli e confrontarli con i dati reali per giudicare la coerenza del modello. L’obiettivo non è aggiungere nuova teoria, ma *imparare un metodo operativo* che userai spesso: generare, guardare, confrontare.

::: {.callout-note title="Idea in una riga"}
La distribuzione predittiva posteriore è una *media pesata* delle predizioni possibili, secondo quanto i parametri sono plausibili alla luce dei dati:

$$
p(\tilde y \mid y) \;=\; \int p(\tilde y \mid \theta)\, p(\theta \mid y)\, d\theta.
$$
In pratica: campioni $\theta$ verosimili → genera dati $\tilde{y}$ → confronta con i dati osservati.
:::

**Cosa faremo concretamente**

1. Genereremo dati *a priori* per verificare che le ipotesi iniziali non producano scenari assurdi.  
2. Genereremo dati *a posteriori* per valutare se il modello allenato “sa” riprodurre i dati.  
3. Useremo grafici semplici per un confronto visivo chiaro.

::: {.callout-caution collapse=true title="Preparazione del Notebook"}

```{r}
#| output: false

here::here("code", "_common.R") |> source()

# Carichiamo i pacchetti necessari
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(cmdstanr, posterior, insight, bayesplot, ggplot2)
```
:::


## La distribuzione predittiva a posteriori 

Ricordiamo la definizione:

$$
p(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta ,
$$
che nei Posterior Predictive Checks (PPC) viene stimata simulando nuovi dati a partire dal posteriore dei parametri.

Perfetto, ti propongo una versione rielaborata e più didattica del testo, pensata per studenti di psicologia non esperti di statistica. Ho mantenuto il codice, ma ho reso più narrativo il commento e più chiaro il legame tra teoria, Stan e R.


## Esempio: il modello normale–normale

Per rendere concreti i concetti di predizione bayesiana, partiamo da un caso molto semplice ma istruttivo: il **modello normale–normale**.  

Immaginiamo di voler stimare l’altezza media di una popolazione. Scegliamo come modello:  

- un *prior* sulla media $\mu$, distribuito normalmente con parametri $\mu_0$ (media a priori) e $\tau_0^2$ (varianza a priori);  
- dei *dati osservati* $y_i$, anch’essi distribuiti normalmente attorno a $\mu$, con varianza fissata $\sigma^2$.  

Matematicamente:

$$
\mu \sim \mathcal{N}(\mu_0, \tau_0^2), \qquad
y_i \sim \mathcal{N}(\mu, \sigma^2).
$$

In questo contesto, la *distribuzione predittiva posteriore* per una nuova osservazione $\tilde{y}$ si scrive:

$$
\tilde{y} \mid Y \sim \mathcal{N}\!\big(\mu_n, \; \tau_n^2 + \sigma^2 \big),
$$
dove $\mu_n$ e $\tau_n^2$ sono la media e la varianza della distribuzione a posteriori di $\mu$.   

La formula in sé non è la parte più importante: ciò che ci interessa è *come realizzare la simulazione* in pratica, così da poter confrontare i dati osservati con i dati replicati dal modello.


### Implementazione con Stan

Creiamo un modello Stan (`normal_pred.stan`) che ci consente di generare:

- *prior predictive* (se `prior_only = 1`): simulazioni solo a partire dal prior, prima di vedere i dati;  
- *posterior predictive* (se `prior_only = 0`): simulazioni aggiornate dopo aver visto i dati.

```stan
data {
  int<lower=0> N;
  vector[N] y;
  real mu0;
  real<lower=0> tau0;
  real<lower=0> sigma;
  int<lower=0,upper=1> prior_only;
}
parameters {
  real mu;
}
model {
  mu ~ normal(mu0, tau0);          // prior
  if (prior_only == 0)
    y ~ normal(mu, sigma);         // likelihood (solo se usiamo i dati)
}
generated quantities {
  real y_rep;
  y_rep = normal_rng(mu, sigma);   // generiamo una nuova osservazione
}
```


### In R

Simuliamo 100 altezze con media 170 cm e deviazione standard 10 cm, poi invochiamo Stan per stimare il modello:

```{r}
#| output: false
N <- 100
y <- rnorm(N, mean = 170, sd = 10)

stan_data <- list(
  N = N, y = y,
  mu0 = 175, tau0 = 5, sigma = 10,
  prior_only = 0
)

mod <- cmdstan_model(here::here("stan", "normal_pred.stan"))
fit <- mod$sample(data = stan_data, chains = 4, iter_sampling = 1000, refresh = 0)
```


### Visualizzazione: Posterior Predictive Checks

Per i controlli predittivi, generiamo i dati replicati e li confrontiamo con i dati osservati:

```{r}
y_rep_vars <- paste0("y_rep[", 1:N, "]")
y_rep <- as_draws_matrix(fit$draws(y_rep_vars))

# Confronto distribuzioni
bayesplot::ppc_dens_overlay(y, y_rep[1:200, ])
```

Il grafico mostra la distribuzione reale delle altezze (`y`) e quelle simulate dal modello (`y_rep`). Se le due curve si sovrappongono in modo plausibile, significa che il modello è in grado di riprodurre le caratteristiche essenziali dei dati.

Altri strumenti utili per controlli più mirati:

```{r}
bayesplot::ppc_stat(y, y_rep, stat = "mean")        # confronto delle medie
```

```{r}
bayesplot::ppc_intervals(y, y_rep[1:100, ])        # confronto di intervalli
```

Questi controlli ci permettono di capire *quanto bene il modello “mima” la realtà*: se i dati simulati divergono fortemente da quelli osservati, vuol dire che le assunzioni di base (prior, varianza fissata, ecc.) non sono adatte e vanno riviste.


## Distribuzione predittiva a priori

Prima di raccogliere un solo dato, possiamo chiederci:  *“Se il mondo fosse davvero come lo descrive il nostro modello, che tipo di dati mi aspetterei di osservare?”*

Questa è la *predizione a priori* (*prior predictive*): generiamo dati simulati *solo dalle assunzioni iniziali* (i prior), senza alcuna informazione empirica. Lo scopo non è “indovinare” i dati reali, ma *verificare che i prior producano scenari plausibili* alla luce della conoscenza di dominio (ordini di grandezza, range, code).

### Perché è utile?

- Se le simulazioni risultano *implausibili* (es. altezze negative o centinaia di cm fuori scala), i prior sono *mal calibrati* (troppo stretti, troppo larghi, con media irrealistica).  
- Se le simulazioni sono compatibili con ciò che ci aspetteremmo in contesti reali, il modello *non parte*, è *già “rotto”* e possiamo procedere.

L’idea pratica è semplice: *campioniamo i parametri dal prior*, poi *generiamo dati fittizi* con il meccanismo del modello, senza “guardare” i dati osservati.


### Stan (prior predictive minimale e robusta)

Usiamo un file dedicato, ad es. `normal_prior_predictive.stan`. Qui non servono i blocchi `parameters` e `model`: in prior predictive *campioniamo i parametri direttamente in `generated quantities`* con le funzioni `_rng`, ed eseguiamo `fixed_param=TRUE` in R.

```stan
// file: normal_prior_predictive.stan
data {
  int<lower=0> N;          // dimensione della replica che vuoi generare
  vector[N] y;             // ignorato (può essere numeric(0) o un vettore qualsiasi)
  real mu0;                // media a priori di mu
  real<lower=0> tau0;      // dev. standard a priori di mu
  real<lower=0> sigma;     // dev. standard nota dei dati
}
generated quantities {
  real mu_prior;           // un draw del parametro dal prior
  vector[N] y_rep;         // una replica completa (lunghezza N)

  mu_prior = normal_rng(mu0, tau0);
  for (n in 1:N) {
    y_rep[n] = normal_rng(mu_prior, sigma);
  }
}
```

> Nota: se vuoi generare più repliche per draw, puoi aggiungere una dimensione `N_rep` e produrre una matrice `y_rep[N, N_rep]`. Per l’uso didattico, una replica per draw (ma tante iterazioni) è di solito sufficiente.


### Codice R (prior predictive)

Dati per la prior predictive** (qui scegliamo una replica di lunghezza `N = 200`):

```{r}
N <- 200
stan_data_prior <- list(
  N    = N,
  y    = rep(0, N),  # ignorato
  mu0  = 175,
  tau0 = 5,
  sigma= 10
)
```

Compilazione e campionamento (solo `generated quantities`):

```{r}
#| message: false
#| warning: false
mod_prior <- cmdstan_model(here::here("stan", "normal_prior_predictive.stan"))

fit_prior <- mod_prior$sample(
  data = stan_data_prior,
  chains = 4,
  iter_sampling = 1000,
  iter_warmup = 0,
  fixed_param = TRUE,   # fondamentale per prior predictive
  refresh = 0,
  seed = 123
)
```

Estrazione e check analitico (normale–normale):

```{r}
# y_rep: matrice (S x N) con S = numero totale di draws (tutte le chain combinate)
yrep_mat <- as_draws_matrix(fit_prior$draws("y_rep"))
df_mc <- data.frame(x = as.numeric(yrep_mat))  # vettore lungo S*N

# Deviazione standard predittiva a priori (analitica): sqrt(tau0^2 + sigma^2)
sd_prior_pred <- sqrt(5^2 + 10^2)   # = sqrt(125) ≈ 11.18034

# Code analitiche (confronto con Monte Carlo)
p_bassa <- pnorm(150, mean = 175, sd = sd_prior_pred)
p_alta  <- 1 - pnorm(200, mean = 175, sd = sd_prior_pred)
c(p_bassa = p_bassa, p_alta = p_alta)
```

Prior Predictive: Monte Carlo vs. analitica..

```{r}
ggplot(df_mc, aes(x = x)) +
  geom_density() +
  stat_function(fun = dnorm,
                args = list(mean = 175, sd = sd_prior_pred),
                linetype = 2) +
  labs(x = "y", y = "Densità")
```

::: {.callout-note title="Cosa stai verificando, in pratica?"}

* **Scala e posizione**: la media simulata è coerente con $\mu_0$?
* **Variabilità**: la dispersione simulata è coerente con $\sqrt{\tau_0^2 + \sigma^2}$?
* **Code e range**: la massa di probabilità in regioni “assurde” (es. <150 cm o >200 cm) è ragionevole?
:::


### Come interpretare

* Se le repliche *a priori* risultano sistematicamente *più grandi/piccole* di ciò che riterresti plausibile, il prior su $\mu$ è **mal calibrato** (es. $\mu_0$ troppo alto, $\tau_0$ troppo stretto/larghissimo).
* Se la *variabilità* è troppo diversa da quella che consideri realistica, rivedi $\tau_0$ (e, in modelli con $\sigma$ ignota, il prior su $\sigma$).
* L’obiettivo **non** è “forzare i dati nel prior”, ma **evitare prior implausibili** rispetto alla conoscenza di dominio e all’ordine di grandezza atteso.

::: {.callout-tip title="Regola pratica"}
Se la prior predictive **non passa il test del buon senso** (scala, range, code), *rivedi i prior* prima di stimare il modello.
:::

::: {.callout-caution title="Errori comuni"}

* **Usare `parameters` in prior predictive con `fixed_param=TRUE`**: i parametri non vengono campionati ⇒ valori costanti. In prior predictive *campiona i parametri in `generated quantities`* con le funzioni `_rng`.
* **Dipendere da `N` quando è 0**: se definisci `vector[N] y_rep` e passi `N=0`, l’oggetto è vuoto: va bene, ma ricorda che alcune pipeline di estrazione richiedono attenzione. In alternativa usa sempre `N>0` per le repliche a priori.
* **Confondere prior vs posterior predictive**: la prior predictive controlla le **assunzioni**; la posterior predictive controlla la **capacità del modello**, dopo aver “visto” i dati, di riprodurre le caratteristiche osservate.
:::


## Riflessioni conclusive {.unnumbered .unlisted}

Le due distribuzioni predittive rispondono a domande complementari ma distinte. La distribuzione predittiva a priori ci permette di valutare la plausibilità delle nostre assunzioni iniziali: dati i prior scelti, il modello è in grado di generare dati che abbiano un senso nel contesto del problema? Si tratta di un controllo fondamentale per assicurarci che le nostre ipotesi partano da basi ragionevoli.

La distribuzione predittiva a posteriori, invece, verifica la capacità del modello – dopo aver incorporato l’evidenza dei dati – di cogliere le caratteristiche salienti del fenomeno osservato. La domanda guida qui diventa: il modello riesce a riprodurre non solo la tendenza centrale, ma anche la variabilità e la struttura dei dati reali?

Da un punto di vista operativo, entrambe le verifiche seguono la stessa logica: si simulano campioni di dati sulla base di parametri estratti dalle distribuzioni (che siano prior o posteriori), per poi confrontarli visivamente e quantitativamente con i dati osservati.

Questi strumenti non costituiscono una validazione definitiva del modello, ma offrono un mezzo agile e intuitivo per individuare incoerenze macroscopiche: prior eccessivamente vincolanti, modelli che sottostimano la variabilità, o strutture probabilistiche inadeguate. Rappresentano perciò una tappa obbligata nel workflow bayesiano, da svolgere prima di affrontare confronti predittivi più formali, come quelli basati sull’ELPD o sulla cross-validazione LOO.


::: {.callout-note collapse=true title="Informazioni sull'ambiente di sviluppo"}

```{r}
sessionInfo()
```
:::

## Bibliografia {.unnumbered .unlisted}
