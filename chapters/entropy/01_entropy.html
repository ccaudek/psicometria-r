<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>72&nbsp; Entropia e informazione di Shannon – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/entropy/02_kl.html" rel="next">
<link href="../../chapters/linear_models/13_poisson_model.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-3e8fa383bad517095c2b42029d2b9125.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a12c8109e4cfb0c0b98cd3996d21797e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori (Prior Predictive Check)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">72.1</span> Introduzione</a></li>
  <li><a href="#che-cos%C3%A8-linformazione" id="toc-che-cosè-linformazione" class="nav-link" data-scroll-target="#che-cos%C3%A8-linformazione"><span class="header-section-number">72.2</span> Che cos’è l’informazione?</a></li>
  <li><a href="#la-sorpresa-e-linformazione-di-shannon" id="toc-la-sorpresa-e-linformazione-di-shannon" class="nav-link" data-scroll-target="#la-sorpresa-e-linformazione-di-shannon"><span class="header-section-number">72.3</span> La Sorpresa e l’Informazione di Shannon</a></li>
  <li><a href="#sorpresa-e-probabilit%C3%A0" id="toc-sorpresa-e-probabilità" class="nav-link" data-scroll-target="#sorpresa-e-probabilit%C3%A0"><span class="header-section-number">72.4</span> Sorpresa e Probabilità</a></li>
  <li><a href="#entropia-come-media-dellinformazione-di-shannon" id="toc-entropia-come-media-dellinformazione-di-shannon" class="nav-link" data-scroll-target="#entropia-come-media-dellinformazione-di-shannon"><span class="header-section-number">72.5</span> Entropia come Media dell’Informazione di Shannon</a></li>
  <li><a href="#interpretazione-dellentropia-1" id="toc-interpretazione-dellentropia-1" class="nav-link" data-scroll-target="#interpretazione-dellentropia-1"><span class="header-section-number">72.6</span> Interpretazione dell’entropia (1)</a></li>
  <li><a href="#interpretazione-dellentropia-2" id="toc-interpretazione-dellentropia-2" class="nav-link" data-scroll-target="#interpretazione-dellentropia-2"><span class="header-section-number">72.7</span> Interpretazione dell’Entropia (2)</a></li>
  <li><a href="#caratteristiche-dellentropia" id="toc-caratteristiche-dellentropia" class="nav-link" data-scroll-target="#caratteristiche-dellentropia"><span class="header-section-number">72.8</span> Caratteristiche dell’entropia</a></li>
  <li><a href="#stimare-lentropia-da-una-distribuzione-di-probabilit%C3%A0" id="toc-stimare-lentropia-da-una-distribuzione-di-probabilità" class="nav-link" data-scroll-target="#stimare-lentropia-da-una-distribuzione-di-probabilit%C3%A0"><span class="header-section-number">72.9</span> Stimare l’Entropia da una Distribuzione di Probabilità</a></li>
  <li><a href="#stimare-lentropia-in-un-campione-di-osservazioni" id="toc-stimare-lentropia-in-un-campione-di-osservazioni" class="nav-link" data-scroll-target="#stimare-lentropia-in-un-campione-di-osservazioni"><span class="header-section-number">72.10</span> Stimare l’Entropia in un Campione di Osservazioni</a></li>
  <li><a href="#entropia-di-una-variabile-casuale-continua" id="toc-entropia-di-una-variabile-casuale-continua" class="nav-link" data-scroll-target="#entropia-di-una-variabile-casuale-continua"><span class="header-section-number">72.11</span> Entropia di una Variabile Casuale Continua</a></li>
  <li><a href="#la-codifica-di-huffman" id="toc-la-codifica-di-huffman" class="nav-link" data-scroll-target="#la-codifica-di-huffman"><span class="header-section-number">72.12</span> La codifica di Huffman</a></li>
  <li><a href="#lentropia-come-lunghezza-media-del-codice-binario" id="toc-lentropia-come-lunghezza-media-del-codice-binario" class="nav-link" data-scroll-target="#lentropia-come-lunghezza-media-del-codice-binario"><span class="header-section-number">72.13</span> L’entropia come lunghezza media del codice binario</a></li>
  <li><a href="#applicazioni-psicologiche" id="toc-applicazioni-psicologiche" class="nav-link" data-scroll-target="#applicazioni-psicologiche"><span class="header-section-number">72.14</span> Applicazioni Psicologiche</a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">72.15</span> Riflessioni Conclusive</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/01_entropy.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-entropy-shannon-information" class="quarto-section-identifier"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi di apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<p>Al termine di questo capitolo, sarai in grado di:</p>
<ul>
<li>Comprendere e sapere calcolare l’informazione di Shannon.</li>
<li>Comprendere e sapere calcolare l’entropia per variabili casuali discrete.</li>
<li>Comprendere il concetto di entropia per variabili casuali continue.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Per i concetti di base sulla teoria dell’informazione, si rimanda ai primi due capitoli di <em>Information Theory: A Tutorial Introduction</em> <span class="citation" data-cites="stone2022information">(<a href="#ref-stone2022information" role="doc-biblioref">Stone, 2022</a>)</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<section id="introduzione" class="level2" data-number="72.1"><h2 data-number="72.1" class="anchored" data-anchor-id="introduzione">
<span class="header-section-number">72.1</span> Introduzione</h2>
<p>In questo capitolo, descriveremo il concetto di entropia, una misura fondamentale sviluppata nell’ambito della teoria dell’informazione. L’entropia ci permette di quantificare l’incertezza associata a una distribuzione di probabilità e, di conseguenza, la quantità di informazione che un evento ci fornisce.</p>
<p>L’entropia è legata alla nostra capacità di prevedere l’esito di un evento: più un risultato è imprevedibile, maggiore sarà l’entropia. In termini più generali, l’entropia di una variabile casuale misura quanto il valore che essa assumerà in media sia incerto o “sorprendente”. Se ogni possibile esito ha la stessa probabilità di verificarsi, l’entropia sarà massima. Se invece alcuni esiti sono molto più probabili di altri, l’entropia diminuisce, poiché l’incertezza complessiva si riduce.</p>
<p>Un’intuizione sull’entropia può essere ottenuta considerando il seguente esempio. Immaginiamo un sacchetto di palline colorate. Se il sacchetto contiene solo palline di un unico colore, possiamo essere sicuri di quale pallina estrarremo ogni volta. Non c’è alcuna incertezza o sorpresa, quindi l’entropia è pari a zero. Tuttavia, se nel sacchetto ci sono palline di diversi colori in numero uguale, ogni estrazione è un’incognita: l’incertezza è massima e, di conseguenza, lo è anche l’entropia.</p>
<p>Il concetto di entropia va ben oltre questo semplice esempio. Si applica a qualunque situazione in cui vi sia un insieme di risultati possibili con probabilità diverse. In questo capitolo ci concentreremo sul significato matematico dell’entropia, esplorando come può essere calcolata per diverse distribuzioni di probabilità e come essa sia correlata alla quantità di informazione che un sistema può fornire. Inizieremo con esempi semplici, come l’entropia di una moneta equa o di un dado, per poi passare a situazioni più complesse.</p>
</section><section id="che-cosè-linformazione" class="level2" data-number="72.2"><h2 data-number="72.2" class="anchored" data-anchor-id="che-cosè-linformazione">
<span class="header-section-number">72.2</span> Che cos’è l’informazione?</h2>
<p>L’informazione è solitamente misurata in bit e un bit di informazione permette di scegliere tra due alternative ugualmente probabili. Il termine bit deriva da “binary digit”, ovvero “cifra binaria”, che può assumere il valore 0 o 1.</p>
<p>Per capire come l’informazione possa essere misurata in bit, consideriamo il seguente esempio. Immaginiamo di trovarci a un incrocio e di dover scegliere una strada tra due possibilità. Ogni volta che ci troviamo di fronte a un incrocio, dobbiamo prendere una decisione: andare a destra o a sinistra. Ogni decisione può essere rappresentata da un bit di informazione: 0 per la sinistra e 1 per la destra.</p>
<p>Consideriamo il percorso con più incroci rappresentato nell’immagine seguente. Ogni percorso completo può essere codificato da una sequenza di bit, dove ogni bit corrisponde a una decisione (binaria) presa a un incrocio. Ad esempio, per raggiungere il punto D011, la sequenza di bit corretta è 011.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="01_entropy_files/figure-html/unnamed-chunk-2-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="01_entropy_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</div>
</div>
<p><strong>Quanti bit sono necessari per identificare una destinazione specifica?</strong></p>
<p>Ogni bit raddoppia il numero di possibili percorsi. Quindi, se abbiamo <span class="math inline">\(n\)</span> bit, possiamo identificare <span class="math inline">\(2^n\)</span> destinazioni distinte. Viceversa, se conosciamo il numero di destinazioni <span class="math inline">\(m\)</span>, possiamo calcolare il numero di bit necessari utilizzando la formula:</p>
<p><span class="math display">\[
n = \log_2 m.
\]</span></p>
<p>Nel nostro esempio, abbiamo otto destinazioni finali. Pertanto, sono necessari 3 bit (3 decisioni binarie) per identificarne una in modo univoco.</p>
<p><strong>Cosa rappresenta un bit in questo contesto?</strong></p>
<p>Un bit rappresenta un’unità elementare di informazione. In questo caso, ogni bit risponde alla domanda: “Devo andare a destra o a sinistra?”.</p>
<p><strong>Perché utilizziamo i logaritmi?</strong></p>
<p>Il logaritmo in base 2 ci permette di calcolare l’esponente a cui elevare 2 per ottenere un dato numero. In altre parole, ci indica quanti bit sono necessari per rappresentare un certo numero di destinazioni. Per l’esempio considerato, per arrivare a <span class="math inline">\(D011\)</span> partendo da <span class="math inline">\(A\)</span>, sono necessarie 3 domande la cui risposta è binaria (destra/sinistra).</p>
<p>Per riassumere:</p>
<ul>
<li>per raggiungere il punto D011 partendo da A, abbiamo bisogno di prendere tre decisioni binarie (sinistra o destra) in corrispondenza di tre incroci;</li>
<li>ogni decisione binaria può essere rappresentata da un bit (0 o 1). Quindi, per l’intero percorso, abbiamo bisogno di una sequenza di tre bit: 011;</li>
<li>per rispondere alla domanda “Come si va da A a D011?”, abbiamo dunque bisogno di 3 bit di informazione.</li>
</ul>
<p>In sintesi, esiste una relazione diretta tra il numero di bit di informazione e il numero di possibili destinazioni in un percorso decisionale binario. Ogni bit ci permette di scegliere tra due alternative, raddoppiando così il numero di possibili percorsi.</p>
</section><section id="la-sorpresa-e-linformazione-di-shannon" class="level2" data-number="72.3"><h2 data-number="72.3" class="anchored" data-anchor-id="la-sorpresa-e-linformazione-di-shannon">
<span class="header-section-number">72.3</span> La Sorpresa e l’Informazione di Shannon</h2>
<p>Nel primo esempio abbiamo visto che l’informazione può essere misurata in bit, dove ogni bit corrisponde a una scelta binaria che ci aiuta a raggiungere una destinazione specifica lungo un percorso. Tuttavia, la quantità di informazione può variare anche in base alla probabilità con cui si verificano determinati eventi o si fanno determinate scelte. È qui che entra in gioco il concetto di “informazione di Shannon”, che prende in considerazione la sorpresa associata a un risultato.</p>
<p>Immaginiamo, ad esempio, di avere una moneta che cade “testa” nel 90% dei casi. Poiché il risultato “testa” è molto probabile, non ci sorprenderemmo molto se lo ottenessimo. Al contrario, il risultato “croce”, che si verifica solo nel 10% dei casi, ci sorprenderà di più. Più un risultato è improbabile, maggiore sarà la sorpresa nel vederlo.</p>
<p>In termini di informazione, i risultati meno probabili forniscono più informazione perché ci sorprendono di più. Una prima idea per misurare questa sorpresa è definirla come inversamente proporzionale alla probabilità del risultato, ovvero <span class="math inline">\(1/p(x)\)</span>. Tuttavia, Shannon ha dimostrato che è più utile esprimere la sorpresa come il logaritmo di <span class="math inline">\(1/p(x)\)</span>. Ciò ci porta alla definizione dell’informazione di Shannon, che si misura in bit (se si utilizza il logaritmo in base 2, lo stesso utilizzato per misurare i percorsi nel primo esempio). L’informazione di Shannon per un risultato <span class="math inline">\(x\)</span> è quindi:</p>
<p><span id="eq-surprise-shannon"><span class="math display">\[
h(x) = \log_2 \frac{1}{p(x)} = -\log_2 p(x) \text{ bit}.
\tag{72.1}\]</span></span></p>
<p>In questo modo, vediamo che l’informazione associata a un evento dipende dalla sua probabilità: eventi meno probabili portano più informazione, e viceversa.</p>
</section><section id="sorpresa-e-probabilità" class="level2" data-number="72.4"><h2 data-number="72.4" class="anchored" data-anchor-id="sorpresa-e-probabilità">
<span class="header-section-number">72.4</span> Sorpresa e Probabilità</h2>
<p>Per comprendere pienamente la sorpresa, dobbiamo conoscere le probabilità dei diversi risultati. Questo significa che l’informazione di Shannon dipende dalla distribuzione di probabilità <span class="math inline">\(p(X)\)</span> della variabile casuale <span class="math inline">\(X\)</span>. In altre parole, per misurare quanta informazione otteniamo da un risultato, dobbiamo sapere quanto è probabile ciascun possibile esito.</p>
<p>Un modo per stimare queste probabilità è osservare i risultati di un esperimento ripetuto nel tempo. Utilizzando le osservazioni, possiamo stimare la probabilità di ciascun risultato e quindi calcolare l’informazione di Shannon associata. Questo approccio ci permette di collegare il concetto di informazione misurata in bit (come nel primo esempio sugli incroci) con la sorpresa generata da eventi più o meno probabili.</p>
</section><section id="entropia-come-media-dellinformazione-di-shannon" class="level2" data-number="72.5"><h2 data-number="72.5" class="anchored" data-anchor-id="entropia-come-media-dellinformazione-di-shannon">
<span class="header-section-number">72.5</span> Entropia come Media dell’Informazione di Shannon</h2>
<p>Quando si lavora con fenomeni casuali, non ci interessa solo la sorpresa associata a un singolo risultato, ma piuttosto la sorpresa media che si può ottenere considerando tutti i possibili risultati di una variabile. Questa sorpresa media è chiamata “entropia” e si indica con <span class="math inline">\(H(X)\)</span>. L’entropia fornisce una misura della quantità di incertezza (o informazione potenziale) contenuta in una variabile casuale <span class="math inline">\(X\)</span> la cui distribuzione di probabilità è data da <span class="math inline">\(p(X)\)</span>.</p>
<p>L’entropia rappresenta quindi la quantità media di informazione che si ottiene osservando i risultati della variabile <span class="math inline">\(X\)</span>. Se, per esempio, lanciamo una moneta molte volte, l’entropia della distribuzione dei risultati riflette la media delle informazioni di Shannon ottenute da ciascun lancio. In altre parole, ci dice quanto ci si può aspettare di “imparare” in media da ogni lancio.</p>
<p>Matematicamente, l’entropia può essere approssimata dalla media delle informazioni di Shannon associate a ciascun possibile risultato <span class="math inline">\(x_i\)</span>:</p>
<p><span id="eq-surprise-shannon"><span class="math display">\[
H(X) \approx \frac{1}{n} \sum_{i=1}^{n} h(x_i).
\tag{72.2}\]</span></span></p>
<p>In questa formula, <span class="math inline">\(h(x_i)\)</span> rappresenta l’informazione di Shannon di un singolo risultato <span class="math inline">\(x_i\)</span>, come discusso in precedenza. L’entropia, quindi, non si riferisce a un evento specifico, ma alla sorpresa media che ci si aspetta quando si osserva ripetutamente una variabile casuale. Più la distribuzione delle probabilità dei risultati è equilibrata (per esempio, se tutti i risultati sono ugualmente probabili), maggiore sarà l’entropia, perché ciascun risultato fornirà una quantità simile di informazione. Al contrario, se alcuni risultati sono molto più probabili di altri (per esempio, una moneta truccata che dà quasi sempre “testa”), l’entropia sarà minore, perché si otterrà meno informazione da ogni osservazione.</p>
<div id="exm-entropy-balanced-coin" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.1</strong></span> Se una moneta è equa, allora <span class="math inline">\(p(x_h) = 0.5\)</span> e la sorpresa di osservare una testa è</p>
<p><span class="math display">\[
\begin{align}
h(x_h) &amp;= \log_2 \frac{1}{p(x_h)} \notag\\
       &amp;= \log_2(1/0.5) = 1 \text{ bit}.\notag
\end{align}
\]</span></p>
<p>Dato che <span class="math inline">\(p(x_t) = 0.5\)</span>, la sorpresa di osservare una testa (o una croce) è di un bit.</p>
<p>Possiamo trovare la sorpresa media lanciando la moneta, diciamo, 100 volte, misurando la sorpresa di ogni risultato e poi calcolando la media dei 100 risultati. Se lanciamo una moneta 100 volte, ci aspettiamo di osservare testa circa 50 volte e croce circa 50 volte. Se osserviamo esattamente 50 teste e 50 croci, la quantità media di sorpresa diventa</p>
<p><span class="math display">\[
\begin{align}
H(X) &amp;= \frac{1}{100} \left( \sum_{i=1}^{50} \log_2 \frac{1}{p(x_h)} + \sum_{i=1}^{50} \log_2 \frac{1}{p(x_t)} \right)\notag\\
&amp;=1 \text{ bit per lancio della moneta}\notag.
\end{align}
\]</span></p>
<p>In sintesi, poiché la quantità di sorpresa o informazione di Shannon fornita dall’osservazione del risultato di ogni lancio di questa moneta equa è di un bit, ne segue che l’informazione media <span class="math inline">\(H(X)\)</span> di ogni lancio è anch’essa di un bit.</p>
</div>
</section><section id="interpretazione-dellentropia-1" class="level2" data-number="72.6"><h2 data-number="72.6" class="anchored" data-anchor-id="interpretazione-dellentropia-1">
<span class="header-section-number">72.6</span> Interpretazione dell’entropia (1)</h2>
<p>Se consideriamo una distribuzione di probabilità uniforme, una variabile con entropia <span class="math inline">\(H(X)\)</span> espressa in bit fornisce un’informazione sufficiente (nel senso della teoria dell’informazione di Shannon) per distinguere tra <span class="math inline">\(m = 2^(H(X))\)</span> alternative ugualmente probabili. In altre parole, l’entropia misura la quantità di informazione contenuta in una variabile, esprimendola in termini del numero di scelte ugualmente probabili possibili per quella variabile.</p>
<div id="exm-entropy-unbalanced-coin" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.2</strong></span> Una moneta sbilanciata ha una quantità media di informazione (o incertezza) inferiore rispetto a una moneta equa.</p>
<p>La sorpresa associata a testa è:</p>
<p><span class="math display">\[
h(\text{testa}) = \log\left(\frac{1}{0.9}\right) = 0.15 \text{ bit},
\]</span></p>
<p>mentre la sorpresa associata a croce è maggiore:</p>
<p><span class="math display">\[
h(\text{croce}) = \log\left(\frac{1}{0.1}\right) = 3.32 \text{ bit}.
\]</span></p>
</div>
</section><section id="interpretazione-dellentropia-2" class="level2" data-number="72.7"><h2 data-number="72.7" class="anchored" data-anchor-id="interpretazione-dellentropia-2">
<span class="header-section-number">72.7</span> Interpretazione dell’Entropia (2)</h2>
<p>Se immaginiamo di lanciare la moneta molte volte, la sorpresa media o entropia di questa moneta, considerando <span class="math inline">\(p(\text{testa}) = 0,9\)</span> e <span class="math inline">\(p(\text{croce}) = 0,1\)</span>, è:</p>
<p><span class="math display">\[H(X) = 0.9 \log_2 \frac{1}{0.9} + 0.1 \log_2 \frac{1}{0.1} = 0.469 \text{ bit per lancio}.\]</span></p>
<p>L’incertezza media di questa moneta sbilanciata è dunque inferiore a quella di una moneta equa (che ha un’entropia di 1 bit), anche se l’incertezza associata all’esito meno probabile (croce) è maggiore (3.32 bit) rispetto a quella di una moneta equa (1 bit). In generale, nessuna moneta sbilanciata può avere un’entropia media maggiore di quella di una moneta equa.</p>
<p>Poiché <span class="math inline">\(p(\text{testa}) = 0.9\)</span> e <span class="math inline">\(p(\text{croce}) = 0.1\)</span>, la formula dell’entropia può essere scritta come:</p>
<p><span class="math display">\[
H(X) = p(\text{testa}) \log\left(\frac{1}{p(\text{testa})}\right) + p(\text{croce}) \log\left(\frac{1}{p(\text{croce})}\right) = 0.469 \text{ bit per lancio}.
\]</span></p>
<p>Per semplificare ulteriormente, possiamo rappresentare l’entropia sommando i due possibili esiti (testa e croce):</p>
<p><span class="math display">\[
H(X) = \sum_{i=1}^{2} p(x_i) \log\left(\frac{1}{p(x_i)}\right) = 0.469 \text{ bit per lancio}.
\]</span></p>
<p>Questa entropia di 0.469 bit implica che l’informazione contenuta in 1000 lanci di questa moneta potrebbe essere rappresentata utilizzando solo 469 bit binari, ovvero <span class="math inline">\(1000 \times 0.469\)</span>.</p>
<p>Possiamo interpretare questo risultato considerando l’entropia in termini di numero di alternative ugualmente probabili. La variabile <span class="math inline">\(X\)</span>, che rappresenta il lancio della moneta, può essere vista come equivalente a una variabile che può assumere i valori:</p>
<p><span class="math display">\[
m = 2^{H(X)} = 2^{0.469} \approx 1.38 \text{ valori equiprobabili}.
\]</span></p>
<p>A prima vista, questo risultato può sembrare strano, dato che stiamo considerando una moneta che ha solo due esiti possibili. Tuttavia, interpretare l’entropia in termini di un numero equivalente di valori ugualmente probabili ci fornisce un’intuizione sull’informazione rappresentata da una variabile. Un modo per comprendere questo concetto è immaginare che una moneta con entropia <span class="math inline">\(H(X) = 0.469\)</span> bit abbia la stessa quantità di incertezza di un ipotetico dado con 1.38 facce.</p>
</section><section id="caratteristiche-dellentropia" class="level2" data-number="72.8"><h2 data-number="72.8" class="anchored" data-anchor-id="caratteristiche-dellentropia">
<span class="header-section-number">72.8</span> Caratteristiche dell’entropia</h2>
<ul>
<li><p><em>Entropia massima:</em> l’entropia raggiunge il suo valore massimo quando tutti gli esiti di un evento hanno la stessa probabilità di verificarsi. In questa situazione, l’incertezza è massima, poiché non ci sono indizi che ci permettano di prevedere il risultato. Ciò rappresenta il massimo grado di imprevedibilità.</p></li>
<li><p><em>Entropia minima:</em> l’entropia è minima quando l’esito di un evento è completamente certo (con probabilità pari a 1) o impossibile (con probabilità pari a 0). In questi casi, non c’è incertezza né sorpresa, quindi non c’è alcuna informazione aggiuntiva da ottenere osservando il risultato.</p></li>
</ul>
<section id="additività-dellentropia-per-eventi-indipendenti" class="level3" data-number="72.8.1"><h3 data-number="72.8.1" class="anchored" data-anchor-id="additività-dellentropia-per-eventi-indipendenti">
<span class="header-section-number">72.8.1</span> Additività dell’entropia per eventi indipendenti</h3>
<p>L’entropia è additiva nel caso di eventi indipendenti. Ciò significa che, se si verificano due o più eventi indipendenti, l’entropia totale della loro combinazione è pari alla somma delle entropie di ciascun evento considerato singolarmente. Questa proprietà deriva dall’additività dei logaritmi che permette di sommare le entropie individuali per ottenere l’entropia complessiva.</p>
</section></section><section id="stimare-lentropia-da-una-distribuzione-di-probabilità" class="level2" data-number="72.9"><h2 data-number="72.9" class="anchored" data-anchor-id="stimare-lentropia-da-una-distribuzione-di-probabilità">
<span class="header-section-number">72.9</span> Stimare l’Entropia da una Distribuzione di Probabilità</h2>
<p>Consideriamo una variabile casuale discreta <span class="math inline">\(X\)</span>, che rappresenta una serie di eventi distinti, ciascuno con una probabilità associata. Per una variabile discreta <span class="math inline">\(X\)</span> con possibili valori <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> e una funzione di massa di probabilità <span class="math inline">\(p(x) = \Pr\{X = x\}\)</span>, l’entropia <span class="math inline">\(H(X)\)</span> misura l’incertezza complessiva associata a questa distribuzione di probabilità e si calcola con la formula:</p>
<p><span id="eq-entropy-collection"><span class="math display">\[
\begin{equation}
H(X) = -\sum_{x \in X} p(x) \log_2 p(x).
\end{equation}
\tag{72.3}\]</span></span></p>
<p>In questo contesto, l’entropia <span class="math inline">\(H(X)\)</span> rappresenta l’incertezza media relativa alla collezione di eventi descritti dalla variabile <span class="math inline">\(X\)</span>. La formula fornisce una somma pesata delle sorprese associate a ciascun esito, dove la sorpresa di un risultato <span class="math inline">\(x\)</span> dipende dalla sua improbabilità, calcolata come <span class="math inline">\(-\log_2 p(x)\)</span>. Il segno negativo è necessario perché i logaritmi delle probabilità, essendo inferiori a 1, sono negativi; il segno negativo li trasforma in valori positivi, che rappresentano correttamente la sorpresa o l’informazione associata.</p>
<p>Ogni termine della somma, <span class="math inline">\(-p(x) \log_2 p(x)\)</span>, esprime la quantità di informazione o sorpresa relativa a un singolo evento, ponderata dalla sua probabilità <span class="math inline">\(p(x)\)</span>. Più le probabilità degli eventi sono distribuite in modo uniforme, maggiore sarà l’entropia complessiva. Al contrario, se uno o più eventi sono molto più probabili rispetto agli altri, l’entropia sarà inferiore, riflettendo una minore incertezza.</p>
<p>In sintesi, l’entropia <span class="math inline">\(H(X)\)</span> misura l’incertezza complessiva associata alla distribuzione di probabilità di una variabile casuale discreta <span class="math inline">\(X\)</span>. Essa quantifica la sorpresa media che ci si può aspettare quando si osserva un evento estratto casualmente da questa collezione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.3</strong></span> Supponiamo di avere un dado con otto facce. Ci sono <span class="math inline">\(m = 8\)</span> esiti possibili:</p>
<p><span class="math display">\[
A_x = \{1,2,3,4,5,6,7,8\}.
\]</span></p>
<p>Poiché il dado è equo, tutti gli otto esiti hanno la stessa probabilità di <span class="math inline">\(p(x) = 1/8\)</span>, definendo così una distribuzione di probabilità uniforme:</p>
<p><span class="math display">\[
p(X) = \left\{\frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}\right\}.
\]</span></p>
<p>L’entropia di questa distribuzione può essere calcolata come:</p>
<p><span class="math display">\[
H(X) = - \sum_{i=1}^{8} \frac{1}{8} \log_2 \frac{1}{8} = \log_2 8 = 3 \text{ bit}.
\]</span></p>
<p>Poiché l’informazione associata a ciascun esito è esattamente 3 bit, anche l’entropia media è di 3 bit, che rappresenta l’incertezza complessiva della variabile <span class="math inline">\(X\)</span>.</p>
<p>Dato che <span class="math inline">\(X\)</span> ha un’entropia di <span class="math inline">\(H(X) = 3\)</span> bit, possiamo dire che <span class="math inline">\(X\)</span> può rappresentare fino a:</p>
<p><span class="math display">\[
m = 2^{H(X)} = 2^3 = 8
\]</span></p>
<p>esiti equiprobabili.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.4</strong></span> Sia <span class="math inline">\(X\)</span> una variabile casuale discreta che può assumere i valori <span class="math inline">\(a, b, c,\)</span> e <span class="math inline">\(d\)</span> con una distribuzione di probabilità di massa <span class="math inline">\(p(a) = \frac{1}{2}\)</span>, <span class="math inline">\(p(b) = \frac{1}{4}\)</span>, <span class="math inline">\(p(c) = \frac{1}{8}\)</span>, e <span class="math inline">\(p(d) = \frac{1}{8}\)</span>, rispettivamente. L’entropia di <span class="math inline">\(X\)</span>, che misura l’incertezza associata alla distribuzione di probabilità, è calcolata come:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{8} \log_2 \frac{1}{8} + \frac{1}{8} \log_2 \frac{1}{8}\right).
\]</span></p>
<p>Calcolando i singoli termini, otteniamo:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \cdot (-1) + \frac{1}{4} \cdot (-2) + \frac{1}{8} \cdot (-3) + \frac{1}{8} \cdot (-3)\right) = \frac{7}{4} \text{ bits}.
\]</span></p>
<p>È importante notare che l’entropia <span class="math inline">\(H(X)\)</span> dipende esclusivamente dalla distribuzione di probabilità dei valori di <span class="math inline">\(X\)</span> e non dai valori stessi.</p>
</div>
</section><section id="stimare-lentropia-in-un-campione-di-osservazioni" class="level2" data-number="72.10"><h2 data-number="72.10" class="anchored" data-anchor-id="stimare-lentropia-in-un-campione-di-osservazioni">
<span class="header-section-number">72.10</span> Stimare l’Entropia in un Campione di Osservazioni</h2>
<p>L’entropia può essere calcolata non solo per distribuzioni teoriche, ma anche per campioni di dati osservati. In questo caso, l’entropia fornisce una misura di quanto la distribuzione dei valori all’interno del campione sia incerta o imprevedibile.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.5</strong></span> Per comprendere meglio questo concetto, possiamo calcolare l’entropia associata a insiemi di osservazioni. Consideriamo i due vettori seguenti:</p>
<p><span class="math display">\[
\begin{align}
x &amp;= \{1, 2, 3, 3, 3, 3, 2, 1, 3, 3, 2, 1, 1, 4, 4, 3, 1, 2\}, \notag\\
y &amp;= \{3, 4, 1, 1, 1, 1, 4, 3, 1, 1, 4, 3, 3, 2, 2, 1, 3, 4\}. \notag
\end{align}
\]</span></p>
<p>Troviamo l’entropia associata a ciascuno di essi.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Vettori x e y</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Conta le frequenze</span></span>
<span><span class="va">x_counts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">y_counts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcola le probabilità relative</span></span>
<span><span class="va">x_probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">x_counts</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">y_probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">y_counts</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funzione per calcolare l'entropia (log in base 2)</span></span>
<span><span class="va">calculate_entropy</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">probabilities</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Calcolo dell'entropia</span></span>
<span><span class="va">x_entropy</span> <span class="op">&lt;-</span> <span class="fu">calculate_entropy</span><span class="op">(</span><span class="va">x_probabilities</span><span class="op">)</span></span>
<span><span class="va">y_entropy</span> <span class="op">&lt;-</span> <span class="fu">calculate_entropy</span><span class="op">(</span><span class="va">y_probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stampa i risultati</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Entropia di x: %.4f bit\n"</span>, <span class="va">x_entropy</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Entropia di x: 1.8776 bit</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Entropia di y: %.4f bit\n"</span>, <span class="va">y_entropy</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Entropia di y: 1.8776 bit</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Entrambi i vettori hanno la stessa entropia di 1.8776 bit.</p>
</div>
</section><section id="entropia-di-una-variabile-casuale-continua" class="level2" data-number="72.11"><h2 data-number="72.11" class="anchored" data-anchor-id="entropia-di-una-variabile-casuale-continua">
<span class="header-section-number">72.11</span> Entropia di una Variabile Casuale Continua</h2>
<p>Nel caso delle variabili casuali continue, il concetto di entropia viene generalizzato sostituendo la somma con un integrale. Questo è necessario perché le variabili continue possono assumere un numero infinito di valori all’interno di un intervallo.</p>
<p>Per una variabile casuale continua <span class="math inline">\(X\)</span> con una funzione di densità di probabilità <span class="math inline">\(p(x)\)</span>, l’entropia (nota anche come entropia differenziale) è definita dalla seguente formula:</p>
<p><span class="math display">\[ H(X) = -\int p(x) \log_2(p(x)) \, dx, \]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(x)\)</span> è la funzione di densità di probabilità di <span class="math inline">\(X\)</span>,</li>
<li>l’integrale è calcolato su tutto il dominio di <span class="math inline">\(X\)</span>.</li>
</ul>
<p>L’entropia di una variabile casuale continua fornisce una misura dell’incertezza o della sorpresa associata alla distribuzione della variabile. Come nel caso discreto, l’entropia continua quantifica l’incertezza associata a <span class="math inline">\(X\)</span>. Una PDF molto concentrata (ad esempio, una distribuzione con picchi stretti) implica bassa entropia, poiché l’evento è più prevedibile. Una PDF distribuita uniformemente implica alta entropia, poiché l’evento è meno prevedibile.</p>
<p>Il segno negativo assicura che l’entropia sia una quantità positiva, in quanto <span class="math inline">\(\log_2(p(x))\)</span> è negativo per <span class="math inline">\(p(x)\)</span> compreso tra 0 e 1.</p>
<p>Esempi relativi al calcolo dell’entropia nel caso di variabili continue sono fornite nel <span class="quarto-unresolved-ref">?sec-entropy-rv-cont</span>.</p>
</section><section id="la-codifica-di-huffman" class="level2" data-number="72.12"><h2 data-number="72.12" class="anchored" data-anchor-id="la-codifica-di-huffman">
<span class="header-section-number">72.12</span> La codifica di Huffman</h2>
<p>Immagina di dover inviare un messaggio usando il minor numero possibile di bit, cioè di <em>0</em> e <em>1</em>. Alcuni simboli (come le lettere A, E, o spazi vuoti) appaiono spesso nei testi, mentre altri (come Z o Q) sono molto rari. Ha senso usare <em>codici più brevi per i simboli più frequenti</em>, e <em>codici più lunghi per quelli rari</em>. Questo è esattamente ciò che fa la <em>codifica di Huffman</em>: costruisce un codice binario che minimizza la lunghezza totale del messaggio, adattandosi alla frequenza dei simboli.</p>
<section id="passaggi-per-creare-una-codifica-di-huffman" class="level3" data-number="72.12.1"><h3 data-number="72.12.1" class="anchored" data-anchor-id="passaggi-per-creare-una-codifica-di-huffman">
<span class="header-section-number">72.12.1</span> Passaggi per creare una codifica di Huffman</h3>
<p>Immaginiamo di avere un messaggio, o un insieme di dati. Prima di tutto, <em>dobbiamo contare quante volte appare ciascun simbolo</em>. Ad esempio:</p>
<table class="caption-top table">
<thead><tr class="header">
<th>Simbolo</th>
<th>Frequenza</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>20</td>
</tr>
<tr class="even">
<td>B</td>
<td>10</td>
</tr>
<tr class="odd">
<td>C</td>
<td>8</td>
</tr>
<tr class="even">
<td>D</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Queste frequenze ci dicono quali simboli sono più “comuni”.</p>
<p>Il secondo passo è quello di costruire l’albero di Huffman. In altre parole, usiamo un metodo visivo per costruire una specie di albero genealogico al contrario:</p>
<ol type="1">
<li>
<em>creiamo un nodo per ogni simbolo</em>, con la sua frequenza;</li>
<li>
<em>uniamo i due simboli meno frequenti</em> creando un nuovo nodo; la frequenza del nuovo nodo è la somma delle due;</li>
<li>
<em>ripetiamo il processo</em> unendo ogni volta i due nodi con frequenza minore, finché non rimane un solo nodo (la radice dell’albero).</li>
</ol>
<p>Esempio passo-passo con i nostri simboli:</p>
<ul>
<li>(D:5) e (C:8) → uniti in (DC:13)</li>
<li>(B:10) e (DC:13) → uniti in (BDC:23)</li>
<li>(A:20) e (BDC:23) → uniti in (ABDC:43)</li>
</ul>
<p>Struttura dell’albero:</p>
<pre><code>       (43)
      /    \
    (20)   (23)
     A     /   \
         (10)  (13)
          B    /  \
              D    C</code></pre>
<p>Ora possiamo assegnare i codici binari:</p>
<ul>
<li>ad ogni <em>ramo sinistro mettiamo uno 0</em>;</li>
<li>ad ogni <em>ramo destro mettiamo un 1</em>;</li>
<li>il <em>codice di un simbolo</em> è la sequenza di 0 e 1 dal nodo radice fino al simbolo.</li>
</ul>
<p>Risultati finali:</p>
<table class="caption-top table">
<thead><tr class="header">
<th>Simbolo</th>
<th>Codice Huffman</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0</td>
</tr>
<tr class="even">
<td>B</td>
<td>10</td>
</tr>
<tr class="odd">
<td>D</td>
<td>110</td>
</tr>
<tr class="even">
<td>C</td>
<td>111</td>
</tr>
</tbody>
</table>
<p>Notiamo che:</p>
<ul>
<li>A, il più frequente, ha il codice più corto (1 bit);</li>
<li>C e D, i meno frequenti, hanno codici più lunghi (3 bit).</li>
</ul></section><section id="perché-la-codifica-di-huffman-funziona-così-bene" class="level3" data-number="72.12.2"><h3 data-number="72.12.2" class="anchored" data-anchor-id="perché-la-codifica-di-huffman-funziona-così-bene">
<span class="header-section-number">72.12.2</span> Perché la codifica di Huffman funziona così bene?</h3>
<ul>
<li>La <em>codifica è senza perdita</em>: possiamo sempre ricostruire il messaggio originale.</li>
<li>È <em>ottimale</em>: tra tutte le codifiche possibili che usano un numero intero di bit per simbolo, la codifica di Huffman è la più efficiente.</li>
<li>È una <em>codifica prefissa</em>: nessun codice è l’inizio di un altro. Questo rende la decodifica univoca e automatica.</li>
</ul></section></section><section id="lentropia-come-lunghezza-media-del-codice-binario" class="level2" data-number="72.13"><h2 data-number="72.13" class="anchored" data-anchor-id="lentropia-come-lunghezza-media-del-codice-binario">
<span class="header-section-number">72.13</span> L’entropia come lunghezza media del codice binario</h2>
<p>L’entropia può essere interpretarla come <em>la quantità media di informazione</em> che riceviamo quando osserviamo il valore di una variabile casuale. In pratica, rappresenta <em>la lunghezza media più breve</em> (in bit) che possiamo usare per codificare gli esiti di quella variabile, tenendo conto delle loro probabilità.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 72.6</strong></span> Supponiamo di avere una variabile casuale <span class="math inline">\(X\)</span> che può assumere quattro valori: <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, e <span class="math inline">\(D\)</span>, con le seguenti probabilità:</p>
<ul>
<li><span class="math inline">\(p(A) = 0.4\)</span></li>
<li><span class="math inline">\(p(B) = 0.3\)</span></li>
<li><span class="math inline">\(p(C) = 0.2\)</span></li>
<li><span class="math inline">\(p(D) = 0.1\)</span></li>
</ul>
<p>Per rappresentare questi esiti con un codice binario efficiente possiamo usare la <em>codifica di Huffman</em>, che assegna codici più brevi ai simboli più probabili, e codici più lunghi a quelli meno probabili.</p>
<p>Supponiamo che Huffman produca la seguente codifica:</p>
<ul>
<li>A = <code>0</code> (1 bit)</li>
<li>B = <code>10</code> (2 bit)</li>
<li>C = <code>110</code> (3 bit)</li>
<li>D = <code>111</code> (3 bit)</li>
</ul>
<p>La <em>lunghezza media del codice</em> si ottiene moltiplicando la probabilità di ciascun simbolo per la lunghezza del suo codice binario, e poi sommando:</p>
<p><span class="math display">\[
\begin{align}
\text{Lunghezza media} &amp;= (0.4 \times 1) + (0.3 \times 2) + (0.2 \times 3) + (0.1 \times 3) \\
&amp;= 0.4 + 0.6 + 0.6 + 0.3 = 1.9 \text{ bit}.
\end{align}
\]</span></p>
<p>Questo significa che, in media, servono <em>1.9 bit</em> per rappresentare un’osservazione della variabile <span class="math inline">\(X\)</span> usando la codifica di Huffman.</p>
<p>Confermiamo il risultato con il seguente codice R:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definizione delle probabilità</span></span>
<span><span class="va">probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>A <span class="op">=</span> <span class="fl">0.4</span>, B <span class="op">=</span> <span class="fl">0.3</span>, C <span class="op">=</span> <span class="fl">0.2</span>, D <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione per la codifica di Huffman</span></span>
<span><span class="va">huffman_encoding</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">sym</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>symbol <span class="op">=</span> <span class="va">sym</span>, prob <span class="op">=</span> <span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span>, left <span class="op">=</span> <span class="cn">NULL</span>, right <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw">while</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">nodes</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">nodes</span>, <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="va">n</span><span class="op">$</span><span class="va">prob</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">left</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">right</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">merged</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>symbol <span class="op">=</span> <span class="cn">NULL</span>, prob <span class="op">=</span> <span class="va">left</span><span class="op">$</span><span class="va">prob</span> <span class="op">+</span> <span class="va">right</span><span class="op">$</span><span class="va">prob</span>, left <span class="op">=</span> <span class="va">left</span>, right <span class="op">=</span> <span class="va">right</span><span class="op">)</span></span>
<span>    <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">nodes</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">merged</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">assign_codes</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">node</span>, <span class="va">prefix</span> <span class="op">=</span> <span class="st">""</span>, <span class="va">code_map</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">symbol</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">code_map</span><span class="op">[[</span><span class="va">node</span><span class="op">$</span><span class="va">symbol</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">prefix</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">left</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">prefix</span>, <span class="st">"0"</span><span class="op">)</span>, <span class="va">code_map</span><span class="op">)</span></span>
<span>      <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">right</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">prefix</span>, <span class="st">"1"</span><span class="op">)</span>, <span class="va">code_map</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">code_map</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">nodes</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">avg_length</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">sym</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/nchar.html">nchar</a></span><span class="op">(</span><span class="va">code_map</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>avg_length <span class="op">=</span> <span class="va">avg_length</span>, huffman_dict <span class="op">=</span> <span class="va">code_map</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Applicazione e stampa dei risultati</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">huffman_encoding</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Lunghezza media del codice di Huffman: %.2f bit/simbolo\n"</span>, <span class="va">result</span><span class="op">$</span><span class="va">avg_length</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Lunghezza media del codice di Huffman: 1.90 bit/simbolo</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Codici di Huffman:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Codici di Huffman:</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">sym</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">result</span><span class="op">$</span><span class="va">huffman_dict</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"%s: %s\n"</span>, <span class="va">sym</span>, <span class="va">result</span><span class="op">$</span><span class="va">huffman_dict</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; A: 0</span></span>
<span><span class="co">#&gt; B: 10</span></span>
<span><span class="co">#&gt; D: 110</span></span>
<span><span class="co">#&gt; C: 111</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ora calcoliamo l’entropia teorica della variabile <span class="math inline">\(X\)</span>, cioè la lunghezza media <strong>minima</strong> che qualsiasi codifica binaria può raggiungere:</p>
<p><span class="math display">\[
\begin{align}
H(X) &amp;= - \sum p(x) \log_2 p(x) \\
     &amp;= -[0.4 \log_2 0.4 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1] \\
     &amp;= 1.8465 \text{ bit}.
\end{align}
\]</span></p>
<p>Il valore dell’entropia è leggermente <em>inferiore alla lunghezza media di Huffman</em> (1.9 bit). Questo è normale: Huffman fornisce <em>codici con lunghezza intera in bit</em>, mentre l’entropia può assumere valori decimali. La codifica di Huffman è quindi <em>quasi ottimale</em>.</p>
<p>In sintesi:</p>
<ul>
<li>
<em>l’entropia <span class="math inline">\(H(X)\)</span></em> rappresenta la <em>lunghezza media teorica minima</em> (in bit) per codificare una variabile casuale;</li>
<li>la <em>codifica di Huffman</em> costruisce un codice binario che si avvicina molto a questo limite, usando <em>più bit per i simboli rari</em> e <em>meno bit per quelli frequenti</em>;</li>
<li>in questo modo, l’entropia ci offre un criterio per valutare <em>quanto efficiente</em> è una codifica: <em>più la lunghezza media si avvicina all’entropia, più è efficiente</em>.</li>
</ul>
</div>
</section><section id="applicazioni-psicologiche" class="level2" data-number="72.14"><h2 data-number="72.14" class="anchored" data-anchor-id="applicazioni-psicologiche">
<span class="header-section-number">72.14</span> Applicazioni Psicologiche</h2>
<p>Un esempio di applicazione dell’entropia dell’informazione in psicologia riguarda l’effetto della sorpresa nello studio dell’umore. La sorpresa, o entropia, è stata documentata sia in laboratorio che in contesti naturali come un fattore in grado di influenzare le emozioni. Ad esempio, <span class="citation" data-cites="spector1956expectations">Spector (<a href="#ref-spector1956expectations" role="doc-biblioref">1956</a>)</span> ha osservato l’effetto della probabilità a priori sulla soddisfazione dei soggetti in risposta a una promozione lavorativa. I risultati indicano che gli esiti meno probabili a priori (e quindi più sorprendenti quando si verificano) hanno un impatto maggiore sull’umore. In altre parole, quando un evento inatteso e sorprendente si verifica, tende a influenzare l’umore in modo più forte rispetto agli eventi previsti e probabili.</p>
</section><section id="riflessioni-conclusive" class="level2" data-number="72.15"><h2 data-number="72.15" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">72.15</span> Riflessioni Conclusive</h2>
<p>In questo capitolo abbiamo introdotto il concetto di <em>entropia</em> come misura dell’incertezza associata a una variabile casuale. Attraverso esempi concreti – dalla moneta al dado, fino alla codifica di Huffman – abbiamo visto come l’entropia permetta di quantificare la <em>sorpresa media</em> di un sistema, e come questa sorpresa sia strettamente legata alla <em>quantità media di informazione</em> necessaria per rappresentare un evento.</p>
<p>L’entropia non è solo uno strumento astratto, ma un ponte tra la probabilità e la codifica dell’informazione: ci dice <em>quanto ci aspettiamo di “imparare”</em> quando osserviamo un dato, e quanto dovrebbe “costare”, in bit, trasmettere quell’informazione. Nella codifica Huffman, per esempio, abbiamo visto che l’entropia rappresenta un limite inferiore teorico alla lunghezza media di qualsiasi codifica binaria senza perdita.</p>
<p>Questi concetti non sono fini a sé stessi. Servono come <em>fondamenta concettuali per comprendere i criteri di confronto tra modelli</em> probabilistici – e in particolare tra modelli bayesiani. Nei prossimi capitoli estenderemo l’idea di entropia alla <em>divergenza di Kullback-Leibler (KL)</em>, che misura quanto una distribuzione di probabilità differisce da un’altra. La divergenza KL sarà fondamentale per definire l’<em>Expected Log Predictive Density (ELPD)</em>, una metrica chiave per valutare e confrontare modelli in termini della loro <em>capacità predittiva fuori campione</em>. Il collegamento è il seguente:</p>
<ul>
<li>l’<em>entropia</em> misura quanto “impariamo” da una distribuzione nota;</li>
<li>la <em>divergenza KL</em> misura <em>quanto un modello si discosta</em> da quella distribuzione;</li>
<li>l’<em>ELPD</em> si basa proprio sulla somma di log-verosimiglianze predittive, valutate su dati non utilizzati per l’adattamento del modello, ed è legato alla minimizzazione della divergenza KL rispetto ai dati generati dal “vero” processo.</li>
</ul>
<p>Capire l’entropia, quindi, non è solo utile per comprendere la codifica e la compressione: è <em>propedeutico per interpretare correttamente cosa significa che un modello “funziona bene” dal punto di vista dell’informazione</em>.</p>
<p>Concludiamo con una nota filosofica. La celebre osservazione di <span class="citation" data-cites="eckhardt2012paradoxes">Eckhardt (<a href="#ref-eckhardt2012paradoxes" role="doc-biblioref">2012</a>)</span> ci ricorda che l’informazione non è solo quantità, ma anche qualità:</p>
<blockquote class="blockquote">
<p>“…our mind, and even the subconscious self resonate. A poet can recall chains of ideas, emotions and memories with a well-turned word. In this sense, writing is magic.”</p>
</blockquote>
<p>Questa immagine della <em>risonanza</em> come effetto dell’informazione sul ricevente ci invita a <em>non ridurre l’informazione al solo contenuto numerico o simbolico</em>. Nei modelli psicologici – specialmente in quelli predittivi – è fondamentale ricordare che <em>l’interpretabilità e il significato dell’informazione</em> dipendono anche dal contesto umano e cognitivo in cui essa è ricevuta. Nel valutare i modelli, dunque, dovremo considerare non solo quanto bene il modello predice, ma anche <em>cosa ci dice sul fenomeno</em>, quanto è interpretabile, e in che misura <em>risuona</em> con le domande psicologiche che ci poniamo.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] tidygraph_1.3.1  ggraph_2.2.1     igraph_2.1.4     thematic_0.1.7  </span></span>
<span><span class="co">#&gt;  [5] MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.11.0       gridExtra_2.3   </span></span>
<span><span class="co">#&gt;  [9] patchwork_1.3.1  bayesplot_1.13.0 psych_2.5.6      scales_1.4.0    </span></span>
<span><span class="co">#&gt; [13] markdown_2.0     knitr_1.50       lubridate_1.9.4  forcats_1.0.0   </span></span>
<span><span class="co">#&gt; [17] stringr_1.5.1    dplyr_1.1.4      purrr_1.1.0      readr_2.1.5     </span></span>
<span><span class="co">#&gt; [21] tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2    tidyverse_2.0.0 </span></span>
<span><span class="co">#&gt; [25] rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] gtable_0.3.6       xfun_0.52          htmlwidgets_1.6.4 </span></span>
<span><span class="co">#&gt;  [4] ggrepel_0.9.6      lattice_0.22-7     tzdb_0.5.0        </span></span>
<span><span class="co">#&gt;  [7] vctrs_0.6.5        tools_4.5.1        generics_0.1.4    </span></span>
<span><span class="co">#&gt; [10] parallel_4.5.1     pacman_0.5.1       pkgconfig_2.0.3   </span></span>
<span><span class="co">#&gt; [13] RColorBrewer_1.1-3 lifecycle_1.0.4    compiler_4.5.1    </span></span>
<span><span class="co">#&gt; [16] farver_2.1.2       mnormt_2.1.1       ggforce_0.5.0     </span></span>
<span><span class="co">#&gt; [19] graphlayouts_1.2.2 htmltools_0.5.8.1  yaml_2.3.10       </span></span>
<span><span class="co">#&gt; [22] pillar_1.11.0      MASS_7.3-65        cachem_1.1.0      </span></span>
<span><span class="co">#&gt; [25] viridis_0.6.5      nlme_3.1-168       tidyselect_1.2.1  </span></span>
<span><span class="co">#&gt; [28] digest_0.6.37      stringi_1.8.7      labeling_0.4.3    </span></span>
<span><span class="co">#&gt; [31] polyclip_1.10-7    rprojroot_2.1.0    fastmap_1.2.0     </span></span>
<span><span class="co">#&gt; [34] grid_4.5.1         cli_3.6.5          magrittr_2.0.3    </span></span>
<span><span class="co">#&gt; [37] withr_3.0.2        timechange_0.3.0   rmarkdown_2.29    </span></span>
<span><span class="co">#&gt; [40] hms_1.1.3          memoise_2.0.1      evaluate_1.0.4    </span></span>
<span><span class="co">#&gt; [43] viridisLite_0.4.2  rlang_1.1.6        Rcpp_1.1.0        </span></span>
<span><span class="co">#&gt; [46] glue_1.8.0         tweenr_2.0.3       rstudioapi_0.17.1 </span></span>
<span><span class="co">#&gt; [49] jsonlite_2.0.0     R6_2.6.1</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-eckhardt2012paradoxes" class="csl-entry" role="listitem">
Eckhardt, W. (2012). <em>Paradoxes in probability theory</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-spector1956expectations" class="csl-entry" role="listitem">
Spector, A. J. (1956). Expectations, fulfillment, and morale. <em>The Journal of Abnormal and Social Psychology</em>, <em>52</em>(1), 51–56.
</div>
<div id="ref-stone2022information" class="csl-entry" role="listitem">
Stone, J. V. (2022). <em>Information theory: a tutorial introduction, 2nd edition</em>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/linear_models/13_poisson_model.html" class="pagination-link" aria-label="Modello di Poisson">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <span class="nav-page-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/01_entropy.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>