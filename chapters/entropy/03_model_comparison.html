<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>74&nbsp; Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/formal_models/introduction.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-3e8fa383bad517095c2b42029d2b9125.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a12c8109e4cfb0c0b98cd3996d21797e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori (Prior Predictive Check)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#distribuzione-predittiva-posteriore" id="toc-distribuzione-predittiva-posteriore" class="nav-link active" data-scroll-target="#distribuzione-predittiva-posteriore"><span class="header-section-number">74.1</span> Distribuzione Predittiva Posteriore</a></li>
  <li><a href="#il-problema-della-valutazione-predittiva" id="toc-il-problema-della-valutazione-predittiva" class="nav-link" data-scroll-target="#il-problema-della-valutazione-predittiva"><span class="header-section-number">74.2</span> Il problema della valutazione predittiva</a></li>
  <li><a href="#expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" id="toc-expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" class="nav-link" data-scroll-target="#expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati"><span class="header-section-number">74.3</span> Expected Log Predictive Density (ELPD): guardare oltre i dati osservati</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv-la-tecnica-per-stimare-lelpd" id="toc-leave-one-out-cross-validation-loo-cv-la-tecnica-per-stimare-lelpd" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv-la-tecnica-per-stimare-lelpd"><span class="header-section-number">74.4</span> Leave-One-Out Cross-Validation (LOO-CV): la tecnica per stimare l’ELPD</a></li>
  <li><a href="#il-fondamento-teorico-loo-cv-e-la-divergenza-di-kullback-leibler" id="toc-il-fondamento-teorico-loo-cv-e-la-divergenza-di-kullback-leibler" class="nav-link" data-scroll-target="#il-fondamento-teorico-loo-cv-e-la-divergenza-di-kullback-leibler"><span class="header-section-number">74.5</span> Il fondamento teorico: LOO-CV e la divergenza di Kullback-Leibler</a></li>
  <li><a href="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" id="toc-criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="nav-link" data-scroll-target="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl"><span class="header-section-number">74.6</span> Criteri di Informazione come Approssimazioni della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span></a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">74.7</span> Riflessioni Conclusive</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi di apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alla fine di questo capitolo, sarai in grado di:</p>
<ul>
<li>comprendere cos’è la distribuzione predittiva posteriore e come si costruisce;</li>
<li>spiegare cosa misura il log-score e come si calcola nella pratica;</li>
<li>distinguere tra LPPD ed ELPD e comprendere il loro significato;</li>
<li>capire come LOO-CV fornisca una stima dell’ELPD;</li>
<li>collegare il confronto tra modelli alla divergenza di Kullback-Leibler.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>In questo capitolo esploreremo gli strumenti essenziali per valutare e confrontare modelli statistici nel contesto bayesiano. L’obiettivo centrale è capire <em>quanto bene un modello riesce a prevedere nuovi dati</em>, non solo quanto si adatta a quelli osservati.</p>
<p>Immaginate di aver sviluppato un test psicologico per predire l’ansia degli studenti prima di un esame. Non vi basta sapere quanto bene il vostro modello “spiega” i dati che avete già raccolto: volete anche essere sicuri che funzionerà con nuovi studenti che non avete ancora incontrato. Questa distinzione tra adattamento ai dati osservati e capacità di predire nuovi casi è il cuore della valutazione predittiva bayesiana.</p>
<p>Per raggiungere questo obiettivo, introdurremo la distribuzione predittiva posteriore, che integra l’incertezza sui parametri, il log-score come misura dell’accuratezza predittiva punto per punto, la LPPD (Log Pointwise Predictive Density) e l’ELPD (Expected Log Predictive Density), la tecnica Leave-One-Out Cross-Validation (LOO-CV), e il legame tra queste misure e la divergenza di Kullback-Leibler (<span class="math inline">\(D_{\text{KL}}\)</span>).</p>
<p>Queste idee, pur complesse, possono essere rese accessibili con intuizioni semplici e strumenti pratici. Vi guideremo passo per passo.</p>
<section id="distribuzione-predittiva-posteriore" class="level2" data-number="74.1"><h2 data-number="74.1" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore">
<span class="header-section-number">74.1</span> Distribuzione Predittiva Posteriore</h2>
<p>Nel contesto bayesiano, non ci limitiamo a una stima puntuale dei parametri. Dopo aver osservato i dati <span class="math inline">\(y\)</span>, otteniamo una distribuzione posteriore <span class="math inline">\(p(\theta \mid y)\)</span>, che riflette la nostra incertezza sui valori di <span class="math inline">\(\theta\)</span>.</p>
<p>Pensate a uno psicologo che vuole stimare il livello medio di ansia in una popolazione. Dopo aver raccolto dati da un campione, non otterrà un singolo numero come “la media è esattamente 4.7”, ma piuttosto una distribuzione che dice “la media è probabilmente tra 4.2 e 5.1, con 4.7 come valore più plausibile”.</p>
<p>La distribuzione predittiva posteriore per <em>nuovi dati</em> <span class="math inline">\(\tilde{y}\)</span> è:</p>
<p><span class="math display">\[
q(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta.
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Intuizione">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuizione
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Se conoscessimo esattamente i parametri <span class="math inline">\(\theta\)</span>, potremmo prevedere <span class="math inline">\(\tilde{y}\)</span> usando <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span>. Poiché non li conosciamo, combiniamo tutte le previsioni possibili (una per ogni <span class="math inline">\(\theta\)</span>), pesandole secondo quanto ciascun <span class="math inline">\(\theta\)</span> è plausibile secondo la posteriore. Il risultato è una previsione media, che tiene conto della nostra incertezza.</p>
</div>
</div>
</div>
<p>Questa formula cattura un principio fondamentale: invece di fare previsioni basandoci su un singolo “migliore” valore dei parametri, consideriamo tutte le possibilità plausibili e le combiniamo in proporzione alla loro credibilità. È come chiedere il parere a diversi esperti, dove il peso dato a ciascun parere dipende dalla fiducia che riponiamo in quell’esperto.</p>
<p>In questo capitolo useremo a volte <span class="math inline">\(q(\cdot \mid y)\)</span> per indicare genericamente la distribuzione predittiva posteriore del modello, ma nella maggior parte dei casi adotteremo la notazione più esplicita <span class="math inline">\(p(y_i \mid y)\)</span>, per evidenziare che si tratta di una previsione marginale, ottenuta integrando la distribuzione dei dati condizionata ai parametri <span class="no-break">(<span class="math inline">\(p(y_i \mid \theta)\)</span>)</span> sulla distribuzione posteriore dei parametri (<span class="math inline">\(p(\theta \mid y)\)</span>).</p>
</section><section id="il-problema-della-valutazione-predittiva" class="level2" data-number="74.2"><h2 data-number="74.2" class="anchored" data-anchor-id="il-problema-della-valutazione-predittiva">
<span class="header-section-number">74.2</span> Il problema della valutazione predittiva</h2>
<p>Vorremmo sapere quanto bene questa distribuzione predittiva <span class="math inline">\(q(\tilde{y} \mid y)\)</span> si avvicina alla distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, ovvero la distribuzione che avrebbe generato i nuovi dati futuri <span class="math inline">\(\tilde{y}\)</span>, se la conoscessimo. Questa distanza si misura idealmente con la <em>divergenza di Kullback-Leibler</em>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p\left[\log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)}\right].
\]</span></p>
<p>Tuttavia, qui incontriamo un problema fondamentale: <em>non conosciamo <span class="math inline">\(p(\tilde{y})\)</span></em> e non possiamo quindi calcolare direttamente <span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span>. È come voler valutare l’accuratezza di una mappa senza conoscere il territorio reale. Per aggirare questo ostacolo, usiamo misure surrogate: il log-score, la LPPD e l’ELPD, che stimano indirettamente la bontà predittiva del modello attraverso tecniche che sfruttano creativamente i dati a nostra disposizione.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled" title="Mappa concettuale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 39%">
<col style="width: 39%">
</colgroup>
<thead><tr class="header">
<th>Concetto</th>
<th>Cosa rappresenta</th>
<th>Dove viene usato</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid \theta)\)</span></td>
<td>Verosimiglianza</td>
<td>Calcolo predittivo</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(\theta \mid y)\)</span></td>
<td>Distribuzione posteriore</td>
<td>Ponderazione</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid y)\)</span></td>
<td>Predizione bayesiana media</td>
<td>Log-score, LPPD</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(y_i \mid y_{-i})\)</span></td>
<td>Predizione LOO</td>
<td>ELPD</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(q(\tilde{y} \mid y)\)</span></td>
<td>Distribuzione predittiva complessiva</td>
<td>KL, confronto tra modelli</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="il-log-score-misurare-laccuratezza-punto-per-punto" class="level3" data-number="74.2.1"><h3 data-number="74.2.1" class="anchored" data-anchor-id="il-log-score-misurare-laccuratezza-punto-per-punto">
<span class="header-section-number">74.2.1</span> Il log-score: misurare l’accuratezza punto per punto</h3>
<p>Per ciascun punto osservato <span class="math inline">\(y_i\)</span>, calcoliamo il logaritmo della probabilità che il modello le assegna:</p>
<p><span class="math display">\[
\log p(y_i \mid y) = \log \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta.
\]</span></p>
<p>Il log-score totale è la somma di questi valori:</p>
<p><span class="math display">\[
S = \sum_{i=1}^n \log p(y_i \mid y).
\]</span></p>
<p>Un log-score più alto (meno negativo) indica che il modello assegna maggiore probabilità ai dati osservati. Pensate al log-score come a un “voto di fiducia” che il modello assegna a ciascuna osservazione: se il modello considera molto probabile quello che è effettivamente accaduto, il log-score sarà alto.</p>
</section><section id="stima-pratica-del-log-score-con-mcmc" class="level3" data-number="74.2.2"><h3 data-number="74.2.2" class="anchored" data-anchor-id="stima-pratica-del-log-score-con-mcmc">
<span class="header-section-number">74.2.2</span> Stima pratica del log-score con MCMC</h3>
<p>Nel calcolo teorico, il log-score richiede la <em>probabilità predittiva</em> per ogni osservazione <span class="math inline">\(y_i\)</span>, ovvero:</p>
<p><span class="math display">\[
p(y_i \mid y) = \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
\]</span></p>
<p>Per comprenderla, possiamo analizzarne le due componenti:</p>
<ul>
<li>
<span class="math inline">\(p(y_i \mid \theta)\)</span> è la <em>distribuzione dei dati futuri</em> (o di una nuova osservazione) <em>condizionata a un valore specifico dei parametri <span class="math inline">\(\theta\)</span></em>. Questo componente descrive cosa si aspetta il modello che accada, <em>se</em> i parametri fossero proprio <span class="math inline">\(\theta\)</span>.</li>
<li>
<span class="math inline">\(p(\theta \mid y)\)</span> è la <em>distribuzione a posteriori dei parametri del modello</em>, ottenuta <em>dopo aver osservato i dati</em> <span class="math inline">\(y\)</span>. Questo componente rappresenta la nostra <em>incertezza residua</em> su quali siano i veri valori dei parametri.</li>
</ul>
<p>L’integrale esprime il fatto che per fare una previsione su <span class="math inline">\(y_i\)</span>, <em>non fissiamo un singolo valore di <span class="math inline">\(\theta\)</span></em>, ma consideriamo <em>tutti i valori plausibili</em> (secondo la posteriore) e <em>facciamo una media ponderata</em> delle previsioni condizionate <span class="math inline">\(p(y_i \mid \theta)\)</span>.</p>
<p>In altre parole, la distribuzione predittiva <span class="math inline">\(p(y_i \mid y)\)</span> è una <em>media (o somma continua)</em> delle distribuzioni <span class="math inline">\(p(y_i \mid \theta)\)</span>, <em>pesate</em> in base a quanto crediamo che ogni <span class="math inline">\(\theta\)</span> sia plausibile <em>dopo aver visto i dati</em>, cioè secondo <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<p>Poiché questo integrale raramente ha una soluzione analitica, in pratica lo approssimiamo con <span class="math inline">\(S\)</span> campioni MCMC dalla posteriore:</p>
<p><span class="math display">\[
p(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) .
\]</span> E quindi:</p>
<p><span class="math display">\[
\text{Log-score} \approx \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)})\right).
\]</span></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.1</strong></span> Supponiamo di avere tre valori dei parametri dalla posteriore: <span class="math inline">\(\theta^{(1)} = 0.3\)</span> con peso <span class="math inline">\(p = 0.2\)</span>, <span class="math inline">\(\theta^{(2)} = 0.5\)</span> con peso <span class="math inline">\(p = 0.5\)</span>, e <span class="math inline">\(\theta^{(3)} = 0.7\)</span> con peso <span class="math inline">\(p = 0.3\)</span>. Se la nuova osservazione è <span class="math inline">\(y = 3\)</span> su <span class="math inline">\(n = 5\)</span> tentativi, calcoliamo:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">3</span>, size <span class="op">=</span> <span class="fl">5</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span><span class="va">p_y_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">posterior_weights</span><span class="op">)</span></span>
<span><span class="va">log_score</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_y_given_y</span><span class="op">)</span></span>
<span><span class="va">log_score</span></span>
<span><span class="co">#&gt; [1] -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il log-score è circa -1.29. Un valore meno negativo sarebbe preferibile.</p>
</div>
</section></section><section id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" class="level2" data-number="74.3"><h2 data-number="74.3" class="anchored" data-anchor-id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati">
<span class="header-section-number">74.3</span> Expected Log Predictive Density (ELPD): guardare oltre i dati osservati</h2>
<p>Il log-score misura l’adattamento ai dati osservati, ma quello che davvero ci interessa è sapere se il modello predice bene <em>nuovi dati</em>. È qui che entra in gioco l’ELPD (<em>Expected Log Predictive Density</em>):</p>
<p><span class="math display">\[
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i}),
\]</span></p>
<p>dove <span class="math inline">\(y_i\)</span> è l’<span class="math inline">\(i\)</span>-esima osservazione e <span class="math inline">\(\mathbf{y}_{-i}\)</span> rappresenta tutte le osservazioni <em>eccetto</em> <span class="math inline">\(y_i\)</span>.</p>
<p>Tornando all’esempio del test per l’ansia: l’ELPD vi direbbe quanto bene il vostro modello riesce a predire il livello di ansia di uno studente utilizzando solo i dati degli altri studenti, senza “imbrogliare” guardando il vero livello di ansia di quello studente specifico.</p>
<p>Un <em>ELPD più alto</em> indica che il modello riesce a predire accuratamente anche osservazioni non utilizzate per stimare i parametri, suggerendo una buona capacità di generalizzazione.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Interpretazione:</strong> l’ELPD è un log-score calcolato <em>fuori campione</em>. Per ogni <span class="math inline">\(y_i\)</span>, lo escludiamo, adattiamo il modello agli altri dati e valutiamo la probabilità predittiva di <span class="math inline">\(y_i\)</span>.</p>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.2</strong></span> Per illustrare il calcolo dell’ELPD, vediamo un esempio semplice con un set di dati molto piccolo. Supponiamo di avere un dataset di tre osservazioni: <span class="math inline">\(y_1, y_2, y_3\)</span>. Supponiamo che il nostro modello stimi le probabilità per ciascuna osservazione in base a tutte le altre osservazioni, cioè utilizziamo la leave-one-out cross-validation (LOO-CV) per calcolare <span class="math inline">\(p(y_i \mid \mathbf{y}_{-i})\)</span>.</p>
<p>Immaginiamo che il modello produca le seguenti probabilità condizionali per ogni osservazione <span class="math inline">\(y_i\)</span>: <span class="math inline">\(p(y_1 \mid y_2, y_3) = 0.6\)</span>, <span class="math inline">\(p(y_2 \mid y_1, y_3) = 0.7\)</span>, <span class="math inline">\(p(y_3 \mid y_1, y_2) = 0.5\)</span>.</p>
<p>L’ELPD si calcola sommando i logaritmi di queste probabilità:</p>
<p><span class="math display">\[
\text{ELPD} = \log p(y_1 \mid y_2, y_3) + \log p(y_2 \mid y_1, y_3) + \log p(y_3 \mid y_1, y_2).
\]</span></p>
<p>Calcoliamo i logaritmi naturali di ciascuna probabilità: <span class="math inline">\(\log p(y_1 \mid y_2, y_3) = \log 0.6 \approx -0.5108\)</span>, <span class="math inline">\(\log p(y_2 \mid y_1, y_3) = \log 0.7 \approx -0.3567\)</span>, <span class="math inline">\(\log p(y_3 \mid y_1, y_2) = \log 0.5 \approx -0.6931\)</span>.</p>
<p>Sommiamo i logaritmi per ottenere l’ELPD:</p>
<p><span class="math display">\[
\text{ELPD} = -0.5108 + (-0.3567) + (-0.6931) = -1.5606.
\]</span></p>
<p>L’ELPD ottenuto è <span class="math inline">\(-1.5606\)</span>. In generale, valori più vicini a 0 o positivi indicano una migliore capacità predittiva del modello, poiché suggeriscono che le probabilità condizionali assegnate dal modello alle osservazioni lasciate fuori non sono troppo basse. Valori molto negativi indicherebbero che il modello ha assegnato probabilità molto basse alle osservazioni effettivamente osservate, suggerendo una scarsa capacità predittiva. L’ELPD è un modo efficace per valutare quanto bene un modello generalizza a nuovi dati, evitando l’overfitting.</p>
<p>Un altro modello con ELPD meno negativo sarebbe preferibile.</p>
</div>
<section id="lppd-vs-elpd-la-differenza-cruciale" class="level3" data-number="74.3.1"><h3 data-number="74.3.1" class="anchored" data-anchor-id="lppd-vs-elpd-la-differenza-cruciale">
<span class="header-section-number">74.3.1</span> LPPD vs ELPD: la differenza cruciale</h3>
<p>La distinzione tra LPPD (<em>Log Pointwise Predictive Density</em>) e ELPD è fondamentale per comprendere il problema dell’overfitting:</p>
<ul>
<li>La <em>LPPD</em> valuta quanto bene il modello predice i dati <em>osservati</em>, mentre l’<em>ELPD</em> valuta quanto bene il modello predice <em>nuovi dati</em>.</li>
<li>La LPPD tende a premiare modelli più complessi, creando il rischio di overfitting. L’ELPD, grazie alla tecnica <em>leave-one-out cross-validation</em>, penalizza l’overfitting e favorisce modelli che generalizzano meglio.</li>
</ul>
<p>Immaginate di allenare un modello per riconoscere volti: la LPPD misurerebbe quanto bene riconosce i volti del dataset di addestramento, mentre l’ELPD misurerebbe quanto bene riconosce volti nuovi, che non ha mai visto prima. Se LPPD è alto ma ELPD è basso, per esempio, questo significa che il modello ha “memorizzato” i volti ma non ha imparato davvero a riconoscere le caratteristiche generalizzabili.</p>
</section><section id="il-collegamento-con-la-divergenza-kl" class="level3" data-number="74.3.2"><h3 data-number="74.3.2" class="anchored" data-anchor-id="il-collegamento-con-la-divergenza-kl">
<span class="header-section-number">74.3.2</span> Il collegamento con la divergenza KL</h3>
<p>Sebbene <em>non possiamo calcolare <span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span> direttamente</em>, differenze in LPPD o ELPD <em>approssimano la differenza tra divergenze KL</em> dei modelli. Se il modello <span class="math inline">\(A\)</span> ha un ELPD più alto di quello di <span class="math inline">\(B\)</span>, possiamo concludere che <span class="math inline">\(A\)</span> è “più vicino” alla distribuzione vera dei dati.</p>
<p>In altre parole, <em>massimizzare l’ELPD <span class="math inline">\(\approx\)</span> minimizzare la divergenza KL</em>.</p>
<p>In sintesi, la distribuzione predittiva posteriore, la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e l’ELPD sono strumenti matematici che ci permettono di valutare la bontà di un modello statistico. La divergenza KL fornisce una misura teoricamente ideale di quanto un modello approssima la vera distribuzione dei dati, mentre l’ELPD offre un’alternativa praticabile che valuta la capacità del modello di fare previsioni su nuovi dati.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.3</strong></span> Consideriamo un secondo esempio per illustrare il concetto di ELPD utilizzando la distribuzione binomiale. Immaginiamo di condurre un esperimento in cui lanciamo una moneta 10 volte e contiamo il numero di volte in cui otteniamo testa. Supponiamo che la vera probabilità di ottenere testa sia 0.6.</p>
<p>La distribuzione reale dei dati segue una distribuzione binomiale con 10 lanci e probabilità di successo pari a 0.6: <span class="math inline">\(y \sim \text{Binomial}(10, 0.6)\)</span>.</p>
<p>Il nostro modello ipotizza che la probabilità di ottenere testa sia 0.5, cioè considera la moneta come equa: <span class="math inline">\(p(y \mid \theta) = \text{Binomial}(10, 0.5)\)</span>.</p>
<p>Ora procediamo al calcolo dell’ELPD. Nel codice R qui sotto, useremo <code>p</code> per rappresentare la <em>vera</em> probabilità di successo (ad esempio, <span class="math inline">\(p = 0.6\)</span>) e <code>q</code> per rappresentare la probabilità assunta dal <em>modello candidato</em> (es. <span class="math inline">\(q = 0.5\)</span>). Per evitare confusione con la notazione matematica <span class="math inline">\(p(y)\)</span> vista nel testo, teniamo presente che in questo contesto <code>p_y</code> indica la vera distribuzione dei dati, mentre <code>dbinom(..., prob = q)</code> rappresenta le previsioni del modello.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># vera probabilità di testa</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità stimata dal modello</span></span>
<span></span>
<span><span class="co"># Calcolo dell'ELPD per il modello con q = 0.5</span></span>
<span><span class="va">elpd</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">y</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>         <span class="co"># probabilità vera</span></span>
<span>  <span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span>  <span class="co"># log-probabilità del modello</span></span>
<span>  <span class="va">elpd</span> <span class="op">&lt;-</span> <span class="va">elpd</span> <span class="op">+</span> <span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD del modello che stima p = 0.5: %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD del modello che stima p = 0.5: -2.0549</span></span>
<span></span>
<span><span class="co"># Calcolo dell'ELPD per il modello vero (q = p)</span></span>
<span><span class="va">elpd_true</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">y</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span>  <span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">elpd_true</span> <span class="op">&lt;-</span> <span class="va">elpd_true</span> <span class="op">+</span> <span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD del modello vero (p = 0.6): %.4f\n"</span>, <span class="va">elpd_true</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD del modello vero (p = 0.6): -1.8536</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’output mostra che l’ELPD del modello vero è più alto (meno negativo), confermando che <span class="math inline">\(q = 0.6\)</span> è più predittivo di <span class="math inline">\(q = 0.5\)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 42%">
<col style="width: 35%">
</colgroup>
<thead><tr class="header">
<th>Concetto</th>
<th>Cosa misura</th>
<th>Vantaggio</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Divergenza KL</strong></td>
<td>Discrepanza tra modello e realtà</td>
<td>Fondamento teorico</td>
</tr>
<tr class="even">
<td><strong>Log-score</strong></td>
<td>Accuratezza su dati osservati</td>
<td>Intuitivo e semplice</td>
</tr>
<tr class="odd">
<td><strong>LPPD</strong></td>
<td>Log-score bayesiano</td>
<td>Tiene conto dell’incertezza</td>
</tr>
<tr class="even">
<td><strong>ELPD</strong></td>
<td>Log-score su dati non visti</td>
<td>Stima la generalizzabilità</td>
</tr>
</tbody>
</table>
<p>In pratica, <em>ELPD è il criterio più utile</em> per scegliere tra modelli bayesiani, perché stima la performance predittiva su nuovi dati, anche in assenza della distribuzione vera.</p>
</div>
</section></section><section id="leave-one-out-cross-validation-loo-cv-la-tecnica-per-stimare-lelpd" class="level2" data-number="74.4"><h2 data-number="74.4" class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv-la-tecnica-per-stimare-lelpd">
<span class="header-section-number">74.4</span> Leave-One-Out Cross-Validation (LOO-CV): la tecnica per stimare l’ELPD</h2>
<p>Nel paragrafo precedente abbiamo introdotto l’ELPD come misura della capacità di un modello di fare buone previsioni su dati futuri e non osservati. Tuttavia, calcolare l’ELPD in modo diretto non è possibile: per definizione, coinvolge dati che non abbiamo ancora osservato, generati da una distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span> che è sconosciuta.</p>
<p>Come possiamo stimare, in pratica, l’ELPD? Entra in gioco la <em>Leave-One-Out Cross-Validation (LOO-CV)</em>, una tecnica che consente di simulare la previsione di nuovi dati usando solo i dati osservati, fornendo così una stima empirica e affidabile dell’ELPD.</p>
<section id="cosè-la-loo-cv" class="level3" data-number="74.4.1"><h3 data-number="74.4.1" class="anchored" data-anchor-id="cosè-la-loo-cv">
<span class="header-section-number">74.4.1</span> Cos’è la LOO-CV?</h3>
<p>La <em>LOO-CV</em> è una tecnica che valuta la capacità predittiva di un modello simulando la situazione in cui ogni osservazione viene trattata come “nuova”. Il procedimento funziona come un esperimento mentale sistematico:</p>
<p>Prima si esclude una sola osservazione dal dataset, poi si adatta il modello sui dati rimanenti, quindi si calcola la densità predittiva del modello per l’osservazione esclusa, si ripete il processo per ogni osservazione nel dataset, e infine si somma il logaritmo delle densità predittive ottenute, ottenendo una stima detta <em>Expected Log Predictive Density</em> (ELPD):</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}) ,
\]</span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> rappresenta il dataset privato dell’osservazione <span class="math inline">\(i\)</span>.</p>
<p>Questa formula ha una struttura identica a quella dell’ELPD che abbiamo introdotto prima, ma con un’importante differenza: ogni termine viene calcolato <em>escludendo</em> l’osservazione corrispondente. In questo modo, la LOO-CV fornisce una stima <em>fuori campione</em> dell’accuratezza predittiva.</p>
</section><section id="perché-loo-cv-stima-lelpd" class="level3" data-number="74.4.2"><h3 data-number="74.4.2" class="anchored" data-anchor-id="perché-loo-cv-stima-lelpd">
<span class="header-section-number">74.4.2</span> Perché LOO-CV stima l’ELPD</h3>
<p>Abbiamo detto che l’ELPD può essere interpretata come:</p>
<p><span class="math display">\[
  \mathbb{E}_p[\log q(\tilde{y} \mid y)],
\]</span></p>
<p>dove <span class="math inline">\(q(\tilde{y} \mid y)\)</span> è la distribuzione predittiva del modello. Tuttavia, non possiamo calcolare questa aspettativa perché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta. L’idea della LOO-CV è semplice ma potente:</p>
<p>Se consideriamo ogni osservazione <span class="math inline">\(y_i\)</span> come se fosse un nuovo dato generato da <span class="math inline">\(p(\tilde{y})\)</span>, allora la media sui dati osservati può approssimare l’aspettativa su <span class="math inline">\(p\)</span>.</p>
<p>Quindi:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid y_{-i}) \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Questo significa che <em>massimizzare l’ELPD stimato con la LOO-CV equivale a minimizzare la divergenza KL tra la distribuzione vera e quella predittiva del modello</em>, almeno nella parte che possiamo stimare empiricamente.</p>
<p>In termini concreti, LOO-CV ci dice quanto bene il modello predice ogni osservazione <em>senza averla vista</em> durante l’adattamento e quanto bene il modello si aspetterebbe di performare su dati nuovi.</p>
<p>Riepilogando: l’ELPD è una misura ideale della capacità predittiva del modello, la LOO-CV fornisce una stima empirica dell’ELPD, l’ELPD stimato via LOO-CV permette di confrontare modelli in modo robusto, e massimizzare l’ELPD (LOO) equivale a minimizzare la divergenza KL nella parte che possiamo osservare.</p>
</section></section><section id="il-fondamento-teorico-loo-cv-e-la-divergenza-di-kullback-leibler" class="level2" data-number="74.5"><h2 data-number="74.5" class="anchored" data-anchor-id="il-fondamento-teorico-loo-cv-e-la-divergenza-di-kullback-leibler">
<span class="header-section-number">74.5</span> Il fondamento teorico: LOO-CV e la divergenza di Kullback-Leibler</h2>
<p>Perché l’ELPD calcolata con LOO-CV è una buona misura per confrontare modelli? Per rispondere, dobbiamo approfondire la connessione con la misura formale della distanza tra un modello predittivo e la distribuzione vera dei dati: la divergenza di Kullback-Leibler.</p>
<section id="la-divergenza-kl-come-criterio-di-confronto" class="level3" data-number="74.5.1"><h3 data-number="74.5.1" class="anchored" data-anchor-id="la-divergenza-kl-come-criterio-di-confronto">
<span class="header-section-number">74.5.1</span> La divergenza KL come criterio di confronto</h3>
<p>Immaginiamo che i dati futuri siano generati da una distribuzione vera, <span class="math inline">\(p(\tilde{y})\)</span>, sconosciuta. Un modello predittivo bayesiano genera una distribuzione <span class="math inline">\(q(\tilde{y} \mid y)\)</span>.</p>
<p>Una misura naturale della distanza tra queste due distribuzioni è la divergenza di Kullback-Leibler (KL):</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_{p} \left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Il primo termine, <span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span>, è l’entropia della distribuzione vera. Non dipende dal modello. Il secondo termine, <span class="math inline">\(\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]\)</span>, misura l’accuratezza predittiva del modello, in media rispetto alla distribuzione vera.</p>
</section><section id="il-confronto-tra-modelli-il-termine-costante-si-elide" class="level3" data-number="74.5.2"><h3 data-number="74.5.2" class="anchored" data-anchor-id="il-confronto-tra-modelli-il-termine-costante-si-elide">
<span class="header-section-number">74.5.2</span> Il confronto tra modelli: il termine costante si elide</h3>
<p>Supponiamo di voler confrontare due modelli, <span class="math inline">\(q_1\)</span> e <span class="math inline">\(q_2\)</span>. La differenza nelle rispettive divergenze KL rispetto a <span class="math inline">\(p(\tilde{y})\)</span> è:</p>
<p><span class="math display">\[
\begin{align}
\Delta &amp;= D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) \notag\\
&amp;= \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)] \right]
  - \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] \right] .
\end{align}
\]</span></p>
<p>Il primo termine <span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span> è comune a entrambi i modelli e quindi si annulla nel confronto. Resta soltanto la differenza tra le <em>accuratezze predittive</em> dei due modelli:</p>
<p><span class="math display">\[
\Delta = \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)]
\]</span></p>
<p>In altre parole, <em>vince il modello che predice meglio i dati futuri</em>.</p>
</section><section id="come-stimare-laccuratezza-predittiva" class="level3" data-number="74.5.3"><h3 data-number="74.5.3" class="anchored" data-anchor-id="come-stimare-laccuratezza-predittiva">
<span class="header-section-number">74.5.3</span> Come stimare l’accuratezza predittiva?</h3>
<p>Il problema è che non conosciamo <span class="math inline">\(p(\tilde{y})\)</span>, quindi non possiamo calcolare l’aspettativa vera. Tuttavia, possiamo usare una stima empirica: se assumiamo che le osservazioni <span class="math inline">\(y_1, \dots, y_n\)</span> siano state generate da <span class="math inline">\(p(\tilde{y})\)</span>, allora possiamo approssimare l’aspettativa come una media sui dati osservati. Questa idea è alla base della <em>Leave-One-Out Cross-Validation</em>.</p>
</section><section id="loo-cv-come-stima-empirica-dellaccuratezza-predittiva" class="level3" data-number="74.5.4"><h3 data-number="74.5.4" class="anchored" data-anchor-id="loo-cv-come-stima-empirica-dellaccuratezza-predittiva">
<span class="header-section-number">74.5.4</span> LOO-CV come stima empirica dell’accuratezza predittiva</h3>
<p>La <em>Leave-One-Out Cross-Validation</em> (LOO-CV) ci fornisce una <em>stima empirica</em> di questa aspettazione. In pratica, stimiamo:</p>
<p><span id="eq-elpd-loo"><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i})
\approx \mathbb{E}_{p}[\log q(\tilde{y} \mid y)] ,
\tag{74.1}\]</span></span></p>
<p>dove <span class="math inline">\(y_i\)</span> è una delle osservazioni effettivamente raccolte, <span class="math inline">\(\mathbf{y}_{-i}\)</span> sono tutti i dati tranne <span class="math inline">\(y_i\)</span>, e <span class="math inline">\(p(y_i \mid \mathbf{y}_{-i})\)</span> è la densità predittiva calcolata escludendo <span class="math inline">\(y_i\)</span> durante l’addestramento.</p>
<p>Ogni osservazione viene trattata come “nuova”, e il modello è valutato sulla sua capacità di predirla senza averla “vista” durante l’adattamento. In questo modo, simuliamo una previsione fuori campione.</p>
<p>Questa procedura approssima l’attesa rispetto a <span class="math inline">\(p(\tilde{y})\)</span>:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Quindi: <em>massimizzare l’ELPD equivale a minimizzare la divergenza KL</em>, nella parte che possiamo stimare empiricamente.</p>
</section><section id="confronto-tra-modelli-tramite-elpd" class="level3" data-number="74.5.5"><h3 data-number="74.5.5" class="anchored" data-anchor-id="confronto-tra-modelli-tramite-elpd">
<span class="header-section-number">74.5.5</span> Confronto tra modelli tramite ELPD</h3>
<p>Quando confrontiamo due modelli <span class="math inline">\(M_1\)</span> e <span class="math inline">\(M_2\)</span>, vogliamo sapere quale dei due è più vicino alla distribuzione vera. Non possiamo calcolare direttamente la differenza tra <span class="math inline">\(D_{\text{KL}}(p \parallel q_1)\)</span> e <span class="math inline">\(D_{\text{KL}}(p \parallel q_2)\)</span>, ma possiamo confrontare le loro <em>ELPD</em>:</p>
<p><span class="math display">\[
\Delta \text{ELPD} = \text{ELPD}(M_1) - \text{ELPD}(M_2) \approx \mathbb{E}_p[\log q_1(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_2(\tilde{y} \mid y)].
\]</span></p>
<p>La differenza tra le ELPD stimate è quindi un’approssimazione alla differenza tra le divergenze KL. Inoltre, possiamo anche calcolare un <em>errore standard</em> su questa differenza, per valutare l’incertezza nel confronto.</p>
</section><section id="riepilogo-intuitivo" class="level3" data-number="74.5.6"><h3 data-number="74.5.6" class="anchored" data-anchor-id="riepilogo-intuitivo">
<span class="header-section-number">74.5.6</span> Riepilogo intuitivo</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 30%">
<col style="width: 12%">
<col style="width: 28%">
</colgroup>
<thead><tr class="header">
<th>Concetto</th>
<th>Cosa misura</th>
<th>Si può calcolare?</th>
<th>Uso principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span></td>
<td>Distanza tra modello e realtà</td>
<td>❌ (p sconosciuta)</td>
<td>Fondamento teorico</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{E}_p[\log q(\tilde{y} \mid y)]\)</span></td>
<td>Accuratezza predittiva media</td>
<td>❌ (p sconosciuta)</td>
<td>Quantità chiave per confrontare modelli</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\text{ELPD}_{\text{LOO}}\)</span></td>
<td>Stima empirica dell’accuratezza predittiva</td>
<td>✅</td>
<td>Criterio di confronto robusto</td>
</tr>
</tbody>
</table>
<p>In sintesi, anche se non conosciamo la vera distribuzione dei dati, possiamo <em>usare l’ELPD stimata con LOO-CV come proxy della divergenza KL</em> e quindi come strumento pratico e teoricamente fondato per selezionare modelli che generalizzano bene.</p>
</section><section id="elpd-loo-e-il-problema-delloverfitting" class="level3" data-number="74.5.7"><h3 data-number="74.5.7" class="anchored" data-anchor-id="elpd-loo-e-il-problema-delloverfitting">
<span class="header-section-number">74.5.7</span> ELPD-LOO e il problema dell’overfitting</h3>
<p>Valutare un modello sugli stessi dati con cui è stato addestrato può portare a sovrastimare la sua capacità predittiva, specialmente per modelli complessi. È come giudicare le capacità di uno studente facendogli ripetere esattamente gli stessi esercizi che ha già risolto durante lo studio: otterrà sicuramente un buon voto, ma questo non ci dice nulla su quanto bene risolverà problemi nuovi.</p>
<p>La LOO-CV evita questo problema: ogni punto viene lasciato fuori a turno, e il modello viene valutato solo su dati che non ha “visto”. In questo modo, l’ELPD-LOO è una <em>valutazione predittiva fuori campione</em>, molto meno sensibile all’overfitting.</p>
<p>Grazie a metodi efficienti come il <em>Pareto-smoothed importance sampling (PSIS)</em>, possiamo stimare l’ELPD-LOO <em>senza dover riadattare il modello <span class="math inline">\(n\)</span> volte</em>. Questo è implementato nella funzione <code>loo()</code> in R, compatibile con pacchetti come <code>brms</code>, <code>rstanarm</code>, e <code>loo</code>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.4</strong></span> Supponiamo che la vera distribuzione sia binomiale con probabilità <span class="math inline">\(p = 0.6\)</span>, ma il modello assume <span class="math inline">\(q = 0.5\)</span>. Calcoliamo l’ELPD come log-score atteso su dati generati da <span class="math inline">\(p\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>  <span class="co"># vera probabilità</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># modello</span></span>
<span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># elpd è l'implementazione computazionale di ELPD</span></span>
<span><span class="va">elpd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD stimato (modello q = 0.5): %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD stimato (modello q = 0.5): -2.0549</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Questo valore è una stima dell’<em>expected log predictive density</em> di un modello che usa <span class="math inline">\(q = 0.5\)</span> per prevedere dati generati da <span class="math inline">\(p = 0.6\)</span>.</p>
</div>
</section></section><section id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="level2" data-number="74.6"><h2 data-number="74.6" class="anchored" data-anchor-id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl">
<span class="header-section-number">74.6</span> Criteri di Informazione come Approssimazioni della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span>
</h2>
<p>Oltre alla Leave-One-Out Cross-Validation, esistono altri approcci per stimare la qualità predittiva di un modello. Molti di questi derivano, direttamente o indirettamente, dalla <em>divergenza di Kullback-Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che come abbiamo visto rappresenta la distanza tra la distribuzione vera e quella stimata dal modello.</p>
<p>Poiché la distribuzione vera è generalmente ignota, sono stati proposti diversi <em>criteri di informazione</em> che mirano a bilanciare due obiettivi opposti:</p>
<ol type="1">
<li>
<em>Bontà di adattamento</em> del modello ai dati osservati;</li>
<li>
<em>Penalizzazione della complessità</em> del modello, per evitare l’overfitting.</li>
</ol>
<p>Tra i più noti troviamo: <em>MSE</em>, <em>AIC</em>, <em>BIC</em>, e <em>WAIC</em>. Vediamoli uno per uno.</p>
<section id="errore-quadratico-medio-mse" class="level3" data-number="74.6.1"><h3 data-number="74.6.1" class="anchored" data-anchor-id="errore-quadratico-medio-mse">
<span class="header-section-number">74.6.1</span> Errore Quadratico Medio (MSE)</h3>
<p>L’<em>Errore Quadratico Medio</em> è un criterio semplice e intuitivo che misura la media delle differenze al quadrato tra valori osservati e valori previsti:</p>
<p><span class="math display">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span></p>
<ul>
<li>Minore è l’MSE, migliore è la capacità del modello di prevedere i dati osservati.</li>
<li>Tuttavia, l’MSE <em>non penalizza la complessità del modello</em> e può favorire modelli troppo flessibili.</li>
</ul>
<p>È quindi utile come indicatore di accuratezza, ma <em>non è sufficiente</em> per selezionare tra modelli con diversa complessità.</p>
</section><section id="akaike-information-criterion-aic" class="level3" data-number="74.6.2"><h3 data-number="74.6.2" class="anchored" data-anchor-id="akaike-information-criterion-aic">
<span class="header-section-number">74.6.2</span> Akaike Information Criterion (AIC)</h3>
<p>L’<em>AIC</em> nasce da un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e stima quanto informazione si perde usando un modello per descrivere i dati:</p>
<p><span class="math display">\[
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k
\]</span></p>
<ul>
<li>
<span class="math inline">\(\hat{\theta}\_{\text{MLE}}\)</span>: stima massima di verosimiglianza dei parametri;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri del modello.</li>
</ul>
<p>Interpretazione:</p>
<ul>
<li>il primo termine misura quanto bene il modello si adatta ai dati;</li>
<li>il secondo termine penalizza la complessità del modello.</li>
</ul>
<p>Un <em>AIC più basso</em> indica un compromesso migliore tra accuratezza e semplicità.</p>
<p>Limitazioni:</p>
<ul>
<li>basato su assunzioni asintotiche (funziona meglio con campioni grandi);</li>
<li>utilizza solo <em>stime puntuali</em>, ignorando l’incertezza dei parametri;</li>
<li>non è pienamente bayesiano.</li>
</ul></section><section id="bayesian-information-criterion-bic" class="level3" data-number="74.6.3"><h3 data-number="74.6.3" class="anchored" data-anchor-id="bayesian-information-criterion-bic">
<span class="header-section-number">74.6.3</span> Bayesian Information Criterion (BIC)</h3>
<p>Il <em>Bayesian Information Criterion</em> (BIC) è un criterio di selezione del modello che, come l’AIC, cerca un compromesso tra <strong>bontà di adattamento ai dati</strong> e <strong>complessità del modello</strong>. Tuttavia, rispetto all’AIC, il BIC applica una <strong>penalizzazione più severa</strong> alla complessità, rendendolo particolarmente adatto a situazioni con grandi quantità di dati.</p>
<p>La formula del BIC è:</p>
<p><span class="math display">\[
\text{BIC} = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(y \mid \hat{\theta})\)</span> è la <em>massima verosimiglianza</em> del modello, ovvero la probabilità dei dati osservati valutata nel punto <span class="math inline">\(\hat{\theta}\)</span> che massimizza la funzione di verosimiglianza (massimo a posteriori in caso di priori piatti);</li>
<li>
<span class="math inline">\(n\)</span> è il numero di osservazioni indipendenti;</li>
<li>
<span class="math inline">\(k\)</span> è il numero di parametri stimati nel modello.</li>
</ul>
<p>Il BIC può anche essere scritto come:</p>
<p><span class="math display">\[
\text{BIC} = \ln(n) \cdot k - 2 \ln L
\]</span></p>
<p>dove <span class="math inline">\(L = p(y \mid \hat{\theta})\)</span> è, appunto, la massima verosimiglianza.</p>
<p>Interpretazione:</p>
<ul>
<li>il primo termine, <span class="math inline">\(-2 \log p(y \mid \hat{\theta})\)</span>, misura quanto bene il modello si adatta ai dati;</li>
<li>il secondo termine, <span class="math inline">\(\log(n) \cdot k\)</span>, è una penalizzazione che aumenta con il numero di parametri e con la dimensione del campione.</li>
</ul>
<p>Un <em>valore più basso del BIC</em> indica un modello preferibile, cioè un miglior compromesso tra accuratezza e parsimonia.</p>
<p>Vantaggi:</p>
<ul>
<li>favorisce <em>modelli più semplici</em>, specialmente quando il numero di osservazioni <span class="math inline">\(n\)</span> è elevato;</li>
<li>ha una <em>giustificazione teorica bayesiana</em>: sotto ipotesi regolari e prior non informativi, il BIC approssima il logaritmo della <em>marginal likelihood</em> del modello (e quindi del modello bayesiano integrato).</li>
</ul>
<p>Limiti:</p>
<ul>
<li>si basa su <em>assunzioni forti</em>, tra cui l’indipendenza delle osservazioni, modelli regolari (es. parametri identificabili) e <em>prior</em> deboli o non informativi;</li>
<li>può <em>sottoselezionare</em> modelli utili in presenza di piccoli campioni, dati rumorosi e strutture complesse (es. modelli gerarchici, a posteriori multimodali).</li>
</ul></section><section id="widely-applicable-information-criterion-waic" class="level3" data-number="74.6.4"><h3 data-number="74.6.4" class="anchored" data-anchor-id="widely-applicable-information-criterion-waic">
<span class="header-section-number">74.6.4</span> Widely Applicable Information Criterion (WAIC)</h3>
<p>Il <em>WAIC</em> è una generalizzazione bayesiana dell’AIC, ed è progettato per:</p>
<ul>
<li>tenere conto dell’intera distribuzione a posteriori dei parametri;</li>
<li>fornire una <em>stima fully Bayesian</em> dell’accuratezza predittiva.</li>
</ul>
<p><span class="math display">\[
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \text{Var}_{\theta^{(s)}} \left( \log p(y_i \mid \theta^{(s)}) \right)
\right]
\]</span></p>
<ul>
<li>
<span class="math inline">\(S\)</span>: numero di campioni dalla distribuzione a posteriori;</li>
<li>
<span class="math inline">\(\theta^{(s)}\)</span>: s-esimo campione dalla posteriori;</li>
<li>la seconda somma rappresenta il numero <em>effettivo</em> di parametri, basato sulla variabilità della log-verosimiglianza.</li>
</ul>
<p>Caratteristiche principali:</p>
<ul>
<li>utilizza campioni MCMC → <em>adatto anche a modelli non regolari</em>;</li>
<li>fornisce un’<em>approssimazione del log score</em> su dati nuovi;</li>
<li>migliore dell’AIC per modelli bayesiani complessi.</li>
</ul>
<p>Riepilogo Comparativo</p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead><tr class="header">
<th>Criterio</th>
<th>Tipo</th>
<th>Penalizza la complessità?</th>
<th>Usa stime puntuali?</th>
<th>Supporta Bayesian MCMC?</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSE</td>
<td>Frequentista</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>AIC</td>
<td>Frequentista</td>
<td>✅ (modesta)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="odd">
<td>BIC</td>
<td>Frequentista/Bayesiano</td>
<td>✅ (forte)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>WAIC</td>
<td>Bayesiano</td>
<td>✅ (effettiva)</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr class="odd">
<td>LOO-CV</td>
<td>Bayesiano</td>
<td>✅ (empirica)</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table></section></section><section id="riflessioni-conclusive" class="level2" data-number="74.7"><h2 data-number="74.7" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">74.7</span> Riflessioni Conclusive</h2>
<p>La selezione del modello in ambito bayesiano si basa su una domanda chiave: <em>quanto bene il modello predice nuovi dati?</em></p>
<p>A questo scopo, il criterio più solido è l’<em>Expected Log Predictive Density</em> (ELPD), che valuta quanto la distribuzione predittiva del modello si avvicina alla (sconosciuta) distribuzione vera dei dati. Sebbene la <em>divergenza di Kullback-Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>) rappresenti una misura ideale per confrontare distribuzioni, il suo uso diretto è raramente possibile perché <span class="math inline">\(p_{\text{vera}}(y)\)</span> è ignota. Tuttavia, massimizzare l’ELPD equivale a minimizzare la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> rispetto alla vera generatrice: entrambi puntano a rappresentare accuratamente la realtà sottostante.</p>
<p>Poiché l’ELPD non è calcolabile in forma esatta, esistono <em>approssimazioni pratiche</em>:</p>
<ul>
<li>
<em>LOO-CV</em> (Leave-One-Out Cross-Validation) è oggi lo strumento più robusto per stimare l’ELPD. Valuta iterativamente ogni osservazione come “nuova” e fornisce una stima attendibile della capacità del modello di generalizzare.</li>
<li>
<em>WAIC</em> offre una stima simile, ma basata interamente sulla distribuzione a posteriori, senza riadattare il modello.</li>
<li>
<em>AIC</em> e <em>BIC</em>, pur derivando da un framework frequentista e basandosi su stime puntuali, offrono soluzioni rapide e utili in contesti semplici.</li>
<li>
<em>MSE</em>, infine, misura solo la distanza tra le previsioni e i valori osservati, ma <em>non penalizza la complessità</em>, e quindi è inadatto alla selezione del modello.</li>
</ul>
<p>Nel confronto tra modelli, la <em>differenza tra valori di ELPD</em> (stimata tramite LOO-CV o WAIC) può essere accompagnata da un <em>errore standard</em> che aiuta a quantificare l’incertezza della differenza. Una regola pratica: se la differenza tra modelli è almeno <em>due volte maggiore</em> dell’errore standard, è probabile che uno dei due modelli sia davvero superiore, in termini predittivi.</p>
<p>Conclusione sintetica:</p>
<ul>
<li>
<em>la buona statistica non è quella che spiega il passato, ma quella che anticipa il futuro</em>;</li>
<li>la <em>divergenza KL</em> ci dà una misura teorica della distanza tra modello e realtà;</li>
<li>l’<em>ELPD</em>, stimato via <em>LOO-CV</em> o <em>WAIC</em>, fornisce una misura pratica della capacità del modello di prevedere nuovi dati;</li>
<li>la <em>selezione del modello ottimale</em> richiede equilibrio tra accuratezza, generalizzazione e parsimonia.</li>
</ul>
<p>Con questi strumenti, possiamo scegliere modelli che <em>catturano i pattern reali nei dati senza farsi ingannare dal rumore</em>, garantendo affidabilità e interpretabilità anche in contesti complessi.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] thematic_0.1.7   MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.11.0      </span></span>
<span><span class="co">#&gt;  [5] gridExtra_2.3    patchwork_1.3.1  bayesplot_1.13.0 psych_2.5.6     </span></span>
<span><span class="co">#&gt;  [9] scales_1.4.0     markdown_2.0     knitr_1.50       lubridate_1.9.4 </span></span>
<span><span class="co">#&gt; [13] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.1.0     </span></span>
<span><span class="co">#&gt; [17] readr_2.1.5      tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2   </span></span>
<span><span class="co">#&gt; [21] tidyverse_2.0.0  rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] generics_0.1.4     stringi_1.8.7      lattice_0.22-7    </span></span>
<span><span class="co">#&gt;  [4] hms_1.1.3          digest_0.6.37      magrittr_2.0.3    </span></span>
<span><span class="co">#&gt;  [7] evaluate_1.0.4     grid_4.5.1         timechange_0.3.0  </span></span>
<span><span class="co">#&gt; [10] RColorBrewer_1.1-3 fastmap_1.2.0      rprojroot_2.1.0   </span></span>
<span><span class="co">#&gt; [13] jsonlite_2.0.0     mnormt_2.1.1       cli_3.6.5         </span></span>
<span><span class="co">#&gt; [16] rlang_1.1.6        withr_3.0.2        tools_4.5.1       </span></span>
<span><span class="co">#&gt; [19] parallel_4.5.1     tzdb_0.5.0         pacman_0.5.1      </span></span>
<span><span class="co">#&gt; [22] vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   </span></span>
<span><span class="co">#&gt; [25] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.11.0     </span></span>
<span><span class="co">#&gt; [28] gtable_0.3.6       glue_1.8.0         xfun_0.52         </span></span>
<span><span class="co">#&gt; [31] tidyselect_1.2.1   rstudioapi_0.17.1  farver_2.1.2      </span></span>
<span><span class="co">#&gt; [34] htmltools_0.5.8.1  nlme_3.1-168       rmarkdown_2.29    </span></span>
<span><span class="co">#&gt; [37] compiler_4.5.1</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/formal_models/introduction.html" class="pagination-link" aria-label="Introduzione">
        <span class="nav-page-text">Introduzione</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>