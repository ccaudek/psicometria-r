# Relazioni tra variabili {#sec-correlation}

::: callout-important
## In questo capitolo imparerai a

- comprendere e calcolare la correlazione e la covarianza;
- interpretare correttamente gli indici di correlazione e covarianza nel contesto dell‚Äôanalisi dei dati.
:::

::: callout-tip
## Prerequisiti

- Leggere l'introduzione dell'articolo "The curious case of the cross-sectional correlation" [@hamaker2024curious].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(readr)
```
:::

## Introduzione 

Nonostante sia un'operazione di base, l'analisi delle associazioni tra variabili rappresenta uno degli aspetti pi√π controversi nell'ambito dell'analisi dei dati psicologici. Sebbene possa sembrare un passaggio naturale dopo l'analisi univariata, questo processo solleva numerose questioni metodologiche e concettuali.

Tradizionalmente, in psicologia, l'analisi delle associazioni tra variabili √® stata considerata come l'obiettivo finale del processo di ricerca. Questa visione si basa sull'idea che la descrizione delle relazioni tra variabili fornisca una spiegazione esaustiva dei fenomeni psicologici. Tale approccio trova le sue radici storiche nel pensiero di Karl Pearson (1911), il quale sosteneva che la spiegazione scientifica si esaurisse una volta delineate le associazioni tra le variabili osservate:

> Quanto spesso, quando √® stato osservato un nuovo fenomeno, sentiamo che viene posta la domanda: 'qual √® la sua causa?'. Questa √® una domanda a cui potrebbe essere assolutamente impossibile rispondere. Invece, pu√≤ essere pi√π facile rispondere alla domanda: 'in che misura altri fenomeni sono associati con esso?'. Dalla risposta a questa seconda domanda possono risultare molte preziose conoscenze.

Sebbene sia indubbio che rispondere alla seconda domanda posta da Pearson sia relativamente semplice, √® altres√¨ evidente che la nostra comprensione di un fenomeno non pu√≤ dipendere unicamente dalle informazioni fornite dalle correlazioni.

In contrasto con questa visione tradizionale, la "Causal Revolution" propone un paradigma radicalmente diverso secondo il quale le associazioni tra variabili sono considerate come epifenomeni, mentre l'obiettivo principale della ricerca √® l'identificazione e la comprensione delle relazioni causali: per comprendere veramente i fenomeni psicologici √® essenziale indagare le cause sottostanti, andando oltre la mera descrizione delle associazioni.

La discussione dei metodi utilizzati per individuare le relazioni causali sar√† trattata successivamente. In questo capitolo, ci concentreremo sui concetti statistici fondamentali necessari per descrivere le associazioni lineari tra variabili. √à importante sottolineare che, sebbene esistano indici statistici per quantificare associazioni non lineari, la maggior parte degli psicologi si limita all'utilizzo di indici lineari.

Nel linguaggio comune, termini come "dipendenza", "associazione" e "correlazione" vengono spesso usati in modo intercambiabile. Tuttavia, da un punto di vista tecnico, √® importante distinguere questi concetti:

1. **Associazione**: questo termine indica una relazione generale tra variabili, dove la conoscenza del valore di una variabile fornisce informazioni su un'altra.
2. **Correlazione**: descrive una relazione specifica e quantificabile, indicando se due variabili tendono a variare insieme in modo sistematico. Ad esempio, in una correlazione positiva, se $X > \mu_X$, √® probabile che anche $Y > \mu_Y$. La correlazione specifica il segno e l'intensit√† di una relazione lineare.
3. **Dipendenza**: indica una relazione causale tra le variabili, dove la variazione della variabile causale porta probabilisticamente alla variazione della variabile dipendente.

√à cruciale comprendere che non tutte le associazioni sono correlazioni e, soprattutto, che la correlazione non implica necessariamente causalit√†. Questa distinzione √® fondamentale per interpretare correttamente i dati e evitare conclusioni errate sulle relazioni tra variabili.

In questo capitolo, esamineremo due misure statistiche fondamentali per valutare la relazione lineare tra due variabili: la covarianza e la correlazione. Questi indici ci permettono di descrivere il grado e la direzione dell'associazione lineare tra variabili, quantificando come queste variano congiuntamente.

## I dati grezzi

Per illustrare la correlazione e la covarianza, analizzeremo i dati raccolti da @zetsche_2019future in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello specifico, i ricercatori si sono proposti di determinare se gli individui depressi sviluppano aspettative accurate riguardo al loro umore futuro o se tali aspettative sono distortamente negative.

Uno dei loro studi ha coinvolto un campione di 30 soggetti con almeno un episodio depressivo maggiore, confrontati con un gruppo di controllo composto da 37 individui sani. La misurazione del livello di depressione √® stata effettuata tramite il *Beck Depression Inventory* (BDI-II).

Il BDI-II √® uno strumento di autovalutazione utilizzato per valutare la gravit√† della depressione in adulti e adolescenti. Il test √® stato sviluppato per identificare e misurare l'intensit√† dei sintomi depressivi sperimentati nelle ultime due settimane. I 21 item del test sono valutati su una scala a 4 punti, dove 0 rappresenta il grado pi√π basso e 3 il grado pi√π elevato di sintomatologia depressiva. 

Nell'esercizio successivo, ci proponiamo di analizzare i punteggi di depressione BDI-II nel campione di dati fornito da @zetsche_2019future.

## Definizione delle relazioni tra variabili

Nel contesto delle indagini statistiche, spesso non ci limitiamo a esaminare la distribuzione di una singola variabile. Invece, il nostro interesse si concentra sulla relazione che emerge nei dati tra due o pi√π variabili. Ma cosa significa esattamente quando diciamo che due variabili hanno una relazione?

Per comprendere ci√≤, prendiamo ad esempio l'altezza e l'et√† tra un gruppo di bambini. In generale, √® possibile notare che all'aumentare dell'et√† di un bambino, aumenta anche la sua altezza. Pertanto, conoscere l'et√† di un bambino, ad esempio tredici anni, e l'et√† di un altro, sei anni, ci fornisce un'indicazione su quale dei due bambini sia pi√π alto.

Nel linguaggio statistico, definiamo questa relazione tra altezza e et√† come positiva, il che significa che all'aumentare dei valori di una delle variabili (in questo caso, l'et√†), ci aspettiamo di vedere valori pi√π elevati anche nell'altra variabile (l'altezza). Tuttavia, esistono anche relazioni negative, in cui l'aumento di una variabile √® associato a un diminuzione dell'altra (ad esempio, pi√π et√† √® correlata a meno pianto).

Non si tratta solo di relazioni positive o negative; ci sono anche situazioni in cui le variabili non hanno alcuna relazione tra loro, definendo cos√¨ una relazione nulla. Inoltre, le relazioni possono variare nel tempo, passando da positive a negative o da fortemente positive a appena positiva. In alcuni casi, una delle variabili pu√≤ essere categorica, rendendo difficile parlare di "maggioranza" o "minoranza" ma piuttosto di "differente" (ad esempio, i bambini pi√π grandi potrebbero semplicemente avere diverse preferenze rispetto ai bambini pi√π piccoli, senza necessariamente essere "migliori" o "peggiori").

## Grafico a dispersione {#sec-scatter-plot}

Il metodo pi√π diretto per visualizzare la relazione tra due variabili continue √® tramite un grafico a dispersione, comunemente noto come "scatterplot". Questo tipo di diagramma rappresenta le coppie di dati ottenute da due variabili, posizionandole sull'asse delle ascisse (orizzontale) e delle ordinate (verticale).

Per rendere l'idea pi√π chiara, consideriamo i dati dello studio condotto da @zetsche_2019future, in cui i ricercatori hanno utilizzato due scale psicometriche, il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D), per misurare il livello di depressione nei partecipanti. Il BDI-II √® uno strumento di autovalutazione che valuta la presenza e l'intensit√† dei sintomi depressivi in pazienti adulti e adolescenti con diagnosi psichiatrica, mentre la CES-D √® una scala di autovalutazione progettata per misurare i sintomi depressivi sperimentati nella settimana precedente nella popolazione generale, in particolare negli adolescenti e nei giovani adulti. Poich√© entrambe le scale misurano lo stesso costrutto, ovvero la depressione, ci aspettiamo una relazione tra i punteggi ottenuti dal BDI-II e dalla CES-D. Un diagramma a dispersione ci consente di esaminare questa relazione in modo visuale e intuitivo.

```{r}
# Leggi i dati dal file CSV
df <- rio::import(here::here("data", "data.mood.csv"))

# Seleziona le colonne di interesse
df <- df |>
  dplyr::select("esm_id", "group", "bdi", "cesd_sum")

# Rimuovi le righe duplicate
df <- df[!duplicated(df), ]

# Rimuovi le righe con valori mancanti nella colonna "bdi"
df <- df[!is.na(df$bdi), ]
```

Posizionando i valori del BDI-II sull'asse delle ascisse e quelli del CES-D sull'asse delle ordinate, ogni punto sul grafico rappresenta un individuo, di cui conosciamo il livello di depressione misurato dalle due scale. √à evidente che i valori delle scale BDI-II e CES-D non possono coincidere per due motivi principali: (1) la presenza di errori di misurazione e (2) l'utilizzo di unit√† di misura arbitrarie per le due variabili. L'errore di misurazione √® una componente inevitabile che influisce in parte su qualsiasi misurazione, ed √® particolarmente rilevante in psicologia, dove la precisione degli strumenti di misurazione √® generalmente inferiore rispetto ad altre discipline, come la fisica. Il secondo motivo per cui i valori delle scale BDI-II e CES-D non possono essere identici √® che l'unit√† di misura della depressione √® una questione arbitraria e non standardizzata. Tuttavia, nonostante le differenze dovute agli errori di misurazione e all'uso di unit√† di misura diverse, ci aspettiamo che, se le due scale misurano lo stesso costrutto (la depressione), i valori prodotti dalle due scale dovrebbero essere associati linearmente tra di loro. Per comprendere meglio il concetto di "associazione lineare", √® possibile esaminare i dati attraverso l'utilizzo di un diagramma a dispersione.

```{r}
#| echo: false
#| tags: [hide-input]

# Crea uno scatterplot con colori diversi per i due gruppi
# Separate data by group
mdd_data <- df[df$group == "mdd", ]
ctl_data <- df[df$group == "ctl", ]

# Calculate linear regression coefficients
coeff_combined <- lm(cesd_sum ~ bdi, data = df)$coefficients

# Define the linear regression line
line_combined <- function(x) coeff_combined[1] + coeff_combined[2] * x

# Generate x values for the regression line
x_values <- seq(min(df$bdi), max(df$bdi), length.out = 100)
okabe_ito_colors <- c("Pazienti" = "#E69F00",  # Orange
                      "Controlli" = "#56B4E9") # Sky Blue

# Plot scatter plot and regression line
ggplot() +
  geom_point(
    data = mdd_data,
    aes(x = bdi, y = cesd_sum, color = "Pazienti"), alpha = 0.7
  ) +
  geom_point(
    data = ctl_data,
    aes(x = bdi, y = cesd_sum, color = "Controlli"), alpha = 0.7
  ) +
  geom_line(
    aes(x = x_values, y = line_combined(x_values)),
    linetype = "dashed", color = okabe_ito_colors["Pazienti"]
  ) +
  geom_vline(
    aes(xintercept = mean(mdd_data$bdi, na.rm = TRUE)),
    color = okabe_ito_colors["Pazienti"], alpha = 0.2
  ) +
  geom_vline(
    aes(xintercept = mean(ctl_data$bdi, na.rm = TRUE)),
    color = okabe_ito_colors["Controlli"], alpha = 0.2
  ) +
  geom_hline(
    aes(yintercept = mean(mdd_data$cesd_sum, na.rm = TRUE)),
    color = okabe_ito_colors["Pazienti"], alpha = 0.2
  ) +
  geom_hline(
    aes(yintercept = mean(ctl_data$cesd_sum, na.rm = TRUE)),
    color = okabe_ito_colors["Controlli"], alpha = 0.2
  ) +
  labs(x = "BDI-II", y = "CESD", color = "Gruppo") +
  scale_color_manual(
    values = okabe_ito_colors
  )
```

Osservando il grafico a dispersione, √® evidente che i dati mostrano una tendenza a distribuirsi in modo approssimativamente lineare. In termini statistici, ci√≤ suggerisce una relazione di associazione lineare tra i punteggi CES-D e BDI-II.

Tuttavia, √® importante notare che la relazione lineare tra le due variabili √® lontana dall'essere perfetta. In una relazione lineare perfetta, tutti i punti nel grafico sarebbero allineati in modo preciso lungo una retta. Nella realt√†, la dispersione dei punti dal comportamento lineare ideale √® evidente.

Di conseguenza, sorge la necessit√† di quantificare numericamente la forza e la direzione della relazione lineare tra le due variabili e di misurare quanto i punti si discostino da una relazione lineare ideale. Esistono vari indici statistici a disposizione per raggiungere questo obiettivo.

## Covarianza

Iniziamo a considerare il pi√π importante di tali indici, chiamato *covarianza*. In realt√† la definizione di questo indice non ci sorprender√† pi√π di tanto in quanto, in una forma solo apparentemente diversa, l'abbiamo gi√† incontrata in precedenza. Ci ricordiamo infatti che la varianza di una generica variabile $X$ √® definita come la media degli scarti quadratici di ciascuna osservazione dalla media:

$$
S_{XX} = \frac{1}{n} \sum_{i=1}^n(X_i - \bar{X}) (X_i - \bar{X}). 
$$

La varianza viene talvolta descritta come la "covarianza di una variabile con s√© stessa". Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di una sola variabile, ci chiediamo come due variabili $X$ e $Y$ "variano insieme" (co-variano). √à facile capire come una risposta a tale domanda possa essere fornita da una semplice trasformazione della formula precedente che diventa:

$$
S_{XY} = \frac{1}{n} \sum_{i=1}^n(X_i - \bar{X}) (Y_i - \bar{Y}).
$$ {#eq-cov-def}

L'@eq-cov-def ci fornisce la definizione della covarianza.

### Interpretazione

Per capire il significato dell'@eq-cov-def, supponiamo di dividere il grafico riportato nella @sec-scatter-plot in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.

Se prevalgono punti nel I e III quadrante, allora la nuvola di punti avr√† un andamento crescente (per cui a valori bassi di $X$ tendono ad associarsi valori bassi di $Y$ e a valori elevati di $X$ tendono ad associarsi valori elevati di $Y$) e la covarianza avr√† segno positivo. Mentre se prevalgono punti nel II e IV quadrante la nuvola di punti avr√† un andamento decrescente (per cui a valori bassi di $X$ tendono ad associarsi valori elevati di $Y$ e a valori elevati di $X$ tendono ad associarsi valori bassi di $Y$) e la covarianza avr√† segno negativo. Dunque, il segno della covarianza ci informa sulla direzione della relazione lineare tra due variabili: l'associazione lineare si dice positiva se la covarianza √® positiva, negativa se la covarianza √® negativa.

::: {#exm-}
Implementiamo l'@eq-cov-def in R.

```{r}
cov_value <- function(x, y) {
  mean_x <- sum(x) / length(x)
  mean_y <- sum(y) / length(y)

  sub_x <- x - mean_x
  sub_y <- y - mean_y

  sum_value <- sum(sub_y * sub_x)
  denom <- length(x)

  cov <- sum_value / denom
  return(cov)
}
```

Per i dati mostrati nel diagramma, la covarianza tra BDI-II e CESD √® 207.4

```{r}
x <- df$bdi
y <- df$cesd_sum

cov_value(x, y)
```

Oppure, in maniera pi√π semplice:

```{r}
mean((x - mean(x)) * (y - mean(y)))
```

Lo stesso risultato si ottiene con la funzione `cov`:

```{r}
cov(x, y) * (length(x) - 1) / length(x)
```

La funzione `cov(x, y)` calcola la covarianza tra due array, `x` e `y` utilizzando $n-1$ al denominatore.
:::

## Correlazione

La direzione della relazione tra le variabili √® indicata dal segno della covarianza, ma il valore assoluto di questo indice non fornisce informazioni utili poich√© dipende dall'unit√† di misura delle variabili. Ad esempio, considerando l'altezza e il peso delle persone, la covarianza sar√† pi√π grande se l'altezza √® misurata in millimetri e il peso in grammi, rispetto al caso in cui l'altezza √® in metri e il peso in chilogrammi. Pertanto, per descrivere la forza e la direzione della relazione lineare tra due variabili in modo adimensionale, si utilizza l'indice di correlazione.

La correlazione √® ottenuta standardizzando la covarianza tramite la divisione delle deviazioni standard ($s_X$, $s_Y$) delle due variabili:

$$ 
r = \frac{S_{XY}}{S_X S_Y}. 
$$ {#eq-cor-def}

La quantit√† che si ottiene dall'@eq-cor-def viene chiamata *correlazione* di Bravais-Pearson (dal nome degli autori che, indipendentemente l'uno dall'altro, l'hanno introdotta).

In maniera equivalente, per una lista di coppie di valori $(x_1, y_1), \dots, (x_n, y_n)$, il coefficiente di correlazione √® definito come la media del prodotto dei valori standardizzati:

$$
r = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{\sigma_x} \right) \left( \frac{y_i - \bar{y}}{\sigma_y} \right),
$$ {#eq-cor-def2}

dove $\bar{x}$ e $\bar{y}$ rappresentano, rispettivamente, le medie dei valori $x$ e $y$, e $\sigma_x$ e $\sigma_y$ sono le rispettive deviazioni standard.

Nell'@eq-cor-def2, i valori $x_i$ e $y_i$ vengono prima standardizzati sottraendo la media e dividendo per la deviazione standard, e poi si calcola la media del prodotto di questi valori standardizzati.

### Propriet√†

Il coefficiente di correlazione ha le seguenti propriet√†:

-   ha lo stesso segno della covarianza, dato che si ottiene dividendo la covarianza per due numeri positivi;
-   √® un numero puro, cio√® non dipende dall'unit√† di misura delle variabili;
-   assume valori compresi tra -1 e +1.

### Interpretazione  

All'indice di correlazione possiamo assegnare la seguente interpretazione:

1.  $r_{XY} = -1$ $\rightarrow$ perfetta relazione negativa: tutti i punti si trovano esattamente su una retta con pendenza negativa (dal quadrante in alto a sinistra al quadrante in basso a destra);
2.  $r_{XY} = +1$ $\rightarrow$ perfetta relazione positiva: tutti i punti si trovano esattamente su una retta con pendenza positiva (dal quadrante in basso a sinistra al quadrante in alto a destra);
3.  $-1 < r_{XY} < +1$ $\rightarrow$ presenza di una relazione lineare di intensit√† diversa;
4.  $r_{XY} = 0$ $\rightarrow$ assenza di relazione lineare tra $X$ e $Y$.

::: {#exm-}
Per i dati riportati nel diagramma della sezione {ref}`sec-zetsche-scatter`, la covarianza √® 207.4. Il segno positivo della covarianza ci dice che tra le due variabili c'√® un'associazione lineare positiva. Per capire quale sia l'intensit√† della relazione lineare calcoliamo la correlazione. Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a $\frac{207.426}{15.38 \cdot 14.93} = 0.904.$ Tale valore √® prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.

Troviamo la correlazione con la funzione `corrcoef()`:

```{r}
cor(x, y)
```

Replichiamo il risultato implementando l'@eq-cor-def:

```{r}
s_xy <- mean((x - mean(x)) * (y - mean(y)))
s_x <- sqrt(mean((x - mean(x))^2)) # Deviazione standard popolazione
s_y <- sqrt(mean((y - mean(y))^2)) # Deviazione standard popolazione
r_xy <- s_xy / (s_x * s_y)
print(r_xy)
```

Un altro modo ancora per trovare la correlazione tra i punteggi BDI-II e CESD √® quello di applicare l'@eq-cor-def2:

```{r}
z_x <- (x - mean(x)) / sqrt(mean((x - mean(x))^2)) 
# Standardizzazione con deviazione standard popolazione
z_y <- (y - mean(y)) / sqrt(mean((y - mean(y))^2)) 
# Standardizzazione con deviazione standard popolazione
mean(z_x * z_y)
```
:::

::: {#exm-}
Un uso interessante delle correlazioni viene fatto in un recente articolo di [Guilbeault et al. (2024)](https://www.nature.com/articles/s41586-024-07068-x). Il concetto di "gender bias" si riferisce alla tendenza sistematica di favorire un sesso rispetto all'altro, spesso a scapito delle donne. Lo studio di Guilbeault et al. (2024) analizza come le immagini online influenzino la diffusione su vasta scala di questo preconcetto di genere.

Attraverso un vasto insieme di immagini e testi raccolti online, gli autori dimostrano che sia le misurazioni basate sulle immagini che quelle basate sui testi catturano la frequenza con cui varie categorie sociali sono associate a rappresentazioni di genere, valutate su una scala da -1 (femminile) a 1 (maschile), con 0 che indica una neutralit√† di genere. Questo consente di quantificare il preconcetto di genere come una forma di bias statistico lungo tre dimensioni: la tendenza delle categorie sociali ad associarsi a un genere specifico nelle immagini e nei testi, la rappresentazione relativa delle donne rispetto agli uomini in tutte le categorie sociali nelle immagini e nei testi, e il confronto tra le associazioni di genere nei dati delle immagini e dei testi con la distribuzione empirica delle donne e degli uomini nella societ√†. Il lavoro di Guilbeault et al. (2024) evidenzia che il preconcetto di genere √® molto pi√π evidente nelle immagini rispetto ai testi, come mostrato nella {numref}`gender-bias-1-fig` C. 

Si noti che, nel grafico della {numref}`gender-bias-1-fig` C, ogni punto pu√≤ essere interpretato come una misura di correlazione. La misura utilizzata da Guilbeault et al. (2024) riflette il grado di associazione tra le categorie sociali e le rappresentazioni di genere presenti nelle immagini e nei testi analizzati. Quando la misura √® vicina a +1, indica una forte associazione positiva tra una categoria sociale specifica e una rappresentazione di genere maschile, mentre un valore vicino a -1 indica una forte associazione negativa con una rappresentazione di genere femminile. Un valore di 0, invece, suggerisce che non vi √® alcuna associazione tra la categoria sociale considerata e un genere specifico, indicando una sorta di neutralit√† di genere. In sostanza, questa misura di frequenza pu√≤ essere interpretata come una correlazione che riflette la tendenza delle categorie sociali a essere rappresentate in un modo o nell'altro nelle immagini e nei testi analizzati, rispetto ai concetti di genere femminile e maschile.

![Il preconcetto di genere √® pi√π prevalente nelle immagini online (da Google Immagini) e nei testi online (da Google News). A. La correlazione tra le associazioni di genere nelle immagini da Google Immagini e nei testi da Google News per tutte le categorie sociali (n = 2.986), organizzate per decili. B. La forza dell'associazione di genere in queste immagini e testi online per tutte le categorie (n = 2.986), suddivisa in base al fatto che queste categorie siano inclinate verso il femminile o il maschile. C. Le associazioni di genere per un campione di occupazioni secondo queste immagini e testi online; questo campione √® stato selezionato manualmente per evidenziare i tipi di categorie sociali e preconcetti di genere esaminati. (Figura tratta da Guilbeault et al. (2024)).](../../figures/gender_bias_1.png)
:::

## Correlazione di Spearman

Un'alternativa per valutare la relazione lineare tra due variabili √® il coefficiente di correlazione di Spearman, che si basa esclusivamente sull'ordine dei dati e non sugli specifici valori. Questo indice di associazione √® particolarmente adatto quando gli psicologi sono in grado di misurare solo le relazioni di ordine tra diverse modalit√† di risposta dei soggetti, ma non l'intensit√† della risposta stessa. Tali variabili psicologiche che presentano questa caratteristica sono definite come "ordinali".

::: {.callout-note}
√à importante ricordare che, nel caso di una variabile ordinale, non √® possibile utilizzare le statistiche descrittive convenzionali come la media e la varianza per sintetizzare le osservazioni. Tuttavia, √® possibile riassumere le osservazioni attraverso una distribuzione di frequenze delle diverse modalit√† di risposta. Come abbiamo appena visto, la direzione e l'intensit√† dell'associazione tra due variabili ordinali possono essere descritte utilizzando il coefficiente di correlazione di Spearman.
:::

Per fornire un esempio, consideriamo due variabili di scala ordinale e calcoliamo la correlazione di Spearman tra di esse.

```{r}
cor.test(c(1, 2, 3, 4, 5), c(5, 6, 7, 8, 7), method = "spearman")
```

## Oltre la correlazione e la covarianza: quando l‚Äôassociazione tra variabili √® pi√π complessa

In questo capitolo abbiamo introdotto due misure fondamentali per descrivere la relazione tra due variabili: **covarianza** e **correlazione**. La covarianza fornisce un‚Äôindicazione del modo in cui due variabili si discostano congiuntamente dalle proprie medie, mentre la correlazione ne standardizza i valori, permettendo un confronto pi√π immediato tra diverse coppie di variabili e garantendo un indice compreso tra -1 e +1. Una correlazione vicina a +1 indica una forte relazione lineare positiva, vicina a -1 una forte relazione lineare negativa, mentre un valore vicino a 0 segnala l‚Äôassenza di una chiara relazione lineare.

Tuttavia, √® cruciale comprendere che la correlazione descrive esclusivamente **la dimensione lineare** della relazione tra due variabili. Questo significa che una correlazione nulla (pari a zero) non implica affatto che non vi sia alcuna relazione tra le variabili, ma semplicemente che non esiste una relazione lineare. Possono esistere relazioni non lineari anche molto forti, non catturate da questo indice. Inoltre, altre situazioni possono trarre in inganno, come nei casi in cui i dati siano raggruppati in sottogruppi con propriet√† differenti o siano frutto di particolari processi di selezione.

### Correlazione nulla e relazioni non lineari

Una correlazione pari a zero pu√≤ nascondere relazioni non lineari anche molto marcate. Per esempio, se una variabile Y aumenta solo quando X √® molto alta o molto bassa, ma rimane costante per valori intermedi di X, questa curva a ‚ÄúU‚Äù pu√≤ generare una correlazione vicina allo zero, pur esistendo un forte legame non lineare.

Un esempio illustrativo √® fornito dal cosiddetto **Datasaurus Dozen**, un insieme di tredici dataset con la stessa media, deviazione standard e correlazione tra le variabili, ma con distribuzioni visivamente e strutturalmente molto diverse. In ognuno di questi dataset la correlazione di Pearson √® pari a zero, ma l‚Äôispezione grafica rivela pattern e forme ben definite. Questo ci ricorda che √® sempre bene accompagnare le misure numeriche con una visualizzazione grafica dei dati.

```{r}
datasaurus_data <- read.csv("../../data/datasaurus.csv")

datasaurus_summary <- datasaurus_data %>%
  group_by(dataset) %>%
  summarise(
    x_count = n(),
    x_mean = mean(x, na.rm = TRUE),
    x_std = sd(x, na.rm = TRUE),
    y_count = n(),
    y_mean = mean(y, na.rm = TRUE),
    y_std = sd(y, na.rm = TRUE)
  )

datasaurus_summary
```

```{r fig.asp=1}
datasaurus_data |>
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = 0.7) +
  facet_wrap(~dataset, nrow = 4, ncol = 4) +
  labs(x = NULL, y = NULL)
```

Tutti questi esempi rafforzano l‚Äôidea che l‚Äôassenza di correlazione lineare non significa assenza di relazione: potremmo avere strutture curve, pattern complessi o punti anomali (outlier) capaci di modificare radicalmente la forma della relazione tra le variabili.

Oltre alle relazioni non lineari, esistono situazioni in cui la correlazione, da sola, pu√≤ fornire un‚Äôimmagine distorta dei dati. Tra queste ricordiamo due fenomeni noti come il **paradosso di Simpson** e il **paradosso di Berkson**.

### Paradosso di Simpson

Il paradosso di Simpson si verifica quando, guardando i dati raggruppati per sottogruppi, si osserva una certa relazione tra due variabili, ma aggregando i dati di tutti i sottogruppi insieme emerge una relazione opposta. In altre parole, la tendenza che appare quando si considerano i dati divisi per gruppi scompare o si inverte quando si esamina l‚Äôintero campione senza tenere conto della suddivisione in sottogruppi.

Immaginiamo di avere due dipartimenti universitari, A e B. All‚Äôinterno di ciascun dipartimento, la relazione tra il voto di laurea (X) e la performance in un successivo programma di specializzazione (Y) √® positiva: all‚Äôaumentare del voto di laurea aumenta, in media, la performance nella scuola di specializzazione.

Tuttavia, supponiamo che il Dipartimento A abbia in generale voti di laurea mediamente pi√π bassi, ma ottime performance nella specializzazione; mentre il Dipartimento B abbia voti di laurea mediamente pi√π alti, ma performance alla specializzazione un po‚Äô pi√π basse. Se uniamo tutti gli studenti dei due dipartimenti senza considerarne l‚Äôappartenenza, potremmo osservare una relazione negativa tra voto di laurea e performance, sovvertendo le conclusioni tratte guardando ai singoli sottogruppi.

Ecco come possiamo simulare questi dati in R:

```{r}
set.seed(123)

# Numero di osservazioni per dipartimento
n <- 100

# Dipartimento A:
# Voti di laurea (X) pi√π bassi, ma performance (Y) pi√π alta.
# Creiamo un legame positivo tra X e Y all'interno di A.
X_A <- rnorm(n, mean = 50, sd = 5)    # Voti laurea mediamente pi√π bassi
Y_A <- X_A + rnorm(n, mean = 20, sd = 5) # Performance alta e correlata positivamente con X

# Dipartimento B:
# Voti di laurea (X) pi√π alti, ma performance (Y) pi√π bassa.
# Anche qui creiamo un legame positivo tra X e Y all'interno di B, 
# ma con un offset tale che globalmente i voti alti coincidano con performance minori.
X_B <- rnorm(n, mean = 60, sd = 5)    # Voti laurea mediamente pi√π alti
Y_B <- X_B - rnorm(n, mean = 10, sd = 5) # Performance pi√π bassa ma comunque correlata positivamente con X all‚Äôinterno di B

# Creiamo un dataframe con tutti i dati
dipartimento <- c(rep("A", n), rep("B", n))
X <- c(X_A, X_B)
Y <- c(Y_A, Y_B)
dati <- data.frame(dipartimento, X, Y)

# Correlazioni all'interno dei dipartimenti
cat("Correlazione nel Dipartimento A:", cor(X_A, Y_A), "\n")
cat("Correlazione nel Dipartimento B:", cor(X_B, Y_B), "\n")

# Correlazione sull'intero dataset (senza distinguere i dipartimenti)
cat("Correlazione globale:", cor(X, Y), "\n")
```

```{r}
# Visualizziamo i dati
ggplot(dati, aes(x = X, y = Y, color = dipartimento)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_fill_okabeito() + 
  labs(
    title = "Paradosso di Simpson",
    x = "Voto di laurea", 
    y = "Performance nella specializzazione",
    color = "Dipartimento"
  )
```

- **All‚Äôinterno del Dipartimento A**: la correlazione tra voto di laurea (X) e performance (Y) √® positiva. Ci√≤ significa che, per gli studenti di A, avere un voto di laurea pi√π alto √® associato a una performance maggiore nella specializzazione. 

- **All‚Äôinterno del Dipartimento B**: la correlazione tra X e Y √® anch‚Äôessa positiva, indicando che anche nel secondo dipartimento voti pi√π alti tendono ad accompagnarsi a performance pi√π alte.

- **Considerando entrambi i dipartimenti insieme**: a causa delle differenze nei livelli medi di X e Y tra i dipartimenti, unendo i dati senza distinguere il gruppo di appartenenza otteniamo una correlazione globale negativa. Questo significa che, ignorando la suddivisione in dipartimenti, sembra che aumentare il voto di laurea sia associato a una diminuzione della performance nella specializzazione ‚Äî una conclusione opposta a quella tratta dall‚Äôanalisi separata dei due sottogruppi.

Questo √® un esempio concreto del paradosso di Simpson: la relazione osservata in sottogruppi omogenei si inverte quando si aggregano i dati, mettendo in guardia sulla necessit√† di considerare con attenzione la struttura dei dati e i fattori confondenti prima di trarre conclusioni.

### Paradosso di Berkson

Il paradosso di Berkson √® un fenomeno legato alla selezione del campione. Se il dataset non √® rappresentativo della popolazione generale, la relazione osservata pu√≤ risultare artificiale o opposta a quella esistente su un campione pi√π ampio. Per esempio, analizzando solo ciclisti professionisti, potremmo non vedere alcuna relazione tra VO2 max e probabilit√† di vincere una gara, poich√© tutti hanno gi√† superato una certa soglia di VO2 max. Considerando la popolazione generale, invece, potrebbe emergere chiaramente una relazione positiva tra questi due fattori. Questo paradosso evidenzia l‚Äôimportanza di considerare il processo di selezione dei dati e di chiedersi se il campione analizzato sia adeguato a rispondere alla domanda di ricerca.

### Limiti delle statistiche riassuntive semplici

Un esempio particolarmente famoso che dimostra i limiti delle semplici statistiche descrittive ‚Äî come media, deviazione standard e correlazione ‚Äî √® il quartetto di Anscombe. Questo insieme di quattro piccoli dataset possiede identiche medie, varianze e correlazioni tra variabili, ma rappresenta relazioni estremamente differenti tra X e Y [@anscombe1973graphs].

Importiamo il dataset `anscombe` gi√† disponibile nel pacchetto `datasets` di R:

```{r}
data("anscombe")

anscombe |> 
  head()
```

Il dataset `anscombe` contiene quattro serie di dati (x1, y1), (x2, y2), (x3, y3) e (x4, y4), ognuna costituita da 11 coppie di valori (x, y). Per comprendere meglio le loro caratteristiche, iniziamo calcolando alcune statistiche descrittive. La funzione `apply()` consente di applicare una funzione (ad esempio, `mean`) a tutte le colonne di un data frame. Usandola sulle colonne di `anscombe`, otteniamo le medie di ciascuna delle otto variabili (le quattro x e le quattro y):

```{r}
apply(anscombe, MARGIN = 2, mean)
```

Osserviamo che le medie delle quattro variabili x sono identiche fino a sei cifre decimali, e le medie delle quattro variabili y differiscono solo in modo trascurabile (circa lo 0.01%). Analogamente, se calcoliamo le deviazioni standard per ciascuna variabile, notiamo una notevole somiglianza:

```{r}
apply(anscombe, MARGIN = 2, sd)
```

Anche in questo caso, le deviazioni standard per le x coincidono fino alla sesta cifra decimale, mentre quelle delle y differiscono di meno dello 0.06%. Inoltre, se calcoliamo i coefficienti di correlazione tra x e y in ognuno dei quattro dataset, scopriamo che sono quasi identici:

```{r}
# Calcoliamo la correlazione per ciascuna coppia (x1,y1), (x2,y2), (x3,y3), (x4,y4)
for (i in 1:4) {
  x_var <- anscombe[[paste0("x", i)]]
  y_var <- anscombe[[paste0("y", i)]]
  
  corr_value <- cor(x_var, y_var)
  cat("Correlazione tra x", i, "e y", i, ":", corr_value, "\n")
}
```

Se ci limitassimo a guardare queste statistiche (media, deviazione standard, correlazione), potremmo facilmente essere indotti a concludere che i quattro dataset sono tra loro sostanzialmente indistinguibili. Tuttavia, la realt√† √® molto diversa. Le statistiche descrittive, prese da sole, non offrono una visione completa e possono nascondere importanti differenze nella struttura dei dati.

Questa differenza diventa evidente non appena decidiamo di visualizzare i dati. La rappresentazione grafica fornisce informazioni che non emergono dalle sole statistiche riassuntive:

```{r fig.asp=1}
anscombe_m <- tibble()

for (i in 1:4) {
  anscombe_m <- rbind(
    anscombe_m, tibble(set = i, x = anscombe[, i], y = anscombe[, i + 4])
  )
}

ggplot(anscombe_m, aes(x, y)) +
  geom_point(size = 3, color = "red", fill = "orange", shape = 21) +
  geom_smooth(method = "lm", fill = NA, fullrange = TRUE) +
  facet_wrap(~set, ncol = 2)
```


Osservando i grafici, notiamo subito che i quattro dataset sono profondamente diversi: 

- **Dataset 1:** Qui la relazione tra x e y √® approssimativamente lineare, e la correlazione riflette in modo appropriato il legame tra le due variabili.
- **Dataset 2:** Sebbene la correlazione sia simile a quella del Dataset 1, i dati mostrano una relazione curvilinea e non lineare. La semplice correlazione lineare non ne cattura la forma.
- **Dataset 3:** La presenza di un singolo outlier distorce la percezione della relazione tra le variabili, altrimenti lineare. La correlazione alta √® influenzata in modo sproporzionato da questo punto anomalo.
- **Dataset 4:** Qui i dati non mostrano alcuna relazione lineare. La correlazione elevata √® il frutto di un pattern fortemente atipico (ad esempio, una sola coppia di punti allineata).

Il quartetto di Anscombe mette in luce un principio fondamentale: statistiche descrittive come media, deviazione standard e correlazione non sono sempre sufficienti per comprendere la natura dei dati. La visualizzazione grafica √® essenziale per cogliere relazioni, pattern non lineari, outlier e altre caratteristiche che le semplici statistiche non riescono a rivelare. In definitiva, questo esempio dimostra che analizzare i dati soltanto attraverso poche statistiche di sintesi pu√≤ portare a conclusioni fuorvianti, mentre l‚Äôintegrazione con la rappresentazione grafica fornisce una visione pi√π completa e accurata.

## Riflessioni Conclusive

La covarianza e la correlazione sono strumenti preziosi per capire l‚Äôintensit√† e la direzione lineare di una relazione tra due variabili. Tuttavia, non dobbiamo confondere la loro semplicit√† con completezza d‚Äôinformazione. Una correlazione nulla non implica assenza di relazione, ma solo assenza di una relazione lineare. Inoltre, il paradosso di Simpson e il paradosso di Berkson dimostrano che la semplice osservazione di correlazioni pu√≤ essere fuorviante se non si tiene conto della struttura dei dati o del processo di selezione del campione.

Infine, esempi come il quartetto di Anscombe sottolineano quanto sia fondamentale accompagnare le statistiche riassuntive con analisi grafiche e un‚Äôattenzione costante ai possibili pattern non lineari, agli outlier e alle caratteristiche peculiari del dataset. Nel prossimo capitolo esamineremo approcci che consentono di avvicinarsi alla comprensione causale dei fenomeni, andando oltre la semplice osservazione dell‚Äôassociazione tra variabili.

## Esercizi

::: {.callout-tip title="Esercizio" collapse="true"}
**Esercizi Teorici**  

1. **Definizioni fondamentali**  

   - Qual √® la differenza tra **associazione, correlazione e dipendenza**?  
   - Perch√© la **correlazione non implica causalit√†**? Fai un esempio.  
   - In quali situazioni la correlazione di **Spearman** √® preferibile rispetto alla correlazione di **Pearson**?  

2. **Interpretazione della correlazione**  

   - Qual √® il range dei valori che pu√≤ assumere la correlazione di Pearson?  
   - Se il coefficiente di correlazione tra due variabili √® **0.85**, come interpreteresti la loro relazione?  
   - Se il coefficiente di correlazione √® **-0.60**, che tipo di relazione esiste tra le due variabili?  
   - Quali fattori potrebbero influenzare il valore della correlazione?  

3. **Covarianza vs Correlazione**  

   - Qual √® la differenza tra **covarianza** e **correlazione**?  
   - Perch√© la covarianza non √® sempre interpretabile come misura della forza della relazione tra due variabili?  
   - Quali sono i vantaggi di usare la correlazione al posto della covarianza?  

4. **Grafico a dispersione e correlazione**  

   - Osservando un **grafico a dispersione**, quali caratteristiche ti permettono di identificare una relazione lineare positiva o negativa?  
   - Disegna (o descrivi verbalmente) un esempio di un dataset con una correlazione di **circa 0**, ma con una chiara relazione non lineare tra le variabili.  

**Esercizi Pratici in R**  

üìå **Obiettivo**: Analizzare le relazioni tra **Satisfaction With Life Scale (SWLS)** e **Scala della Rete Sociale di Lubben (LSNS-6)**, calcolare covarianza e correlazione, e visualizzare i dati per individuare pattern.  

**Dati disponibili**:  

Usa i dati raccolti per le variabili SWLS e LSNS, oltre al genere dei partecipanti.  

**1. Esplorazione e Visualizzazione della Relazione tra SWLS e LSNS**  

1. **Carica i dati raccolti dagli studenti** e verifica la struttura del dataset.  
2. **Calcola le statistiche descrittive**: media, deviazione standard, minimo, massimo e quartili delle variabili SWLS e LSNS.  
3. **Crea un grafico a dispersione** tra SWLS e LSNS:  
   - Colora i punti in base al **genere** del partecipante.  
   - Aggiungi una **linea di regressione** per evidenziare il trend della relazione.  

**2. Calcolo della Covarianza e della Correlazione tra SWLS e LSNS**  

1. **Calcola la covarianza tra SWLS e LSNS** usando la formula matematica della covarianza e confrontala con il valore ottenuto con `cov()`.  
2. **Calcola la correlazione di Pearson** e commenta il risultato:  
   - La relazione √® forte o debole?  
   - Ha segno positivo o negativo?  
   - √à coerente con quanto osservato nel grafico a dispersione?  
3. **Calcola la correlazione di Spearman** e confrontala con quella di Pearson. Quale delle due √® pi√π appropriata per questi dati?  

**3. Analisi delle Associazioni per Gruppi**  

1. **Calcola la correlazione separatamente per i partecipanti di genere maschile e femminile.**  
2. **Confronta i risultati**: la relazione tra SWLS e LSNS √® simile nei due gruppi o ci sono differenze?  
3. **Visualizza i dati con due grafici a dispersione distinti** per maschi e femmine.  

**4. Correlazione Nulla e Pattern Non Lineari**  

1. **Simula un dataset** in cui la correlazione di Pearson √® **vicina a 0**, ma esiste una chiara relazione **non lineare** tra le variabili.  
2. **Costruisci un grafico a dispersione** per osservare il pattern nei dati.  
3. **Calcola la correlazione di Spearman** e confrontala con quella di Pearson. Quale delle due cattura meglio la relazione nei dati?  

Consegna il file `.qmd` compilato in PDF contenente il codice, le visualizzazioni e le interpretazioni.
:::

::: {.callout-tip title="Soluzione" collapse="true"}

**1. Definizioni fondamentali**

1.1 Differenza tra **associazione, correlazione e dipendenza**:

- **Associazione**: Indica una relazione generica tra due variabili, senza specificare la natura o la direzione della relazione.
- **Correlazione**: Misura la forza e la direzione di una relazione **lineare** tra due variabili. Pu√≤ essere positiva (variabili aumentano insieme) o negativa (una variabile aumenta mentre l'altra diminuisce).
- **Dipendenza**: Indica che una variabile √® influenzata da un'altra, ma non implica necessariamente una relazione lineare. La dipendenza pu√≤ essere causale o statistica.

1.2 Perch√© la **correlazione non implica causalit√†**:

La correlazione misura solo la relazione lineare tra due variabili, ma non indica se una variabile causa l'altra. Un esempio classico √® la correlazione tra il consumo di gelati e il numero di annegamenti: entrambi aumentano in estate, ma non c'√® un nesso causale diretto. Un terzo fattore (il caldo) influenza entrambe le variabili.

1.3 Quando preferire la correlazione di **Spearman** rispetto a quella di **Pearson**:

- Quando i dati non sono distribuiti normalmente.
- Quando ci sono outlier che potrebbero distorcere la correlazione di Pearson.
- Quando la relazione tra le variabili √® **monotona** (sempre crescente o decrescente) ma non lineare.

**2. Interpretazione della correlazione**

2.1 Range dei valori della correlazione di Pearson:

La correlazione di Pearson assume valori compresi tra **-1** e **1**:
- **1**: Correlazione lineare positiva perfetta.
- **-1**: Correlazione lineare negativa perfetta.
- **0**: Nessuna correlazione lineare.

2.2 Interpretazione di un coefficiente di **0.85**:

Un coefficiente di 0.85 indica una **forte relazione lineare positiva** tra le due variabili. All'aumentare di una variabile, l'altra tende ad aumentare in modo consistente.

2.3 Interpretazione di un coefficiente di **-0.60**:

Un coefficiente di -0.60 indica una **relazione lineare negativa moderata**. All'aumentare di una variabile, l'altra tende a diminuire.

2.4 Fattori che influenzano la correlazione:

- **Outlier**: Possono distorcere il valore della correlazione.
- **Distribuzione non lineare**: La correlazione di Pearson non cattura relazioni non lineari.
- **Range ristretto delle variabili**: Se i dati coprono solo una piccola parte del range possibile, la correlazione potrebbe essere sottostimata.

**3. Covarianza vs Correlazione**

3.1 Differenza tra **covarianza** e **correlazione**:

- **Covarianza**: Misura la direzione della relazione tra due variabili, ma il suo valore dipende dalle unit√† di misura delle variabili.
- **Correlazione**: Standardizza la covarianza, rendendola adimensionale e consentendo confronti tra diverse coppie di variabili.

3.2 Perch√© la covarianza non √® sempre interpretabile:

La covarianza non √® standardizzata, quindi il suo valore non fornisce informazioni sulla forza della relazione. Ad esempio, una covarianza di 1000 potrebbe indicare una relazione forte o debole, a seconda delle unit√† di misura.

3.3 Vantaggi della correlazione rispetto alla covarianza:

- √à adimensionale, quindi pu√≤ essere confrontata tra diverse coppie di variabili.
- Assume valori compresi tra -1 e 1, facilitando l'interpretazione della forza e della direzione della relazione.

**4. Grafico a dispersione e correlazione**

4.1 Caratteristiche di un grafico a dispersione per identificare relazioni lineari:

- **Relazione lineare positiva**: I punti si dispongono lungo una linea retta con pendenza positiva.
- **Relazione lineare negativa**: I punti si dispongono lungo una linea retta con pendenza negativa.
- **Nessuna relazione lineare**: I punti sono sparsi senza un pattern evidente.

4.2 Esempio di dataset con correlazione circa 0 ma relazione non lineare:

Immagina un dataset in cui una variabile $x$ assume valori simmetrici intorno a 0 (ad esempio, da -5 a 5), e la variabile $y$ √® uguale a $x^2$. In questo caso:

- La correlazione di Pearson sar√† **circa 0**, perch√© non c'√® una relazione lineare.
- Tuttavia, esiste una chiara relazione **non lineare** (quadratica) tra $x$ e $y$.

**Descrizione verbale**:

I punti formano una parabola, con $y$ che aumenta sia quando $x$ √® positivo che negativo. La correlazione di Pearson non cattura questa relazione, mentre la correlazione di Spearman potrebbe farlo.

**1. Esplorazione e Visualizzazione della Relazione tra SWLS e LSNS**

1.1 Carica i dati raccolti dagli studenti e verifica la struttura del dataset
Simuliamo un dataset con 100 partecipanti, includendo le variabili **SWLS** (Soddisfazione di Vita), **LSNS** (Rete Sociale), e **Genere**.

```r
# Simulazione dei dati
set.seed(123)
n <- 100
genere <- sample(c("Maschio", "Femmina"), n, replace = TRUE)
swls <- round(rnorm(n, mean = 20, sd = 5), 1)  # SWLS: Scala 5-35
lsns <- round(rnorm(n, mean = 12, sd = 4), 1)   # LSNS: Scala 0-30

# Creazione del dataset
dati <- data.frame(Genere = genere, SWLS = swls, LSNS = lsns)

# Verifica della struttura
str(dati)
head(dati)
```

1.2 Calcola le statistiche descrittive

```r
# Statistiche descrittive per SWLS e LSNS
summary(dati$SWLS)
summary(dati$LSNS)

# Media e deviazione standard
mean(dati$SWLS)
sd(dati$SWLS)
mean(dati$LSNS)
sd(dati$LSNS)
```

1.3 Crea un grafico a dispersione

```r
library(ggplot2)

ggplot(dati, aes(x = SWLS, y = LSNS, color = Genere)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  labs(title = "Relazione tra SWLS e LSNS",
       x = "Soddisfazione di Vita (SWLS)",
       y = "Rete Sociale (LSNS)") +
  theme_minimal()
```

**2. Calcolo della Covarianza e della Correlazione tra SWLS e LSNS**

2.1 Calcola la covarianza

```r
# Covarianza manuale
cov_manual <- sum((dati$SWLS - mean(dati$SWLS)) * (dati$LSNS - mean(dati$LSNS))) / (n - 1)
cov_manual

# Covarianza con funzione R
cov(dati$SWLS, dati$LSNS)
```

2.2 Calcola la correlazione di Pearson

```r
cor_pearson <- cor(dati$SWLS, dati$LSNS, method = "pearson")
cor_pearson
```
- **Commento**: La correlazione √® [valore], indicando una relazione [forte/debole] e [positiva/negativa]. Questo √® coerente con il grafico a dispersione.

2.3 Calcola la correlazione di Spearman

```r
cor_spearman <- cor(dati$SWLS, dati$LSNS, method = "spearman")
cor_spearman
```
- **Confronto**: La correlazione di Spearman √® pi√π appropriata se i dati non sono distribuiti normalmente o presentano outlier.

**3. Analisi delle Associazioni per Gruppi**

3.1 Calcola la correlazione separatamente per genere

```r
# Maschi
cor_maschi <- cor(dati$SWLS[dati$Genere == "Maschio"], dati$LSNS[dati$Genere == "Maschio"], method = "pearson")

# Femmine
cor_femmine <- cor(dati$SWLS[dati$Genere == "Femmina"], dati$LSNS[dati$Genere == "Femmina"], method = "pearson")

cor_maschi
cor_femmine
```

3.2 Confronta i risultati

- **Commento**: La correlazione √® [simile/diversa] tra maschi e femmine, suggerendo [presenza/assenza] di differenze di genere.

3.3 Visualizza i dati con grafici distinti

```r
ggplot(dati, aes(x = SWLS, y = LSNS, color = Genere)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ Genere) +
  labs(title = "Relazione tra SWLS e LSNS per Genere",
       x = "Soddisfazione di Vita (SWLS)",
       y = "Rete Sociale (LSNS)") +
  theme_minimal()
```

**4. Correlazione Nulla e Pattern Non Lineari**

4.1 Simula un dataset con correlazione nulla ma relazione non lineare

```r
set.seed(123)
x <- rnorm(100, mean = 0, sd = 1)
y <- x^2 + rnorm(100, mean = 0, sd = 0.5)  # Relazione quadratica
dati_non_lineari <- data.frame(x = x, y = y)

# Correlazione di Pearson
cor_pearson_non_lineare <- cor(dati_non_lineari$x, dati_non_lineari$y, method = "pearson")
cor_pearson_non_lineare  # Dovrebbe essere vicina a 0
```

4.2 Costruisci un grafico a dispersione

```r
ggplot(dati_non_lineari, aes(x = x, y = y)) +
  geom_point(size = 3) +
  labs(title = "Relazione Non Lineare con Correlazione Nulla",
       x = "Variabile X",
       y = "Variabile Y") +
  theme_minimal()
```

4.3 Calcola la correlazione di Spearman

```r
cor_spearman_non_lineare <- cor(dati_non_lineari$x, dati_non_lineari$y, method = "spearman")
cor_spearman_non_lineare  # Dovrebbe catturare la relazione non lineare
```
- **Confronto**: La correlazione di Spearman √® pi√π adatta per catturare relazioni non lineari.
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}
