<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>42&nbsp; La verosimiglianza – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/probability/16_likelihood_gauss.html" rel="next">
<link href="../../chapters/probability/14_gauss.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-51e9ba6732aa6bc0134376f05742e55b.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-520e231445e9edbb3e5f59e93d436564.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/15_likelihood.html"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La riforma metodologica in psicologia: dalla crisi alla rivoluzione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelli cognitivi</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/16_likelihood_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">La verosimiglianza gaussiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/17_likelihood_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Il rapporto di verosimiglianze</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata: un approccio frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Stima bayesiana della grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-principio-della-verosimiglianza" id="toc-il-principio-della-verosimiglianza" class="nav-link" data-scroll-target="#il-principio-della-verosimiglianza"><span class="header-section-number">42.1</span> Il Principio della Verosimiglianza</a></li>
  <li><a href="#la-log-verosimiglianza" id="toc-la-log-verosimiglianza" class="nav-link" data-scroll-target="#la-log-verosimiglianza"><span class="header-section-number">42.2</span> La Log-Verosimiglianza</a></li>
  <li><a href="#sequenza-di-lanci-di-una-moneta" id="toc-sequenza-di-lanci-di-una-moneta" class="nav-link" data-scroll-target="#sequenza-di-lanci-di-una-moneta"><span class="header-section-number">42.3</span> Sequenza di Lanci di una Moneta</a></li>
  <li><a href="#verosimiglianza-binomiale" id="toc-verosimiglianza-binomiale" class="nav-link" data-scroll-target="#verosimiglianza-binomiale"><span class="header-section-number">42.4</span> Verosimiglianza binomiale</a></li>
  <li><a href="#la-stima-di-massima-verosimiglianza" id="toc-la-stima-di-massima-verosimiglianza" class="nav-link" data-scroll-target="#la-stima-di-massima-verosimiglianza"><span class="header-section-number">42.5</span> La Stima di Massima Verosimiglianza</a></li>
  <li><a href="#calcolare-la-mle-in-r" id="toc-calcolare-la-mle-in-r" class="nav-link" data-scroll-target="#calcolare-la-mle-in-r"><span class="header-section-number">42.6</span> Calcolare la MLE in R</a></li>
  <li><a href="#verosimiglianza-congiunta" id="toc-verosimiglianza-congiunta" class="nav-link" data-scroll-target="#verosimiglianza-congiunta"><span class="header-section-number">42.7</span> Verosimiglianza Congiunta</a></li>
  <li><a href="#perch%C3%A9-%C3%A8-importante-la-verosimiglianza-congiunta" id="toc-perché-è-importante-la-verosimiglianza-congiunta" class="nav-link" data-scroll-target="#perch%C3%A9-%C3%A8-importante-la-verosimiglianza-congiunta"><span class="header-section-number">42.8</span> Perché è Importante la Verosimiglianza Congiunta?</a></li>
  <li><a href="#esempio-osservazioni-raggruppate" id="toc-esempio-osservazioni-raggruppate" class="nav-link" data-scroll-target="#esempio-osservazioni-raggruppate"><span class="header-section-number">42.9</span> Esempio: Osservazioni Raggruppate</a></li>
  <li><a href="#implementazione-in-r" id="toc-implementazione-in-r" class="nav-link" data-scroll-target="#implementazione-in-r"><span class="header-section-number">42.10</span> Implementazione in R</a></li>
  <li><a href="#la-verosimiglianza-marginale" id="toc-la-verosimiglianza-marginale" class="nav-link" data-scroll-target="#la-verosimiglianza-marginale"><span class="header-section-number">42.11</span> La Verosimiglianza Marginale</a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">42.12</span> Riflessioni Conclusive</a></li>
  <li><a href="#esercizi" id="toc-esercizi" class="nav-link" data-scroll-target="#esercizi">Esercizi</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/15_likelihood.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/15_likelihood.html"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-prob-likelihood" class="quarto-section-identifier"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In questo capitolo, imparerai a:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Comprendere il concetto di verosimiglianza</strong>: Scoprirai il ruolo fondamentale che la verosimiglianza svolge nella stima dei parametri statistici.</li>
<li>
<strong>Generare grafici della funzione di verosimiglianza</strong>:
<ul>
<li>Implementare grafici per la funzione di verosimiglianza nel caso binomiale.</li>
</ul>
</li>
<li>
<strong>Interpretare i grafici della funzione di verosimiglianza</strong>: Sviluppare le competenze necessarie per analizzare e trarre conclusioni dai grafici generati.</li>
<li>
<strong>Capire il principio di stima di massima verosimiglianza (MLE)</strong>: Approfondiremo il metodo di stima di massima verosimiglianza.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Leggere il capitolo <em>Estimation</em> <span class="citation" data-cites="schervish2014probability">(<a href="#ref-schervish2014probability" role="doc-biblioref">Schervish &amp; DeGroot, 2014</a>)</span>.</li>
<li>Leggere il capitolo <em>Bayes’ rule</em> <span class="citation" data-cites="Johnson2022bayesrules">(<a href="#ref-Johnson2022bayesrules" role="doc-biblioref">Johnson et al., 2022</a>)</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<section id="introduzione" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>I ricercatori utilizzano diversi modelli matematici per descrivere e prevedere il comportamento dei dati osservati. Questi modelli si distinguono tra loro per la struttura funzionale, ovvero il modo in cui collegano le variabili osservate con parametri teorici. La scelta del modello migliore avviene confrontando le previsioni teoriche generate dal modello con i dati effettivamente osservati. Il modello che produce previsioni più vicine ai dati reali viene considerato il più adeguato per descrivere il fenomeno studiato.</p>
<p>In questo processo di confronto, la funzione di verosimiglianza gioca un ruolo fondamentale. Essa quantifica la probabilità che i dati osservati siano stati generati da un particolare modello con determinati valori dei suoi parametri. In altre parole, la verosimiglianza misura quanto i dati siano compatibili con il modello ipotizzato.</p>
</section><section id="il-principio-della-verosimiglianza" class="level2" data-number="42.1"><h2 data-number="42.1" class="anchored" data-anchor-id="il-principio-della-verosimiglianza">
<span class="header-section-number">42.1</span> Il Principio della Verosimiglianza</h2>
<p>La <strong>verosimiglianza</strong> misura quanto sia plausibile ciascun valore dei parametri alla luce dei dati osservati. In altre parole, indica quanto ogni possibile valore dei parametri sia compatibile con i dati raccolti.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definizione 42.1</strong></span> Consideriamo un vettore aleatorio <span class="math inline">\(Y\)</span>, la cui distribuzione è descritta da una funzione di densità (nel caso continuo) oppure da una funzione di massa di probabilità (nel caso discreto), indicata con <span class="math inline">\(f(y \mid \theta)\)</span>, dove <span class="math inline">\(\theta\)</span> è un vettore di parametri appartenente allo spazio parametrico <span class="math inline">\(\Theta\)</span>.</p>
<p>Una volta osservato un valore specifico <span class="math inline">\(y\)</span> del vettore <span class="math inline">\(Y\)</span>, definiamo la <strong>funzione di verosimiglianza</strong> come una funzione che associa a ciascun valore possibile dei parametri <span class="math inline">\(\theta\)</span> la plausibilità di aver osservato proprio quei dati:</p>
<p><span class="math display">\[
L(\theta; y) = f(y \mid \theta).
\]</span></p>
<p>In questa espressione, i <strong>dati osservati <span class="math inline">\(y\)</span> sono considerati fissi</strong>, mentre la <strong>variabile di interesse è <span class="math inline">\(\theta\)</span></strong>. La funzione di verosimiglianza esprime quindi <strong>quanto ogni possibile valore di <span class="math inline">\(\theta\)</span> sia compatibile con i dati osservati</strong>.</p>
</div>
<section id="relazione-tra-verosimiglianza-e-funzione-di-probabilità" class="level3" data-number="42.1.1"><h3 data-number="42.1.1" class="anchored" data-anchor-id="relazione-tra-verosimiglianza-e-funzione-di-probabilità">
<span class="header-section-number">42.1.1</span> Relazione tra Verosimiglianza e Funzione di Probabilità</h3>
<p>Sia la funzione di probabilità (o densità) sia la funzione di verosimiglianza sono costruite sulla <strong>stessa espressione matematica</strong>: <span class="math inline">\(f(y \mid \theta)\)</span>. Tuttavia, <strong>il significato che attribuiamo a questa espressione cambia radicalmente</strong> a seconda del contesto inferenziale in cui ci troviamo.</p>
<p>La differenza tra funzione di probabilità e funzione di verosimiglianza riguarda il <strong>ruolo epistemologico</strong> assegnato ai dati e ai parametri:</p>
<ul>
<li><p><strong>Funzione di densità (o massa) di probabilità</strong>:<br>
In questo caso, assumiamo che i <strong>parametri <span class="math inline">\(\theta\)</span> siano noti</strong> e consideriamo i <strong>dati <span class="math inline">\(y\)</span> come variabili aleatorie</strong>. La funzione <span class="math inline">\(f(y \mid \theta)\)</span> rappresenta quindi il <strong>meccanismo generativo</strong> dei dati: ci dice quanto è probabile (o densa) l’osservazione di <span class="math inline">\(y\)</span>, se <span class="math inline">\(\theta\)</span> è fissato.</p></li>
<li><p><strong>Funzione di verosimiglianza</strong>:<br>
Qui la prospettiva si inverte: i <strong>dati <span class="math inline">\(y\)</span> sono fissi perché già osservati</strong>, mentre i <strong>parametri <span class="math inline">\(\theta\)</span> sono incogniti</strong> e rappresentano l’oggetto dell’inferenza. La funzione <span class="math inline">\(L(\theta; y) = f(y \mid \theta)\)</span> misura <strong>quanto ciascun valore possibile di <span class="math inline">\(\theta\)</span> sia compatibile con i dati osservati</strong>.</p></li>
</ul>
<p>Formalmente, la relazione è:</p>
<p><span class="math display">\[
L(\theta; y) = f(y \mid \theta)
\]</span></p>
<p>ma l’interpretazione è diversa:</p>
<ul>
<li>
<span class="math inline">\(f(y \mid \theta)\)</span> → <strong>probabilità di osservare <span class="math inline">\(y\)</span></strong>, dato <span class="math inline">\(\theta\)</span> (parametri fissi, dati variabili);</li>
<li>
<span class="math inline">\(L(\theta; y)\)</span> → <strong>plausibilità di <span class="math inline">\(\theta\)</span></strong>, dati gli <span class="math inline">\(y\)</span> osservati (dati fissi, parametri variabili).</li>
</ul>
<p>In sintesi:</p>
<ul>
<li>La <strong>funzione di probabilità</strong> risponde alla domanda:<br><em>“Se i parametri fossero questi, quanto è probabile osservare questi dati?”</em>
</li>
<li>La <strong>funzione di verosimiglianza</strong> risponde alla domanda:<br><em>“Dati questi dati, quali valori dei parametri sono più plausibili?”</em>
</li>
</ul>
<p>Questa distinzione è fondamentale per l’inferenza statistica: mentre la funzione di probabilità descrive il <strong>processo generativo</strong> dei dati, la verosimiglianza gioca un ruolo centrale nell’<strong>aggiornamento delle credenze sui parametri</strong>.</p>
<p>In particolare, nella prospettiva bayesiana, la verosimiglianza fornisce l’informazione derivante dai dati osservati, che viene <strong>combinata con le credenze precedenti</strong> (prior) per ottenere la <strong>distribuzione a posteriori</strong> dei parametri.</p>
<p>In sintesi, la verosimiglianza ci dice quanto i dati supportano i diversi valori dei parametri. Nella visione bayesiana, essa è lo strumento attraverso cui i dati <strong>modificano le nostre credenze</strong>.</p>
</section></section><section id="la-log-verosimiglianza" class="level2" data-number="42.2"><h2 data-number="42.2" class="anchored" data-anchor-id="la-log-verosimiglianza">
<span class="header-section-number">42.2</span> La Log-Verosimiglianza</h2>
<p>Per ragioni sia analitiche che computazionali, si lavora spesso con la <strong>log-verosimiglianza</strong>, ovvero il logaritmo della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\ell(\theta; y) = \log L(\theta; y) = \log f(y \mid \theta).
\]</span></p>
<p>La log-verosimiglianza presenta numerosi vantaggi:</p>
<ul>
<li>
<strong>Numerici</strong>: i prodotti di probabilità molto piccole possono causare problemi di underflow numerico. Il logaritmo trasforma questi prodotti in somme, che sono numericamente più stabili.</li>
<li>
<strong>Analitici</strong>: derivare la log-verosimiglianza rispetto ai parametri è spesso più semplice, facilitando la stima mediante metodi come il massimo della verosimiglianza (MLE).</li>
<li>
<strong>Additività</strong>: se i dati sono indipendenti, la log-verosimiglianza totale è la somma delle log-verosimiglianze individuali:</li>
</ul>
<p><span class="math display">\[
\ell(\theta; y_1, \dots, y_n) = \sum_{i=1}^{n} \log f(y_i \mid \theta).
\]</span></p>
<p>Questa proprietà è fondamentale nei modelli statistici in cui si assumono osservazioni indipendenti e identicamente distribuite (i.i.d.).</p>
<p>Infine, è importante ricordare che massimizzare la log-verosimiglianza è equivalente a massimizzare la verosimiglianza, poiché il logaritmo è una funzione monotona crescente. Per questo motivo, molte tecniche di stima e modellazione in statistica e machine learning sono formulate in termini di log-verosimiglianza.</p>
</section><section id="sequenza-di-lanci-di-una-moneta" class="level2" data-number="42.3"><h2 data-number="42.3" class="anchored" data-anchor-id="sequenza-di-lanci-di-una-moneta">
<span class="header-section-number">42.3</span> Sequenza di Lanci di una Moneta</h2>
<p>Per comprendere il concetto di <strong>verosimiglianza</strong>, iniziamo con un esempio semplice e intuitivo. Immagina di voler stimare la probabilità che una moneta cada su <strong>testa</strong>, e chiamiamo questa probabilità <span class="math inline">\(p_H\)</span>.</p>
<p>Il nostro obiettivo è capire <strong>quali valori di <span class="math inline">\(p_H\)</span> rendono più plausibile ciò che abbiamo osservato</strong>. Per farlo, vedremo come calcolare la probabilità di osservare determinate sequenze di lanci, assumendo che ogni lancio sia <strong>indipendente</strong> dagli altri.</p>
<section id="perché-moltiplichiamo-le-probabilità" class="level3" data-number="42.3.1"><h3 data-number="42.3.1" class="anchored" data-anchor-id="perché-moltiplichiamo-le-probabilità">
<span class="header-section-number">42.3.1</span> Perché moltiplichiamo le probabilità?</h3>
<p>Se lanciamo una moneta più volte, ogni risultato è indipendente dai precedenti: il fatto che esca “testa” o “croce” in un lancio <strong>non influenza</strong> il risultato del lancio successivo. Questo significa che la probabilità di osservare una specifica sequenza si ottiene <strong>moltiplicando le probabilità di ciascun singolo lancio</strong>.</p>
<p>Ad esempio, supponiamo che la probabilità di testa sia <span class="math inline">\(p_H\)</span>, e quindi la probabilità di croce sarà <span class="math inline">\(1 - p_H\)</span>. Se osserviamo la sequenza <strong>HTHT</strong>, allora la probabilità di ottenere questa sequenza è:</p>
<p><span class="math display">\[
P(\text{HTHT} \mid p_H) = p_H \cdot (1 - p_H) \cdot p_H \cdot (1 - p_H) = p_H^2 (1 - p_H)^2.
\]</span></p>
<p>Se invece osserviamo una sequenza come <strong>THTH</strong> o <strong>HHTT</strong>, la probabilità è la stessa: <span class="math inline">\(p_H^2 (1 - p_H)^2.\)</span> Questo perché <strong>non ci interessa l’ordine dei lanci</strong>, ma solo <strong>quante teste</strong> e <strong>quante croci</strong> abbiamo ottenuto.</p>
</section><section id="generalizzazione" class="level3" data-number="42.3.2"><h3 data-number="42.3.2" class="anchored" data-anchor-id="generalizzazione">
<span class="header-section-number">42.3.2</span> Generalizzazione</h3>
<p>In generale, se lanciamo una moneta <strong><span class="math inline">\(n\)</span> volte</strong> e osserviamo <strong><span class="math inline">\(y\)</span> teste</strong> (successi) e <strong><span class="math inline">\(n - y\)</span> croci</strong> (insuccessi), la probabilità di ottenere una qualsiasi sequenza con questa configurazione è:</p>
<p><span id="eq-like-seq-tossess-coin"><span class="math display">\[
P(Y = y \mid p_H) = p_H^y (1 - p_H)^{n - y}.
\tag{42.1}\]</span></span></p>
<p>Questa espressione è la <strong>funzione di verosimiglianza</strong>, perché ci dice <strong>quanto un certo valore di <span class="math inline">\(p_H\)</span> è compatibile con i dati osservati</strong>.</p>
</section><section id="connessione-con-la-distribuzione-binomiale" class="level3" data-number="42.3.3"><h3 data-number="42.3.3" class="anchored" data-anchor-id="connessione-con-la-distribuzione-binomiale">
<span class="header-section-number">42.3.3</span> Connessione con la Distribuzione Binomiale</h3>
<p>L’<a href="#eq-like-seq-tossess-coin" class="quarto-xref">Equazione&nbsp;<span class="quarto-unresolved-ref">eq-like-seq-tossess-coin</span></a> è anche il <strong>nucleo della distribuzione binomiale</strong>. La formula completa per la probabilità binomiale è:</p>
<p><span class="math display">\[
P(Y = y \mid p_H) = \binom{n}{y} p_H^y (1 - p_H)^{n - y}.
\]</span></p>
<p>L’unica differenza è il <strong>coefficiente binomiale</strong> <span class="math inline">\(\binom{n}{y}\)</span>, che conta <strong>quante sequenze diverse</strong> portano allo stesso numero di successi e insuccessi.</p>
<p>Quando calcoliamo la <strong>verosimiglianza</strong>, possiamo <strong>ignorare questo coefficiente</strong>: non dipende da <span class="math inline">\(p_H\)</span>, quindi <strong>non influisce sul valore che massimizza la funzione</strong>. Per questo motivo, usiamo solo il “nucleo” della funzione binomiale:</p>
<p><span id="eq-like-binom-nucleo"><span class="math display">\[
L(p_H \mid y) = p_H^y (1 - p_H)^{n - y}.
\tag{42.2}\]</span></span></p>
</section><section id="esempio-1-due-lanci" class="level3" data-number="42.3.4"><h3 data-number="42.3.4" class="anchored" data-anchor-id="esempio-1-due-lanci">
<span class="header-section-number">42.3.4</span> Esempio 1: Due lanci</h3>
<p>Immaginiamo di lanciare una moneta <strong>due volte</strong> e di osservare una <strong>testa</strong> e una <strong>croce</strong>. Il nostro obiettivo è stimare <span class="math inline">\(p_H\)</span>, cioè la probabilità che la moneta cada su testa. Per iniziare, valutiamo <strong>quanto sono plausibili</strong> due diversi valori di <span class="math inline">\(p_H\)</span> alla luce dei dati osservati, calcolando la funzione di verosimiglianza.</p>
<ul>
<li>
<p>Se <span class="math inline">\(p_H = 0.5\)</span> (una moneta equa), allora:</p>
<p><span class="math display">\[
L(0.5) = 0.5^1 \cdot (1 - 0.5)^1 = 0.25 .
\]</span></p>
</li>
<li>
<p>Se <span class="math inline">\(p_H = 0.4\)</span>, allora:</p>
<p><span class="math display">\[
L(0.4) = 0.4^1 \cdot 0.6^1 = 0.24 .
\]</span></p>
</li>
</ul>
<p>In entrambi i casi, la <strong>funzione di verosimiglianza</strong> ci dice quanto sia plausibile quel valore di <span class="math inline">\(p_H\)</span>, dati i risultati che abbiamo osservato (una testa e una croce). Come possiamo notare, il valore <span class="math inline">\(p_H = 0.5\)</span> è <strong>più compatibile</strong> con i dati osservati rispetto a <span class="math inline">\(p_H = 0.4\)</span>.</p>
</section><section id="calcolo-della-verosimiglianza-su-una-griglia-di-valori" class="level3" data-number="42.3.5"><h3 data-number="42.3.5" class="anchored" data-anchor-id="calcolo-della-verosimiglianza-su-una-griglia-di-valori">
<span class="header-section-number">42.3.5</span> Calcolo della verosimiglianza su una griglia di valori</h3>
<p>Ora ripetiamo questo calcolo per <strong>molti valori diversi</strong> di <span class="math inline">\(p_H\)</span>, compresi tra 0 e 1. Questo ci permette di <strong>visualizzare la funzione di verosimiglianza</strong> e di vedere per quali valori di <span class="math inline">\(p_H\)</span> essa è più alta.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri osservati</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">2</span>             <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span>             <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Griglia di valori per p_H da 0 a 1</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">p_H</span><span class="op">^</span><span class="va">y</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p_H</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="rappresentazione-grafica" class="level3" data-number="42.3.6"><h3 data-number="42.3.6" class="anchored" data-anchor-id="rappresentazione-grafica">
<span class="header-section-number">42.3.6</span> Rappresentazione grafica</h3>
<p>Il grafico seguente mostra la <strong>funzione di verosimiglianza</strong> per i 100 valori di <span class="math inline">\(p_H\)</span>. Ogni punto della curva indica <strong>quanto è compatibile</strong> quel valore di <span class="math inline">\(p_H\)</span> con i dati che abbiamo osservato.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="va">okabe_ito_palette</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Verosimiglianza per 2 Lanci\n(1 Testa, 1 Croce)"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="15_likelihood_files/figure-html/unnamed-chunk-3-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="15_likelihood_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</div>
</div>
</section><section id="cosa-ci-dice-il-grafico" class="level3" data-number="42.3.7"><h3 data-number="42.3.7" class="anchored" data-anchor-id="cosa-ci-dice-il-grafico">
<span class="header-section-number">42.3.7</span> Cosa ci dice il grafico?</h3>
<ul>
<li>La funzione di verosimiglianza raggiunge il suo massimo per <strong><span class="math inline">\(p_H = 0.5\)</span></strong>, che corrisponde alla proporzione osservata (1 testa su 2 lanci).</li>
<li>Questo valore di <span class="math inline">\(p_H\)</span> è quindi il <strong>più plausibile</strong> secondo i dati: rappresenta la <strong>stima di massima verosimiglianza</strong>.</li>
<li>I valori estremi (vicini a 0 o a 1) hanno verosimiglianza molto bassa: <strong>non spiegano bene</strong> il fatto che abbiamo ottenuto <strong>una testa e una croce</strong>.</li>
</ul>
<p>Ecco una versione migliorata e più didattica del tuo <strong>Esempio 2: Tre lanci</strong>, pensata per studenti che stanno imparando il concetto di verosimiglianza passo dopo passo. Mantiene l’approccio del primo esempio e rafforza la continuità logica, l’intuizione e l’interpretazione dei risultati, con un linguaggio semplice ma preciso.</p>
</section><section id="esempio-2-tre-lanci" class="level3" data-number="42.3.8"><h3 data-number="42.3.8" class="anchored" data-anchor-id="esempio-2-tre-lanci">
<span class="header-section-number">42.3.8</span> Esempio 2: Tre lanci</h3>
<p>Proseguiamo con un secondo esperimento: lanciamo una moneta <strong>tre volte</strong>, e otteniamo <strong>una testa e due croci</strong>. Anche in questo caso, vogliamo stimare <span class="math inline">\(p_H\)</span>, la probabilità che la moneta cada su testa, e valutare <strong>quali valori di <span class="math inline">\(p_H\)</span> sono più compatibili</strong> con i dati osservati.</p>
<p>Iniziamo calcolando la <strong>verosimiglianza</strong> per due valori specifici:</p>
<ul>
<li><p>Se <span class="math inline">\(p_H = 0.5\)</span> (moneta equa): <span class="math display">\[
L(0.5) = 0.5^1 \cdot (1 - 0.5)^2 = 0.5 \cdot 0.25 = 0.125
\]</span></p></li>
<li><p>Se <span class="math inline">\(p_H = 0.4\)</span>: <span class="math display">\[
L(0.4) = 0.4^1 \cdot 0.6^2 = 0.4 \cdot 0.36 = 0.144
\]</span></p></li>
</ul>
<p>Come vediamo, in questo caso <strong><span class="math inline">\(p_H = 0.4\)</span> ha una verosimiglianza maggiore</strong> rispetto a <span class="math inline">\(p_H = 0.5\)</span>: questo suggerisce che il valore 0.4 spiega meglio i dati (1 testa su 3 lanci) rispetto alla moneta equa.</p>
</section><section id="calcolo-su-una-griglia-di-valori" class="level3" data-number="42.3.9"><h3 data-number="42.3.9" class="anchored" data-anchor-id="calcolo-su-una-griglia-di-valori">
<span class="header-section-number">42.3.9</span> Calcolo su una griglia di valori</h3>
<p>Calcoliamo ora la verosimiglianza per <strong>100 valori compresi tra 0 e 1</strong> per visualizzare la funzione su tutto l’intervallo di probabilità.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri osservati</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">3</span>             <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span>             <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Sequenza di valori possibili per p_H</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">p_H</span><span class="op">^</span><span class="va">y</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p_H</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="rappresentazione-grafica-1" class="level3" data-number="42.3.10"><h3 data-number="42.3.10" class="anchored" data-anchor-id="rappresentazione-grafica-1">
<span class="header-section-number">42.3.10</span> Rappresentazione grafica</h3>
<p>Il grafico seguente mostra la <strong>funzione di verosimiglianza</strong>: ci dice quanto ogni valore di <span class="math inline">\(p_H\)</span> è compatibile con l’osservazione di <strong>1 testa e 2 croci</strong>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Mostra la curva della verosimiglianza</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="va">okabe_ito_palette</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Verosimiglianza per 3 Lanci\n(1 Testa, 2 Croci)"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="15_likelihood_files/figure-html/unnamed-chunk-5-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="15_likelihood_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</div>
</div>
</section><section id="cosa-osserviamo" class="level3" data-number="42.3.11"><h3 data-number="42.3.11" class="anchored" data-anchor-id="cosa-osserviamo">
<span class="header-section-number">42.3.11</span> Cosa osserviamo?</h3>
<ul>
<li>Il <strong>massimo</strong> della funzione di verosimiglianza <strong>non è più in <span class="math inline">\(p_H = 0.5\)</span></strong>, ma <strong>più vicino a 0.33</strong>, cioè alla proporzione osservata (1 testa su 3 lanci).</li>
<li>Questo riflette il fatto che il valore di <span class="math inline">\(p_H\)</span> che <strong>meglio spiega i dati</strong> è quello che riproduce la frequenza osservata.</li>
<li>La curva è <strong>più stretta</strong> rispetto al caso con 2 lanci: abbiamo più informazioni, quindi possiamo stimare <span class="math inline">\(p_H\)</span> con maggiore precisione.</li>
<li>Anche qui, i valori estremi di <span class="math inline">\(p_H\)</span> (vicino a 0 o 1) hanno verosimiglianze basse, perché <strong>non giustificano bene</strong> l’osservazione di <strong>una testa e due croci</strong>.</li>
</ul></section><section id="interpretazione-dei-risultati" class="level3" data-number="42.3.12"><h3 data-number="42.3.12" class="anchored" data-anchor-id="interpretazione-dei-risultati">
<span class="header-section-number">42.3.12</span> Interpretazione dei Risultati</h3>
<ul>
<li>La funzione di verosimiglianza <strong>raggiunge il massimo</strong> per valori di <span class="math inline">\(p_H\)</span> vicini alla <strong>proporzione di teste osservata</strong>.</li>
<li>Quando il numero di lanci <strong>aumenta</strong>, la curva diventa più “stretta”: i dati ci permettono di stimare <span class="math inline">\(p_H\)</span> in modo più preciso.</li>
<li>Valori estremi di <span class="math inline">\(p_H\)</span> (vicini a 0 o 1) hanno <strong>verosimiglianze basse</strong>: non spiegano bene i dati osservati.</li>
</ul>
<p><strong>In sintesi</strong>, abbiamo visto come costruire la funzione di verosimiglianza <strong>a partire dai dati osservati</strong>, senza usare direttamente la distribuzione binomiale completa.<br>
La tua sezione è già molto chiara, ma possiamo migliorarla ulteriormente per renderla ancora più <strong>didattica e accessibile a studenti principianti</strong>, mantenendo un linguaggio preciso e coerente con le sezioni precedenti.<br>
L’obiettivo è aiutare gli studenti a vedere <strong>come la distribuzione binomiale si collega alla verosimiglianza</strong> e alla stima del parametro.</p>
</section></section><section id="verosimiglianza-binomiale" class="level2" data-number="42.4"><h2 data-number="42.4" class="anchored" data-anchor-id="verosimiglianza-binomiale">
<span class="header-section-number">42.4</span> Verosimiglianza binomiale</h2>
<p>Torniamo al nostro esperimento con la moneta, ma questa volta lo rendiamo un po’ più realistico: lanciamo la moneta <strong><span class="math inline">\(n = 30\)</span> volte</strong> e otteniamo <strong><span class="math inline">\(y = 23\)</span> teste</strong>.</p>
<p>Per modellare il numero totale di teste osservate, utilizziamo la <strong>distribuzione binomiale</strong>, che ci dice qual è la probabilità di ottenere esattamente <span class="math inline">\(y\)</span> successi su <span class="math inline">\(n\)</span> prove, quando la probabilità di successo in ogni singolo lancio è <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
P(Y = y \mid \theta) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p>In questo contesto:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> è una <strong>variabile casuale</strong> che rappresenta il numero di teste ottenute,</li>
<li>
<span class="math inline">\(y = 23\)</span> è il <strong>valore osservato</strong>,</li>
<li>
<span class="math inline">\(\theta\)</span> (o <span class="math inline">\(p_H\)</span>) è la <strong>probabilità incognita</strong> di ottenere testa in un singolo lancio.</li>
</ul>
<section id="dalla-distribuzione-alla-verosimiglianza" class="level3" data-number="42.4.1"><h3 data-number="42.4.1" class="anchored" data-anchor-id="dalla-distribuzione-alla-verosimiglianza">
<span class="header-section-number">42.4.1</span> Dalla distribuzione alla verosimiglianza</h3>
<p>Una volta osservati i dati (<span class="math inline">\(y = 23\)</span>), possiamo considerarli <strong>fissati</strong> e analizzare <strong>quanto ciascun valore possibile del parametro <span class="math inline">\(\theta\)</span></strong> (la probabilità di ottenere testa) <strong>sia compatibile con questi dati</strong>. Per farlo, possiamo <strong>riutilizzare direttamente la formula della distribuzione binomiale</strong>, trattandola come <strong>funzione di <span class="math inline">\(\theta\)</span></strong> anziché come funzione di <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
L(\theta \mid y = 23) = \binom{30}{23} \theta^{23} (1 - \theta)^7.
\]</span></p>
<p>Questa è la <strong>funzione di verosimiglianza</strong>, che ci dice quanto ogni valore di <span class="math inline">\(\theta\)</span> sia plausibile alla luce dei dati osservati. A differenza degli esempi precedenti (in cui abbiamo ignorato le costanti moltiplicative), qui <strong>non abbiamo bisogno di semplificare</strong> la formula: la funzione <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> in R calcola <strong>automaticamente l’intera espressione</strong>, costante inclusa.</p>
</section><section id="visualizzazione-della-funzione-di-verosimiglianza" class="level3" data-number="42.4.2"><h3 data-number="42.4.2" class="anchored" data-anchor-id="visualizzazione-della-funzione-di-verosimiglianza">
<span class="header-section-number">42.4.2</span> Visualizzazione della funzione di verosimiglianza</h3>
<p>Il codice seguente costruisce il <strong>grafico della funzione di verosimiglianza</strong>, calcolando per ogni valore di <span class="math inline">\(\theta\)</span> la probabilità di osservare <strong>23 successi su 30 prove</strong>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri osservati</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span>       <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">23</span>       <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Griglia di valori possibili per p_H</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza (usando la binomiale completa)</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p_H</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Creazione del data frame</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Grafico della funzione di verosimiglianza</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">data</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="va">okabe_ito_palette</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Verosimiglianza per 30 Lanci di Moneta (23 Teste)"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="15_likelihood_files/figure-html/unnamed-chunk-6-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="15_likelihood_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</div>
</div>
</section><section id="interpretazione-del-risultato" class="level3" data-number="42.4.3"><h3 data-number="42.4.3" class="anchored" data-anchor-id="interpretazione-del-risultato">
<span class="header-section-number">42.4.3</span> Interpretazione del risultato</h3>
<p>Osservando il grafico, vediamo che:</p>
<ul>
<li>la funzione di verosimiglianza raggiunge il suo <strong>massimo</strong> in corrispondenza di <strong><span class="math inline">\(p_H \approx 0.77\)</span></strong>;</li>
<li>questo significa che il valore di <span class="math inline">\(\theta\)</span> (cioè <span class="math inline">\(p_H\)</span>) che rende <strong>più plausibile</strong> l’osservazione di 23 teste su 30 è circa <strong>0.77</strong>;</li>
<li>in altre parole, <strong>la stima di massima verosimiglianza (MLE)</strong> per la probabilità di ottenere testa è: <span class="math display">\[
\hat{p}_H = \frac{23}{30} \approx 0.767.
\]</span>
</li>
</ul>
<p>Questo risultato è del tutto intuitivo: <strong>la stima migliore</strong> è la <strong>proporzione osservata</strong> di teste.<br>
La verosimiglianza ci mostra <strong>quali altri valori di <span class="math inline">\(\theta\)</span> sono meno compatibili</strong> con i dati.</p>
<p><strong>In sintesi</strong>, abbiamo visto come utilizzare <strong>direttamente la distribuzione binomiale</strong> per costruire la funzione di verosimiglianza. Questa funzione è uno <strong>strumento fondamentale</strong> per confrontare diversi valori possibili del parametro <span class="math inline">\(\theta\)</span> (cioè la probabilità di successo) <strong>alla luce dei dati osservati</strong>.</p>
<p>Nel caso della moneta lanciata 30 volte con 23 teste, abbiamo trattato il numero di successi osservati (<span class="math inline">\(y = 23\)</span>) come un dato fisso, e abbiamo valutato per <strong>quali valori di <span class="math inline">\(\theta\)</span></strong> questa osservazione sarebbe più plausibile.</p>
<p>In R, per calcolare la funzione di verosimiglianza <strong>non serve riscrivere manualmente la formula della binomiale</strong>. Possiamo usare la funzione <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code>, che calcola le probabilità (o masse) della <strong>distribuzione binomiale</strong> per un dato numero di successi <span class="math inline">\(y\)</span>, un numero totale di prove <span class="math inline">\(n\)</span>, e una certa probabilità di successo <span class="math inline">\(\theta\)</span>.</p>
<p>Ad esempio, nel codice:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p_H</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>
<code>y</code> è il numero di successi osservati (23),</li>
<li>
<code>n</code> è il numero totale di prove (30),</li>
<li>
<code>p_H</code> è un vettore di valori di <span class="math inline">\(\theta\)</span> tra 0 e 1.</li>
</ul>
<p>Il risultato <code>likelihood</code> è un vettore che contiene, per ciascun valore di <span class="math inline">\(\theta\)</span>, la <strong>verosimiglianza</strong> associata a quel valore: cioè, <strong>quanto quel valore di <span class="math inline">\(\theta\)</span> è compatibile con i dati</strong> che abbiamo osservato.</p>
<p>Riassumendo:</p>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> fornisce la <strong>funzione di probabilità</strong> della binomiale,</li>
<li>fissando <span class="math inline">\(y\)</span> e variando <span class="math inline">\(\theta\)</span>, usiamo <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> per costruire la <strong>funzione di verosimiglianza</strong>,</li>
<li>possiamo poi <strong>tracciare un grafico</strong> di questa funzione per <strong>visualizzare quali valori di <span class="math inline">\(\theta\)</span> sono più plausibili</strong>.</li>
</ul>
<p>Questa strategia mostra come la verosimiglianza possa essere costruita <strong>direttamente a partire da una distribuzione conosciuta</strong> (in questo caso, la binomiale) e <strong>implementata facilmente in R</strong> con strumenti standard.</p>
</section></section><section id="la-stima-di-massima-verosimiglianza" class="level2" data-number="42.5"><h2 data-number="42.5" class="anchored" data-anchor-id="la-stima-di-massima-verosimiglianza">
<span class="header-section-number">42.5</span> La Stima di Massima Verosimiglianza</h2>
<p>Nel momento in cui osserviamo dei dati e vogliamo stimare un parametro incognito (ad esempio, la probabilità <span class="math inline">\(\theta\)</span> che una moneta dia “testa”), un metodo classico è la <strong>stima di massima verosimiglianza</strong> (<em>Maximum Likelihood Estimation</em>, o <strong>MLE</strong>).</p>
<p>Anche se nella prospettiva bayesiana ci interessa la <strong>distribuzione completa</strong> dei valori plausibili del parametro (e non un singolo valore stimato), è utile conoscere il concetto di <strong>MLE</strong>, che rappresenta il <strong>valore di <span class="math inline">\(\theta\)</span> che rende i dati osservati più compatibili</strong> con il modello. Più avanti vedremo come la MLE corrisponde, nel caso di una prior uniforme, al <strong>valore massimo della distribuzione a posteriori</strong>.</p>
<section id="lidea-di-fondo" class="level3" data-number="42.5.1"><h3 data-number="42.5.1" class="anchored" data-anchor-id="lidea-di-fondo">
<span class="header-section-number">42.5.1</span> L’idea di Fondo</h3>
<p>La logica è semplice. Una volta osservati i dati, ci chiediamo: <strong>quali valori del parametro sono più compatibili con ciò che abbiamo visto?</strong> Il valore che risulta più plausibile è la stima di massima verosimiglianza.</p>
<p>Immagina di “provare” tanti valori di <span class="math inline">\(\theta\)</span>, e per ciascuno chiederti: <em>“se fosse questo il vero valore di <span class="math inline">\(\theta\)</span>, quanto sarebbe plausibile osservare questi dati?”</em> Il valore che <strong>meglio spiega i dati</strong> è quello scelto come stima.</p>
</section><section id="un-esempio-concreto" class="level3" data-number="42.5.2"><h3 data-number="42.5.2" class="anchored" data-anchor-id="un-esempio-concreto">
<span class="header-section-number">42.5.2</span> Un Esempio Concreto</h3>
<p>Hai lanciato una moneta <strong>30 volte</strong> e hai osservato <strong>23 teste</strong>. La tua domanda è: <em>quanto è sbilanciata la moneta?</em></p>
<p>Utilizziamo la <strong>funzione di verosimiglianza</strong>, che ci dice <strong>quanto ogni valore possibile di <span class="math inline">\(\theta\)</span> è compatibile con i dati osservati</strong>. Più alta è la verosimiglianza, più plausibile è il valore di <span class="math inline">\(\theta\)</span>.</p>
</section><section id="una-metafora-visiva" class="level3" data-number="42.5.3"><h3 data-number="42.5.3" class="anchored" data-anchor-id="una-metafora-visiva">
<span class="header-section-number">42.5.3</span> Una metafora visiva</h3>
<p>Immagina di osservare una curva che rappresenta la <strong>verosimiglianza</strong> al variare del parametro <span class="math inline">\(\theta\)</span>.<br>
La curva si alza, raggiunge un punto massimo, e poi scende: proprio come una collina.</p>
<p>Quel punto più alto della curva rappresenta il valore di <span class="math inline">\(\theta\)</span> che è <strong>più compatibile con i dati osservati</strong>: è il valore per cui i dati risultano <strong>più verosimili</strong>, secondo il modello.</p>
<p>Quel valore è chiamato <strong>stima di massima verosimiglianza</strong> (<em>maximum likelihood estimate</em>): è il punto in cima alla collina della verosimiglianza.</p>
</section><section id="come-si-trova-il-massimo" class="level3" data-number="42.5.4"><h3 data-number="42.5.4" class="anchored" data-anchor-id="come-si-trova-il-massimo">
<span class="header-section-number">42.5.4</span> Come si Trova il Massimo?</h3>
<p>Dal punto di vista matematico, il massimo di una funzione si trova dove la sua <strong>pendenza</strong> (la derivata) è <strong>zero</strong>: cioè, dove smette di salire e inizia a scendere.</p>
<p>Per la verosimiglianza binomiale (trasformata in log-verosimiglianza), questo calcolo si può fare in modo esatto. Se hai osservato <span class="math inline">\(y\)</span> successi su <span class="math inline">\(n\)</span> prove, la log-verosimiglianza è:</p>
<p><span class="math display">\[
\ell(\theta) = y \log \theta + (n - y) \log(1 - \theta),
\]</span></p>
<p>e la derivata si annulla quando:</p>
<p><span class="math display">\[
\hat{\theta} = \frac{y}{n}.
\]</span></p>
<p>In altre parole, <strong>la MLE è semplicemente la proporzione osservata di successi</strong>.</p>
</section><section id="un-punto-importante-per-lapproccio-bayesiano" class="level3" data-number="42.5.5"><h3 data-number="42.5.5" class="anchored" data-anchor-id="un-punto-importante-per-lapproccio-bayesiano">
<span class="header-section-number">42.5.5</span> Un punto Importante per l’Approccio Bayesiano</h3>
<p>Nel contesto bayesiano, <strong>non ci limitiamo a cercare il valore più plausibile del parametro</strong>, ma costruiamo una distribuzione completa che rappresenta <strong>l’incertezza su <span class="math inline">\(\theta\)</span></strong>. Tuttavia, <strong>il punto in cui la distribuzione a posteriori raggiunge il suo massimo</strong> viene chiamato <strong>MAP (Maximum A Posteriori)</strong>, e se assumiamo una <strong>distribuzione a priori uniforme</strong>, <strong>MLE e MAP coincidono</strong>. Quindi, la MLE può essere vista come un <strong>caso speciale</strong> del ragionamento bayesiano, utile per introdurre il concetto di compatibilità tra parametri e dati.</p>
</section></section><section id="calcolare-la-mle-in-r" class="level2" data-number="42.6"><h2 data-number="42.6" class="anchored" data-anchor-id="calcolare-la-mle-in-r">
<span class="header-section-number">42.6</span> Calcolare la MLE in R</h2>
<section id="metodo-1-valutazione-su-griglia" class="level3" data-number="42.6.1"><h3 data-number="42.6.1" class="anchored" data-anchor-id="metodo-1-valutazione-su-griglia">
<span class="header-section-number">42.6.1</span> Metodo 1: Valutazione su Griglia</h3>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">23</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">10000</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della verosimiglianza binomiale</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Trova il massimo</span></span>
<span><span class="va">max_index</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="va">likelihood</span><span class="op">)</span></span>
<span><span class="va">optimal_theta</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="va">max_index</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Risultato</span></span>
<span><span class="va">optimal_theta</span></span>
<span><span class="co">#&gt; [1] 0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Spiegazione:</strong></p>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> calcola la verosimiglianza per ogni valore di <span class="math inline">\(\theta\)</span>;</li>
<li>
<code><a href="https://rdrr.io/r/base/which.min.html">which.max()</a></code> individua il massimo;</li>
<li>
<code>theta[max_index]</code> restituisce la stima MLE.</li>
</ul></section><section id="metodo-2-ottimizzazione-numerica" class="level3" data-number="42.6.2"><h3 data-number="42.6.2" class="anchored" data-anchor-id="metodo-2-ottimizzazione-numerica">
<span class="header-section-number">42.6.2</span> Metodo 2: Ottimizzazione Numerica</h3>
<p>In alternativa, possiamo trovare la MLE <strong>senza usare griglie</strong>, con un approccio più efficiente.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione log-verosimiglianza negativa</span></span>
<span><span class="va">neg_log_likelihood</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">-</span> <span class="op">(</span><span class="va">y</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Ottimizzazione numerica</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  fn <span class="op">=</span> <span class="va">neg_log_likelihood</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"Brent"</span>,</span>
<span>  lower <span class="op">=</span> <span class="fl">1e-6</span>,</span>
<span>  upper <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fl">1e-6</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">optimal_theta_numerical</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">optimal_theta_numerical</span></span>
<span><span class="co">#&gt; [1] 0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Nota:</strong> abbiamo calcolato la <strong>log-verosimiglianza negativa</strong>, perché <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> cerca <strong>minimi</strong> per default.</p>
</section><section id="confronto-tra-le-soluzioni" class="level3" data-number="42.6.3"><h3 data-number="42.6.3" class="anchored" data-anchor-id="confronto-tra-le-soluzioni">
<span class="header-section-number">42.6.3</span> Confronto tra le Soluzioni</h3>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"Griglia"</span> <span class="op">=</span> <span class="va">optimal_theta</span>, </span>
<span>  <span class="st">"Ottimizzazione"</span> <span class="op">=</span> <span class="va">optimal_theta_numerical</span>, </span>
<span>  <span class="st">"Analitica"</span> <span class="op">=</span> <span class="va">y</span> <span class="op">/</span> <span class="va">n</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;        Griglia Ottimizzazione      Analitica </span></span>
<span><span class="co">#&gt;         0.7667         0.7667         0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Tutti i metodi restituiscono lo stesso risultato:<br><span class="math display">\[
\hat{\theta} = \frac{23}{30} \approx 0.767.
\]</span></p>
</section></section><section id="verosimiglianza-congiunta" class="level2" data-number="42.7"><h2 data-number="42.7" class="anchored" data-anchor-id="verosimiglianza-congiunta">
<span class="header-section-number">42.7</span> Verosimiglianza Congiunta</h2>
<p>Abbiamo visto che, nel caso di una sequenza di <span class="math inline">\(n\)</span> lanci di una moneta, la funzione di <strong>verosimiglianza</strong> si basa sulla distribuzione binomiale. In questo caso, trattiamo un esperimento Bernoulliano ripetuto <span class="math inline">\(n\)</span> volte, e la nostra osservazione è <strong>il numero totale di successi</strong> (teste). Il numero complessivo di successi segue una <strong>distribuzione binomiale</strong>, e la funzione di verosimiglianza assume la forma:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = P(Y = y \mid \theta) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p>Qui la verosimiglianza è espressa direttamente in termini del numero totale di successi e insuccessi, senza dover scrivere il contributo di ogni singolo lancio.</p>
<p>Tuttavia, possiamo affrontare la questione da una prospettiva diversa: <strong>invece di considerare il numero totale di successi, possiamo pensare alla verosimiglianza come il prodotto delle probabilità di ogni singolo lancio</strong>. Questo ci porta a una generalizzazione importante: la <strong>verosimiglianza congiunta</strong> di più osservazioni indipendenti.</p>
<section id="dal-caso-binomiale-alla-verosimiglianza-congiunta" class="level3" data-number="42.7.1"><h3 data-number="42.7.1" class="anchored" data-anchor-id="dal-caso-binomiale-alla-verosimiglianza-congiunta">
<span class="header-section-number">42.7.1</span> Dal Caso Binomiale alla Verosimiglianza Congiunta</h3>
<p>Nel caso dei lanci della moneta, le singole osservazioni sono <strong>prove Bernoulliane indipendenti</strong>, ovvero ogni singolo lancio è un’osservazione indipendente che segue una distribuzione Bernoulli con parametro <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
P(Y_i = 1 \mid \theta) = \theta, \quad P(Y_i = 0 \mid \theta) = 1 - \theta.
\]</span></p>
<p>Se trattiamo ogni prova individualmente, la funzione di verosimiglianza per una <strong>singola osservazione</strong> è:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y_i) = \theta^{y_i} (1 - \theta)^{1 - y_i}.
\]</span></p>
<p>Ora, per un campione di <span class="math inline">\(n\)</span> osservazioni indipendenti, la <strong>verosimiglianza congiunta</strong> è il prodotto delle verosimiglianze delle singole osservazioni:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y_1, y_2, \dots, y_n) = \prod_{i=1}^{n} \theta^{y_i} (1 - \theta)^{1 - y_i}.
\]</span></p>
<p>Riconosciamo che questa espressione è identica alla funzione di verosimiglianza della distribuzione binomiale, perché il numero totale di successi è:</p>
<p><span class="math display">\[
y = \sum_{i=1}^{n} y_i.
\]</span></p>
<p>Quindi, riscrivendo la verosimiglianza congiunta, otteniamo:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = \theta^{\sum y_i} (1 - \theta)^{n - \sum y_i} = \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p><strong>Questa è proprio la verosimiglianza della distribuzione binomiale!</strong> Questo mostra che il caso binomiale può essere visto come una forma compatta di verosimiglianza congiunta per prove Bernoulliane indipendenti.</p>
</section></section><section id="perché-è-importante-la-verosimiglianza-congiunta" class="level2" data-number="42.8"><h2 data-number="42.8" class="anchored" data-anchor-id="perché-è-importante-la-verosimiglianza-congiunta">
<span class="header-section-number">42.8</span> Perché è Importante la Verosimiglianza Congiunta?</h2>
<p>L’idea della <strong>verosimiglianza congiunta</strong> è fondamentale perché ci permette di estendere i concetti di verosimiglianza dal caso di una singola osservazione al caso di molte osservazioni indipendenti. Questo è utile in molti contesti statistici:</p>
<ol type="1">
<li>
<strong>stimare parametri</strong> basandosi su un intero campione invece che su una singola osservazione;</li>
<li>
<strong>definire modelli statistici più complessi</strong>, in cui le osservazioni sono indipendenti ma possono avere diverse distribuzioni.</li>
</ol>
<p>In sintesi, l’esempio binomiale mostra che la verosimiglianza congiunta di prove Bernoulliane indipendenti si riconduce alla verosimiglianza binomiale. Tuttavia, la vera potenza della verosimiglianza congiunta si manifesta in distribuzioni continue come la normale, dove la produttoria delle densità di probabilità per singole osservazioni è chiaramente distinta dalla funzione di verosimiglianza per il campione intero.</p>
<p>La chiave per comprendere il concetto è rendersi conto che <strong>la verosimiglianza di un’intera sequenza di prove indipendenti è il prodotto delle singole verosimiglianze</strong>.</p>
</section><section id="esempio-osservazioni-raggruppate" class="level2" data-number="42.9"><h2 data-number="42.9" class="anchored" data-anchor-id="esempio-osservazioni-raggruppate">
<span class="header-section-number">42.9</span> Esempio: Osservazioni Raggruppate</h2>
<p>Per illustrare il concetto di <strong>verosimiglianza congiunta</strong> nel caso della distribuzione binomiale, consideriamo quattro gruppi distinti di osservazioni binomiali indipendenti:</p>
<ul>
<li>
<strong>gruppo 1</strong>: 23 successi su 30 prove,</li>
<li>
<strong>gruppo 2</strong>: 20 successi su 28 prove,</li>
<li>
<strong>gruppo 3</strong>: 29 successi su 40 prove,</li>
<li>
<strong>gruppo 4</strong>: 29 successi su 36 prove.</li>
</ul>
<p>Poiché ogni gruppo segue una distribuzione binomiale indipendente con lo stesso parametro <span class="math inline">\(\theta\)</span>, la log-verosimiglianza congiunta si ottiene <strong>sommando</strong> le log-verosimiglianze di ciascun gruppo:</p>
<p><span class="math display">\[
\log \mathcal{L}(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(n_i\)</span> è il numero totale di prove nel gruppo <span class="math inline">\(i\)</span>,</li>
<li>
<span class="math inline">\(y_i\)</span> è il numero di successi nel gruppo <span class="math inline">\(i\)</span>.</li>
</ul>
<p>Sostituendo i valori specifici:</p>
<p><span class="math display">\[
\begin{aligned}
\log \mathcal{L}(\theta) &amp;= [23\log(\theta) + (30-23)\log(1 - \theta)] \\
&amp;\quad + [20\log(\theta) + (28-20)\log(1 - \theta)] \\
&amp;\quad + [29\log(\theta) + (40-29)\log(1 - \theta)] \\
&amp;\quad + [29\log(\theta) + (36-29)\log(1 - \theta)].
\end{aligned}
\]</span></p>
<p>Questa formula ci permette di calcolare <strong>quanto è plausibile</strong> il valore del parametro <span class="math inline">\(\theta\)</span>, tenendo conto simultaneamente di tutte le osservazioni nei quattro gruppi.</p>
</section><section id="implementazione-in-r" class="level2" data-number="42.10"><h2 data-number="42.10" class="anchored" data-anchor-id="implementazione-in-r">
<span class="header-section-number">42.10</span> Implementazione in R</h2>
<p>Supponiamo di avere i dati di <strong>quattro gruppi indipendenti</strong>, e vogliamo trovare la stima del parametro <span class="math inline">\(\theta\)</span> che <strong>massimizza la log-verosimiglianza congiunta</strong>. Procederemo passo dopo passo.</p>
<p><strong>1. Definire una funzione per la log-verosimiglianza congiunta.</strong></p>
<p>Questa funzione riceve un valore di <span class="math inline">\(\theta\)</span> e una lista di gruppi. Ogni gruppo contiene il numero totale di prove e il numero di successi. La funzione calcola la somma delle log-verosimiglianze per ciascun gruppo.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione che calcola la log-verosimiglianza congiunta</span></span>
<span><span class="va">log_verosimiglianza_congiunta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>, <span class="va">dati</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Limitiamo theta per evitare log(0)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">theta</span> <span class="op">&lt;=</span> <span class="fl">0</span><span class="op">)</span> <span class="va">theta</span> <span class="op">&lt;-</span> <span class="fl">1e-10</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">theta</span> <span class="op">&gt;=</span> <span class="fl">1</span><span class="op">)</span> <span class="va">theta</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fl">1e-10</span></span>
<span></span>
<span>  <span class="co"># Inizializziamo il totale</span></span>
<span>  <span class="va">somma_loglik</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span></span>
<span>  <span class="co"># Per ogni gruppo, calcoliamo il contributo alla log-verosimiglianza</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dati</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">gruppo</span> <span class="op">&lt;-</span> <span class="va">dati</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span>   <span class="co"># Estrae il gruppo i-esimo</span></span>
<span>    <span class="va">n</span> <span class="op">&lt;-</span> <span class="va">gruppo</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>        <span class="co"># Numero totale di prove</span></span>
<span>    <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">gruppo</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>        <span class="co"># Numero di successi</span></span>
<span></span>
<span>    <span class="co"># Aggiunge il contributo del gruppo alla somma totale</span></span>
<span>    <span class="va">somma_loglik</span> <span class="op">&lt;-</span> <span class="va">somma_loglik</span> <span class="op">+</span> <span class="va">y</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Restituiamo il valore negativo (perché optim() cerca minimi)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">somma_loglik</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>2. Inserire i dati.</strong></p>
<p>Qui definiamo i dati per ciascun gruppo come coppie (numero di prove, numero di successi):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati osservati per ciascun gruppo</span></span>
<span><span class="va">dati_gruppi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">23</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">20</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">40</span>, <span class="fl">29</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">36</span>, <span class="fl">29</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>3. Trovare la stima di θ che massimizza la verosimiglianza.</strong></p>
<p>Usiamo <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> per trovare numericamente il valore di <span class="math inline">\(\theta\)</span> che minimizza il <strong>valore negativo della log-verosimiglianza</strong>, ovvero che <strong>massimizza la log-verosimiglianza</strong>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Ricerca del valore ottimale di theta</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">0.5</span>,                            <span class="co"># Valore iniziale</span></span>
<span>  fn <span class="op">=</span> <span class="va">log_verosimiglianza_congiunta</span>,  <span class="co"># Funzione da minimizzare</span></span>
<span>  dati <span class="op">=</span> <span class="va">dati_gruppi</span>,                  <span class="co"># Passiamo i dati</span></span>
<span>  method <span class="op">=</span> <span class="st">"L-BFGS-B"</span>,                 <span class="co"># Metodo con vincoli</span></span>
<span>  lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span>                 <span class="co"># Vincoli: theta tra 0 e 1</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stima ottimale trovata</span></span>
<span><span class="va">result</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="co">#&gt; [1] 0.7537</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>4. Visualizzare la log-verosimiglianza.</strong></p>
<p>Costruiamo ora un grafico che mostri <strong>come varia la log-verosimiglianza</strong> al variare di <span class="math inline">\(\theta\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Valori di theta da esplorare</span></span>
<span><span class="va">theta_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.01</span>, <span class="fl">0.99</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Vettore per salvare i valori di log-verosimiglianza</span></span>
<span><span class="va">log_likelihood_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta_values</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcoliamo il valore della funzione per ogni valore di theta</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta_values</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">t</span> <span class="op">&lt;-</span> <span class="va">theta_values</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">log_likelihood_values</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">log_verosimiglianza_congiunta</span><span class="op">(</span><span class="va">t</span>, <span class="va">dati_gruppi</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ora costruiamo il grafico:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_values</span>, log_likelihood <span class="op">=</span> <span class="va">log_likelihood_values</span><span class="op">)</span>,</span>
<span>  <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="va">log_likelihood</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Log-Verosimiglianza Congiunta"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Log-verosimiglianza negativa"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="15_likelihood_files/figure-html/unnamed-chunk-14-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="15_likelihood_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</div>
</div>
<p>Con questo esempio abbiamo visto che:</p>
<ul>
<li>la <strong>log-verosimiglianza congiunta</strong> si ottiene <strong>sommando</strong> le log-verosimiglianze dei singoli gruppi;</li>
<li>è possibile trovare la stima ottimale di <span class="math inline">\(\theta\)</span> <strong>numericamente</strong>, senza formule complicate;</li>
<li>il grafico della log-verosimiglianza ci aiuta a <strong>visualizzare il punto in cui il modello è più compatibile con i dati</strong>.</li>
</ul>
<p>Questo approccio — basato sull’uso della verosimiglianza e della sua somma tra gruppi indipendenti — sarà anche <strong>la base per il passaggio all’inferenza bayesiana</strong>, dove aggiungeremo <strong>una distribuzione a priori</strong> per ottenere una distribuzione a posteriori di <span class="math inline">\(\theta\)</span>.</p>
</section><section id="la-verosimiglianza-marginale" class="level2" data-number="42.11"><h2 data-number="42.11" class="anchored" data-anchor-id="la-verosimiglianza-marginale">
<span class="header-section-number">42.11</span> La Verosimiglianza Marginale</h2>
<p>La <strong>verosimiglianza marginale</strong> è un concetto fondamentale nell’inferenza bayesiana, utilizzato per valutare <strong>quanto un modello sia compatibile con i dati osservati</strong>, tenendo conto dell’incertezza sui parametri.</p>
<p>A differenza della verosimiglianza standard, che misura la plausibilità dei dati per un valore <strong>fisso</strong> del parametro, la <strong>verosimiglianza marginale</strong> considera <strong>tutti i possibili valori del parametro</strong>, pesandoli in base alla loro probabilità a priori. Questo approccio permette di integrare l’incertezza nella valutazione del modello.</p>
<section id="caso-con-parametri-discreti" class="level3" data-number="42.11.1"><h3 data-number="42.11.1" class="anchored" data-anchor-id="caso-con-parametri-discreti">
<span class="header-section-number">42.11.1</span> Caso con Parametri Discreti</h3>
<p>Per comprendere meglio il concetto, consideriamo un esperimento in cui eseguiamo <strong>10 tentativi</strong> e otteniamo <strong>7 successi</strong>. Supponiamo che la probabilità di successo <span class="math inline">\(\theta\)</span> possa assumere solo tre valori discreti:</p>
<p><span class="math display">\[
\theta \in \{0.1, 0.5, 0.9\}.
\]</span></p>
<p>Per calcolare la <strong>verosimiglianza marginale</strong>, dobbiamo:</p>
<ol type="1">
<li>
<p><strong>Assegnare una probabilità a priori</strong> a ciascun valore di <span class="math inline">\(\theta\)</span>, ad esempio:</p>
<ul>
<li>Distribuzione uniforme: <span class="math display">\[
p(\theta = 0.1) = p(\theta = 0.5) = p(\theta = 0.9) = \frac{1}{3}.
\]</span>
</li>
<li>Distribuzione non uniforme (ad esempio, dando più peso a <span class="math inline">\(\theta = 0.5\)</span>): <span class="math display">\[
p(\theta = 0.1) = \frac{1}{4}, \quad p(\theta = 0.5) = \frac{1}{2}, \quad p(\theta = 0.9) = \frac{1}{4}.
\]</span>
</li>
</ul>
</li>
<li>
<p><strong>Calcolare la probabilità di osservare 7 successi su 10 prove per ogni valore di <span class="math inline">\(\theta\)</span></strong>:</p>
<p><span class="math display">\[
p(k=7 \mid \theta) = \binom{10}{7} \theta^7 (1 - \theta)^3.
\]</span></p>
</li>
<li>
<p><strong>Moltiplicare ciascuna di queste probabilità per la corrispondente probabilità a priori e sommare i risultati</strong>:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \sum_{i} p(k=7 \mid \theta_i) p(\theta_i).
\]</span></p>
</li>
</ol>
<p>Sostituendo i valori per la distribuzione uniforme:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \binom{10}{7} 0.1^7 (0.9)^3 \cdot \frac{1}{3} +
\binom{10}{7} 0.5^7 (0.5)^3 \cdot \frac{1}{3} +
\binom{10}{7} 0.9^7 (0.1)^3 \cdot \frac{1}{3}.
\]</span></p>
<p>Questa somma rappresenta la <strong>verosimiglianza marginale</strong>, ossia la probabilità di ottenere 7 successi su 10, considerando tutte le possibili incertezze su <span class="math inline">\(\theta\)</span>.</p>
</section><section id="caso-con-parametri-continui" class="level3" data-number="42.11.2"><h3 data-number="42.11.2" class="anchored" data-anchor-id="caso-con-parametri-continui">
<span class="header-section-number">42.11.2</span> Caso con Parametri Continui</h3>
<p>Nella maggior parte delle situazioni, il parametro <span class="math inline">\(\theta\)</span> non assume solo pochi valori discreti, ma può variare <strong>continuamente</strong> in un intervallo (ad esempio, tra 0 e 1). In questo caso, invece di sommare, dobbiamo <strong>integrare</strong>:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 p(\theta) \, d\theta.
\]</span></p>
<p>Qui:</p>
<ul>
<li>
<span class="math inline">\(p(\theta)\)</span> è la <strong>distribuzione a priori</strong> di <span class="math inline">\(\theta\)</span>.</li>
<li>L’integrale rappresenta una media ponderata della probabilità di ottenere i dati, considerando tutti i valori di <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Ad esempio, se <span class="math inline">\(\theta \sim \text{Beta}(2,2)\)</span>, la verosimiglianza marginale diventa:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 \frac{\theta (1-\theta)}{B(2,2)} \, d\theta.
\]</span></p>
<p>Questo tipo di calcolo viene spesso risolto numericamente.</p>
</section><section id="calcolo-numerico-della-verosimiglianza-marginale-in-r" class="level3" data-number="42.11.3"><h3 data-number="42.11.3" class="anchored" data-anchor-id="calcolo-numerico-della-verosimiglianza-marginale-in-r">
<span class="header-section-number">42.11.3</span> Calcolo Numerico della Verosimiglianza Marginale in R</h3>
<p>Se vogliamo calcolare la verosimiglianza marginale numericamente, possiamo usare l’integrazione numerica in R.</p>
<p><strong>Caso con Parametri Discreti.</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definiamo i valori possibili di theta e le probabilità a priori</span></span>
<span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span><span class="op">)</span></span>
<span><span class="va">prior_probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span>  <span class="co"># Distribuzione uniforme</span></span>
<span></span>
<span><span class="co"># Calcoliamo la verosimiglianza per ciascun valore di theta</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">7</span>, size <span class="op">=</span> <span class="fl">10</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcoliamo la verosimiglianza marginale sommando i contributi ponderati</span></span>
<span><span class="va">marginal_likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">prior_probs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">marginal_likelihood</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0582</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Caso con Parametri Continui.</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definiamo la funzione di verosimiglianza pesata dalla prior</span></span>
<span><span class="va">integrand</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">7</span>, size <span class="op">=</span> <span class="fl">10</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, shape1 <span class="op">=</span> <span class="fl">2</span>, shape2 <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Eseguiamo l'integrazione numerica</span></span>
<span><span class="va">marginal_likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">integrand</span>, lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">marginal_likelihood</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1119</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interpretazione-della-verosimiglianza-marginale" class="level3" data-number="42.11.4"><h3 data-number="42.11.4" class="anchored" data-anchor-id="interpretazione-della-verosimiglianza-marginale">
<span class="header-section-number">42.11.4</span> Interpretazione della Verosimiglianza Marginale</h3>
<p>La <strong>verosimiglianza marginale</strong> rappresenta la probabilità complessiva dei dati, <strong>tenendo conto di tutte le possibili incertezze sul parametro <span class="math inline">\(\theta\)</span></strong>.</p>
<ul>
<li>Se la verosimiglianza marginale è <strong>alta</strong>, significa che il modello nel suo insieme è compatibile con i dati osservati.</li>
<li>Se la verosimiglianza marginale è <strong>bassa</strong>, significa che, indipendentemente dal valore di <span class="math inline">\(\theta\)</span>, il modello non spiega bene i dati.</li>
</ul>
<p>A differenza della verosimiglianza classica, che valuta quanto siano probabili i dati per un <strong>singolo valore</strong> di <span class="math inline">\(\theta\)</span>, la verosimiglianza marginale <strong>considera tutte le possibili ipotesi sul parametro</strong>.</p>
</section><section id="ruolo-nella-statistica-bayesiana" class="level3" data-number="42.11.5"><h3 data-number="42.11.5" class="anchored" data-anchor-id="ruolo-nella-statistica-bayesiana">
<span class="header-section-number">42.11.5</span> Ruolo nella Statistica Bayesiana</h3>
<p>La verosimiglianza marginale svolge un ruolo cruciale nell’inferenza bayesiana perché appare nella <strong>formula di Bayes</strong>:</p>
<p><span class="math display">\[
p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},
\]</span></p>
<p>dove <span class="math inline">\(p(D)\)</span> è la <strong>verosimiglianza marginale</strong>. Questa quantità:</p>
<ol type="1">
<li><p><strong>serve da fattore di normalizzazione</strong> per la distribuzione a posteriori <span class="math inline">\(p(\theta \mid D)\)</span>;</p></li>
<li>
<p><strong>permette di confrontare modelli diversi</strong>, attraverso il <strong>fattore di Bayes</strong>:</p>
<p><span class="math display">\[
BF = \frac{p(D \mid M_1)}{p(D \mid M_2)},
\]</span></p>
<p>dove <span class="math inline">\(M_1\)</span> e <span class="math inline">\(M_2\)</span> sono due modelli diversi.</p>
</li>
</ol>
<p>In conclusione, la <strong>verosimiglianza marginale</strong> è un concetto chiave nell’inferenza bayesiana, che ci permette di valutare quanto un modello sia coerente con i dati, tenendo conto di tutte le possibili incertezze sui parametri:</p>
<ul>
<li>per parametri <strong>discreti</strong>, si calcola come una somma ponderata;</li>
<li>per parametri <strong>continui</strong>, si calcola con un <strong>integrale</strong>;</li>
<li>è essenziale per il calcolo della distribuzione <strong>a posteriori</strong> e per il <strong>confronto tra modelli</strong>.</li>
</ul></section></section><section id="riflessioni-conclusive" class="level2" data-number="42.12"><h2 data-number="42.12" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">42.12</span> Riflessioni Conclusive</h2>
<p>La funzione di verosimiglianza rappresenta un ponte fondamentale tra i dati osservati e i parametri di un modello statistico, offrendo una misura della plausibilità dei dati rispetto a diversi valori possibili dei parametri. La sua costruzione richiede l’integrazione di tre elementi chiave: il modello statistico ipotizzato come generatore dei dati, lo spazio dei parametri associato al modello e le osservazioni empiriche disponibili.</p>
<p>Nell’ambito dell’inferenza statistica, la funzione di verosimiglianza svolge un ruolo centrale. Essa consente di valutare quanto bene diversi valori dei parametri siano in grado di spiegare i dati osservati, diventando così uno strumento essenziale per la stima dei parametri e la selezione del modello. La sua corretta applicazione è determinante per garantire analisi dati rigorose e interpretazioni affidabili dei risultati.</p>
<p>In sintesi, padroneggiare il concetto di verosimiglianza e saperlo applicare in modo appropriato sono competenze indispensabili per chi si occupa di ricerca empirica e analisi di dati complessi. La verosimiglianza non è solo uno strumento tecnico, ma un pilastro metodologico che supporta la comprensione e l’interpretazione dei fenomeni osservati attraverso modelli statistici.</p>
</section><section id="esercizi" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="esercizi">Esercizi</h2>
<div class="callout callout-style-simple callout-important no-icon callout-titled" title="Problemi">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Problemi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Spiega ciascuno dei concetti seguenti con una frase:</p>
<ul>
<li>probabilità.</li>
<li>funzione di massa di probabilità.</li>
<li>funzione di densità di probabilità.</li>
<li>distribuzione di probabilità.</li>
<li>distribuzione di probabilità discreta.</li>
<li>distribuzione di probabilità continua.</li>
<li>funzione di distribuzione cumulativa (cdf).</li>
<li>verosimiglianza</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled" title="Problemi">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Problemi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>All’esame ti verrà chiesto di:</p>
<ul>
<li>Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\theta\)</span>.</li>
<li>Calcolare la stima di massima verosimiglianza.</li>
<li>Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.</li>
</ul>
</div>
</div>
</div>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.0 (2025-04-11)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] thematic_0.1.6   MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.11.0      </span></span>
<span><span class="co">#&gt;  [5] gridExtra_2.3    patchwork_1.3.0  bayesplot_1.12.0 psych_2.5.3     </span></span>
<span><span class="co">#&gt;  [9] scales_1.4.0     markdown_2.0     knitr_1.50       lubridate_1.9.4 </span></span>
<span><span class="co">#&gt; [13] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.0.4     </span></span>
<span><span class="co">#&gt; [17] readr_2.1.5      tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.2   </span></span>
<span><span class="co">#&gt; [21] tidyverse_2.0.0  rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] generics_0.1.4     stringi_1.8.7      lattice_0.22-7    </span></span>
<span><span class="co">#&gt;  [4] hms_1.1.3          digest_0.6.37      magrittr_2.0.3    </span></span>
<span><span class="co">#&gt;  [7] evaluate_1.0.3     grid_4.5.0         timechange_0.3.0  </span></span>
<span><span class="co">#&gt; [10] RColorBrewer_1.1-3 fastmap_1.2.0      rprojroot_2.0.4   </span></span>
<span><span class="co">#&gt; [13] jsonlite_2.0.0     mnormt_2.1.1       cli_3.6.5         </span></span>
<span><span class="co">#&gt; [16] rlang_1.1.6        withr_3.0.2        tools_4.5.0       </span></span>
<span><span class="co">#&gt; [19] parallel_4.5.0     tzdb_0.5.0         pacman_0.5.1      </span></span>
<span><span class="co">#&gt; [22] vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   </span></span>
<span><span class="co">#&gt; [25] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.10.2     </span></span>
<span><span class="co">#&gt; [28] gtable_0.3.6       glue_1.8.0         xfun_0.52         </span></span>
<span><span class="co">#&gt; [31] tidyselect_1.2.1   rstudioapi_0.17.1  farver_2.1.2      </span></span>
<span><span class="co">#&gt; [34] htmltools_0.5.8.1  nlme_3.1-168       labeling_0.4.3    </span></span>
<span><span class="co">#&gt; [37] rmarkdown_2.29     compiler_4.5.0</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Johnson2022bayesrules" class="csl-entry" role="listitem">
Johnson, A. A., Ott, M., &amp; Dogucu, M. (2022). <em><span>Bayes Rules! An Introduction to Bayesian Modeling with R</span></em>. CRC Press.
</div>
<div id="ref-schervish2014probability" class="csl-entry" role="listitem">
Schervish, M. J., &amp; DeGroot, M. H. (2014). <em>Probability and statistics</em> (Vol. 563). Pearson Education London, UK:
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/probability/14_gauss.html" class="pagination-link" aria-label="Assunzione di gaussianità e trasformazioni dei dati">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/probability/16_likelihood_gauss.html" class="pagination-link" aria-label="La verosimiglianza gaussiana">
        <span class="nav-page-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">La verosimiglianza gaussiana</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/15_likelihood.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>