# La regressione verso la media {#sec-linear-models-regression-toward_mean}

::: {.epigraph}
> “The mean filial regression towards mediocrity was directly proportional to the parental deviation from it.”
>
> -- **Francis Galton**, Regression Towards Mediocrity in Hereditary Stature, Journal of the Anthropological Institute 15, 1886, pp. 246–263.
:::

## Introduzione {.unnumbered .unlisted} 

::: {.lead .dropcap}
Il concetto di *regressione verso la media* è stato introdotto da Francis Galton, un pioniere della statistica, mentre studiava la trasmissione ereditaria di tratti fisici, in particolare l'altezza. Galton osservò che, quando si confronta l'altezza dei padri con quella dei figli, i figli tendono ad essere più vicini alla media della popolazione rispetto ai loro padri. Questo fenomeno è noto come *regressione verso la media*.
:::

Immaginiamo di avere un padre che è più alto della media della popolazione. Ci aspettiamo che suo figlio sia anch'esso più alto della media, ma *non* tanto quanto il padre. In altre parole, l'altezza del figlio "regredisce" parzialmente verso la media della popolazione. Lo stesso principio si applica ai padri più bassi della media: i loro figli tenderanno ad essere più bassi della media, ma non tanto quanto i padri.

*Perché succede?* 
Quando un padre è alto 75 pollici (mentre la media magari è 69.1), essere così alto potrebbe essere dovuto a molti fattori “eccezionali” combinati (genetici, ambientali, casuali). Il figlio, tuttavia, eredita solo una parte di quei fattori, e probabilmente avrà altri fattori (positivi o negativi) in modo casuale, cosicché la sua altezza si sposta verso la media della popolazione. Questo non vuol dire che il figlio sia basso: rimane comunque al di sopra della media, ma non raggiunge l’estremo del padre.

Il cuore statistico del fenomeno si trova nella correlazione tra due variabili (in questo caso, altezza del padre e altezza del figlio). Se la correlazione fosse 1 (perfetta), un padre alto in modo eccezionale avrebbe sempre un figlio proporzionalmente alto, senza “riavvicinarsi” alla media. Se invece la correlazione è minore di 1, come succede quasi sempre nel mondo reale, la relazione padre-figlio non è perfetta e vi è una certa variabilità.

Galton misurò in media una correlazione di circa 0.5 tra altezza paterna e altezza del figlio maschio. Questo valore implica che, se un padre si discosta di 2-3 deviazioni standard sopra la media, il figlio di solito ne recupererà una parte, ma non interamente.


### Panoramica del capitolo {.unnumbered .unlisted}

- Origine storica e intuizione della regressione verso la media (Galton).
- Correlazione e regressione: forma standardizzata e non standardizzata.
- Visualizzare/quantificare la RTM: stime condizionate e retta di regressione; interpretazione predittiva.
- RTM ≠ causalità: effetti di selezione e rumore di misura; come evitare false inferenze.

---

::: {.callout-tip collapse=true}
## Prerequisiti

- Leggere il capitolo *Basic Regression* di [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition)](https://moderndive.com/v2/).
- Leggere il capitolo *Linear Statistical Models* [@schervish2014probability].
:::

::: {.callout-caution collapse=true title="Preparazione del Notebook"}

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(HistData)
```
:::

### I dati di Galton

Esaminiamo il fenomeno della regressione verso la media usando i dati di Galton. Nel pacchetto `HistData` di R sono disponibili i dati originali raccolti da Galton, che includono informazioni sull'altezza di padri, madri, figli maschi e femmine. Per semplificare l'analisi, possiamo creare un dataset che include solo l'altezza del padre e l'altezza di un figlio maschio scelto casualmente da ogni famiglia:

```{r}
set.seed(1234)

galton_heights <- GaltonFamilies |>
  filter(gender == "male") |>
  group_by(family) |>
  sample_n(1) |>
  ungroup() |>
  select(father, childHeight) |>
  rename(son = childHeight)
```

Questo dataset contiene due colonne: `father` (altezza del padre) e `son` (altezza del figlio maschio). Calcolando la media e la deviazione standard delle altezze dei padri e dei figli, otteniamo:

```{r}
galton_heights |> 
  summarize(
    mean_father = mean(father), 
    sd_father   = sd(father),
    mean_son    = mean(son), 
    sd_son      = sd(son)
  )
```

I risultati mostrano che, in media, i padri e i figli hanno altezze simili, anche se le distribuzioni non sono identiche. Un grafico di dispersione (scatterplot) evidenzia una chiara tendenza: padri più alti tendono ad avere figli più alti:

```{r}
galton_heights |>
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)
```

### Il coefficiente di correlazione

Per quantificare la relazione lineare tra l'altezza del padre e quella del figlio, utilizziamo il *coefficiente di correlazione* (indicato con $r$ o $\rho$). La formula del coefficiente di correlazione è:

$$
\rho = \frac{1}{n}\sum_{i=1}^n 
\left(\frac{x_i - \mu_x}{\sigma_x}\right)
\left(\frac{y_i - \mu_y}{\sigma_y}\right).
$$

In R, possiamo calcolare la correlazione tra l'altezza del padre e quella del figlio con il seguente codice:

```{r}
galton_heights |> 
  summarize(r = cor(father, son)) |> 
  pull(r)
```

Nel nostro caso, la correlazione è circa 0.5. Questo valore positivo indica che padri più alti tendono ad avere figli più alti, ma la correlazione non è perfetta. Il coefficiente di correlazione varia tra -1 e 1, dove il valore assoluto misura la forza della relazione lineare.

### Stime condizionate: previsioni basate sull'altezza del padre

Un modo per fare previsioni è chiedersi: “Se un padre è alto 72 pollici, quale sarà l'altezza media dei figli di tutti i padri di 72 pollici?” In termini statistici, questa è l'*aspettativa condizionata* $\mathbb{E}(\text{altezza figlio} \mid \text{altezza padre} = 72)$.

Se filtriamo i dati prendendo solo i padri di altezza 72 pollici, possiamo calcolare la media dell'altezza dei figli in quel sottogruppo. Tuttavia, questo metodo può essere instabile se il numero di osservazioni nel sottogruppo è piccolo. Ad esempio:

```{r}
galton_heights |> 
  filter(round(father) == 72) |>
  summarize(avg_son = mean(son))
```

Questo ci dà la stima condizionata dell'altezza media del figlio di un padre di 72 pollici. Spesso, questa media è maggiore della media generale dei figli, ma *meno* di quanto il padre (72 pollici) sia sopra la media dei padri. Questo fenomeno è noto come *regressione verso la media*.

## Visualizzare la regressione verso la media

Ripetiamo ora lo stesso procedimento per tutti i dati.  *Stratifichiamo* i dati  raggruppadoli in base a valori simili di altezza del padre. Calcoliamo poi in ogni gruppo la media dell’altezza del figlio:

```{r}
galton_heights |>
  mutate(father_strata = factor(round(father))) |>
  group_by(father_strata) |>
  summarize(avg_son = mean(son)) |>
  ggplot(aes(x = father_strata, y = avg_son)) +
  geom_point()
```

Nel grafico risultante, ogni punto rappresenta la media dei figli corrispondenti a un determinato “strato” di padri. Se tracciamo anche la *retta di regressione*, noteremo che i punti si dispongono in modo approssimativamente lineare: *a padri più alti corrispondono figli più alti*, ma in media *meno* alti di quanto ci si aspetterebbe se la correlazione fosse perfetta.

## La retta di regressione

Per capire perché, dal punto di vista statistico, si verifica la regressione verso la media, consideriamo il modello di *regressione lineare semplice* che prevede l’altezza del figlio $\hat{Y}$ in base all’altezza del padre $X$:

$$
\hat{Y} = \beta_0 + \beta_1 X.
$$

- Quando *standardizziamo* i dati (cioè trasformiamo sia $X$ sia $Y$ in “punti z”, sottraendo la media e dividendo per la deviazione standard), la pendenza $\beta_1$ della retta di regressione *diventa esattamente la correlazione* $\rho$.  
- Se $\rho = 1$, la pendenza sarebbe 1 e non ci sarebbe alcun “riavvicinamento” alla media: i valori alti di $X$ corrisponderebbero a valori altrettanto alti di $Y$.  
- Se $\rho < 1$, la pendenza risulta *minore di 1* e ciò significa che, partendo da un valore molto alto (o molto basso) di $X$, la nostra previsione di $Y$ si colloca in una posizione *parzialmente più vicina* alla media di $Y$ rispetto alla distanza del padre dalla media di $X$. È proprio questo il fenomeno della regressione verso la media.

### Forma non standardizzata

Nella forma originale (non standardizzata), i coefficienti si calcolano con:

$$
\beta_1 
= \rho \,\frac{\sigma_Y}{\sigma_X}, 
\quad
\beta_0 
= \mu_Y 
- \beta_1 \,\mu_X,
$$

dove:

- $\mu_X, \mu_Y$ sono le medie di $X$ (altezza del padre) e $Y$ (altezza del figlio);
- $\sigma_X, \sigma_Y$ sono le rispettive deviazioni standard;
- $\rho$ è la correlazione tra $X$ e $Y$.

### Forma standardizzata

Nella forma standardizzata, le deviazioni standard di $X$ e $Y$ sono pari a 1 e i coefficienti di regressione diventano:

$$
\beta_1 
= \rho, 
\quad
\beta_0 
= 0,
$$

dato che le medie di $X$ e $Y$ sono uguali a zero.

### Regressione verso la media 

Ne segue dunque che, se la correlazione tra le variabili è minore di 1, si verificherà necessariamente il fenomeno della regressione verso la media. In R, possiamo stimare la retta di regressione tramite la *stima dei minimi quadrati*:

```{r}
fit <- lm(son ~ father, data = galton_heights)
fit$coefficients
```

La funzione `lm` calcola gli stimatori $\hat{\beta}_0$ e $\hat{\beta}_1$ che minimizzano la somma dei quadrati degli scarti:

$$
\text{RSS} = \sum_{i} \left[y_i - (\beta_0 + \beta_1 x_i)\right]^2.
$$

Possiamo quindi tracciare la retta sullo scatterplot dei dati:

```{r}
galton_heights |>
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(
    slope = coef(fit)[2], 
    intercept = coef(fit)[1],
    color = okabe_ito["sky"]
  )
```

Se standardizziamo i dati, la pendenza della retta di regressione diventa uguale alla correlazione:

```{r}
fit_2 <- lm(scale(son) ~ scale(father), data = galton_heights)
fit_2$coefficients
```

```{r}
galton_heights |>
  ggplot(aes(scale(father), scale(son))) +
  geom_point(alpha = 0.5) +
  geom_abline(
    slope = coef(fit_2)[2], 
    intercept = coef(fit_2)[1],
    color = okabe_ito["sky"]
  )
```

Il punto cruciale è ricordare che la pendenza $\beta_1$ *è proporzionale* alla correlazione $\rho$. Se la correlazione non è perfetta ($\rho < 1$), allora qualsiasi previsione basata su $X$ (ad esempio, l’altezza del padre) risulterà *meno estrema* di quanto sia $X$ stesso rispetto alla sua media. In altre parole, un padre altissimo (molto sopra la media) avrà, in media, un figlio sopra la media ma non altrettanto estremo, “regredendo” parzialmente verso il centro della distribuzione.

In sintesi, la correlazione imperfetta ($\rho < 1$) è la ragione principale per cui un valore estremo di $X$ (ad esempio, un padre molto alto) porta a un valore $\hat{Y}$ che è sì superiore (o inferiore) alla media, ma *meno* estremo del padre. Questo “ritorno verso il centro” è ciò che chiamiamo *regressione verso la media*.


## Riflessioni conclusive {.unnumbered .unlisted}

Il fenomeno della regressione verso la media rappresenta un principio statistico fondamentale con profonde implicazioni per la ricerca psicologica. In presenza di una correlazione imperfetta (0 < $\rho$ < 1), i valori estremi della variabile X generano previsioni $\hat{Y}$ meno estreme. Espresso in termini di punteggi standardizzati, l'atteso condizionato di $Z_Y$ dato $Z_X = z$ è pari a $\rho z$. Questo "richiamo verso il centro" della distribuzione costituisce l'essenza del fenomeno della regressione verso la media.

Questo fenomeno si verifica perché le relazioni tra le variabili psicologiche sono raramente (o mai) deterministiche. La presenza di variabilità non spiegata, di errori di misurazione e di processi di selezione che privilegiano i casi estremi, determina, in contesti di ripetizione della misurazione o di previsione di $Y$ da $X$, uno spostamento sistematico verso il valore medio. È importante sottolineare che l'entità di questo effetto è inversamente proporzionale all'affidabilità della misura: minore è l'affidabilità, più marcato sarà l'effetto di regressione.

Nella pratica della ricerca, specialmente nei disegni pre-post privi di gruppo di controllo, un apparente miglioramento successivo a un intervento in soggetti con valori di base molto bassi potrebbe essere semplicemente attribuibile all'atteso statistico di regressione verso la media, *anche in assenza di un qualsiasi effetto dell'intervento*. Lo stesso principio si applica al presunto peggioramento osservato dopo picchi eccezionalmente elevati.

Per evitare interpretazioni fuorvianti, è essenziale adottare buone pratiche metodologiche. L'impiego di gruppi di controllo e la randomizzazione sono le strategie preferibili, quando fattibili. L'aggiustamento del valore di base tramite modelli di regressione o ANCOVA è un'alternativa migliore rispetto al semplice confronto delle differenze grezze. L'utilizzo di misurazioni multiple consente di stimare e incrementare l'affidabilità, riducendo l'errore di misura. È inoltre opportuno evitare, quando possibile, la selezione basata esclusivamente su casi estremi; quando ciò non è evitabile, è necessario modellare esplicitamente tale processo selettivo. La trasparenza nel reporting, che include scatterplot, pendenze in scala standardizzata e intervalli di incertezza, insieme a una discussione critica sull'affidabilità degli strumenti, completa l'approccio metodologicamente rigoroso.

Dal punto di vista della psicologia come disciplina, è cruciale concettualizzare la regressione verso la media come un fenomeno statistico prevedibile e non come una spiegazione psicologica dei cambiamenti osservati. Le interpretazioni di natura causale devono basarsi su adeguati disegni di ricerca e su modelli statistici che incorporino esplicitamente considerazioni sull'affidabilità, sulla variabilità non osservata e sui processi di selezione del campione.

In sintesi, la regressione verso la media emerge sistematicamente in tutte le situazioni caratterizzate da $\rho$ < 1, la sua entità è inversamente proporzionale all'affidabilità della misurazione e il suo controllo richiede un attento disegno sperimentale e appropriati aggiustamenti statistici.


::: {.callout-note collapse=true title="Informazioni sull'ambiente di sviluppo"}
```{r}
sessionInfo()
```
:::


## Bibliografia {.unnumbered .unlisted} 

