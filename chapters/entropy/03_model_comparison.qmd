# Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score {#sec-div-kl-lppd-elpd}


::: callout-important
## Obiettivi di apprendimento

Alla fine di questo capitolo, sarai in grado di:

* comprendere cos'è la distribuzione predittiva posteriore e come si costruisce;
* spiegare cosa misura il log-score e come si calcola nella pratica;
* distinguere tra LPPD ed ELPD e comprendere il loro significato;
* capire come LOO-CV fornisca una stima dell’ELPD;
* collegare il confronto tra modelli alla divergenza di Kullback-Leibler.
:::

::: callout-tip
## Prerequisiti

- Per comprendere appieno questo capitolo è utile leggere il capitolo 7 *Ulysses’ Compass* di *Statistical Rethinking* (@McElreath_rethinking).
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


In questo capitolo esploreremo gli strumenti essenziali per valutare e confrontare modelli statistici nel contesto bayesiano. L'obiettivo centrale è capire *quanto bene un modello riesce a prevedere nuovi dati*, non solo quanto si adatta a quelli osservati.

Immaginate di aver sviluppato un test psicologico per predire l'ansia degli studenti prima di un esame. Non vi basta sapere quanto bene il vostro modello "spiega" i dati che avete già raccolto: volete anche essere sicuri che funzionerà con nuovi studenti che non avete ancora incontrato. Questa distinzione tra adattamento ai dati osservati e capacità di predire nuovi casi è il cuore della valutazione predittiva bayesiana.

Per raggiungere questo obiettivo, introdurremo la distribuzione predittiva posteriore, che integra l'incertezza sui parametri, il log-score come misura dell'accuratezza predittiva punto per punto, la LPPD (Log Pointwise Predictive Density) e l'ELPD (Expected Log Predictive Density), la tecnica Leave-One-Out Cross-Validation (LOO-CV), e il legame tra queste misure e la divergenza di Kullback-Leibler ($D_{\text{KL}}$).

Queste idee, pur complesse, possono essere rese accessibili con intuizioni semplici e strumenti pratici. Vi guideremo passo per passo.


## Distribuzione Predittiva Posteriore

Nel contesto bayesiano, non ci limitiamo a una stima puntuale dei parametri. Dopo aver osservato i dati $y$, otteniamo una distribuzione posteriore $p(\theta \mid y)$, che riflette la nostra incertezza sui valori di $\theta$.

Pensate a uno psicologo che vuole stimare il livello medio di ansia in una popolazione. Dopo aver raccolto dati da un campione, non otterrà un singolo numero come "la media è esattamente 4.7", ma piuttosto una distribuzione che dice "la media è probabilmente tra 4.2 e 5.1, con 4.7 come valore più plausibile".

La distribuzione predittiva posteriore per *nuovi dati* $\tilde{y}$ è:

$$
q(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta.
$$


::: {.callout-note collapse=true title="Intuizione"}
Se conoscessimo esattamente i parametri $\theta$, potremmo prevedere $\tilde{y}$ usando $p(\tilde{y} \mid \theta)$. Poiché non li conosciamo, combiniamo tutte le previsioni possibili (una per ogni $\theta$), pesandole secondo quanto ciascun $\theta$ è plausibile secondo la posteriore. Il risultato è una previsione media, che tiene conto della nostra incertezza.
:::

Questa formula cattura un principio fondamentale: invece di fare previsioni basandoci su un singolo "migliore" valore dei parametri, consideriamo tutte le possibilità plausibili e le combiniamo in proporzione alla loro credibilità. È come chiedere il parere a diversi esperti, dove il peso dato a ciascun parere dipende dalla fiducia che riponiamo in quell'esperto.

In questo capitolo useremo a volte $q(\cdot \mid y)$ per indicare genericamente la distribuzione predittiva posteriore del modello, ma nella maggior parte dei casi adotteremo la notazione più esplicita $p(y_i \mid y)$, per evidenziare che si tratta di una previsione marginale, ottenuta integrando la distribuzione dei dati condizionata ai parametri <span class="no-break">($p(y_i \mid \theta)$)</span> sulla distribuzione posteriore dei parametri ($p(\theta \mid y)$).


## Il problema della valutazione predittiva
  
Vorremmo sapere quanto bene questa distribuzione predittiva $q(\tilde{y} \mid y)$ si avvicina alla distribuzione vera dei dati, $p(\tilde{y})$, ovvero la distribuzione che avrebbe generato i nuovi dati futuri $\tilde{y}$, se la conoscessimo. Questa distanza si misura idealmente con la *divergenza di Kullback-Leibler*:

$$
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p\left[\log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)}\right].
$$

Tuttavia, qui incontriamo un problema fondamentale: *non conosciamo $p(\tilde{y})$* e non possiamo quindi calcolare direttamente $D_{\text{KL}}(p \parallel q)$. È come voler valutare l'accuratezza di una mappa senza conoscere il territorio reale. Per aggirare questo ostacolo, usiamo misure surrogate: il log-score, la LPPD e l'ELPD, che stimano indirettamente la bontà predittiva del modello attraverso tecniche che sfruttano creativamente i dati a nostra disposizione.


::: {.callout-tip title="Mappa concettuale"}
| Concetto | Cosa rappresenta | Dove viene usato |
|----------|------------------|------------------|
| $p(y_i \mid \theta)$ | Verosimiglianza | Calcolo predittivo |
| $p(\theta \mid y)$ | Distribuzione posteriore | Ponderazione |
| $p(y_i \mid y)$ | Predizione bayesiana media | Log-score, LPPD |
| $p(y_i \mid y_{-i})$ | Predizione LOO | ELPD |
| $q(\tilde{y} \mid y)$ | Distribuzione predittiva complessiva | KL, confronto tra modelli |
:::


### Il log-score: misurare l'accuratezza punto per punto

Per ciascun punto osservato $y_i$, calcoliamo il logaritmo della probabilità che il modello le assegna:

$$
\log p(y_i \mid y) = \log \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta.
$$
  
Il log-score totale è la somma di questi valori:
  
$$
S = \sum_{i=1}^n \log p(y_i \mid y).
$$
  
Un log-score più alto (meno negativo) indica che il modello assegna maggiore probabilità ai dati osservati. Pensate al log-score come a un "voto di fiducia" che il modello assegna a ciascuna osservazione: se il modello considera molto probabile quello che è effettivamente accaduto, il log-score sarà alto.


### Stima pratica del log-score con MCMC

Nel calcolo teorico, il log-score richiede la *probabilità predittiva* per ogni osservazione $y_i$, ovvero:

$$
p(y_i \mid y) = \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
$$

Per comprenderla, possiamo analizzarne le due componenti:
  
* $p(y_i \mid \theta)$ è la *distribuzione dei dati futuri* (o di una nuova osservazione) *condizionata a un valore specifico dei parametri $\theta$*. Questo componente descrive cosa si aspetta il modello che accada, *se* i parametri fossero proprio $\theta$.
* $p(\theta \mid y)$ è la *distribuzione a posteriori dei parametri del modello*, ottenuta *dopo aver osservato i dati* $y$. Questo componente rappresenta la nostra *incertezza residua* su quali siano i veri valori dei parametri.

L'integrale esprime il fatto che per fare una previsione su $y_i$, *non fissiamo un singolo valore di $\theta$*, ma consideriamo *tutti i valori plausibili* (secondo la posteriore) e *facciamo una media ponderata* delle previsioni condizionate $p(y_i \mid \theta)$.

In altre parole, la distribuzione predittiva $p(y_i \mid y)$ è una *media (o somma continua)* delle distribuzioni $p(y_i \mid \theta)$, *pesate* in base a quanto crediamo che ogni $\theta$ sia plausibile *dopo aver visto i dati*, cioè secondo $p(\theta \mid y)$.

Poiché questo integrale raramente ha una soluzione analitica, in pratica lo approssimiamo con $S$ campioni MCMC dalla posteriore:

$$
p(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) .
$$
E quindi:

$$
\text{Log-score} \approx \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)})\right).
$$

::: {#exm-}
Supponiamo di avere tre valori dei parametri dalla posteriore: $\theta^{(1)} = 0.3$ con peso $p = 0.2$, $\theta^{(2)} = 0.5$ con peso $p = 0.5$, e $\theta^{(3)} = 0.7$ con peso $p = 0.3$. Se la nuova osservazione è $y = 3$ su $n = 5$ tentativi, calcoliamo:
  
```{r}
theta_vals <- c(0.3, 0.5, 0.7)
posterior_weights <- c(0.2, 0.5, 0.3)
likelihoods <- dbinom(3, size = 5, prob = theta_vals)
p_y_given_y <- sum(likelihoods * posterior_weights)
log_score <- log(p_y_given_y)
log_score
```

Il log-score è circa -1.29. Un valore meno negativo sarebbe preferibile.
:::


## Expected Log Predictive Density (ELPD): guardare oltre i dati osservati

Il log-score misura l'adattamento ai dati osservati, ma quello che davvero ci interessa è sapere se il modello predice bene *nuovi dati*. È qui che entra in gioco l'ELPD (*Expected Log Predictive Density*):

$$
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i}),
$$

dove $y_i$ è l'$i$-esima osservazione e $\mathbf{y}_{-i}$ rappresenta tutte le osservazioni *eccetto* $y_i$.

Tornando all'esempio del test per l'ansia: l'ELPD vi direbbe quanto bene il vostro modello riesce a predire il livello di ansia di uno studente utilizzando solo i dati degli altri studenti, senza "imbrogliare" guardando il vero livello di ansia di quello studente specifico.

Un *ELPD più alto* indica che il modello riesce a predire accuratamente anche osservazioni non utilizzate per stimare i parametri, suggerendo una buona capacità di generalizzazione.

::: callout-note
**Interpretazione:** l'ELPD è un log-score calcolato *fuori campione*. Per ogni $y_i$, lo escludiamo, adattiamo il modello agli altri dati e valutiamo la probabilità predittiva di $y_i$.
:::

::: {#exm-}
Per illustrare il calcolo dell'ELPD, vediamo un esempio semplice con un set di dati molto piccolo. Supponiamo di avere un dataset di tre osservazioni: $y_1, y_2, y_3$. Supponiamo che il nostro modello stimi le probabilità per ciascuna osservazione in base a tutte le altre osservazioni, cioè utilizziamo la leave-one-out cross-validation (LOO-CV) per calcolare $p(y_i \mid \mathbf{y}_{-i})$.

Immaginiamo che il modello produca le seguenti probabilità condizionali per ogni osservazione $y_i$: $p(y_1 \mid y_2, y_3) = 0.6$, $p(y_2 \mid y_1, y_3) = 0.7$, $p(y_3 \mid y_1, y_2) = 0.5$.

L'ELPD si calcola sommando i logaritmi di queste probabilità:

$$
\text{ELPD} = \log p(y_1 \mid y_2, y_3) + \log p(y_2 \mid y_1, y_3) + \log p(y_3 \mid y_1, y_2).
$$

Calcoliamo i logaritmi naturali di ciascuna probabilità: $\log p(y_1 \mid y_2, y_3) = \log 0.6 \approx -0.5108$, $\log p(y_2 \mid y_1, y_3) = \log 0.7 \approx -0.3567$, $\log p(y_3 \mid y_1, y_2) = \log 0.5 \approx -0.6931$.

Sommiamo i logaritmi per ottenere l'ELPD:

$$
\text{ELPD} = -0.5108 + (-0.3567) + (-0.6931) = -1.5606.
$$

L'ELPD ottenuto è $-1.5606$. In generale, valori più vicini a 0 o positivi indicano una migliore capacità predittiva del modello, poiché suggeriscono che le probabilità condizionali assegnate dal modello alle osservazioni lasciate fuori non sono troppo basse. Valori molto negativi indicherebbero che il modello ha assegnato probabilità molto basse alle osservazioni effettivamente osservate, suggerendo una scarsa capacità predittiva. L'ELPD è un modo efficace per valutare quanto bene un modello generalizza a nuovi dati, evitando l'overfitting.

Un altro modello con ELPD meno negativo sarebbe preferibile.
:::


### LPPD vs ELPD: la differenza cruciale

La distinzione tra LPPD (*Log Pointwise Predictive Density*) e ELPD è fondamentale per comprendere il problema dell'overfitting:

* La *LPPD* valuta quanto bene il modello predice i dati *osservati*, mentre l'*ELPD* valuta quanto bene il modello predice *nuovi dati*.
* La LPPD tende a premiare modelli più complessi, creando il rischio di overfitting. L'ELPD, grazie alla tecnica *leave-one-out cross-validation*, penalizza l'overfitting e favorisce modelli che generalizzano meglio.

Immaginate di allenare un modello per riconoscere volti: la LPPD misurerebbe quanto bene riconosce i volti del dataset di addestramento, mentre l'ELPD misurerebbe quanto bene riconosce volti nuovi, che non ha mai visto prima. Se LPPD è alto ma ELPD è basso, per esempio, questo significa che il modello ha "memorizzato" i volti ma non ha imparato davvero a riconoscere le caratteristiche generalizzabili.


### Il collegamento con la divergenza KL

Sebbene *non possiamo calcolare $D_{\text{KL}}(p \parallel q)$ direttamente*, differenze in LPPD o ELPD *approssimano la differenza tra divergenze KL* dei modelli. Se il modello $A$ ha un ELPD più alto di quello di $B$, possiamo concludere che $A$ è "più vicino" alla distribuzione vera dei dati.

In altre parole, *massimizzare l'ELPD $\approx$ minimizzare la divergenza KL*.

In sintesi, la distribuzione predittiva posteriore, la divergenza $D_{\text{KL}}$ e l'ELPD sono strumenti matematici che ci permettono di valutare la bontà di un modello statistico. La divergenza KL fornisce una misura teoricamente ideale di quanto un modello approssima la vera distribuzione dei dati, mentre l'ELPD offre un'alternativa praticabile che valuta la capacità del modello di fare previsioni su nuovi dati. 


::: {#exm-}
Consideriamo un secondo esempio per illustrare il concetto di ELPD utilizzando la distribuzione binomiale. Immaginiamo di condurre un esperimento in cui lanciamo una moneta 10 volte e contiamo il numero di volte in cui otteniamo testa. Supponiamo che la vera probabilità di ottenere testa sia 0.6.

La distribuzione reale dei dati segue una distribuzione binomiale con 10 lanci e probabilità di successo pari a 0.6: $y \sim \text{Binomial}(10, 0.6)$.

Il nostro modello ipotizza che la probabilità di ottenere testa sia 0.5, cioè considera la moneta come equa: $p(y \mid \theta) = \text{Binomial}(10, 0.5)$.

Ora procediamo al calcolo dell'ELPD. Nel codice R qui sotto, useremo `p` per rappresentare la *vera* probabilità di successo (ad esempio, $p = 0.6$) e `q` per rappresentare la probabilità assunta dal *modello candidato* (es. $q = 0.5$). Per evitare confusione con la notazione matematica $p(y)$ vista nel testo, teniamo presente che in questo contesto `p_y` indica la vera distribuzione dei dati, mentre `dbinom(..., prob = q)` rappresenta le previsioni del modello.

```{r}
# Parametri
n <- 10          # numero di lanci
p <- 0.6         # vera probabilità di testa
q <- 0.5         # probabilità stimata dal modello

# Calcolo dell'ELPD per il modello con q = 0.5
elpd <- 0
for (y in 0:n) {
  p_y <- dbinom(y, size = n, prob = p)         # probabilità vera
  log_q_y <- log(dbinom(y, size = n, prob = q))  # log-probabilità del modello
  elpd <- elpd + p_y * log_q_y
}

cat(sprintf("ELPD del modello che stima p = 0.5: %.4f\n", elpd))

# Calcolo dell'ELPD per il modello vero (q = p)
elpd_true <- 0
for (y in 0:n) {
  p_y <- dbinom(y, size = n, prob = p)
  log_p_y <- log(dbinom(y, size = n, prob = p))
  elpd_true <- elpd_true + p_y * log_p_y
}

cat(sprintf("ELPD del modello vero (p = 0.6): %.4f\n", elpd_true))
```

L'output mostra che l'ELPD del modello vero è più alto (meno negativo), confermando che $q = 0.6$ è più predittivo di $q = 0.5$.

  | Concetto          | Cosa misura                      | Vantaggio                   |
  | ----------------- | -------------------------------- | --------------------------- |
  | **Divergenza KL** | Discrepanza tra modello e realtà | Fondamento teorico          |
  | **Log-score**     | Accuratezza su dati osservati    | Intuitivo e semplice        |
  | **LPPD**          | Log-score bayesiano              | Tiene conto dell'incertezza |
  | **ELPD**          | Log-score su dati non visti      | Stima la generalizzabilità  |
  
In pratica, *ELPD è il criterio più utile* per scegliere tra modelli bayesiani, perché stima la performance predittiva su nuovi dati, anche in assenza della distribuzione vera.
:::


## Leave-One-Out Cross-Validation (LOO-CV): la tecnica per stimare l'ELPD

Nel paragrafo precedente abbiamo introdotto l'ELPD come misura della capacità di un modello di fare buone previsioni su dati futuri e non osservati. Tuttavia, calcolare l'ELPD in modo diretto non è possibile: per definizione, coinvolge dati che non abbiamo ancora osservato, generati da una distribuzione vera $p(\tilde{y})$ che è sconosciuta.

Come possiamo stimare, in pratica, l'ELPD? Entra in gioco la *Leave-One-Out Cross-Validation (LOO-CV)*, una tecnica che consente di simulare la previsione di nuovi dati usando solo i dati osservati, fornendo così una stima empirica e affidabile dell'ELPD.


### Cos’è la LOO-CV?

La *LOO-CV* è una tecnica che valuta la capacità predittiva di un modello simulando la situazione in cui ogni osservazione viene trattata come "nuova". Il procedimento funziona come un esperimento mentale sistematico:

Prima si esclude una sola osservazione dal dataset, poi si adatta il modello sui dati rimanenti, quindi si calcola la densità predittiva del modello per l'osservazione esclusa, si ripete il processo per ogni osservazione nel dataset, e infine si somma il logaritmo delle densità predittive ottenute, ottenendo una stima detta *Expected Log Predictive Density* (ELPD):

$$
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}) ,
$$

dove $y_{-i}$ rappresenta il dataset privato dell'osservazione $i$.

Questa formula ha una struttura identica a quella dell'ELPD che abbiamo introdotto prima, ma con un'importante differenza: ogni termine viene calcolato *escludendo* l'osservazione corrispondente. In questo modo, la LOO-CV fornisce una stima *fuori campione* dell'accuratezza predittiva.


### Perché LOO-CV stima l’ELPD
 
Abbiamo detto che l'ELPD può essere interpretata come:
  
$$
  \mathbb{E}_p[\log q(\tilde{y} \mid y)],
$$
  
dove $q(\tilde{y} \mid y)$ è la distribuzione predittiva del modello. Tuttavia, non possiamo calcolare questa aspettativa perché $p(\tilde{y})$ è sconosciuta. L'idea della LOO-CV è semplice ma potente:
  
Se consideriamo ogni osservazione $y_i$ come se fosse un nuovo dato generato da $p(\tilde{y})$, allora la media sui dati osservati può approssimare l'aspettativa su $p$.

Quindi:
  
$$
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid y_{-i}) \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$
  
Questo significa che *massimizzare l'ELPD stimato con la LOO-CV equivale a minimizzare la divergenza KL tra la distribuzione vera e quella predittiva del modello*, almeno nella parte che possiamo stimare empiricamente.

In termini concreti, LOO-CV ci dice quanto bene il modello predice ogni osservazione *senza averla vista* durante l'adattamento e quanto bene il modello si aspetterebbe di performare su dati nuovi.

Riepilogando: l'ELPD è una misura ideale della capacità predittiva del modello, la LOO-CV fornisce una stima empirica dell'ELPD, l'ELPD stimato via LOO-CV permette di confrontare modelli in modo robusto, e massimizzare l'ELPD (LOO) equivale a minimizzare la divergenza KL nella parte che possiamo osservare.


## Il fondamento teorico: LOO-CV e la divergenza di Kullback-Leibler

Perché l'ELPD calcolata con LOO-CV è una buona misura per confrontare modelli? Per rispondere, dobbiamo approfondire la connessione con la misura formale della distanza tra un modello predittivo e la distribuzione vera dei dati: la divergenza di Kullback-Leibler.

### La divergenza KL come criterio di confronto

Immaginiamo che i dati futuri siano generati da una distribuzione vera, $p(\tilde{y})$, sconosciuta. Un modello predittivo bayesiano genera una distribuzione $q(\tilde{y} \mid y)$.

Una misura naturale della distanza tra queste due distribuzioni è la divergenza di Kullback-Leibler (KL):

$$
D_{\text{KL}}(p \parallel q) = \mathbb{E}_{p} \left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
$$

Il primo termine, $\mathbb{E}_{p}[\log p(\tilde{y})]$, è l'entropia della distribuzione vera. Non dipende dal modello. Il secondo termine, $\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]$, misura l'accuratezza predittiva del modello, in media rispetto alla distribuzione vera.

### Il confronto tra modelli: il termine costante si elide

Supponiamo di voler confrontare due modelli, $q_1$ e $q_2$. La differenza nelle rispettive divergenze KL rispetto a $p(\tilde{y})$ è:

$$
\begin{align}
\Delta &= D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) \notag\\
&= \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)] \right] 
  - \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] \right] .
\end{align}
$$

Il primo termine $\mathbb{E}_{p}[\log p(\tilde{y})]$ è comune a entrambi i modelli e quindi si annulla nel confronto. Resta soltanto la differenza tra le *accuratezze predittive* dei due modelli:

$$
\Delta = \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)]
$$

In altre parole, *vince il modello che predice meglio i dati futuri*.

### Come stimare l'accuratezza predittiva?

Il problema è che non conosciamo $p(\tilde{y})$, quindi non possiamo calcolare l'aspettativa vera. Tuttavia, possiamo usare una stima empirica: se assumiamo che le osservazioni $y_1, \dots, y_n$ siano state generate da $p(\tilde{y})$, allora possiamo approssimare l'aspettativa come una media sui dati osservati. Questa idea è alla base della *Leave-One-Out Cross-Validation*.

### LOO-CV come stima empirica dell'accuratezza predittiva

La *Leave-One-Out Cross-Validation* (LOO-CV) ci fornisce una *stima empirica* di questa aspettazione. In pratica, stimiamo:

$$
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i})
\approx \mathbb{E}_{p}[\log q(\tilde{y} \mid y)] ,
$$ {#eq-elpd-loo}

dove $y_i$ è una delle osservazioni effettivamente raccolte, $\mathbf{y}_{-i}$ sono tutti i dati tranne $y_i$, e $p(y_i \mid \mathbf{y}_{-i})$ è la densità predittiva calcolata escludendo $y_i$ durante l'addestramento.

Ogni osservazione viene trattata come "nuova", e il modello è valutato sulla sua capacità di predirla senza averla "vista" durante l'adattamento. In questo modo, simuliamo una previsione fuori campione.

Questa procedura approssima l'attesa rispetto a $p(\tilde{y})$:

$$
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$

Quindi: *massimizzare l'ELPD equivale a minimizzare la divergenza KL*, nella parte che possiamo stimare empiricamente.

### Confronto tra modelli tramite ELPD

Quando confrontiamo due modelli $M_1$ e $M_2$, vogliamo sapere quale dei due è più vicino alla distribuzione vera. Non possiamo calcolare direttamente la differenza tra $D_{\text{KL}}(p \parallel q_1)$ e $D_{\text{KL}}(p \parallel q_2)$, ma possiamo confrontare le loro *ELPD*:

$$
\Delta \text{ELPD} = \text{ELPD}(M_1) - \text{ELPD}(M_2) \approx \mathbb{E}_p[\log q_1(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_2(\tilde{y} \mid y)].
$$

La differenza tra le ELPD stimate è quindi un'approssimazione alla differenza tra le divergenze KL. Inoltre, possiamo anche calcolare un *errore standard* su questa differenza, per valutare l'incertezza nel confronto.


### Riepilogo intuitivo

| Concetto                                 | Cosa misura                                | Si può calcolare? | Uso principale                          |
| ---------------------------------------- | ------------------------------------------ | ----------------- | --------------------------------------- |
| $D_{\text{KL}}(p \parallel q)$           | Distanza tra modello e realtà              | ❌ (p sconosciuta) | Fondamento teorico                      |
| $\mathbb{E}_p[\log q(\tilde{y} \mid y)]$ | Accuratezza predittiva media               | ❌ (p sconosciuta) | Quantità chiave per confrontare modelli |
| $\text{ELPD}_{\text{LOO}}$               | Stima empirica dell'accuratezza predittiva | ✅                 | Criterio di confronto robusto           |

In sintesi, anche se non conosciamo la vera distribuzione dei dati, possiamo *usare l'ELPD stimata con LOO-CV come proxy della divergenza KL* e quindi come strumento pratico e teoricamente fondato per selezionare modelli che generalizzano bene.


### ELPD-LOO e il problema dell'overfitting

Valutare un modello sugli stessi dati con cui è stato addestrato può portare a sovrastimare la sua capacità predittiva, specialmente per modelli complessi. È come giudicare le capacità di uno studente facendogli ripetere esattamente gli stessi esercizi che ha già risolto durante lo studio: otterrà sicuramente un buon voto, ma questo non ci dice nulla su quanto bene risolverà problemi nuovi.

La LOO-CV evita questo problema: ogni punto viene lasciato fuori a turno, e il modello viene valutato solo su dati che non ha "visto". In questo modo, l'ELPD-LOO è una *valutazione predittiva fuori campione*, molto meno sensibile all'overfitting.

Grazie a metodi efficienti come il *Pareto-smoothed importance sampling (PSIS)*, possiamo stimare l'ELPD-LOO *senza dover riadattare il modello $n$ volte*. Questo è implementato nella funzione `loo()` in R, compatibile con pacchetti come `brms`, `rstanarm`, e `loo`.


::: {#exm-}
Supponiamo che la vera distribuzione sia binomiale con probabilità $p = 0.6$, ma il modello assume $q = 0.5$. Calcoliamo l'ELPD come log-score atteso su dati generati da $p$:

```{r}
n <- 10
p <- 0.6  # vera probabilità
q <- 0.5  # modello

y_vals <- 0:n
p_y <- dbinom(y_vals, size = n, prob = p)
log_q_y <- log(dbinom(y_vals, size = n, prob = q))

# elpd è l'implementazione computazionale di ELPD
elpd <- sum(p_y * log_q_y)
cat(sprintf("ELPD stimato (modello q = 0.5): %.4f\n", elpd))
```

Questo valore è una stima dell'*expected log predictive density* di un modello che usa $q = 0.5$ per prevedere dati generati da $p = 0.6$.
:::


## Criteri di Informazione come Approssimazioni della Divergenza $D_{\text{KL}}$

Oltre alla Leave-One-Out Cross-Validation, esistono altri approcci per stimare la qualità predittiva di un modello. Molti di questi derivano, direttamente o indirettamente, dalla *divergenza di Kullback-Leibler* ($D_{\text{KL}}$), che come abbiamo visto rappresenta la distanza tra la distribuzione vera e quella stimata dal modello.

Poiché la distribuzione vera è generalmente ignota, sono stati proposti diversi *criteri di informazione* che mirano a bilanciare due obiettivi opposti:

1. *Bontà di adattamento* del modello ai dati osservati;
2. *Penalizzazione della complessità* del modello, per evitare l’overfitting.

Tra i più noti troviamo: *MSE*, *AIC*, *BIC*, e *WAIC*. Vediamoli uno per uno.


### Errore Quadratico Medio (MSE)

L’*Errore Quadratico Medio* è un criterio semplice e intuitivo che misura la media delle differenze al quadrato tra valori osservati e valori previsti:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

* Minore è l’MSE, migliore è la capacità del modello di prevedere i dati osservati.
* Tuttavia, l’MSE *non penalizza la complessità del modello* e può favorire modelli troppo flessibili.

È quindi utile come indicatore di accuratezza, ma *non è sufficiente* per selezionare tra modelli con diversa complessità.


### Akaike Information Criterion (AIC)

L’*AIC* nasce da un'approssimazione della divergenza $D_{\text{KL}}$ e stima quanto informazione si perde usando un modello per descrivere i dati:

$$
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k
$$

* $\hat{\theta}\_{\text{MLE}}$: stima massima di verosimiglianza dei parametri;
* $k$: numero di parametri del modello.

Interpretazione:

* il primo termine misura quanto bene il modello si adatta ai dati;
* il secondo termine penalizza la complessità del modello.

Un *AIC più basso* indica un compromesso migliore tra accuratezza e semplicità.

Limitazioni:

* basato su assunzioni asintotiche (funziona meglio con campioni grandi);
* utilizza solo *stime puntuali*, ignorando l’incertezza dei parametri;
* non è pienamente bayesiano.


### Bayesian Information Criterion (BIC)

Il *Bayesian Information Criterion* (BIC) è un criterio di selezione del modello che, come l’AIC, cerca un compromesso tra **bontà di adattamento ai dati** e **complessità del modello**. Tuttavia, rispetto all’AIC, il BIC applica una **penalizzazione più severa** alla complessità, rendendolo particolarmente adatto a situazioni con grandi quantità di dati.

La formula del BIC è:

$$
\text{BIC} = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k
$$

dove:

* $p(y \mid \hat{\theta})$ è la *massima verosimiglianza* del modello, ovvero la probabilità dei dati osservati valutata nel punto $\hat{\theta}$ che massimizza la funzione di verosimiglianza (massimo a posteriori in caso di priori piatti);
* $n$ è il numero di osservazioni indipendenti;
* $k$ è il numero di parametri stimati nel modello.

Il BIC può anche essere scritto come:

$$
\text{BIC} = \ln(n) \cdot k - 2 \ln L
$$

dove $L = p(y \mid \hat{\theta})$ è, appunto, la massima verosimiglianza.


Interpretazione:

* il primo termine, $-2 \log p(y \mid \hat{\theta})$, misura quanto bene il modello si adatta ai dati;
* il secondo termine, $\log(n) \cdot k$, è una penalizzazione che aumenta con il numero di parametri e con la dimensione del campione.

Un *valore più basso del BIC* indica un modello preferibile, cioè un miglior compromesso tra accuratezza e parsimonia.

Vantaggi:

* favorisce *modelli più semplici*, specialmente quando il numero di osservazioni $n$ è elevato;
* ha una *giustificazione teorica bayesiana*: sotto ipotesi regolari e prior non informativi, il BIC approssima il logaritmo della *marginal likelihood* del modello (e quindi del modello bayesiano integrato).

Limiti:

* si basa su *assunzioni forti*, tra cui l'indipendenza delle osservazioni, modelli regolari (es. parametri identificabili) e *prior* deboli o non informativi;
* può *sottoselezionare* modelli utili in presenza di piccoli campioni, dati rumorosi e strutture complesse (es. modelli gerarchici, a posteriori multimodali).


### Widely Applicable Information Criterion (WAIC)

Il *WAIC* è una generalizzazione bayesiana dell’AIC, ed è progettato per:

* tenere conto dell’intera distribuzione a posteriori dei parametri;
* fornire una *stima fully Bayesian* dell’accuratezza predittiva.

$$
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) - 
\sum_{i=1}^{n} \text{Var}_{\theta^{(s)}} \left( \log p(y_i \mid \theta^{(s)}) \right)
\right]
$$

* $S$: numero di campioni dalla distribuzione a posteriori;
* $\theta^{(s)}$: s-esimo campione dalla posteriori;
* la seconda somma rappresenta il numero *effettivo* di parametri, basato sulla variabilità della log-verosimiglianza.

Caratteristiche principali:

* utilizza campioni MCMC → *adatto anche a modelli non regolari*;
* fornisce un’*approssimazione del log score* su dati nuovi;
* migliore dell’AIC per modelli bayesiani complessi.

Riepilogo Comparativo

| Criterio | Tipo                   | Penalizza la complessità? | Usa stime puntuali? | Supporta Bayesian MCMC? |
| -------- | ---------------------- | ------------------------- | ------------------- | ----------------------- |
| MSE      | Frequentista           | ❌                         | ✅                   | ❌                       |
| AIC      | Frequentista           | ✅ (modesta)               | ✅                   | ❌                       |
| BIC      | Frequentista/Bayesiano | ✅ (forte)                 | ✅                   | ❌                       |
| WAIC     | Bayesiano              | ✅ (effettiva)             | ❌                   | ✅                       |
| LOO-CV   | Bayesiano              | ✅ (empirica)              | ❌                   | ✅                       |

## Riflessioni Conclusive

La selezione del modello in ambito bayesiano si basa su una domanda chiave: *quanto bene il modello predice nuovi dati?*

A questo scopo, il criterio più solido è l’*Expected Log Predictive Density* (ELPD), che valuta quanto la distribuzione predittiva del modello si avvicina alla (sconosciuta) distribuzione vera dei dati. Sebbene la *divergenza di Kullback-Leibler* ($D_{\text{KL}}$) rappresenti una misura ideale per confrontare distribuzioni, il suo uso diretto è raramente possibile perché $p_{\text{vera}}(y)$ è ignota. Tuttavia, massimizzare l’ELPD equivale a minimizzare la divergenza $D_{\text{KL}}$ rispetto alla vera generatrice: entrambi puntano a rappresentare accuratamente la realtà sottostante.

Poiché l'ELPD non è calcolabile in forma esatta, esistono *approssimazioni pratiche*:

* *LOO-CV* (Leave-One-Out Cross-Validation) è oggi lo strumento più robusto per stimare l’ELPD. Valuta iterativamente ogni osservazione come "nuova" e fornisce una stima attendibile della capacità del modello di generalizzare.
* *WAIC* offre una stima simile, ma basata interamente sulla distribuzione a posteriori, senza riadattare il modello.
* *AIC* e *BIC*, pur derivando da un framework frequentista e basandosi su stime puntuali, offrono soluzioni rapide e utili in contesti semplici.
* *MSE*, infine, misura solo la distanza tra le previsioni e i valori osservati, ma *non penalizza la complessità*, e quindi è inadatto alla selezione del modello.

Nel confronto tra modelli, la *differenza tra valori di ELPD* (stimata tramite LOO-CV o WAIC) può essere accompagnata da un *errore standard* che aiuta a quantificare l’incertezza della differenza. Una regola pratica: se la differenza tra modelli è almeno *due volte maggiore* dell’errore standard, è probabile che uno dei due modelli sia davvero superiore, in termini predittivi.

Conclusione sintetica:

* *la buona statistica non è quella che spiega il passato, ma quella che anticipa il futuro*;
* la *divergenza KL* ci dà una misura teorica della distanza tra modello e realtà;
* l’*ELPD*, stimato via *LOO-CV* o *WAIC*, fornisce una misura pratica della capacità del modello di prevedere nuovi dati;
* la *selezione del modello ottimale* richiede equilibrio tra accuratezza, generalizzazione e parsimonia.

Con questi strumenti, possiamo scegliere modelli che *catturano i pattern reali nei dati senza farsi ingannare dal rumore*, garantendo affidabilità e interpretabilità anche in contesti complessi.


## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

