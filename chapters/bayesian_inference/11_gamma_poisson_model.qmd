# Modello coniugato Gamma-Poisson {#sec-bayes-inference-gamma-poisson}

::: callout-important
## In questo capitolo imparerai a

- Comprendere la distribuzione di Poisson come un modello probabilistico adatto per descrivere eventi rari in un intervallo di tempo o spazio fisso.
- Sapere applicare il metodo basato su griglia per derivare la distribuzione a posteriori del parametro $\lambda$ della distribuzione di Poisson.
- Conoscere il modello coniugato Gamma-Poisson, dimostrando come la distribuzione a priori Gamma si combini con la verosimiglianza di Poisson per produrre una distribuzione a posteriori Gamma.
- Sapere come calcolare e interpretare le probabilità utilizzando la distribuzione a posteriori. 
:::

::: callout-tip
## Prerequisiti

- Leggere il @sec-prob-discrete-prob-distr e il @sec-prob-cont-prob-distr della dispensa.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(ggdist)
```
:::


## Introduzione {.unnumbered .unlisted}

::: {.dropcap}
In psicologia capita spesso di lavorare con *variabili di conteggio*, cioè variabili che registrano quante volte un certo evento si verifica. Pensiamo, ad esempio, al numero di sintomi riportati da un paziente in una settimana, oppure al numero di parole pronunciate da un bambino durante un’interazione osservata in laboratorio. In altri casi, le variabili di conteggio vengono usate in psicolinguistica per misurare la frequenza con cui certi vocaboli compaiono in un testo.
:::

Queste variabili hanno una caratteristica importante: possono assumere solo valori interi non negativi (0, 1, 2, …). Non possiamo trattarle come se fossero variabili continue, perché la loro natura discreta richiede modelli statistici specifici. Il modello di riferimento per questo tipo di dati è la *distribuzione di Poisson*, che descrive la probabilità di osservare un certo numero di eventi in un intervallo di tempo (o di spazio), dato un tasso medio di incidenza.

Il parametro centrale della distribuzione di Poisson è $\lambda$, che rappresenta proprio il *numero medio di eventi per unità di misura* (ad esempio, la media di compulsioni per ora, oppure la media di parole per frase). Stimare questo parametro significa, in pratica, descrivere l’intensità con cui l’evento che ci interessa tende a verificarsi.

In questo capitolo ci proponiamo due obiettivi:

1. mostrare come stimare il parametro $\lambda$ in un contesto bayesiano, utilizzando una distribuzione *a priori di tipo Gamma*, che si combina bene con la Poisson;
2. confrontare due approcci diversi per ottenere la distribuzione a posteriori: da un lato la derivazione analitica, che sfrutta la coniugatezza tra le due distribuzioni, dall’altro una procedura di *simulazione Monte Carlo*, che ci permette di approssimare la distribuzione anche quando non disponiamo di formule esatte.

Questo doppio percorso è molto utile per chi studia: ci consente infatti di verificare che i metodi numerici funzionano correttamente in un caso semplice, dove conosciamo già la soluzione analitica, e ci prepara ad affrontare situazioni più complesse in cui la soluzione chiusa non esiste e la simulazione diventa indispensabile.


## Distribuzione di Poisson

La distribuzione di *Poisson* è uno degli strumenti fondamentali per descrivere fenomeni di *conteggio*, cioè situazioni in cui vogliamo sapere quante volte un certo evento si verifica in un intervallo di tempo (o di spazio) fissato. L’idea di fondo è semplice: immaginiamo che gli eventi si verifichino con una frequenza media costante e che ogni evento avvenga in modo indipendente dagli altri.

Se una variabile casuale $Y$ segue una distribuzione di Poisson con parametro $\lambda$, la probabilità di osservare un certo numero $y_i$ di eventi è data da:

$$
f(y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{y_i}}{y_i!}, \quad y_i \in \{0,1,2,\dots\},\ \lambda > 0.
$$

Il parametro $\lambda$ rappresenta il *tasso medio di occorrenza* degli eventi. Una caratteristica importante della distribuzione di Poisson è che la sua media e la sua varianza coincidono:

$$
E(Y) = \lambda, \qquad \text{Var}(Y) = \lambda.
$$

Questo significa che, se in media osserviamo 2 eventi per intervallo, anche la variabilità attesa attorno a questo valore sarà pari a 2.


### Un esempio psicologico

Per rendere più concreta l’idea, immaginiamo un paziente con disturbo ossessivo-compulsivo. Supponiamo che, in media, questo paziente compia *2 azioni compulsive all’ora*. In questo caso il parametro della distribuzione di Poisson è $\lambda = 2$.

La formula ci permette di calcolare la probabilità di osservare esattamente $k$ eventi in un’ora. Ad esempio:

* la probabilità di osservare *0 eventi* è $\frac{e^{-2} \cdot 2^0}{0!} = e^{-2} \approx 0.1353$;
* la probabilità di osservare *1 evento* è $\frac{e^{-2} \cdot 2^1}{1!} = 2e^{-2} \approx 0.2707$;
* la probabilità di osservare *2 eventi* è $\frac{e^{-2} \cdot 2^2}{2!} = 2e^{-2} \approx 0.2707$.


### Calcoli in R

Possiamo calcolare queste probabilità direttamente con R, usando la funzione `dpois`:

```{r}
lam_true <- 2
k_values <- 0:9

probabilities <- dpois(k_values, lambda = lam_true)

for (i in seq_along(k_values)) {
  cat(sprintf("Probabilità di %d eventi: %.4f\n", k_values[i], probabilities[i]))
}
```

Questo codice calcola la probabilità di osservare tra 0 e 9 eventi in un’ora, dato che il tasso medio è 2.

Notiamo che i valori più probabili sono proprio 1 o 2 eventi per ora, mentre la probabilità di osservare molti più eventi diminuisce rapidamente.


### Visualizzazione grafica

Un modo ancora più intuitivo per comprendere la distribuzione di Poisson è guardare il suo grafico. Con il seguente codice possiamo rappresentare la funzione di massa di probabilità (PMF) per $\lambda = 2$:

```{r}
lambd <- 2
x <- 0:9  
y <- dpois(x, lambda = lambd)

df <- data.frame(
  numero_eventi = x,
  probabilita = y
)

ggplot(df, aes(x = numero_eventi, y = probabilita)) +
  geom_segment(aes(x = numero_eventi, xend = numero_eventi, y = 0, yend = probabilita)) +
  geom_point(size = 3) +
  labs(title = "Distribuzione di Poisson (λ = 2)",  
       x = "Numero di eventi", 
       y = "Probabilità") +
  theme(plot.title = element_text(hjust = 0.5))
```

Il grafico mostra con chiarezza che i valori centrati attorno a 2 hanno probabilità maggiore, mentre la probabilità decresce rapidamente per valori più alti.


## Distribuzione Gamma

Per costruire un modello bayesiano con dati di tipo Poisson abbiamo bisogno di scegliere una distribuzione a priori per il parametro di tasso, $\lambda$. La scelta più comune – e anche la più conveniente dal punto di vista matematico – è la *distribuzione Gamma*. Questa distribuzione, infatti, è *coniugata* alla Poisson: significa che, partendo da una prior Gamma e aggiornandola con dati che seguono una Poisson, la distribuzione a posteriori appartiene ancora alla famiglia delle Gamma. In pratica, la coniugatezza ci permette di aggiornare le nostre credenze in modo molto semplice e diretto, senza doverci avventurare in calcoli complicati.

La densità della distribuzione Gamma è definita dalla formula:

$$
f(x \mid \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x}, \quad x > 0
$$

dove compaiono due parametri fondamentali:

* *$\alpha$ (forma)*: controlla la forma della distribuzione. Con valori piccoli, la distribuzione è molto asimmetrica, concentrata vicino allo zero; aumentando $\alpha$, la distribuzione diventa più regolare e tende ad assumere una forma simile a una normale.
* *$\beta$ (tasso o “rate”)*: regola la scala della distribuzione. Con valori alti, la probabilità si concentra verso valori più piccoli di $x$; con valori bassi, la distribuzione si allarga e assegna probabilità a valori più grandi.

Alcuni esempi aiutano a chiarire:

* con $\alpha = 2$ e $\beta = 3$, la distribuzione descrive un processo in cui gli eventi sono relativamente rari, ma non impossibili;
* con $\alpha = 10$ e $\beta = 1$, invece, la distribuzione è molto più concentrata e simmetrica, riflettendo un processo più regolare e prevedibile.

::: {.callout-note}
In R la distribuzione Gamma è parametrizzata usando direttamente $\beta$ come *rate*. Attenzione: a volte, in altri contesti, la stessa distribuzione viene scritta usando il parametro di *scala* (scale), che è semplicemente l’inverso del rate: $scale = 1/\beta$.
:::

In R possiamo calcolare la densità della distribuzione Gamma con la funzione `dgamma`:

```r
dgamma(x, shape = alpha, rate = beta)
```

Ad esempio, per una Gamma con $\alpha = 2$ e $\beta = 3$ possiamo scrivere:

```{r}
alpha <- 2
beta <- 3

ggplot(data.frame(x = c(0, 3)), aes(x = x)) +
  stat_function(
    fun = dgamma,
    args = list(shape = alpha, rate = beta)
  ) +
  labs(
    title = expression("Distribuzione Gamma con " ~ alpha == 2 ~ "," ~ beta == 3),
    x = "x",
    y = "Densità di probabilità"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

Il grafico mostra come la distribuzione si concentri vicino a valori piccoli di $x$, ma lasci comunque una coda lunga verso destra. È proprio questa flessibilità che rende la distribuzione Gamma molto utile come modello a priori: può descrivere sia situazioni in cui ci aspettiamo valori piccoli ma incerti, sia casi in cui prevediamo valori medi o alti con maggiore regolarità.


## La coppia coniugata Gamma–Poisson

Abbiamo visto separatamente due elementi: la distribuzione di *Poisson*, che descrive il numero di eventi osservati, e la distribuzione *Gamma*, che scegliamo come distribuzione a priori per il parametro di tasso $\lambda$. Ora possiamo metterli insieme per costruire un modello bayesiano completo.

Il cuore del ragionamento è questo:

* i *dati osservati* ($y_1, y_2, \dots, y_N$) sono modellati come Poisson con parametro $\lambda$;
* la nostra *conoscenza a priori* su $\lambda$ è rappresentata da una distribuzione Gamma con parametri $\alpha_{\text{prior}}$ e $\beta_{\text{prior}}$.

Questa combinazione è molto conveniente perché la Gamma è *coniugata* alla Poisson. Ma che cosa significa, concretamente, “coniugata”? Significa che, dopo aver osservato i dati, la distribuzione a posteriori di $\lambda$ rimane della stessa famiglia della priori, cioè una distribuzione Gamma. In altre parole, se parto con una Gamma e la aggiorno con dati di tipo Poisson, ottengo ancora una Gamma: cambia soltanto il valore dei parametri, che incorporano l’informazione proveniente dai dati.


### La distribuzione a posteriori

La regola di Bayes ci dice che:

$$
p(\lambda \mid y) \propto p(\lambda) \cdot p(y \mid \lambda).
$$

Sostituendo le due distribuzioni (Gamma per la priori e Poisson per la verosimiglianza), si vede facilmente che la forma risultante è ancora quella di una Gamma. I parametri aggiornati sono:

$$
\alpha_{\text{post}} = \alpha_{\text{prior}} + \sum_{i=1}^N y_i
$$

$$
\beta_{\text{post}} = \beta_{\text{prior}} + N
$$

Dunque, la nostra credenza su $\lambda$ dopo aver osservato i dati è rappresentata da una distribuzione:

$$
\lambda \mid y \sim \text{Gamma}(\alpha_{\text{post}}, \beta_{\text{post}})
$$


### Intuizione

Che cosa sta succedendo qui? L’aggiornamento dei parametri ha un’interpretazione molto intuitiva:

* *$\alpha$* aumenta con la somma degli eventi osservati: più eventi contiamo, più la distribuzione a posteriori si sposta verso valori alti di $\lambda$.
* *$\beta$* aumenta con il numero di osservazioni $N$: più intervalli di tempo osserviamo, più aumenta la precisione con cui stimiamo il tasso medio.

In altre parole, i parametri della posteriori rappresentano una *sintesi tra la nostra conoscenza a priori e i dati raccolti*.


### Un esempio numerico

Riprendiamo l’esempio visto prima, con i dati:

```{r}
y <- c(2, 1, 3, 2, 2, 1, 1, 1)
```

Abbiamo $N = 8$ osservazioni, e come priori scegliamo $\alpha_{\text{prior}} = 9$ e $\beta_{\text{prior}} = 2$.

Calcoliamo i nuovi parametri:

```{r}
alpha_post <- 9 + sum(y)
beta_post  <- 2 + length(y)

alpha_post; beta_post
```

Il risultato è $\alpha_{\text{post}} = 22$ e $\beta_{\text{post}} = 10$. La nostra distribuzione a posteriori è quindi una Gamma(22, 10), che rappresenta la nuova convinzione su $\lambda$ dopo aver visto i dati.


### Visualizzare la posteriori

Possiamo visualizzare il cambiamento confrontando la priori e la posteriori:

```{r}
x <- seq(0, 6, length.out = 500)

prior_density <- dgamma(x, shape = 9, rate = 2)
post_density  <- dgamma(x, shape = alpha_post, rate = beta_post)

df <- data.frame(
  x = rep(x, 2),
  densita = c(prior_density, post_density),
  Distribuzione = rep(c("Prior", "Posterior"), each = length(x))
)

ggplot(df, aes(x = x, y = densita, color = Distribuzione)) +
  geom_line(size = 1) +
  labs(
    title = "Aggiornamento da Prior a Posterior (Gamma–Poisson)",
    x = expression(lambda),
    y = "Densità"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

Il grafico mostra chiaramente che la posteriori è più concentrata: osservare gli otto dati ci ha reso più sicuri sul valore di $\lambda$. Inoltre, il centro della distribuzione si è spostato verso il valore suggerito dai dati, integrando così le informazioni empiriche con le credenze iniziali.


## Metodo basato su griglia

Ora che abbiamo introdotto il modello *Gamma–Poisson*, possiamo provare a calcolare la distribuzione *a posteriori* del parametro $\lambda$ non con formule chiuse, ma con un procedimento puramente numerico. Questo approccio prende il nome di *metodo su griglia* (*grid approximation*), ed è molto utile come strumento didattico: ci mostra, passo dopo passo, come funziona l’aggiornamento bayesiano, trasformando le regole matematiche in un algoritmo semplice da eseguire al computer.

L’idea è la seguente:

1. fissiamo una “griglia” di possibili valori per $\lambda$;
2. calcoliamo la probabilità di ciascun valore secondo la *priori*;
3. calcoliamo, per gli stessi valori, la *verosimiglianza* dei dati osservati;
4. moltiplichiamo i due risultati per ottenere la distribuzione *a posteriori* (non ancora normalizzata);
5. normalizziamo in modo che la somma totale sia pari a 1.


### 1. I dati e la priori

Prendiamo gli stessi dati già usati in precedenza, cioè il numero di compulsioni osservate in otto intervalli di tempo:

```{r}
y <- c(2, 1, 3, 2, 2, 1, 1, 1)
```

La priori scelta per $\lambda$ è una Gamma con parametri $\alpha = 9$ e $\beta = 2$, che riflette la nostra convinzione iniziale che il tasso medio sia intorno a 4–5 eventi, con un certo margine di incertezza.

Creiamo una griglia di valori plausibili di $\lambda$, ad esempio da 0 a 10, divisa in 1000 punti:

```{r}
alpha_prior <- 9
beta_prior  <- 2

lambda_grid <- seq(0.01, 10, length.out = 1000)

prior <- dgamma(lambda_grid, shape = alpha_prior, rate = beta_prior)
```

Questa curva rappresenta ciò che crediamo possibile *prima di vedere i dati*.


### 2. La verosimiglianza

La seconda componente è la *verosimiglianza*, cioè quanto ciascun valore ipotetico di $\lambda$ rende probabili i dati osservati. Per una Poisson, la verosimiglianza è il prodotto delle probabilità individuali:

$$
\mathcal{L}(\lambda) = \prod_{i=1}^N P(Y = y_i \mid \lambda).
$$

In R possiamo calcolarla in modo vettoriale:

```{r}
likelihood <- sapply(lambda_grid, function(l) prod(dpois(y, l)))
```

Così, per ogni punto della griglia otteniamo il valore della verosimiglianza: se il dato osservato è molto coerente con un certo $\lambda$, la verosimiglianza sarà alta; altrimenti sarà vicina a zero.


### 3. La distribuzione a posteriori

Il passo successivo è combinare *priori* e *verosimiglianza*. La regola di Bayes ci dice che:

$$
p(\lambda \mid y) \propto p(\lambda) \cdot \mathcal{L}(\lambda).
$$

In pratica, moltiplichiamo i valori calcolati ai due passi precedenti:

```{r}
posterior_unnormalized <- prior * likelihood
```

Questo ci dà una curva che ha la forma giusta, ma che non è ancora una vera distribuzione di probabilità: non somma a 1.


### 4. Normalizzazione

Per trasformarla in una distribuzione valida, dobbiamo normalizzare, cioè dividere per la somma totale (che qui approssimiamo come una somma numerica sulla griglia):

```{r}
grid_width <- lambda_grid[2] - lambda_grid[1]
normalization_factor <- sum(posterior_unnormalized) * grid_width

posterior <- posterior_unnormalized / normalization_factor
```

Ora abbiamo davvero la distribuzione *a posteriori* di $\lambda$.


### 5. Interpretazione e confronto visivo

A questo punto possiamo confrontare la priori e la posteriori in un unico grafico:

```{r}
df <- data.frame(
  lambda     = lambda_grid,
  Prior      = prior,
  Posteriori = posterior
)

df_long <- pivot_longer(df, cols = c("Prior", "Posteriori"),
                        names_to = "Distribuzione",
                        values_to = "Densita")

ggplot(df_long, aes(x = lambda, y = Densita, color = Distribuzione)) +
  geom_line(size = 1) +
  labs(title = "Aggiornamento bayesiano con il metodo su griglia",
       x = expression(lambda),
       y = "Densità")
```

Il grafico rivela due aspetti fondamentali:

* la *posteriori* è *spostata* rispetto alla *priori*, perché i dati suggeriscono valori di $\lambda$ più bassi di quanto ci aspettassimo inizialmente;
* la *posteriori* è anche più *concentrata*, cioè meno incerta: dopo aver osservato i dati, non solo abbiamo aggiornato la nostra convinzione, ma siamo anche diventati più sicuri del valore plausibile di $\lambda$.


### Perché è utile il metodo su griglia?

Questo metodo non è molto efficiente in problemi complessi (dove i parametri sono tanti e lo spazio da esplorare è enorme), ma è *molto utile a scopo didattico*. Ci mostra infatti in modo visivo e intuitivo come funziona l’aggiornamento bayesiano: partiamo da una convinzione iniziale (la prior), la confrontiamo con i dati (la likelihood) e otteniamo una nuova convinzione aggiornata (la posterior).

Nei prossimi capitoli vedremo come affrontare lo stesso problema con strumenti più generali, come il campionamento MCMC in Stan, che diventano indispensabili quando i modelli si complicano e il metodo su griglia non è più praticabile.


## Riflessioni conclusive

Il modello **Gamma–Poisson** rappresenta un esempio chiaro ed elegante di come l’inferenza bayesiana possa essere applicata ai dati di conteggio, molto frequenti in psicologia. L’idea di fondo è semplice: partiamo da una convinzione iniziale sul tasso medio di occorrenza degli eventi, espressa attraverso una distribuzione *a priori* di tipo Gamma. Quando osserviamo nuovi dati, questa convinzione viene aggiornata in modo sistematico, e il risultato è una nuova distribuzione, la *a posteriori*, che incorpora sia ciò che sapevamo prima sia l’informazione proveniente dai dati.

Questo meccanismo ha due grandi vantaggi. Da un lato, ci fornisce **stime più realistiche** del parametro \$\lambda\$, perché non si limita a un singolo numero ma restituisce una distribuzione che descrive tutti i valori plausibili. Dall’altro, ci permette di **quantificare l’incertezza**: non diciamo soltanto “qual è” il tasso medio, ma anche “quanto siamo incerti” sulla sua stima.

In questo modo, il modello Gamma–Poisson non è soltanto un esercizio teorico: diventa uno strumento pratico che ci aiuta a ragionare in modo più cauto e informato quando prendiamo decisioni basate sui dati psicologici. È un primo passo per comprendere il potere dell’approccio bayesiano, che non si limita a calcolare parametri, ma ci offre un vero e proprio linguaggio per descrivere e aggiornare le nostre conoscenze.


## Esercizi

::: {.callout-tip title="Esercizio" collapse="true"}
Consideriamo uno studio longitudinale su coppie, dove i partecipanti registrano quotidianamente la frequenza con cui nascondono il loro comportamento di fumo al partner. Basandoci sui dati di @scholz2021people, assumiamo che il tasso medio di nascondere il fumo sia di 1.52 volte al giorno. Supponiamo di avere i seguenti dati giornalieri per un partecipante:

- Giorno 1: 2 volte.
- Giorno 2: 0 volte.
- Giorno 3: 1 volta.
- Giorno 4: 3 volte.

Utilizzare un modello Gamma-Poisson per stimare la distribuzione a posteriori del tasso individuale di nascondere il fumo per un partecipante specifico, dato il suo insieme di osservazioni giornaliere.
:::

::: {.callout-tip title="Soluzione" collapse="true"}
Vogliamo stimare quante volte al giorno una persona tende a nascondere il proprio fumo al partner. Abbiamo quattro giorni di dati su questa persona e, grazie a un precedente studio, sappiamo che in media le persone lo fanno circa *1.52 volte al giorno*.

Ma i dati di una sola persona sono pochi, quindi useremo un *approccio bayesiano* per combinare:

1. *Le informazioni preesistenti* (dal precedente studio),
2. *Le nuove osservazioni* (quanti eventi sono stati registrati nei quattro giorni).

Con un modello *Gamma-Poisson*, possiamo aggiornare la nostra stima e ottenere una *distribuzione a posteriori*, che ci dirà quali sono i valori più probabili per il tasso di nascondimento di questa persona.

```r
# Dati osservati: numero di volte che la persona ha nascosto il fumo ogni giorno
osservazioni <- c(2, 0, 1, 3)

# Numero totale di giorni osservati
n_giorni <- length(osservazioni)

# Numero totale di eventi (quante volte ha nascosto il fumo in totale)
eventi_totali <- sum(osservazioni)

# Informazione a priori dallo studio precedente
media_priori <- 1.52
forma_priori <- 3   # Parametro di forma scelto per un'incertezza moderata
tasso_priori <- forma_priori / media_priori  # Parametro di scala

# Aggiornamento bayesiano: parametri della distribuzione a posteriori
forma_post <- forma_priori + eventi_totali
tasso_post <- tasso_priori + n_giorni

# Creazione della griglia di valori possibili per il tasso giornaliero (lambda)
lambda_valori <- seq(0, 5, length.out = 100)

# Calcolo delle densità per le distribuzioni a priori e a posteriori
densita_priori <- dgamma(lambda_valori, shape = forma_priori, rate = tasso_priori)
densita_post <- dgamma(lambda_valori, shape = forma_post, rate = tasso_post)

# Creazione di un dataframe per il grafico
dati_plot <- data.frame(
  lambda = rep(lambda_valori, 2),
  densita = c(densita_priori, densita_post),
  distribuzione = rep(c("Priori", "Posteriori"), each = length(lambda_valori))
)

# Creazione del grafico per confrontare la distribuzione a priori e quella aggiornata (posteriori)
ggplot(dati_plot, aes(x = lambda, y = densita, color = distribuzione)) +
  geom_line(size = 1.2) +
  labs(
    title = "Stima del tasso di nascondimento del fumo",
    x = "Tasso giornaliero (λ)",
    y = "Densità"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

*Cosa fa questo codice*

1. *Carica i dati*: abbiamo registrato per 4 giorni quante volte questa persona ha nascosto il fumo.
2. *Imposta una conoscenza iniziale (priori)*: basata sullo studio precedente.
3. *Applica il teorema di Bayes* per aggiornare la stima con i nuovi dati.
4. *Genera il grafico*: confronta la distribuzione prima e dopo l’aggiornamento con i dati osservati.

*Interpretazione del risultato*

- La curva *rossa (priori)* rappresenta la nostra stima iniziale, basata sullo studio precedente.
- La curva *blu (posteriori)* è la nostra nuova stima dopo aver considerato i dati della persona.
- Se i dati raccolti sono molto diversi dal valore medio dello studio, la curva blu si sposterà rispetto alla rossa.

Questa analisi ci permette di stimare il comportamento specifico di una persona integrando dati generali e osservazioni individuali, in modo più informativo rispetto a una semplice media.
:::

## Informazioni sull'ambiente di sviluppo {.unnumbered .unlisted}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered .unlisted}

