{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Modello bayesiano di regressione lineare bivariata\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Prerequisiti**\n",
        "\n",
        "- Leggere *Regression and Other Stories* [@gelman2020regression].\n",
        "  - Prestare particolare attenzione ai capitoli 1 \"Overeview, 6, \"Background on Regression Modeling,\" 7, \"Linear Regression with a Single Predictor\" e 8, \"Fitting regression models\", che offrono una guida dettagliata al modello di regressione bivariato.\n",
        "\n",
        "**Concetti e Competenze Chiave**\n",
        "\n",
        "- Comprendere il significato del modello di regressione lineare bayesiano e le differenze rispetto al modello frequentista.\n",
        "- Interpretare le stime dei parametri in un contesto bayesiano e confrontarle con l'interpretazione dei parametri nell'approccio frequentista.\n",
        "- Capire il metodo dei minimi quadrati.\n",
        "- Capire il concetto di verosimiglianza per il modello di regressione.\n",
        "- Capire il codice Stan per il modello di regressione.\n",
        "- Comprendere il significato delle previsioni del modello di regressione nell'ambito bayesiano.\n",
        "- Capire e sapere interpretare il Posterior Predictive Check nel modello bayesiano.\n",
        "- Capire il concetto di regressione verso la media.\n",
        "\n",
        "**Preparazione del Notebook**\n"
      ],
      "id": "8e53e6c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:00:47.833443Z",
          "start_time": "2024-07-13T07:00:46.436472Z"
        }
      },
      "source": [
        "# Importazioni dalla libreria standard\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Importazioni di librerie di terze parti\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import cmdstanpy\n",
        "cmdstanpy.utils.get_logger().setLevel(logging.ERROR)\n",
        "from cmdstanpy import CmdStanModel\n",
        "import pingouin as pg\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
      ],
      "id": "6c8b2699",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configurazione\n",
        "seed = sum(map(ord, \"bayesian_bivariate_regression\"))  # Genera un seme basato sulla somma dei valori ASCII della stringa \"linear_algebra\"\n",
        "rng = np.random.default_rng(seed=seed)  # Crea un generatore di numeri casuali con il seme specificato\n",
        "sns.set_theme(palette=\"colorblind\")  # Imposta il tema di Seaborn per grafici accessibili ai daltonici\n",
        "az.style.use(\"arviz-darkgrid\")  # Imposta lo stile dei grafici di ArviZ\n",
        "%config InlineBackend.figure_format = \"retina\"  # Migliora la risoluzione dei grafici inline\n",
        "\n",
        "# Definizione delle directory\n",
        "home_directory = os.path.expanduser(\"~\")  # Ottiene la directory home dell'utente\n",
        "project_directory = f\"{home_directory}/_repositories/psicometria\"  # Definisce la directory del progetto\n",
        "\n",
        "# Stampa la directory del progetto per verificarla\n",
        "print(f\"Directory del progetto: {project_directory}\")\n",
        "\n",
        "def standardize(series):\n",
        "    \"\"\"Standardize a pandas series with n degrees of freedom\"\"\"\n",
        "    return (series - series.mean()) / series.std(ddof=0)"
      ],
      "id": "289bb280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduzione {.unnumbered}\n",
        "\n",
        "I modelli lineari sono stati impiegati in molteplici contesti per lungo tempo. Come descritto da @stigler1986, il metodo dei minimi quadrati, una tecnica per adattare una semplice regressione lineare, veniva già utilizzato nel XVIII secolo per affrontare problemi di analisi dei dati in astronomia. Ad esempio, questo metodo era impiegato per determinare il moto della Luna e per riconciliare i movimenti non periodici di Giove e Saturno. All'epoca, gli astronomi erano tra i primi a sentirsi a proprio agio nell'uso di tali metodi, poiché raccoglievano personalmente le loro osservazioni e sapevano che le condizioni di raccolta dei dati erano omogenee, anche se i valori osservati potevano differire. Questo contrastava con l'approccio più cauto delle scienze sociali, dove la riluttanza a combinare dati eterogenei ritardava l'adozione dei modelli lineari [@stigler1986].\n",
        "\n",
        "In questa sezione della dispensa, esploreremo due modelli statistici fondamentali: la regressione lineare bivariata e la regressione lineare multipla. Il primo modello considera una sola variabile esplicativa, mentre il secondo ne include diverse.\n",
        "\n",
        "È cruciale sottolineare che i modelli statistici sono principalmente utilizzati per due scopi: inferenza e previsione. La previsione si limita a descrivere l'associazione tra le variabili, mentre l'inferenza mira a stabilire relazioni di causa-effetto attraverso l'uso del modello lineare. Mentre la previsione non è una tecnica controversa e può essere facilmente verificata sulla base della sua effettiva capacità di predire la variabile dipendente, l'uso della regressione per l'inferenza causale è molto più problematico. Esso richiede una profonda comprensione del fenomeno in esame e una progettazione sperimentale o quasi-sperimentale adeguata per giustificare le assunzioni necessarie.\n",
        "\n",
        "Indipendentemente dall'approccio scelto, è fondamentale tenere presente che l'analisi di regressione è essenzialmente una forma di media ponderata. Di conseguenza, i risultati ottenuti riflettono inevitabilmente i bias e le peculiarità del dataset utilizzato.\n",
        "\n",
        "Per ciascun modello, esamineremo due approcci distinti:\n",
        "\n",
        "1. L'utilizzo delle funzioni di `pingouin`, particolarmente utile per l'analisi esplorativa dei dati (EDA) quando si necessita di risultati rapidi.\n",
        "2. L'approccio bayesiano, ideale quando l'obiettivo principale è l'inferenza statistica.\n",
        "\n",
        "## Adattare una Retta di Regressione a Dati Simulati\n",
        "\n",
        "Simuliamo 20 osservazioni di $x$ e $y$, dove $y$ è generato in base a un modello di regressione teorico con i parametri specificati di seguito.\n"
      ],
      "id": "02f53a60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define variables\n",
        "x = np.arange(1, 21)\n",
        "n = len(x)\n",
        "a = 0.2\n",
        "b = 0.3\n",
        "sigma = 0.5\n",
        "\n",
        "# Generate y values\n",
        "y = a + b * x + sigma * np.random.normal(size=n)\n",
        "\n",
        "fake = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "fake.head()"
      ],
      "id": "304bcd7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adattiamo ai dati un modello di regressione bayesiano. Compiliamo e stampiamo il codice Stan.\n"
      ],
      "id": "795c8ddf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_file = os.path.join(project_directory, \"stan\", \"bivariate_linreg_model.stan\")\n",
        "model = CmdStanModel(stan_file=stan_file)\n",
        "print(model.code())"
      ],
      "id": "ad520e66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definiamo un dizionario che contiene i dati nel formato attesto da Stan.\n"
      ],
      "id": "be4a258c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stan_data = {\"N\": len(fake[\"x\"]), \"x\": fake[\"x\"], \"y\": fake[\"y\"]}"
      ],
      "id": "538e33dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eseguiamo il campionamento.\n"
      ],
      "id": "afef828b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = model.sample(\n",
        "    data=stan_data,\n",
        "    iter_warmup=2_000,\n",
        "    iter_sampling=2_000,\n",
        "    seed=123,\n",
        "    show_progress=False,\n",
        "    show_console=False\n",
        ")"
      ],
      "id": "94a7f710",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esaminiamo una sintesi della distribuzione a posteriori dei parametri.\n"
      ],
      "id": "9b4aa60a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(fit, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94).round(2)"
      ],
      "id": "51708a44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le prime due righe dell'output ci indicano che l'intercetta stimata è -0.15 con un'incertezza di 0.30, e la pendenza stimata è 0.34 con un'incertezza di 0.02. La deviazione standard residua $\\sigma$ è stimata a 0.64 con un'incertezza di 0.12.\n",
        "\n",
        "È utile disegnare un diagramma a dispersione con la retta di regressione stimata.\n"
      ],
      "id": "9828c390"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Estrarre i parametri stimati dal modello bayesiano\n",
        "alpha_hat = -0.15  # Stima media di alpha\n",
        "beta_hat = 0.34  # Stima media di beta\n",
        "\n",
        "# Scatterplot dei dati\n",
        "plt.scatter(fake[\"x\"], fake[\"y\"], color=\"blue\", label=\"Dati osservati\")\n",
        "\n",
        "# Disegnare la retta di regressione usando i parametri stimati\n",
        "x_values = np.linspace(fake[\"x\"].min(), fake[\"x\"].max(), 100)\n",
        "y_values = alpha_hat + beta_hat * x_values\n",
        "plt.plot(x_values, y_values, color=\"red\", label=\"Retta di regressione stimata\")\n",
        "\n",
        "# Aggiungere titoli e etichette\n",
        "plt.title(\"Scatterplot con Retta di Regressione\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "\n",
        "# Mostrare il grafico\n",
        "plt.show()"
      ],
      "id": "1577c28a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vediamo che la retta stimata si adatta bene alla nube di punti del campione. Possiamo ora confrontare le stime con i valori dei parametri assunti. Cominciamo con l'intercetta $a$, che abbiamo impostato a 0.2 nelle simulazioni. Dopo aver adattato il modello ai dati simulati, la stima risulta essere \n",
        "−0.15, che è molto diversa dal valore assunto di 0.2 -- ma l'incertezza, o errore standard, nella stima è di 0.30. Approssimativamente, ci aspettiamo che la differenza tra la stima e il valore reale rientri in 1 errore standard nel 68% dei casi, e in 2 errori standard nel 95% dei casi. Quindi, se il valore reale è 0.2, e l'errore standard è 0.30, non è sorprendente che la stima risulti essere −0.15. Allo stesso modo, per le stime di $b$ e σ, l'intervallo di credibilità al 95% contiene i loro valori reali.\n",
        "\n",
        "Come appena illustrato, una qualsiasi simulazione di dati fittizi con dati continui non riprodurrà esattamente i valori dei parametri assunti. Tuttavia, sotto simulazioni ripetute, dovremmo osservare una copertura adeguata.\n",
        "\n",
        "Per semplicità, adottiamo l'approccio frequentista e consideriamo l'intervallo di confidenza al 95%. Per livello di copertura si intende la proporzione di volte in cui, nelle simulazioni, il valore reale del parametro rientra nell'intervallo di confidenza stimato a partire dai dati.\n"
      ],
      "id": "3a444866"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imposta il seme per la riproducibilità\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parametri veri\n",
        "a_true = 0.2\n",
        "b_true = 0.3\n",
        "sigma_true = 0.5\n",
        "\n",
        "# Numero di simulazioni\n",
        "num_simulations = 1000\n",
        "\n",
        "# Per tracciare il livello di copertura\n",
        "coverage_a = 0\n",
        "coverage_b = 0\n",
        "\n",
        "# Intervalli di confidenza al 95%\n",
        "z_value = 1.96  # valore z per il 95% di confidenza\n",
        "\n",
        "for _ in range(num_simulations):\n",
        "    # Simula i dati\n",
        "    x = np.arange(1, 21)\n",
        "    n = len(x)\n",
        "    y = a_true + b_true * x + sigma_true * np.random.normal(size=n)\n",
        "\n",
        "    # Crea un DataFrame con i dati simulati\n",
        "    fake = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "\n",
        "    # Fitting del modello\n",
        "    X = sm.add_constant(fake[\"x\"])  # Aggiunge il termine di intercetta\n",
        "    model = sm.OLS(fake[\"y\"], X).fit()\n",
        "\n",
        "    # Estrazione dei parametri stimati\n",
        "    a_hat = model.params[0]\n",
        "    b_hat = model.params[1]\n",
        "\n",
        "    # Calcolo degli errori standard\n",
        "    se_a = model.bse[0]\n",
        "    se_b = model.bse[1]\n",
        "\n",
        "    # Calcolo degli intervalli di confidenza al 95%\n",
        "    ci_a_low = a_hat - z_value * se_a\n",
        "    ci_a_high = a_hat + z_value * se_a\n",
        "    ci_b_low = b_hat - z_value * se_b\n",
        "    ci_b_high = b_hat + z_value * se_b\n",
        "\n",
        "    # Controlla se il valore vero è all'interno degli intervalli di confidenza\n",
        "    if ci_a_low <= a_true <= ci_a_high:\n",
        "        coverage_a += 1\n",
        "    if ci_b_low <= b_true <= ci_b_high:\n",
        "        coverage_b += 1\n",
        "\n",
        "# Calcola e stampa i livelli di copertura\n",
        "coverage_a_rate = coverage_a / num_simulations * 100\n",
        "coverage_b_rate = coverage_b / num_simulations * 100\n",
        "\n",
        "print(f\"Livello di copertura per l'intercetta a: {coverage_a_rate:.2f}%\")\n",
        "print(f\"Livello di copertura per la pendenza b: {coverage_b_rate:.2f}%\")"
      ],
      "id": "4068e5b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I valori di copertura ottenuti empiricamente mediante la simulazione sono prossimi ai valori teorici attesi.\n",
        "\n",
        "## Modellare l'associazione statistica tra variabili\n",
        "\n",
        "Avendo verificato che il modello di regressione è in grado di recuperare in modo attendibile i valori teorici dell'intercetta e della pendenza della retta di regressione, passiamo ora ad applicare il modello a un insieme di dati reali.\n",
        "\n",
        "Esamineremo un set di [dati](../../data/affect.csv) che riguarda la relazione tra i punteggi di affect e arousal. Questi dati provengono da due studi condotti presso il Personality, Motivation, and Cognition Laboratory della Northwestern University, nei quali sono stati utilizzati film per indurre stati affettivi [@rafaeli2006premature].\n",
        "\n",
        "Ci concentreremo sull'associazione tra l'ansia di stato, considerata come variabile indipendente, e la scala di *Tense Arousal* del *Motivational State Questionnaire* (MSQ), considerata come variabile dipendente.\n",
        "\n",
        "In precedenza, abbiamo applicato il modello normale a una singola variabile. Tuttavia, solitamente siamo interessati a modellare come una variabile di esito sia associata a una variabile predittiva. Se esiste un'associazione statistica tra la variabile predittiva e quella di esito, possiamo utilizzarla per predire il risultato. Quando la variabile predittiva viene incorporata nel modello in un modo specifico, otteniamo una regressione lineare.\n",
        "\n",
        "I dati dell'esempio sono forniti di seguito.\n"
      ],
      "id": "eeafa5ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Definire il percorso del file CSV\n",
        "file_path = os.path.join(project_directory, \"data\", \"affect.csv\")\n",
        "\n",
        "# Leggere il file CSV in un DataFrame pandas\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Selezionare le colonne state1 e TA1\n",
        "df = data[[\"state1\", \"TA1\"]]\n",
        "df.head()"
      ],
      "id": "0b6452e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'associazione tra le due variabili, ansia di stato e Tense Arousal, è rappresentata nel grafico sottostante. Il grafico suggerisce che l'associazione può essere approssimata da una semplice funzione matematica, come una retta. Tuttavia, è evidente che una funzione lineare sia troppo semplicistica per rappresentare accuratamente questi dati, poiché non è possibile trovare una singola retta che passi per tutti i punti del diagramma di dispersione.\n"
      ],
      "id": "2fe73019"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:00:57.143715Z",
          "start_time": "2024-07-13T07:00:57.000582Z"
        }
      },
      "source": [
        "plt.scatter(df[\"state1\"], df[\"TA1\"])\n",
        "plt.xlabel(\"State Anxiety\")\n",
        "plt.ylabel(\"Tense Arousal\")\n",
        "plt.show()"
      ],
      "id": "e08ba0c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimi Quadrati\n",
        "\n",
        "Ci poniamo il duplice obiettivo di identificare la retta che meglio si adatta ai dati del diagramma e di valutare la qualità di tale adattamento. In altre parole, vogliamo misurare quanto, in media, i punti del diagramma si discostano dalla retta trovata.\n",
        "\n",
        "Nel modello di regressione lineare classica, espresso come $y_i = \\beta_0 + \\beta_1 x_i + e_i$, i coefficienti $\\beta_0$ e $\\beta_1$ vengono stimati minimizzando la somma dei quadrati degli errori $\\epsilon_i$.\n",
        "\n",
        "Possiamo utilizzare la funzione `linear_regression()` del pacchetto `pingouin` per calcolare i coefficienti del modello seguendo questo approccio:\n"
      ],
      "id": "c679498d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = df[\"state1\"]\n",
        "y = df[\"TA1\"]\n",
        "\n",
        "lm = pg.linear_regression(x, y)\n",
        "lm.round(2)"
      ],
      "id": "b3c47df0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recuperiamo i coefficienti `b0` e `b1` dall'oggetto `lm` creato da `linear_regression()`.\n"
      ],
      "id": "de08ebd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta = lm[\"coef\"]  # Coefficienti\n",
        "beta"
      ],
      "id": "4161593e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "b0 = beta[0]\n",
        "b1 = beta[1]"
      ],
      "id": "2f0d29fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calcoliamo i valori predetti dal modello di regressione:\n"
      ],
      "id": "0088670f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "yhat = b0 + b1 * x\n",
        "yhat"
      ],
      "id": "b9ac0de6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I valori predetti $\\hat{y}$ corrispondono alla retta di regressione:\n"
      ],
      "id": "64fa8a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(x, yhat)\n",
        "plt.xlabel(\"Ansia di stato\")\n",
        "plt.ylabel(\"Tense Arousal, $\\\\hat{y}$\")\n",
        "_ = plt.title(\"Retta di regressione\")"
      ],
      "id": "85e66ba6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aggiungiamo i dati osservati al grafico.\n"
      ],
      "id": "d2784abc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(x, yhat)\n",
        "plt.plot(x, y, \"x\")\n",
        "plt.xlabel(\"Ansia di stato\")\n",
        "plt.ylabel(\"Tense Arousal, $\\\\hat{y}$\")\n",
        "_ = plt.title(\"Retta di regressione\")"
      ],
      "id": "73c039a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretazione\n",
        "\n",
        "Il coefficiente $\\beta_0$ indica il valore atteso della distribuzione condizionata $p(y_i \\mid x_i = 0)$. Nel caso presente, indica la media di Tense Arousal quando l'ansia di stato è uguale a 0. Ovviamente questa non è un'informazione di una qualche importanza pratica. Vedremo come migliorare l'interpretabilità dell'intercetta usando una parametrizzazione alternativa dei dati.\n",
        "\n",
        "Il coefficiente $\\beta_1$ indica il cambiamento del valore atteso della variabile dipendente quando la variabile indipendente aumenta di un'unità. Nel caso presente abbiamo che il punteggio di Tense Arousal aumenta in media di 0.27 punti quando l'ansia di stato aumenta di un punto. In una parametrizzazione alternativa, standardizzando la variabile indipendente, $\\beta_1$ indicherebbe di quanto varia in media Tense Arousal quando l'ansia di stato aumenta di una deviazione standard.\n",
        "\n",
        "## Residui\n",
        "\n",
        "Calcoliamo i residui\n",
        "\n",
        "$$\n",
        "e_i = y_i - \\hat{y}_i\n",
        "$$\n"
      ],
      "id": "b269f938"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e = y - yhat"
      ],
      "id": "c4a1170a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La retta di regressine calcolata con il metodo della massima verosimiglianza ha le seguenti proprietà:\n",
        "\n",
        "- il valore atteso dei residui è zero,\n",
        "- i residui sono incorrelati con i valori predetti.\n",
        "\n",
        "Valutiamo la media dei residui:\n"
      ],
      "id": "ac1382ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.mean(e)"
      ],
      "id": "e438236e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calcoliamo la correlazione tra i residui $e$ e i valori predetti $\\hat{y}$:\n"
      ],
      "id": "116f10b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.corrcoef(e, yhat)[0, 1]"
      ],
      "id": "b91d902b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il modello di regressione bivariato \n",
        "\n",
        "$$\n",
        "y_i = \\beta_0 + \\beta_1 x_i + e_i\n",
        "$$\n",
        "\n",
        "scompone la variabile dipendente $y_i$ in due componenti tra loro incorrelate, una componente deterministica\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\beta_0 + \\beta_1 x_i \n",
        "$$\n",
        "\n",
        "e una componente aleatoria\n",
        "\n",
        "$$\n",
        "e_i = y_i - \\hat{y}_i.\n",
        "$$\n"
      ],
      "id": "5779950f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"x\"] = x\n",
        "df[\"y\"] = y\n",
        "df[\"yhat\"] = yhat\n",
        "df[\"e\"] = e\n",
        "df[\"sum\"] = df[\"yhat\"] + df[\"e\"]\n",
        "df"
      ],
      "id": "b93338d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Errore Standard della Regressione\n",
        "\n",
        "L'errore standard della regressione rappresenta la stima della deviazione standard dei residui nell'intera popolazione. Questo parametro può essere calcolato attraverso la formula:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}_e = \\sqrt{\\frac{\\sum_i (e_i - \\bar{e})^2}{n-2}},\n",
        "$$\n",
        "\n",
        "dove $ \\bar{e} $ indica la media dei residui, che teoricamente è zero dato che si assume che la media degli errori sia zero.\n",
        "\n",
        "Il denominatore \"n-2\" deriva dalla perdita di due gradi di libertà, necessaria per la stima dei due coefficienti, $ \\beta_0 $ (intercetta) e $ \\beta_1 $ (pendenza), che sono utilizzati per calcolare le stime previste $ \\hat{y}_i = \\beta_0 + \\beta_1 x_i $. Questi gradi di libertà vengono sottratti perché ciascun parametro stimato consuma un grado di libertà dal totale disponibile.\n",
        "\n",
        "Nel caso dell'esempio, la numerosità campionaria è\n"
      ],
      "id": "ccf37986"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = len(x)\n",
        "n"
      ],
      "id": "9be73d67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'errore standard della regressione diventa\n"
      ],
      "id": "fa5e8099"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.sqrt(np.sum(e**2) / (n - 2))"
      ],
      "id": "361771a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questo valore indica che, in media, nella popolazione la distanza tra i valori osservati e la retta di regressione è di 2.67 punti.\n",
        "\n",
        "Come discusso da {cite}`gelman2020regression`, la radice quadrata media dei residui, $ \\frac{1}{n} \\sum_{i=1}^n (y_i - (\\hat{a} + \\hat{b}x_i))^2 $, tende a sottostimare la deviazione standard $\\sigma$ dell'errore nel modello di regressione. Questa sottostima è spesso il risultato di un sovradimensionamento, dato che i parametri $a$ e $b$ sono stimati utilizzando gli stessi $n$ punti dati usati anche per calcolare i residui.\n",
        "\n",
        "La validazione incrociata rappresenta un approccio alternativo per valutare l'errore predittivo che evita alcuni dei problemi legati al sovradimensionamento. La versione più semplice della validazione incrociata è l'approccio leave-one-out, in cui il modello è adattato $n$ volte, escludendo ogni volta un punto dati, adattando il modello ai rimanenti $n-1$ punti dati, e utilizzando questo modello adattato per predire l'osservazione esclusa:\n",
        "- Per $i = 1, \\ldots, n$:\n",
        "  - Adatta il modello $y = a + bx + \\text{errore}$ ai $n-1$ punti dati $(x,y)_j, j \\neq i$. Denomina i coefficienti di regressione stimati come $\\hat{a}_{-i}, \\hat{b}_{-i}$.\n",
        "  - Calcola il residuo validato incrociato, $ r_{\\text{CV}} = y_i - (\\hat{a}_{-i} + \\hat{b}_{-i} x_i) $.\n",
        "  - Calcola la stima di $\\sigma_{\\text{CV}} = \\frac{1}{n} \\sum_{i=1}^n r_{\\text{CV}}^2$.\n",
        "\n",
        "Per fare un esempio, eseguiamo i passaggi sopra descritti per il modello che predice Tense Arousal dall'ansia di stato. \n"
      ],
      "id": "bce6ae13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inizializzazione di un modello di regressione lineare\n",
        "model = LinearRegression()\n",
        "\n",
        "# Array per salvare i residui cross-validated\n",
        "residuals_cv = []\n",
        "\n",
        "# Loop per la validazione incrociata leave-one-out\n",
        "for i in range(len(df)):\n",
        "    # Dati di training escludendo l'i-esimo punto\n",
        "    X_train = df.loc[df.index != i, [\"x\"]]\n",
        "    y_train = df.loc[df.index != i, \"y\"]\n",
        "\n",
        "    # Dati di test\n",
        "    X_test = df.loc[[i], [\"x\"]]\n",
        "    y_test = df.loc[i, \"y\"]\n",
        "\n",
        "    # Addestramento del modello\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predizione sull'i-esimo punto\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calcolo del residuo validato incrociato\n",
        "    residual_cv = y_test - y_pred[0]\n",
        "    residuals_cv.append(residual_cv**2)\n",
        "\n",
        "# Calcolo di sigma_cv\n",
        "sigma_cv = np.sqrt(np.mean(residuals_cv))\n",
        "\n",
        "print(\"Stima di σ_CV:\", sigma_cv)"
      ],
      "id": "823ac48a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nel caso dei dati analizzati, si osserva che la stima ottenuta attraverso la validazione incrociata è leggermente superiore rispetto a quella calcolata usando la formula \n",
        "$\n",
        "\\hat{\\sigma}_e = \\sqrt{\\frac{\\sum_i (e_i - \\bar{e})^2}{n-2}}\n",
        "$.\n",
        "Questo incremento, sebbene minimo, riflette le differenze metodologiche tra i due approcci di stima dell'errore standard.\n",
        "\n",
        "### Parametrizzazione Alternativa\n",
        "\n",
        "Per consentire una migliore interpretazione dell'intercetta, centriamo i valori della variabile indipendente.\n"
      ],
      "id": "fe77c5f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = df[\"state1\"]\n",
        "y = df[\"TA1\"]\n",
        "\n",
        "xc = x - np.mean(x)\n",
        "np.mean(xc)"
      ],
      "id": "ec2e798d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eseguiamo l'analisi di regressione.\n"
      ],
      "id": "c3b39f45"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lm2 = pg.linear_regression(xc, y)\n",
        "lm2.round(2)"
      ],
      "id": "d9a2908d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notiamo che la stima della pendenza della retta di regressione è rimasta immutata, mentre cambia il coefficiente $\\beta_0$. Nel caso in cui la variabile indipendente sia centrata, il coefficiente $\\beta_0$ rappresenta il valore atteso della variabile dipendente quando la variabile indipendente assume il suo valore medio.\n",
        "\n",
        "Nel caso presente, il valore 12.62 indica la media di Tense Arousal quando l'ansia di stato assume il valore medio nel campione.\n",
        "\n",
        "Adesso standardizziamo sia la variabile dipendente che la variabile indipendente.\n"
      ],
      "id": "c38f7e8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Standardizzazione\n",
        "zx = standardize(x)\n",
        "zy = standardize(y)"
      ],
      "id": "93aa3cdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(np.mean(zx), np.std(zx))"
      ],
      "id": "800788d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(np.mean(zy), np.std(zy))"
      ],
      "id": "0809ef7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lm3 = pg.linear_regression(zx, zy)\n",
        "lm3.round(2)"
      ],
      "id": "dd2fbd65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dopo aver standardizzato entrambe le variabili, i coefficienti di regressione possono essere interpretati nel seguente modo:\n",
        "\n",
        "- **$\\beta_0$ = 0**: Questo si verifica perché la retta di regressione, calcolata attraverso il metodo dei minimi quadrati (ML), interseca il punto delle medie delle variabili standardizzate, ovvero $(\\bar{X}, \\bar{Y})$.\n",
        "- **$\\beta_1$**: Rappresenta la variazione media della variabile dipendente, espressa in termini di deviazioni standard, per ogni aumento di una deviazione standard nella variabile indipendente.\n",
        "\n",
        "### Derivazione delle stime dei minimi quadrati\n",
        "\n",
        "L'approccio classico al modello di regresione fa uso del metodo dei minimi quadrati per trovare la retta che meglio si adatta a un insieme di dati. L'obiettivo è minimizzare la somma dei quadrati delle differenze (residui) tra i valori osservati e quelli predetti dal modello lineare.\n",
        "\n",
        "Supponiamo di avere un insieme di dati $(x_i, y_i)$, dove $i = 1, 2, \\dots, n$. Il modello lineare si esprime come:\n",
        "\n",
        "$$\n",
        "y_i = \\alpha + \\beta x_i + \\epsilon_i,\n",
        "$$\n",
        "\n",
        "dove:\n",
        "\n",
        "- $\\alpha$ è l'intercetta,\n",
        "- $\\beta$ è il coefficiente angolare (pendenza),\n",
        "- $\\epsilon_i$ è il residuo, ossia l'errore associato al punto $i$.\n",
        "\n",
        "Il metodo dei minimi quadrati mira a minimizzare la somma dei quadrati dei residui $\\epsilon_i$:\n",
        "\n",
        "$$\n",
        "S(\\alpha, \\beta) = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - (\\alpha + \\beta x_i))^2\n",
        "$$\n",
        "\n",
        "Geometricamente, trovare i valori di $\\alpha$ e $\\beta$ che minimizzano la funzione $S(\\alpha, \\beta)$ significa trovare il punto in cui la funzione è piatta, ossia dove la sua pendenza è zero. Questo si fa calcolando le derivate parziali di $S(\\alpha, \\beta)$ rispetto a $\\alpha$ e $\\beta$, e ponendole uguali a zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial S}{\\partial \\alpha} = 0 \\quad \\text{e} \\quad \\frac{\\partial S}{\\partial \\beta} = 0.\n",
        "$$\n",
        "\n",
        "Risolvendo questo sistema di equazioni, si trovano le espressioni per $\\alpha$ e $\\beta$:\n",
        "\n",
        "$$\n",
        "\\beta = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha = \\bar{y} - \\beta \\bar{x},\n",
        "$$\n",
        "\n",
        "dove $\\bar{x}$ e $\\bar{y}$ sono le medie dei valori $x_i$ e $y_i$ rispettivamente.\n",
        "\n",
        "Per chiarire, ora consideriamo il caso in cui i dati sono standardizzati. Quando i dati sono standardizzati (cioè $x_i$ e $y_i$ hanno media 0 e deviazione standard 1), l'intercetta $\\alpha$ è 0, quindi il modello diventa:\n",
        "\n",
        "$$\n",
        "y_i = \\beta x_i + \\epsilon_i\n",
        "$$\n",
        "\n",
        "Di conseguenza, dobbiamo solo stimare $\\beta$.\n",
        "\n",
        "Ecco come eseguire una simulazione in Python per questo caso:\n"
      ],
      "id": "03c21ec5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Serie di valori di beta tra 0 e 1\n",
        "beta_values = np.linspace(0.5, 1, 1000)\n",
        "\n",
        "# Calcolo della somma dei quadrati dei residui (SSE) per ciascun valore di beta\n",
        "SSE = []\n",
        "for beta in beta_values:\n",
        "    residuals = zy - beta * zx\n",
        "    SSE.append(np.sum(residuals**2))\n",
        "\n",
        "# Convertiamo SSE in un array numpy per maggiore facilità nella visualizzazione\n",
        "SSE = np.array(SSE)\n",
        "\n",
        "beta_true = 0.72 # trovato da pingouin\n",
        "\n",
        "# Visualizzazione della curva SSE in funzione di beta\n",
        "plt.plot(beta_values, SSE, label=\"SSE vs Beta\", color=\"blue\")\n",
        "plt.axvline(beta_true, color=\"red\", linestyle=\"--\", label=f\"Beta True = {beta_true}\")\n",
        "plt.xlabel(\"Beta\")\n",
        "plt.ylabel(\"SSE (Somma dei quadrati dei residui)\")\n",
        "plt.title(\"Curva SSE in funzione di Beta\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Troviamo il valore di beta che minimizza SSE\n",
        "beta_min_index = np.argmin(SSE)\n",
        "beta_min = beta_values[beta_min_index]\n",
        "\n",
        "print(f\"Valore di beta stimato con la formula dei minimi quadrati: {beta_true}\")\n",
        "print(f\"Valore stimato di beta (minimo SSE): {beta_min}\")"
      ],
      "id": "bab333bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metodo della Massima Verosimiglianza\n",
        "\n",
        "Dopo aver discusso il metodo dei minimi quadrati, possiamo affrontare il metodo della massima verosimiglianza, che in molti casi porta agli stessi risultati ma con un approccio leggermente diverso.\n",
        "\n",
        "#### Connessione con il Metodo dei Minimi Quadrati\n",
        "\n",
        "Nel contesto della regressione lineare, se gli errori del modello sono indipendenti e distribuiti normalmente, cioè se $y_i \\sim \\text{Normale}(\\alpha + \\beta x_i, \\sigma^2)$ per ogni $i$, allora la stima dei parametri ottenuta con il metodo dei minimi quadrati coincide con quella ottenuta usando il metodo della massima verosimiglianza. Questo significa che i valori di $\\alpha$ e $\\beta$ che minimizzano la somma dei quadrati degli errori residui sono anche quelli che massimizzano la probabilità di osservare i dati dati quei parametri.\n",
        "\n",
        "#### La Funzione di Verosimiglianza\n",
        "\n",
        "In un modello di regressione, la funzione di verosimiglianza è definita come la probabilità (o densità di probabilità) di osservare i dati effettivi in funzione dei parametri e dei predittori del modello. In altre parole, dato un insieme di parametri $\\alpha$, $\\beta$ e $\\sigma$, la funzione di verosimiglianza ci dice quanto è probabile osservare i dati $y$ dati quei parametri.\n",
        "\n",
        "Matematicamente, la funzione di verosimiglianza per un modello di regressione lineare è espressa come:\n",
        "\n",
        "$$\n",
        "p(y|\\alpha, \\beta, \\sigma, X) = \\prod_{i=1}^{n} N(y_i|\\alpha + \\beta x_i, \\sigma^2).\n",
        "$$\n",
        "\n",
        "Qui, $N(\\cdot|\\cdot, \\cdot)$ rappresenta la funzione di densità della distribuzione normale.\n",
        "\n",
        "Questa formula indica che la verosimiglianza totale è il prodotto delle densità di probabilità per ciascun punto dati $y_i$, considerando che ogni $y_i$ segue una distribuzione normale con media $\\alpha + \\beta x_i$ e varianza $\\sigma^2$.\n",
        "\n",
        "#### Minimizzare i Residui Quadratici\n",
        "\n",
        "Un'analisi della funzione di verosimiglianza mostra che massimizzare questa funzione equivale a minimizzare la somma dei quadrati dei residui, proprio come si fa nel metodo dei minimi quadrati. Questo perché la funzione di densità normale $N(y_i|\\alpha + \\beta x_i, \\sigma^2)$ ha un massimo quando il valore di $y_i$ è vicino alla media prevista $\\alpha + \\beta x_i$, e il prodotto delle densità è massimizzato quando i residui sono minimi.\n",
        "\n",
        "#### Differenza nella Stima di $\\sigma$\n",
        "\n",
        "C'è una piccola differenza nella stima della deviazione standard $\\sigma$ tra i due metodi. Nel metodo della massima verosimiglianza, la stima di $\\sigma$ viene calcolata come:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - (\\hat{\\alpha} + \\hat{\\beta} x_i)\\right)^2\n",
        "$$\n",
        "\n",
        "In questo caso, il denominatore è $n$, mentre nel metodo dei minimi quadrati il denominatore è $n-2$, che riflette l'aggiustamento per i gradi di libertà.\n",
        "\n",
        "In sintesi, il metodo della massima verosimiglianza trova i parametri che rendono i dati osservati i più probabili, dato il modello. Quando gli errori sono normalmente distribuiti, questo approccio porta agli stessi risultati del metodo dei minimi quadrati, ma con un'interpretazione basata sulla probabilità. \n",
        "\n",
        "## Coefficiente di Determinazione\n",
        "\n",
        "I modelli lineari, sia quelli stimati con il metodo dei minimi quadrati che quelli ottenuti tramite massima verosimiglianza, permettono una decomposizione della varianza totale della variabile dipendente $y$ in due componenti indipendenti: la devianza spiegata e la devianza residua. Questa decomposizione deriva dal teorema della decomposizione della devianza, come descritto nell'@sec-theo-deviance-decomposition.\n",
        "\n",
        "La *devianza spiegata* (DS) rappresenta la parte della varianza totale che è attribuibile al modello, ed è definita come:\n",
        "\n",
        "$$\n",
        "DS = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2,\n",
        "$$\n",
        "\n",
        "dove $\\hat{y}_i$ è il valore predetto dal modello per l'osservazione $i$, e $\\bar{y}$ è la media delle osservazioni di $y$. \n",
        "\n",
        "La *devianza residua* (DR), invece, rappresenta la parte della varianza totale che non è spiegata dal modello, ovvero l'errore, ed è calcolata come:\n",
        "\n",
        "$$\n",
        "DR = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2.\n",
        "$$\n",
        "\n",
        "La somma delle due componenti, ovvero la devianza spiegata e la devianza residua, è pari alla *devianza totale* (DT):\n",
        "\n",
        "$$\n",
        "DT = \\sum_{i=1}^n (y_i - \\bar{y})^2.\n",
        "$$\n",
        "\n",
        "Il *coefficiente di determinazione* $R^2$ è definito come il rapporto tra la devianza spiegata e la devianza totale:\n",
        "\n",
        "$$\n",
        "R^2 = \\frac{DS}{DT}.\n",
        "$$\n",
        "\n",
        "Il coefficiente di determinazione $R^2$ fornisce una misura della proporzione della varianza totale di $y$ che viene spiegata dal modello di regressione. Un valore di $R^2$ vicino a 1 indica che il modello spiega una grande parte della variabilità osservata nei dati, mentre un valore vicino a 0 suggerisce che il modello spiega poco della variabilità della variabile dipendente. \n",
        "\n",
        "Questo coefficiente è particolarmente utile per valutare l'adeguatezza di un modello lineare, permettendo di comprendere quanto del fenomeno studiato viene catturato dalle variabili indipendenti incluse nel modello. Tuttavia, è importante ricordare che un alto valore di $R^2$ non implica necessariamente che il modello sia il migliore in senso assoluto; altri fattori come la complessità del modello e la presenza di potenziali errori di specificazione devono essere considerati nella valutazione complessiva del modello.\n",
        "\n",
        "## Modello di Regressione Bayesiano\n",
        "\n",
        "L'approccio bayesiano si distingue dal metodo dei minimi quadrati o dalla massima verosimiglianza, poiché non si limita a determinare i parametri che meglio si adattano ai dati osservati secondo un criterio fisso. Al contrario, integra queste stime con informazioni a priori sui parametri stessi, combinando la verosimiglianza dei dati con una distribuzione a priori che rappresenta le ipotesi o le conoscenze preesistenti. In questo modo, l'inferenza bayesiana diventa un processo di aggiornamento delle credenze: la distribuzione a posteriori dei parametri riflette la conoscenza aggiornata dopo aver osservato i dati. Mentre i metodi classici forniscono stime puntuali, l'inferenza bayesiana genera distribuzioni a posteriori che descrivono la probabilità di ogni possibile valore dei parametri, tenendo conto dell'incertezza complessiva nel modello.\n",
        "\n",
        "Nel contesto di un modello lineare bayesiano, si utilizzano le seguenti convenzioni: le variabili di risposta sono indicate con $y$, le variabili predittive (note anche come covariate o caratteristiche) con $x$, e l'indice di osservazione con $n$, che va da 1 al numero totale di osservazioni $N$. Con questa notazione, la verosimiglianza di un semplice modello lineare (gaussiano) si può esprimere come:\n",
        "\n",
        "$$\n",
        "y_n \\sim \\text{Normale}(\\mu_n, \\sigma),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu_n = b_0 + b_1 x_n.\n",
        "$$\n",
        "\n",
        "In questo caso, la distribuzione normale univariata è definita in termini di media $\\mu$ e deviazione standard $\\sigma$.\n",
        "\n",
        "Nel modello bayesiano lineare, i parametri principali sono l'intercetta $b_0$ e il coefficiente $b_1$ associato alla variabile predittiva $x$. Questi parametri insieme formano il predittore lineare $\\mu$. Il parametro $\\sigma$, invece, rappresenta la deviazione standard residua, ovvero la variabilità che non può essere spiegata dal modello lineare e che cattura l'errore o il \"rumore\" presente nei dati.\n",
        "\n",
        "Ad esempio, se si vuole modellare la relazione tra ansia di stato ($x$) e Tense Arousal ($y$), l'approccio bayesiano permette di strutturare il modello in modo simile a quanto fatto con i metodi classici. Anche qui, si assume che gli errori siano indipendenti tra loro, distribuiti normalmente con media zero e varianza costante $\\sigma^2$. Tuttavia, l'approccio bayesiano consente anche di specificare distribuzioni a priori per i parametri del modello ($b_0$, $b_1$ e $\\sigma$), che rappresentano la conoscenza iniziale sui parametri prima di osservare i dati.\n",
        "\n",
        "Dopo aver raccolto i dati, si utilizza il teorema di Bayes per aggiornare queste distribuzioni a priori e ottenere le distribuzioni a posteriori dei parametri. Le distribuzioni a posteriori combinano l'informazione fornita dai dati con le credenze iniziali, offrendo così stime dei parametri che riflettono sia l'evidenza empirica che le conoscenze preesistenti, garantendo un'inferenza più robusta e flessibile.\n",
        "\n",
        "### Verosimiglianza\n",
        "\n",
        "Il modello di verosimiglianza per descrivere la relazione tra $x$ (ansia di stato) e $y$ (Tense Arousal) assume che:\n",
        "\n",
        "$$ y \\sim \\text{Normale}(\\alpha + \\beta x, \\sigma) $$\n",
        "\n",
        "Questo implica che i valori osservati di $y$ sono distribuiti normalmente attorno alla retta di regressione $\\alpha + \\beta x$, con una deviazione standard $\\sigma$. In altre parole, ogni osservazione di $y$ è una combinazione lineare dell'intercetta $\\alpha$, del coefficiente $\\beta$ che moltiplica la variabile $x$, e di un termine di errore normalmente distribuito.\n",
        "\n",
        "### Distribuzioni a Priori\n",
        "\n",
        "Per implementare l'approccio bayesiano, definiamo delle distribuzioni a priori per i parametri $\\alpha$, $\\beta$, e $\\sigma$. In una prima versione del modello, possiamo utilizzare delle distribuzioni a priori uniformi, che esprimono una mancanza di conoscenza specifica o una neutralità nelle credenze iniziali sui valori di questi parametri.\n",
        "\n",
        "### Distribuzioni a Posteriori\n",
        "\n",
        "Le distribuzioni a posteriori sono ottenute combinando la verosimiglianza con le distribuzioni a priori mediante il teorema di Bayes. Queste distribuzioni a posteriori riflettono il nostro stato di conoscenza sui parametri dopo aver osservato i dati, incorporando sia le informazioni contenute nei dati che le credenze iniziali espresse dalle distribuzioni a priori. L'approccio bayesiano, quindi, non solo fornisce stime dei parametri, ma anche una quantificazione dell'incertezza associata a queste stime, rendendolo particolarmente utile in situazioni con dati limitati o incertezza significativa.\n",
        "\n",
        "Questa metodologia ci permette di modellare e comprendere in modo più completo e robusto la relazione tra ansia di stato e Tense Arousal, integrando informazioni preesistenti con nuove evidenze empiriche.\n",
        "\n",
        "In sintesi, il modello di regressione bayesiano può essere riassunto come segue. La verosimiglianza è data da:\n",
        "\n",
        "$$\n",
        "y_i \\sim \\text{Normal}(\\alpha + \\beta \\cdot x_i, \\sigma).\n",
        "$$\n",
        "\n",
        "In una prima formulazione del modello, possiamo utilizzare prior uniformi per ciascuno dei parametri $\\alpha$, $\\beta$ e $\\sigma$:\n",
        "\n",
        "$$\n",
        "\\alpha \\sim \\text{Uniform}(-\\infty, \\infty),\n",
        "$$\n",
        "$$\n",
        "\\beta \\sim \\text{Uniform}(-\\infty, \\infty),\n",
        "$$\n",
        "$$\n",
        "\\sigma \\sim \\text{Uniform}(0, \\infty).\n",
        "$$\n",
        "\n",
        "### Codice Stan\n",
        "\n",
        "Il codice Stan che implementa il modello descritto in precedenza è contenuto nel file `arousal_model_1.stan`. Compiliamo e stampiamo il modello.\n"
      ],
      "id": "3e96a423"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:06.728618Z",
          "start_time": "2024-07-13T07:01:05.671604Z"
        }
      },
      "source": [
        "stan_file = os.path.join(project_directory, 'stan', 'arousal_model1.stan')\n",
        "model1 = CmdStanModel(stan_file=stan_file)\n",
        "print(model1.code())"
      ],
      "id": "67f6866c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si osservi che, in questa prima istanziazione del modello bayesiano, non avendo specificato le distribuzioni a priori per i parametri $\\alpha$, $\\beta$ e $\\sigma$, Stan assume distribuzioni a priori uniformi per questi parametri.\n",
        "\n",
        "Sistemiamo i dati in un dizionario come richiesto dal modello Stan.\n"
      ],
      "id": "f9ba151a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:10.217268Z",
          "start_time": "2024-07-13T07:01:10.214117Z"
        }
      },
      "source": [
        "stan_data = {\n",
        "    \"N\": len(df[\"TA1\"]),\n",
        "    \"x\": df[\"state1\"],\n",
        "    \"y\": df[\"TA1\"]\n",
        "}\n",
        "print(stan_data)"
      ],
      "id": "a983e5ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eseguiamo il campionamento MCMC.\n"
      ],
      "id": "39bf6af3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:14.524548Z",
          "start_time": "2024-07-13T07:01:13.793216Z"
        }
      },
      "source": [
        "fit1 = model1.sample(\n",
        "    data=stan_data,\n",
        "    iter_warmup=1_000,\n",
        "    iter_sampling=2_000,\n",
        "    seed=123,\n",
        "    show_progress=False,\n",
        "    show_console=False\n",
        ")"
      ],
      "id": "1cfa2063",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esaminiamo le distribuzioni a posteriori dei parametri.\n"
      ],
      "id": "daf0e956"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:19.008797Z",
          "start_time": "2024-07-13T07:01:17.795902Z"
        }
      },
      "source": [
        "_ = az.plot_trace(fit1, var_names=([\"alpha\", \"beta\", \"sigma\"]))"
      ],
      "id": "71087181",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le tracce delle quattro catene indicano che sono ben mescolate e convergono verso una distribuzione stazionaria, segnalando una buona esplorazione dello spazio dei parametri. Questo è evidenziato dal fatto che le tracce non mostrano trend evidenti e oscillano intorno a un valore centrale, suggerendo che le catene hanno raggiunto l'equilibrio.\n",
        "\n",
        "La forma della distribuzione a posteriori, visibile nei grafici a densità, appare approssimativamente gaussiana per ciascun parametro (`alpha`, `beta`, e `sigma`). Questo suggerisce che, dato il modello e i dati, le stime a posteriori sono stabili e ben definite, con una concentrazione delle probabilità attorno ai valori medi e una simmetria che riflette una distribuzione normale.\n",
        "\n",
        "In sintesi, i grafici di traccia indicano una buona convergenza e una distribuzione a posteriori stabile e ben definita, rafforzando la fiducia nelle stime bayesiane ottenute.\n",
        "\n",
        "L'oggetto `fit` generato da `cmdstanpy` appartiene alla classe `cmdstanpy.stanfit.mcmc.CmdStanMCMC`. Questo oggetto è funzionalmente equivalente a un oggetto della classe `InferenceData`, consentendo la sua manipolazione tramite le funzioni offerte da ArviZ. Procediamo quindi con l'esame di un sommario delle distribuzioni a posteriori dei parametri del modello lineare.\n"
      ],
      "id": "9c8f2c68"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:23.493778Z",
          "start_time": "2024-07-13T07:01:23.419555Z"
        }
      },
      "source": [
        "az.summary(fit1, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)"
      ],
      "id": "a398cf95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Alpha (Intercetta):** La stima media di `alpha` è 1.552, con un intervallo di credibilità (HDI - Highest Density Interval) al 3% e 97% che va da -0.859 a 3.816. Questo significa che, date le informazioni disponibili e il modello specificato, c'è una probabilità del 94% che l'intercetta reale si trovi all'interno di questo intervallo. L'intercetta corrisponde al valore atteso di Tense Arousal quando l'ansia di stato vale 0.\n",
        "  \n",
        "- **Beta (Coefficiente angolare):** La stima media di `beta` è 0.267. Anche qui, l'intervallo di credibilità al 94% va da 0.213 a 0.323, suggerendo che è molto probabile che l'effetto del predittore `x` sulla variabile di risposta `y` sia positivo e compreso in questo intervallo. La pendenza $\\beta$ ci informa sull'incremento atteso di Tense Arousal quando l'ansia di stato aumenta di un'unità.\n",
        "\n",
        "- **Sigma (Deviazione standard residua):** La stima media di `sigma` è 2.716, con un intervallo di credibilità da 2.314 a 3.149. Questa è una misura della variabilità residua, ovvero la deviazione standard degli errori rispetto alla linea di regressione.\n",
        "\n",
        "La colonna `mean` dell'output riporta la media della distribuzione a posteriori di ciasccun parametro, mentre nella colonna `sd` troviamo una misura di dispersione della distribuzione a posteriori del parametro, ovvero la quantificazione dell'intertezza della stima a posteriori. In pratica è la deviazione standard della distribuzione a posteriori, ovvero la radice quadrata della varianza dei campioni della distribuzione a posteriori del parametro.\n",
        "\n",
        "Supponiamo di avere $S$ campioni per il parametro $\\theta$. Questi campioni possono essere denotati come $\\theta_1, \\theta_2, \\dots, \\theta_S$. La media campionaria (o stima puntuale bayesiana) del parametro $\\theta$ si calcola come:\n",
        "\n",
        "$$\n",
        "\\bar{\\theta} = \\frac{1}{S} \\sum_{i=1}^{S} \\theta_i.\n",
        "$$\n",
        "\n",
        "La deviazione standard della distribuzione a posteriori, che è ciò che è indicato con `sd`, si calcola come la radice quadrata della varianza campionaria dei campioni posteriori:\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\theta) = \\frac{1}{S-1} \\sum_{i=1}^{S} (\\theta_i - \\bar{\\theta})^2,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{sd}(\\theta) = \\sqrt{\\text{Var}(\\theta)}.\n",
        "$$\n",
        "\n",
        "In sintesi, `sd` è calcolato come la deviazione standard dei campioni ottenuti dalla distribuzione a posteriori di un parametro.\n",
        "\n",
        "Le altre colonne sono le seguenti.\n",
        "\n",
        "- **HDI (Intervallo di Massima Densità):** Questo intervallo rappresenta la regione più densa dell'intera distribuzione a posteriori, contenente il 94% delle probabilità. È l'equivalente bayesiano dell'intervallo di confidenza, ma con un'interpretazione probabilistica diretta.\n",
        "\n",
        "- **R_hat:** È un indicatore di convergenza per le catene di Markov Monte Carlo (MCMC). Un valore di `R_hat` prossimo a 1 segnala che la catena è probabilmente convergente, suggerendo che le stime a posteriori sono affidabili.\n",
        "\n",
        "- **ESS (Dimensione Campionaria Effettiva):** Indica l'equivalente di un campione indipendente in un'analisi MCMC, valutando quanto efficacemente i campioni generati dalla catena rappresentano la distribuzione a posteriori.\n",
        "\n",
        "Infine, `mcse_mean` che `mcse_sd` sono misure che valutano la precisione delle stime ottenute tramite MCMC, quantificando quanto queste stime possono variare a causa della natura stocastica del processo di campionamento.\n",
        "\n",
        "- **mcse_mean (Monte Carlo Standard Error of the Mean):** Questo valore rappresenta l'errore standard Monte Carlo associato alla stima della media del parametro. In altre parole, `mcse_mean` quantifica l'incertezza introdotta dal processo di campionamento MCMC stesso. Un valore basso indica che la catena di Markov Monte Carlo ha fornito una stima della media del parametro con un'alta precisione.\n",
        "\n",
        "- **mcse_sd (Monte Carlo Standard Error of the Standard Deviation):** Analogamente, `mcse_sd` è l'errore standard Monte Carlo associato alla stima della deviazione standard della distribuzione a posteriori del parametro. Questo valore misura l'incertezza nella stima della dispersione del parametro, dovuta al processo di campionamento MCMC. Anche qui, un valore basso indica che la stima della deviazione standard è stabile e precisa.\n",
        "\n",
        "Possiamo confrontare i valori ottenuti con l'approccio bayesiano con quelli trovati usando la procedura di massima verosimiglianza.\n"
      ],
      "id": "5f371f8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:01:27.411444Z",
          "start_time": "2024-07-13T07:01:27.372191Z"
        }
      },
      "source": [
        "lm = pg.linear_regression(df[\"state1\"], df[\"TA1\"])\n",
        "lm.round(2)"
      ],
      "id": "f739ce22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La somiglianza tra le due soluzioni indica che, quando usiamo dei prior uniformi per i parametri,  i due approcci producono risultati sostanzialmente equivalenti.\n",
        "\n",
        "L'interpretazione del significato dei parametri è la stessa anche per l'approccio frequentista: \n",
        "\n",
        "- L'intercetta rappresenta il valore atteso della variabile di risposta `TA1` quando il predittore `state1` è pari a zero. \n",
        "- Il coefficiente `beta` rappresenta la variazione attesa nella variabile di risposta `TA1` per ogni unità di incremento in `state1`. \n",
        "\n",
        "Ci sono però delle differenze sostanziali nell'interpretazione dell'incertezza associata alle stime dei parametri.\n",
        "\n",
        "- **Stima Puntuale vs Distribuzione a Posteriori:**\n",
        "  - **Frequentista:** Le stime di `alpha` e `beta` sono considerate come valori puntuali, ottenuti attraverso il metodo dei minimi quadrati. Gli errori standard associati a queste stime forniscono un'indicazione della variabilità delle stime se ripetessimo il campionamento molte volte.\n",
        "  - **Bayesiano:** Le stime di `alpha` e `beta` sono presentate come distribuzioni a posteriori. La media di queste distribuzioni può essere considerata la stima puntuale, ma l'intera distribuzione riflette la nostra incertezza attorno a queste stime, basata sia sui dati osservati che sulle informazioni a priori.\n",
        "\n",
        "- **Intervallo di Confidenza vs Intervallo Credibile:**\n",
        "  - **Frequentista:** L'intervallo di confidenza al 95% indica che, se ripetessimo l'esperimento molte volte, il 95% di tali intervalli conterrà il vero valore del parametro. Questo intervallo si basa sulla stima puntuale e sull'assunzione di distribuzione normale degli errori.\n",
        "  - **Bayesiano:** L'intervallo credibile al 94% (ad esempio l'HDI - Highest Density Interval) rappresenta la probabilità che il parametro si trovi entro quell'intervallo dato il modello, i dati osservati e le informazioni a priori. È un'intervallo che ha una diretta interpretazione probabilistica.\n",
        "\n",
        "- **p-value vs Significato Bayesiano:**\n",
        "  - **Frequentista:** Il p-value è utilizzato per testare l'ipotesi nulla che il coefficiente sia uguale a zero. Un p-value molto basso (come in questo caso per `beta`) suggerisce che c'è una forte evidenza contro l'ipotesi nulla.\n",
        "  - **Bayesiano:** In un'analisi bayesiana, non si fa riferimento a p-value; l'accento è posto sulla distribuzione a posteriori e sull'intervallo credibile, che forniscono una comprensione diretta dell'incertezza attorno ai parametri senza bisogno di test di ipotesi tradizionali.\n",
        "\n",
        "In sintesi,\n",
        "\n",
        "- **Interpretazione delle stime:** Nell'approccio frequentista, le stime dei parametri sono valori puntuali accompagnati da un intervallo di confidenza che riflette la variabilità campionaria. Nell'approccio bayesiano, ogni parametro è rappresentato come una distribuzione a posteriori che incorpora sia i dati osservati sia le informazioni a priori.\n",
        "  \n",
        "- **Gestione dell'incertezza:** L'approccio frequentista usa errori standard e intervalli di confidenza, mentre l'approccio bayesiano utilizza l'intera distribuzione a posteriori per descrivere l'incertezza.\n",
        "- **Probabilità e significatività:** Nell'approccio frequentista, il p-value è cruciale per determinare la significatività statistica, mentre nell'approccio bayesiano si utilizza l'intervallo credibile e la probabilità a posteriori per descrivere quanto è probabile un parametro dato i dati e le informazioni a priori.\n",
        "\n",
        "## Interpretare i coefficienti di regressione come confronti, non come effetti\n",
        "\n",
        "@gelman2021regression sottolineano che i coefficienti di regressione sono spesso chiamati \"effetti\", ma questa terminologia può essere fuorviante. Gli effetti, infatti, sono conseguenze di una relazione causale. Tuttavia, ciò che il modello di regressione stima non è necessariamente un effetto causale, ma piuttosto un pattern osservazionale. In particolare, quello che viene osservato è che la media della variabile $y$ nella sottopopolazione con $X = x + 1$ è $b$ volte maggiore o minore (a seconda del segno di $\\beta$) rispetto alla media della sottopopolazione con $X = x$.\n",
        "\n",
        "La regressione è uno strumento matematico utilizzato principalmente per fare previsioni. I coefficienti di regressione devono essere sempre interpretati come confronti medi. Solo in circostanze specifiche, quando la regressione descrive un processo causale ben definito, è possibile interpretarli come effetti. Tuttavia, questa interpretazione causale deve essere giustificata dal disegno dello studio e non può essere derivata unicamente dall'uso del modello statistico.\n",
        "\n",
        "## Ricodifica dei dati\n",
        "\n",
        "L'intercetta ($\\alpha$) rappresenta il valore atteso di Tense Arousal quando l'ansia di stato è pari a 0. Tuttavia, poiché l'ansia di stato è misurata su una scala ad intervalli, l'origine è arbitraria e non rappresenta l'assenza della proprietà. Lo stesso vale per la variabile Tense Arousal. Per entrambe le variabili, inoltre, anche l'unità di misura è arbitraria.\n",
        "\n",
        "In queste circostanze, una trasformazione utile è la standardizzazione. La standardizzazione fa sì che il valore 0 corrisponda alla media campionaria e che l'unità di misura sia una deviazione standard.\n",
        "\n",
        "Quando standardizziamo l'ansia di stato, il valore 0 della variabile standardizzata corrisponde alla media della variabile originale. Dato che la retta di regressione passa per il punto $(\\bar{x}, \\bar{y})$, utilizzando i valori standardizzati di $x$ e $y$, la nuova intercetta ($\\alpha$) sarà 0. La pendenza ($\\beta$) avrà un'interpretazione utile: nel caso di dati standardizzati, la pendenza stima l'incremento (o decremento) atteso di $y$ quando $x$ aumenta di una deviazione standard.\n"
      ],
      "id": "e359cb15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calcolo della media e della deviazione standard di state1\n",
        "mean_state1 = np.mean(df[\"state1\"])\n",
        "std_state1 = np.std(df[\"state1\"])\n",
        "# Standardizzazione \n",
        "df[\"state1_z\"] = (df[\"state1\"] - mean_state1) / std_state1\n",
        "\n",
        "# Calcolo della media e della deviazione standard di TA1\n",
        "mean_ta1 = np.mean(df[\"TA1\"])\n",
        "std_ta1 = np.std(df[\"TA1\"])\n",
        "# Standardizzazione \n",
        "df[\"ta1_z\"] = (df[\"TA1\"] - mean_ta1) / std_ta1"
      ],
      "id": "04b5274f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creiamo il dizionario dei dati con le nuove variabli standardizzate.\n"
      ],
      "id": "4474b11c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:28.616378Z",
          "start_time": "2024-07-13T07:02:28.608697Z"
        }
      },
      "source": [
        "stan_data2 = {\n",
        "    \"N\": len(df[\"state1_z\"]), \n",
        "    \"x\": df[\"state1_z\"], \n",
        "    \"y\": df[\"ta1_z\"]\n",
        "}"
      ],
      "id": "114a5133",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eseguiamo il campionamento.\n"
      ],
      "id": "1321b646"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:32.624353Z",
          "start_time": "2024-07-13T07:02:32.337744Z"
        }
      },
      "source": [
        "fit2 = model1.sample(\n",
        "    data=stan_data2,\n",
        "    iter_warmup=1_000,\n",
        "    iter_sampling=2_000,\n",
        "    seed=123,\n",
        "    show_progress=False,\n",
        "    show_console=False,\n",
        ")"
      ],
      "id": "b7324f95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esaminiamo le distribuzioni a posteriori dei parametri.\n"
      ],
      "id": "d866f3a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:35.265226Z",
          "start_time": "2024-07-13T07:02:35.213028Z"
        }
      },
      "source": [
        "az.summary(fit2, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)"
      ],
      "id": "62002867",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ora possiamo assegnare al parametro $\\beta$ la seguente interpretazione: quando l'ansia di stato aumenta di una deviazione standard Tense Arousal aumenta, in media, di 0.72 deviazioni standard.\n",
        "\n",
        "## Distribuzioni a Priori sui Parametri\n",
        "\n",
        "Nei modelli precedenti, abbiamo adottato distribuzioni a priori uniformi per i parametri $\\alpha$, $\\beta$ e $\\sigma$. Tuttavia, in generale, quando non disponiamo di informazioni pregresse sul valore dei parametri, è preferibile specificare distribuzioni debolmente informative. Queste distribuzioni sono progettate per essere centrate su un valore neutro, come lo zero, in modo tale da non influenzare in modo significativo la distribuzione a posteriori nella direzione \"desiderata\" dal ricercatore. L'obiettivo delle distribuzioni a priori debolmente informative è, infatti, quello di regolarizzare il modello, penalizzando le osservazioni più estreme e contribuendo a una stima più robusta dei parametri.\n",
        "\n",
        "Per il caso in esame, specificheremo le seguenti distribuzioni a priori debolmente informative sui parametri del modello.\n",
        "\n",
        "1. **Intercetta ($\\alpha$)**:\n",
        "\n",
        "   - $\\alpha \\sim \\text{Normale}(0, 1)$\n",
        "   - La scelta di una deviazione standard ampia (2) riflette l'incertezza riguardo al valore iniziale dell'intercetta. Si crede che l'intercetta possa essere qualsiasi valore vicino a 0, ma con una variazione significativa.\n",
        "\n",
        "2. **Coefficiente Angolare ($\\beta$)**:\n",
        "\n",
        "   - $\\beta \\sim \\text{Normale}(0, 2)$\n",
        "   - Un'ampia deviazione standard (2) per $\\beta$ permette di incorporare l'incertezza riguardo all'influenza della temperatura sui ricavi del gelato. Questo prior permette che $\\beta$ possa essere sia positivo che negativo con una vasta gamma di valori.\n",
        "\n",
        "3. **Deviazione Standard Residua ($\\sigma$)**:\n",
        "\n",
        "   - $\\sigma \\sim \\text{Cauchy}^+(0, 2)$\n",
        "   - La distribuzione Half-Cauchy è scelta perché è debolmente informativa e adatta per i parametri di scala come la deviazione standard residua. La scala di 2 consente a $\\sigma$ di assumere una vasta gamma di valori positivi, riflettendo l'incertezza riguardo alla variabilità residua.\n",
        "\n",
        "Le distribuzioni normali per $\\alpha$ e $\\beta$ con deviazioni standard ampie permettono una grande flessibilità, mentre la distribuzione Half-Cauchy per $\\sigma$ è scelta per la sua capacità di gestire bene i parametri di scala. Queste scelte garantiscono che il modello sia debolmente informativo, permettendo ai dati osservati di avere un'influenza predominante sulle stime posteriori dei parametri.\n",
        "\n",
        "Compiliamo e stampiamo il modello Stan che include le specificazioni delle distribuzioni a priori dei parametri su elencate.\n"
      ],
      "id": "1f7f4c66"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:39.597858Z",
          "start_time": "2024-07-13T07:02:39.459995Z"
        }
      },
      "source": [
        "stan_file = os.path.join(project_directory, \"stan\", \"arousal_model_prior_raw.stan\")\n",
        "model3 = CmdStanModel(stan_file=stan_file)\n",
        "print(model3.code())"
      ],
      "id": "183c908c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adattiamo il modello ai dati.\n"
      ],
      "id": "c6f45886"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:42.838670Z",
          "start_time": "2024-07-13T07:02:42.566294Z"
        }
      },
      "source": [
        "fit3 = model3.sample(\n",
        "    data=stan_data2,\n",
        "    iter_warmup=1_000,\n",
        "    iter_sampling=2_000,\n",
        "    seed=123,\n",
        "    show_progress=False,\n",
        "    show_console=False,\n",
        ")"
      ],
      "id": "1832c1cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esaminiamo le distribuzioni a posteriori dei parametri.\n"
      ],
      "id": "b39d46ee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:02:45.375219Z",
          "start_time": "2024-07-13T07:02:45.327353Z"
        }
      },
      "source": [
        "az.summary(fit3, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)"
      ],
      "id": "b4a6aa34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si noti che, utilizzando distribuzioni a priori debolmente informative, le distribuzioni a posteriori dei parametri risultano molto simili a quelle ottenute usando distribuzioni uniformi. Tuttavia, le distribuzioni a priori debolmente informative sono preferibili poiché forniscono una maggiore stabilità numerica e sono generalmente più affidabili e robuste, specialmente quando si lavora con dati reali. L'uso di distribuzioni uniformi è sconsigliato per via delle possibili instabilità numeriche che possono introdurre nei modelli.\n",
        "\n",
        "## Verifica della procedura di fitting del modello utilizzando una simulazione con dati fittizi\n",
        "\n",
        "L'esempio precedente è abbastanza semplice da permetterci di tracciare un grafico e vedere se la linea di regressione attraversa i punti. Tuttavia, in generale, è una buona pratica verificare l'adattamento del modello eseguendo la procedura in condizioni controllate, dove conosciamo la verità. Mostriamo questo approccio utilizzando il modello precedente.\n",
        "\n",
        "**Passo 1: Creazione di un mondo fittizio.**\n",
        "\n",
        "Iniziamo assumendo dei valori reali per tutti i parametri del modello. In questo caso, abbiamo già adattato un modello ai dati, quindi procediamo assumendo che questi particolari valori dei parametri siano la verità. In altre parole, assumiamo che la relazione $y = 1.126 + 2.2x + \\text{errore}$ sia vera, con gli errori estratti da una distribuzione normale con media 0 e deviazione standard 2.688. Successivamente, utilizzando i valori predittori $x$ già presenti nel nostro dataset, esaminiamo se questi predittori generano una distribuzione di $y$ coerente con i valori osservati di $y$.\n"
      ],
      "id": "c45463e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = 1.126\t\n",
        "b = 0.277\n",
        "sigma = 2.688\n",
        "x = df[\"state1\"]\n",
        "n = len(x)"
      ],
      "id": "0aafaff5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Passo 2: Simulazione di dati fittizi.**\n",
        "\n",
        "Successivamente, simuleremo un vettore $y$ di dati fittizi e inseriremo tutto questo in un data frame:\n"
      ],
      "id": "3e5c76a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = a + b * x + np.random.normal(0, sigma, size=n)\n",
        "fake = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "fake.head()"
      ],
      "id": "2a1cc063",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Passo 3: Adattamento del modello e confronto tra i valori stimati e quelli assunti.**\n",
        "\n",
        "Il passo successivo è adattare un modello di regressione a questi dati. Durante l'adattamento, non si fa alcun uso dei valori veri assunti di α, β e σ.\n"
      ],
      "id": "ffa2164e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lm = pg.linear_regression(fake[\"x\"], fake[\"y\"])\n",
        "lm.round(2)"
      ],
      "id": "db15ec55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le stime ottenute dai dati fittizzi sono molto simili a quelle ottenute con i dati veri.\n",
        "\n",
        "**Passo 4: Inserire la simulazione in un loop.**\n",
        "\n",
        "Per ottenere una stima dell'incertezza delle nostre stime, ripetiamo la simulazione molte volte e calcoliamo il livello di copertura dei parametri.\n",
        "\n",
        "Il livello di copertura rappresenta la proporzione delle volte in cui l'intervallo di confidenza calcolato contiene il vero valore del parametro $b$. In altre parole, se l'intervallo di confidenza al 68% (o 95%) è calcolato correttamente, ci aspetteremmo che, rispettivamente, il 68% (o 95%) di questi intervalli contenga il vero valore di $b$.\n",
        "\n",
        "- Il codice seguente esegue `n_fake = 10_000` simulazioni, ciascuna delle quali genera un set di dati fittizio e adatta un modello di regressione a questi dati.\n",
        "\n",
        "- I valori critici `t_68` e `t_95` sono calcolati utilizzando la funzione `t.ppf` di `scipy.stats`, che fornisce i quantili della distribuzione t di Student per il livello di confidenza desiderato:\n",
        "    - `t_68` corrisponde al quantile dell'84%, che definisce l'intervallo di confidenza al 68%.\n",
        "    - `t_95` corrisponde al quantile del 97,5%, che definisce l'intervallo di confidenza al 95%.\n",
        "\n",
        "- Per ogni simulazione (`s` da 0 a `n_fake - 1`):\n",
        "    - Vengono generati dati fittizi per la variabile indipendente `x` e per la variabile dipendente `y` usando i valori di `a`, `b`, e `sigma`.\n",
        "    - Viene adattato un modello di regressione lineare ai dati fittizi usando la libreria `pingouin`.\n",
        "    - Il coefficiente stimato `b_hat` e il suo errore standard `b_se` sono estratti dai risultati della regressione.\n",
        "    - Viene verificato se il vero valore di $b$ si trova all'interno dell'intervallo $b_hat \\pm t_68 \\times b_se$.\n",
        "        - `cover_68[s] = np.abs(b - b_hat) < t_68 * b_se` memorizza `True` (1) se il vero valore di $b$ è all'interno dell'intervallo di confidenza al 68%, altrimenti `False` (0).\n",
        "   - Viene verificato se il vero valore di $b$ si trova all'interno dell'intervallo $b_hat \\pm t_95 \\times b_se$.\n",
        "    - `cover_95[s] = np.abs(b - b_hat) < t_95 * b_se` memorizza `True` (1) se il vero valore di $b$ è all'interno dell'intervallo di confidenza al 95%, altrimenti `False` (0).\n",
        "\n",
        "Dopo aver completato tutte le simulazioni, i livelli di copertura sono calcolati come la media dei valori in `cover_68` e `cover_95`:\n",
        "\n",
        "- `cover_68.mean()` fornisce la proporzione di simulazioni in cui l'intervallo di confidenza al 68% ha contenuto il vero valore di $b$.\n",
        "- `cover_95.mean()` fornisce la proporzione di simulazioni in cui l'intervallo di confidenza al 95% ha contenuto il vero valore di $b$.\n",
        "\n",
        "Se il risultato è vicino a 0.68, significa che l'intervallo di confidenza al 68% calcolato per ogni simulazione ha contenuto il vero valore di $b$ nel 68% delle simulazioni, come previsto teoricamente.\n",
        "Se il risultato è vicino a 0.95, significa che l'intervallo di confidenza al 95% calcolato per ogni simulazione ha contenuto il vero valore di $b$ nel 95% delle simulazioni, in linea con le aspettative teoriche.\n",
        "\n",
        "Se i livelli di copertura risultano sostanzialmente inferiori ai valori teorici (68% e 95%), potrebbe indicare problemi nella stima degli intervalli di confidenza o nelle assunzioni del modello.\n"
      ],
      "id": "8f32da00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import t\n",
        "\n",
        "# Parametri della simulazione\n",
        "n_fake = 10_000  # numero di simulazioni\n",
        "\n",
        "# Inizializzazione delle liste di copertura\n",
        "cover_68 = np.zeros(n_fake)\n",
        "cover_95 = np.zeros(n_fake)\n",
        "\n",
        "# Calcola i valori critici t per il 68% e il 95% utilizzando scipy.stats.t.ppf\n",
        "t_68 = t.ppf(0.84, df=n - 2)\n",
        "t_95 = t.ppf(0.975, df=n - 2)\n",
        "\n",
        "# Ciclo per la simulazione\n",
        "for s in range(n_fake):\n",
        "    x = np.random.normal(size=n)\n",
        "    y = a + b * x + np.random.normal(0, sigma, size=n)\n",
        "    fake = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "\n",
        "    # Fit del modello usando pingouin\n",
        "    fit = pg.linear_regression(fake[[\"x\"]], fake[\"y\"])\n",
        "    b_hat = fit[\"coef\"][1]\n",
        "    b_se = fit[\"se\"][1]\n",
        "\n",
        "    # Calcolo della copertura\n",
        "    cover_68[s] = np.abs(b - b_hat) < t_68 * b_se\n",
        "    cover_95[s] = np.abs(b - b_hat) < t_95 * b_se\n",
        "\n",
        "# Output dei risultati\n",
        "print(f\"68% coverage: {cover_68.mean()}\")\n",
        "print(f\"95% coverage: {cover_95.mean()}\")"
      ],
      "id": "ef5f8112",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si noti come la simulazione produce una copertura molto prossima a quella teorica. Ciò significa che, nel caso di questa analisi, possiamo assegnare agli intervalli di confidenza o credibilità l'interpretazione usuale. Se il livello di copertura della simulazione fosse stato inferiore a quello teorico (per modelli più complessi), allora questo sarebbe un'indicazione che si dovrebbero interpetare gli intervalli di confidenza o credibilità con cautela.\n",
        "\n",
        "## Il Paradosso della Regressione verso la Media\n",
        "\n",
        "Il fenomeno della regressione verso la media è un concetto statistico importante, spesso frainteso e talvolta interpretato erroneamente come un effetto causale. Questo fenomeno fu osservato inizialmente da Galton in uno studio classico sull'ereditarietà dell'altezza.\n",
        "\n",
        "@gelman2021regression discutono questo fenomeno analizzando i dati pubblicati nel 1903 da Karl Pearson e Alice Lee. Applicando un modello di regressione lineare a questi dati, si ottiene la seguente equazione:\n",
        "\n",
        "$$\n",
        "y = 63.9 + 0.54(x − 62.5) + \\text{errore},\n",
        "$$\n",
        "\n",
        "dove $y$ rappresenta l'altezza delle figlie e $x$ l'altezza delle madri. La variabile indipendente è stata centrata per evitare interpretazioni prive di senso dell'intercetta.\n",
        "\n",
        "Il paradosso emerge dal coefficiente di regressione, che è inferiore a 1. Questo implica che:\n",
        "\n",
        "1. Se una madre ha un'altezza nella media, si prevede che sua figlia adulta avrà anch'essa un'altezza nella media.\n",
        "2. Per ogni pollice in più (o in meno) rispetto alla media dell'altezza materna, ci si aspetta che la figlia sia circa mezzo pollice più alta (o più bassa) rispetto alla media della sua generazione.\n",
        "\n",
        "Questo porta a una domanda apparentemente paradossale: se le madri alte tendono ad avere figlie solo leggermente alte, e le madri basse figlie solo leggermente basse, non significa che le figlie saranno più vicine alla media rispetto alle loro madri? E se questo processo continua, non dovremmo aspettarci che dopo poche generazioni tutti abbiano un'altezza vicina alla media?\n",
        "\n",
        "La risoluzione di questo apparente paradosso sta nel fatto che la previsione dell'altezza di una donna è più vicina alla media rispetto all'altezza di sua madre, ma l'altezza effettiva non è la stessa cosa della previsione, che ha un margine di errore. Le previsioni puntuali regrediscono verso la media - ecco perché il coefficiente è inferiore a 1 - e questo riduce la variazione. Allo stesso tempo, però, l'errore nel modello - l'imperfezione della previsione - aggiunge variazione, sufficiente a mantenere la variazione totale dell'altezza approssimativamente costante da una generazione all'altra.\n",
        "\n",
        "La regressione verso la media si verifica sempre in qualche forma quando le previsioni sono imperfette in un ambiente stabile. L'imperfezione della previsione induce variazione, e la regressione nella previsione puntuale è necessaria per mantenere costante la variazione totale.\n",
        "\n",
        "Questo fenomeno è controintuitivo e spesso porta a interpretazioni causali errate. Per chiarire come ciò possa accadere, possiamo considerare uno scenario matematicamente equivalente: studenti che affrontano due esami. Coloro che ottengono punteggi alti nel primo esame tendono a ottenere risultati solo leggermente superiori alla media nel secondo; d'altra parte, chi ottiene punteggi bassi nel primo esame tende a migliorare leggermente, ottenendo risultati nel secondo esame che, pur restando inferiori alla media, non sono così bassi come i primi.\n",
        "\n",
        "Potrebbe sembrare naturale dare a questo fenomeno una spiegazione causale, suggerendo che gli studenti che eccellono nel primo esame possano avere alte capacità ma poi, diventando troppo sicuri di sé, tendano a rilassarsi, con il risultato di non ripetere la stessa performance nel secondo. Dall'altro lato, si potrebbe ipotizzare che gli studenti con punteggi bassi nel primo esame siano motivati a impegnarsi di più, migliorando così i loro risultati nel secondo.\n",
        "\n",
        "In realtà, il fenomeno della regressione verso la media si verifica anche in assenza di fattori motivazionali, come dimostrano simulazioni in cui sia il primo che il secondo esame sono determinati dalla vera abilità dell'individuo, più un elemento di rumore casuale. La regressione verso la media è un fenomeno puramente statistico, privo di una spiegazione causale intrinseca. Comprendere correttamente questo concetto è essenziale per evitare di trarre conclusioni errate dai dati.\n",
        "\n",
        "## Commenti e considerazioni finali\n",
        "\n",
        "In questo capitolo abbiamo esplorato la stima dei parametri di un modello di regressione bivariato utilizzando l'approccio bayesiano. Questo percorso ci ha portato a riflettere sulla natura e sul ruolo dei modelli statistici nella ricerca scientifica, in particolare nel campo della psicologia.\n",
        "\n",
        "Come sottolineato da @alexander2023telling, è fondamentale comprendere che i modelli statistici non sono strumenti per scoprire una verità assoluta, ma piuttosto mezzi per esplorare e interpretare i dati a nostra disposizione. Questa prospettiva ci invita a considerare i modelli non come rappresentazioni perfette della realtà, ma come lenti attraverso le quali osserviamo e cerchiamo di comprendere il mondo che ci circonda.\n",
        "\n",
        "L'affermazione di McElreath che \"la regressione è in effetti un oracolo, ma un oracolo crudele. Parla per enigmi e si diletta nel punirci per aver posto domande sbagliate\" [@McElreath_rethinking] mette in luce la natura complessa e talvolta insidiosa dell'uso dei modelli statistici. Questa metafora ci ricorda che l'applicazione dei modelli richiede non solo competenza tecnica, ma anche una profonda comprensione del contesto e una costante riflessione critica.\n",
        "\n",
        "Nel processo di modellizzazione statistica, è cruciale considerare due dimensioni interconnesse: il \"mondo del modello\", con le sue assunzioni e semplificazioni, e il \"mondo reale\", caratterizzato da una complessità spesso difficile da catturare pienamente. Questa distinzione ci invita a riflettere costantemente sulla relazione tra il modello e la realtà che cerchiamo di comprendere, ponendoci domande sulla misura in cui il modello ci insegna qualcosa sui dati a disposizione e su quanto accuratamente questi dati riflettano la realtà oggetto del nostro studio.\n",
        "\n",
        "L'evoluzione dei metodi statistici, dalle loro origini in campi come l'astronomia e l'agricoltura fino alle applicazioni moderne in psicologia, evidenzia la necessità di adattare e riconsiderare costantemente questi strumenti. Il lavoro pioneristico di Ronald Fisher, sviluppato in gran parte in un contesto di ricerca agricola, pone interrogativi sulla validità delle sue assunzioni fondamentali quando applicate alla psicologia contemporanea. @McElreath_rethinking sottolinea l'importanza di sviluppare modelli basati su ipotesi relative ai meccanismi psicologici sottostanti al comportamento, suggerendo che questi possano offrire intuizioni più profonde rispetto a un approccio puramente descrittivo come quello della regressione lineare.\n",
        "\n",
        "Nonostante queste considerazioni, il modello di regressione rimane uno strumento di grande valore per la psicologia. Tuttavia, il suo utilizzo efficace richiede un equilibrio tra una solida conoscenza del fenomeno oggetto di studio e la flessibilità necessaria per adattarsi a contesti di ricerca in continua evoluzione. Gli psicologi sono chiamati a considerare una gamma più ampia di strumenti statistici, cercando quelli più appropriati per descrivere i complessi fenomeni psicologici, superando i limiti di un approccio puramente descrittivo.\n",
        "\n",
        "In conclusione, questo capitolo ci ha permesso di esplorare l'approccio bayesiano alla regressione, offrendo una prospettiva critica sull'uso dei modelli statistici in psicologia. Per un confronto più ampio, l'appendice presenta un'introduzione all'approccio frequentista per il modello di regressione lineare bivariato, consentendo di apprezzare le differenze tra i due metodi nella stima dei parametri e nell'interpretazione dei risultati. Per approfondimenti ulteriori, si consiglia la lettura di *Applied Regression Analysis and Generalized Linear Models* [@fox2015applied], in particolare il capitolo 2, e, in italiano, *Statistica per psicologi* [@caudek2001statistica].\n",
        "\n",
        "## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n"
      ],
      "id": "5c9af688"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T07:03:16.259171Z",
          "start_time": "2024-07-13T07:03:16.203222Z"
        }
      },
      "source": [
        "%load_ext watermark\n",
        "%watermark -n -u -v -iv -m  "
      ],
      "id": "ad5cc756",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}