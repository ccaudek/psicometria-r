# Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV  {#sec-div-kl-lppd-elpd}


::: callout-important
## Obiettivi di apprendimento

Alla fine di questo capitolo, sarai in grado di:

* comprendere cos'è la distribuzione predittiva posteriore e come si costruisce;
* spiegare cosa misura il log-score e come si calcola nella pratica;
* distinguere tra LPPD ed ELPD e comprendere il loro significato;
* capire come LOO-CV fornisca una stima dell’ELPD;
* collegare il confronto tra modelli alla divergenza di Kullback-Leibler.
:::

::: callout-tip
## Prerequisiti

- Per comprendere appieno questo capitolo è utile leggere il capitolo 7 *Ulysses’ Compass* di *Statistical Rethinking* (@McElreath_rethinking).
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


## Introduzione {.unnumbered .unlisted}

Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’*entropia*, che misura l’incertezza insita in una distribuzione, e la *divergenza di Kullback–Leibler* ($D_{\text{KL}}$), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per *valutare e confrontare modelli statistici* nel contesto bayesiano.

Il punto di partenza è una domanda cruciale: *quanto bene il modello riesce a prevedere nuovi dati?* Un buon modello non deve solo adattarsi bene ai dati già osservati, ma anche saper *generalizzare* a situazioni future o a campioni mai visti. Questa distinzione — adattamento vs. generalizzazione — è il cuore della valutazione predittiva.

Per rendere concreta questa idea, immaginiamo di aver sviluppato un test psicologico per prevedere il livello di ansia degli studenti alla vigilia di un esame. Non basta sapere che il modello descrive bene i dati del campione che abbiamo usato per costruirlo: vogliamo anche essere ragionevolmente sicuri che le stesse previsioni funzionino per studenti che non hanno partecipato allo studio.

In questo capitolo introdurremo:

* la *distribuzione predittiva posteriore*, che integra l’incertezza sui parametri e ci permette di fare previsioni coerenti con il nostro stato di conoscenza;
* il *log-score*, come misura punto per punto dell’accuratezza predittiva;
* due sintesi fondamentali: la *LPPD* (*Log Pointwise Predictive Density*) e la *ELPD* (*Expected Log Predictive Density*);
* la tecnica *Leave-One-Out Cross-Validation* (LOO-CV), che fornisce una stima empirica dell’ELPD;
* il legame concettuale tra ELPD e *divergenza di Kullback–Leibler*, che permette di interpretare il confronto tra modelli come una ricerca del modello “più vicino” alla distribuzione vera dei dati.

L’obiettivo è fornire un quadro chiaro e operativo di come, in Bayes, si possa passare dai principi teorici dell’informazione a strumenti concreti per scegliere, in modo razionale, il modello più adatto.


## Distribuzione Predittiva Posteriore

Nel capitolo sul modello *beta–binomiale* abbiamo già incontrato il concetto di *distribuzione predittiva posteriore*: è lo strumento che, in Bayes, consente di fare previsioni su nuovi dati incorporando sia il modello che la nostra incertezza sui parametri. Qui riprendiamo quell’idea per applicarla al problema più generale della *valutazione e confronto tra modelli*.

In ambito bayesiano, dopo aver osservato i dati $y$ non otteniamo un singolo valore “migliore” dei parametri, ma una *distribuzione posteriore* $p(\theta \mid y)$, che descrive tutti i valori plausibili di $\theta$ e la nostra incertezza su di essi.

Per esempio, uno psicologo che voglia stimare il livello medio di ansia in una popolazione, dopo aver raccolto un campione, non dirà semplicemente “la media è 4.7”, ma piuttosto “il valore più plausibile è 4.7, ma potrebbe ragionevolmente essere compreso tra 4.2 e 5.1”.

Quando vogliamo predire un nuovo dato $\tilde{y}$, non fissiamo un singolo $\theta$, ma combiniamo tutte le possibili previsioni condizionate, pesandole in base alla probabilità a posteriori di ciascun valore di $\theta$:

$$
q(\tilde{y} \mid y) \;=\; \int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta .
$$

::: {.callout-note collapse=true title="Intuizione"}
Se conoscessimo esattamente i parametri $\theta$, useremmo semplicemente $p(\tilde{y} \mid \theta)$ per prevedere il nuovo dato. Poiché non li conosciamo, prendiamo tutte le previsioni possibili e le combiniamo, pesando ciascuna secondo quanto $\theta$ è plausibile nella posteriore. È come consultare più esperti: il parere di ciascuno viene ponderato in base alla fiducia che riponiamo in lui.
:::

In questo capitolo useremo talvolta la notazione compatta $q(\cdot \mid y)$ per riferirci alla distribuzione predittiva posteriore del modello. Più spesso, per chiarezza, scriveremo $p(y_i \mid y)$, per evidenziare che si tratta della previsione marginale per la singola osservazione $y_i$, ottenuta integrando la verosimiglianza $p(y_i \mid \theta)$ rispetto alla posteriore $p(\theta \mid y)$.


### Il problema della valutazione predittiva

Vorremmo sapere *quanto* la distribuzione predittiva posteriore $q(\tilde{y} \mid y)$ si avvicina alla *vera distribuzione generatrice* dei dati futuri, $p(\tilde{y})$. In teoria, questa distanza si misura con la *divergenza di Kullback–Leibler*:

$$
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right] .
$$

Qui incontriamo subito un ostacolo: *non conosciamo $p(\tilde{y})$*. È come voler valutare la precisione di una mappa senza avere accesso al territorio reale. Per aggirare questo problema, useremo *misure surrogate* — come il *log-score*, la *LPPD* e l’*ELPD* — che stimano indirettamente la bontà predittiva del modello sfruttando in modo ingegnoso i dati osservati.


::: {.callout-tip title="Mappa concettuale"}

| Quantità                 | Significato                          | Uso principale                   |
| ------------------------ | ------------------------------------ | -------------------------------- |
| $p(y_i \mid \theta)$  | Verosimiglianza                      | Calcolo predittivo               |
| $p(\theta \mid y)$     | Distribuzione posteriore             | Ponderazione                     |
| $p(y_i \mid y)$       | Predizione bayesiana media           | Log-score, LPPD                  |
| $p(y_i \mid y_{-i})$ | Predizione LOO (*leave-one-out*)     | ELPD                             |
| $q(\tilde{y} \mid y)$  | Distribuzione predittiva complessiva | Divergenza KL, confronto modelli |
:::


### Il log-score: accuratezza predittiva punto per punto

Una volta definita la *distribuzione predittiva posteriore*, possiamo chiederci: *quanto bene il modello “aveva previsto” ciascun dato osservato?* Il *log-score* risponde a questa domanda, misurando per ogni osservazione $y_i$ il logaritmo della probabilità che il modello le assegna:

$$
\log p(y_i \mid y) = \log \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
$$ {#eq-log-score-def}

Il *log-score totale* è la somma su tutte le osservazioni:

$$
S = \sum_{i=1}^n \log p(y_i \mid y).
$$ {#eq-log-score-sum-def}

Un log-score *più alto* (cioè meno negativo) indica che il modello attribuisce maggiore probabilità ai dati effettivamente osservati. È, in pratica, un *voto di fiducia* del modello per ogni osservazione: se ciò che accade era plausibile per il modello, il punteggio sarà alto.


#### Come si calcola in pratica con MCMC

Nella formula teorica, $p(y_i \mid y)$ è una *media predittiva* calcolata integrando:

* $p(y_i \mid \theta)$: la distribuzione dei dati futuri se i parametri fossero $\theta$ (cioè la verosimiglianza condizionata);
* $p(\theta \mid y)$: la distribuzione posteriore dei parametri, che rappresenta l’incertezza residua dopo aver osservato i dati.

Poiché l’integrale non ha quasi mai una soluzione analitica, in pratica lo *approssimiamo con i campioni MCMC* dalla posteriore:

$$
p(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}),
$$ {#eq-mcmc-posterior-parameter-distr}

e quindi:

$$
\text{Log-score} \approx \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right].
$$ {#eq-mcmc-log-score}


::: {.callout-note collapse=true title="Esempio."}
Supponiamo di avere tre valori posteriori di $\theta$: 0.3 (peso 0.2), 0.5 (peso 0.5) e 0.7 (peso 0.3). Se la nuova osservazione è $y = 3$ su $n = 5$ tentativi:

```{r}
theta_vals <- c(0.3, 0.5, 0.7)
posterior_weights <- c(0.2, 0.5, 0.3)
likelihoods <- dbinom(3, size = 5, prob = theta_vals)
p_y_given_y <- sum(likelihoods * posterior_weights)
log_score <- log(p_y_given_y)
log_score
```

Il log-score è circa -1.29. Un valore meno negativo indica una previsione migliore.
:::


Con il log-score $S = \sum_{i=1}^n \log p(y_i \mid y)$ otteniamo *una misura complessiva* di quanto bene il modello assegna alta probabilità ai dati osservati (*in-sample*). L’idea alla base — calcolare il logaritmo della probabilità predittiva per ciascun dato e poi sommare — sarà la stessa anche per gli indici che introdurremo subito dopo:

* la *LPPD* (*Log Pointwise Predictive Density*) usa esattamente questa struttura, ma la formalizza come misura bayesiana di adattamento ai dati osservati, tenendo conto dell’incertezza sui parametri;
* l’*ELPD* (*Expected Log Predictive Density*) applica lo stesso principio *fuori campione*, stimando quanto bene il modello predirebbe dati nuovi non utilizzati nell’adattamento.

In altre parole, la LPPD e l’ELPD sono estensioni del log-score: la prima ne è la versione “in-sample” formalizzata, la seconda ne è l’analogo “out-of-sample” pensato per valutare la generalizzazione.


### Dalla LPPD all’ELPD: in-sample vs. out-of-sample

Come abbiamo visto, il log-score complessivo

$$
S = \sum_{i=1}^n \log p(y_i \mid y)
$$

ci dice quanto bene il modello assegna alta probabilità ai dati *che ha già visto* (*in-sample*). La *LPPD* (*Log Pointwise Predictive Density*) è sostanzialmente questa stessa quantità, calcolata in modo coerente con il framework bayesiano e con la media predittiva su tutta la posteriore:

$$
\text{LPPD} = \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right].
$$ {#eq-lppd-def}

La LPPD è quindi un’estensione diretta del log-score: misura la bontà predittiva sui dati osservati, incorporando l’incertezza sui parametri.

**Limite:** proprio perché valuta l’adattamento in-sample, tende a favorire modelli più complessi e può sovrastimare la capacità di generalizzare (*overfitting*).


### Expected Log Predictive Density (ELPD): guardare oltre i dati osservati

Per valutare la *generalizzazione* dobbiamo chiederci: *quanto bene il modello predirebbe dati che non ha mai visto?* Questo porta all’*ELPD* (*Expected Log Predictive Density*), che ha la stessa struttura della LPPD, ma con una differenza cruciale: la predizione di $y_i$ è fatta *escludendo $y_i$ dall’adattamento* (*Leave-One-Out*):

$$
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid y_{-i}),
$$ {#eq-elpd-def}

dove $y_{-i}$ indica il dataset privato dell’osservazione $i$.

Tornando all’esempio del test per l’ansia: la LPPD valuta quanto bene il modello predice i punteggi di ansia degli studenti del campione già osservato; l’ELPD valuta quanto bene predirebbe il punteggio di un nuovo studente usando solo i dati degli altri.


::: callout-note
**Interpretazione:** l’ELPD è un *log-score out-of-sample*: per ogni $y_i$, lo escludiamo, adattiamo il modello agli altri dati, e valutiamo la probabilità predittiva di $y_i$.
Più alto è l’ELPD, migliore è la capacità del modello di generalizzare a dati nuovi.
:::


::: {.callout-note collapse=true title="Esempio."}
Supponiamo di avere tre osservazioni $y_1, y_2, y_3$ e che il modello stimi:

$$
p(y_1 \mid y_2,y_3)=0.6,\quad p(y_2 \mid y_1,y_3)=0.7,\quad p(y_3 \mid y_1,y_2)=0.5.
$$

L’ELPD è:

$$
\log 0.6 + \log 0.7 + \log 0.5 \; \approx\; -0.5108 -0.3567 -0.6931 = -1.5606.
$$

Un valore meno negativo indica maggiore capacità predittiva fuori campione.
:::


### LPPD vs. ELPD in sintesi

| Misura   | Dati usati per predire $y_i$    | Valuta                | Rischio principale |
| -------- | ---------------------------------- | --------------------- | ------------------ |
| **LPPD** | Tutti i dati, incluso $y_i$     | Adattamento in-sample | Overfitting        |
| **ELPD** | Tutti i dati tranne $y_i$ (LOO) | Generalizzazione      | —                  |

**Metafora.**
Consideriamo un esperimento sul riconoscimento di volti. Mostriamo a un partecipante 100 fotografie e lo alleniamo a riconoscerle.

* *LPPD* misura quanto bene il partecipante riconosce *quelle stesse 100 foto* già viste durante l’allenamento (*in-sample*).
* *ELPD* misura quanto bene riconosce *nuove foto di persone già viste o di volti mai incontrati prima*, cioè immagini non incluse nell’allenamento (*out-of-sample*).

Se il punteggio LPPD è alto ma l’ELPD è basso, significa che il partecipante — o il modello — ha semplicemente *memorizzato* le foto specifiche, senza aver appreso le caratteristiche generali dei volti che permettono di riconoscerne di nuovi.


### Il collegamento con la divergenza KL
  
La *divergenza di Kullback–Leibler* ($D_{\text{KL}}$) è la misura teoricamente ideale della distanza tra la distribuzione vera dei dati, $p(\tilde{y})$, e la distribuzione predittiva posteriore di un modello, $q(\tilde{y} \mid y)$. Nel confronto tra due modelli $A$ e $B$, la differenza tra le rispettive $D_{\text{KL}}$ equivale alla differenza tra le loro *accuratezze predittive medie* rispetto a $p(\tilde{y})$.

Poiché $p(\tilde{y})$ è sconosciuta, non possiamo calcolare direttamente la KL. Ma possiamo *stimarla indirettamente* con l’*ELPD* (*Expected Log Predictive Density*): un ELPD più alto significa che il modello è più “vicino” alla distribuzione vera.

In sintesi:
  
$$
\text{Massimizzare ELPD} \;\; \approx \;\; \text{Minimizzare la divergenza KL} .
$$

::: {.callout-note collapse=true title="Esempio."}
Vogliamo confrontare due modelli predittivi per il numero di “teste” in $n=10$ lanci.

- La **distribuzione vera** è $p(y)=\text{Binom}(n=10,\;p=0.6)$.
- Il **modello candidato** prevede $q(y)=\text{Binom}(n=10,\;q=0.5)$.

L’*ELPD* di un modello è l’aspettativa, rispetto alla distribuzione vera $p$, del *log-score* del modello: $\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]$. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori $y=0,\dots,n$.

```{r}
# Parametri del problema
n <- 10          # numero di lanci
p <- 0.6         # probabilità vera di "testa"
q <- 0.5         # probabilità ipotizzata dal modello candidato

# 1) Supporto dei possibili esiti
y_vals <- 0:n

# 2) Distribuzione vera p(y) su tutto il supporto
p_y <- dbinom(y_vals, size = n, prob = p)

# 3) Log-predittiva del modello candidato q su tutto il supporto
log_q_y <- log(dbinom(y_vals, size = n, prob = q))

# 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)
elpd_q <- sum(p_y * log_q_y)

# 5) "Modello vero": usa q = p. Log-predittiva del modello vero
log_p_y <- log(dbinom(y_vals, size = n, prob = p))

# 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)
elpd_p <- sum(p_y * log_p_y)

# 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]
kl_pq <- sum(p_y * (log_p_y - log_q_y))

cat(sprintf("ELPD modello candidato (q=0.5): %.4f\n", elpd_q))
cat(sprintf("ELPD modello vero      (q=0.6): %.4f\n", elpd_p))
cat(sprintf("Differenza ELPD (vero - candidato): %.4f\n", elpd_p - elpd_q))
cat(sprintf("KL(p || q): %.4f\n", kl_pq))
```

**Cosa stiamo verificando?**

1. $\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)$ è *più basso* (più negativo) del valore ottenuto dal modello vero $\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)$.
   → Il modello con $q=0.6$ è *più predittivo* di quello con $q=0.5$.

2. La *differenza* tra i due ELPD è *uguale* (vicina numericamente) alla *divergenza di Kullback–Leibler*:

$$
\mathrm{ELPD}(p)-\mathrm{ELPD}(q)
= \sum_y p(y)\big[\log p(y)-\log q(y)\big]
= D_{\mathrm{KL}}(p\|q)\;>\;0.
$$

→ Questo mostra *algebricamente e numericamente* il legame: *massimizzare l’ELPD equivale a minimizzare la KL*.

> Nota sul log: nel codice usiamo il log naturale (unità in **nat**). Se si preferisce il log in base 2 (unità in *bit*), basta sostituire `log()` con `log2()`; tutte le quantità cambiano di una costante di scala, ma i *confronti* tra modelli restano identici.

**In pratica.**

In questo esempio abbiamo potuto calcolare l’ELPD *vero* perché conoscevamo l’intera distribuzione generatrice $p(y)$ e potevamo integrare esattamente. Nella realtà, $p(y)$ è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD *empiricamente*, ad esempio con la *Leave-One-Out Cross-Validation* (LOO-CV), che sostituisce l’aspettativa rispetto a $p$ con una media sui dati raccolti, lasciando fuori una osservazione alla volta. Questa procedura ci consente di avvicinarci al calcolo ideale della KL, anche senza conoscere $p(y)$.
:::


## Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica

Abbiamo visto che l’*ELPD* è la misura ideale della capacità predittiva di un modello su dati futuri e non osservati. Il problema è che, per definizione, richiede di calcolare un’aspettativa rispetto alla *vera* distribuzione generatrice $p(\tilde{y})$, che non conosciamo.

**Come possiamo stimarlo in pratica?**
La risposta è la *Leave-One-Out Cross-Validation* (LOO-CV), che simula la previsione di nuovi dati usando soltanto i dati osservati.


### Cos’è la LOO-CV

La LOO-CV è un esperimento concettuale molto semplice:

1. Scegli un’osservazione $y_i$ dal dataset.
2. *Escludila* dal set di addestramento.
3. Adatta il modello ai dati rimanenti $y_{-i}$.
4. Calcola la densità predittiva del modello per l’osservazione esclusa: $p(y_i \mid y_{-i})$.
5. Ripeti per ogni osservazione e somma i logaritmi ottenuti.

La formula è:

$$
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}),
$$ {#eq-loo-def}

dove $y_{-i}$ indica il dataset senza l’osservazione $i$.

La struttura è identica a quella dell’ELPD “ideale”, ma ogni termine è calcolato *fuori campione*, escludendo l’osservazione corrispondente.


### Perché LOO-CV funziona

L’ELPD può essere scritto come:

$$
\mathbb{E}_p[\log q(\tilde{y} \mid y)],
$$ {#eq-loo-def2}

dove $q(\tilde{y} \mid y)$ è la distribuzione predittiva del modello.
Non possiamo calcolare l’aspettativa rispetto a $p(\tilde{y})$, ma se trattiamo ogni osservazione $y_i$ come “nuovo dato” generato da $p$, la *media sui dati osservati* approssima bene l’aspettativa:

$$
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$

In altre parole, LOO-CV ci dice *quanto bene il modello predirebbe ciascun punto senza averlo mai visto*.


### Legame teorico con la divergenza KL

La divergenza di Kullback–Leibler misura la distanza tra la distribuzione vera $p(\tilde{y})$ e la distribuzione predittiva del modello $q(\tilde{y} \mid y)$:

$$
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p[\log p(\tilde{y})] - \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$

Come abbiamo già visto in precedenza, il primo termine, l’entropia di $p$, è identico per tutti i modelli e quindi *scompare nel confronto*. La differenza tra due modelli $q_1$ e $q_2$ dipende solo dalle loro accuratezze predittive medie:

$$
D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) =
\mathbb{E}_p[\log q_2(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_1(\tilde{y} \mid y)].
$$

Quindi *vince il modello con ELPD più alto*.


### Confronto tra modelli

Poiché $p(\tilde{y})$ è sconosciuta, sostituiamo l’aspettativa con la stima empirica LOO-CV:

$$
\Delta\text{ELPD} = \text{ELPD}_{\text{LOO}}(M_1) - \text{ELPD}_{\text{LOO}}(M_2)
$$ {#eq-delta-elpd-def}

$\Delta\text{ELPD}$ approssima la differenza tra le divergenze KL dei modelli rispetto alla distribuzione vera. Oltre alla differenza, possiamo calcolare un *errore standard* per valutare se la superiorità di un modello è statisticamente convincente.


### Sintesi

* **Problema:** L’ELPD ideale richiede $p(\tilde{y})$, che è sconosciuta.
* **Soluzione:** LOO-CV stima l’ELPD simulando previsioni out-of-sample.
* **Teoria:** L’ELPD è proporzionale (a meno di una costante) al termine di accuratezza nella KL.
* **Pratica:** Massimizzare l’ELPD stimato equivale a scegliere il modello più vicino alla distribuzione vera.

Anche se non conosciamo la vera distribuzione dei dati, possiamo *usare l'ELPD stimata con LOO-CV come proxy della divergenza KL* e quindi come strumento pratico e teoricamente fondato per selezionare modelli che generalizzano bene.


::: {.callout-note collapse=true title="Esempio."}

**Dati.** Cinque prove: $y=\{1,1,1,0,1\}$ (quattro “successi”, uno “insuccesso”).

**Modello A (bayesiano).** Bernoulli$(\theta)$ con prior $\theta\sim \text{Beta}(1,1)$ (uninformativa).
Per la LOO: per ciascun $i$, escludi $y_i$, calcola la posteriore $\theta\mid y_{-i}\sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})$, dove $s_{-i}=\sum_{j\neq i} y_j$, $n_{-i}=n-1$.
La **predittiva LOO** per l’osservazione esclusa è:

* se $y_i=1$: $p(y_i\mid y_{-i}) = \frac{1+s_{-i}}{(1+s_{-i})+(1+n_{-i}-s_{-i})}$;
* se $y_i=0$: $p(y_i\mid y_{-i}) = \frac{1+n_{-i}-s_{-i}}{(1+s_{-i})+(1+n_{-i}-s_{-i})}$.

**Modello B (di confronto).** Moneta equa fissa: $q=0.5$. Allora $p(y_i\mid y_{-i})=0.5$ per ogni $i$.

Calcoliamo le **log-densità predittive punto-per-punto** e poi l’ELPD-LOO (somma delle log-densità).

```{r}
# Dati
y <- c(1, 1, 1, 0, 1)
n <- length(y)

# Funzione: log-densità predittiva LOO per il Modello A (Beta(1,1) + Bernoulli)
loo_log_pred_beta <- function(i, y, a0 = 1, b0 = 1) {
  n  <- length(y)
  yi <- y[i]
  s_minus <- sum(y) - yi      # successi escluso i
  n_minus <- n - 1
  alpha <- a0 + s_minus
  beta  <- b0 + (n_minus - s_minus)
  p1 <- alpha / (alpha + beta)  # pred prob di 1
  p  <- if (yi == 1) p1 else (1 - p1)
  log(p)
}

# Log-densità LOO punto-per-punto
lp_beta  <- sapply(seq_along(y), loo_log_pred_beta, y = y)
lp_fixed <- rep(log(0.5), n)  # Modello B: moneta equa

# ELPD-LOO (somma delle log-densità)
elpd_beta  <- sum(lp_beta)
elpd_fixed <- sum(lp_fixed)

# Differenza punto-per-punto e SE della differenza di ELPD (stima classica)
diff_pt  <- lp_beta - lp_fixed
se_diff  <- sqrt(length(diff_pt) * var(diff_pt))  # SE della somma

# Piccola tabella riassuntiva
res <- data.frame(
  i = 1:n, y = y,
  lp_beta = round(lp_beta, 6),
  lp_fixed = round(lp_fixed, 6),
  diff = round(diff_pt, 6)
)

print(res)
cat(sprintf("\nELPD-LOO Modello A (Beta-Bernoulli): %.6f\n", elpd_beta))
cat(sprintf("ELPD-LOO Modello B (q=0.5 fisso)   : %.6f\n", elpd_fixed))
cat(sprintf("Differenza (A - B)                 : %.6f\n", elpd_beta - elpd_fixed))
cat(sprintf("SE della differenza                : %.6f\n", se_diff))
```

**Interpretazione.**

* Le **righe** mostrano, per ogni $i$, la log-densità LOO del Modello A (che “vede” $n-1$ dati) e quella del Modello B (sempre $\log 0.5$).
* In questo dataset (4 su 5 successi) il Modello A tende a dare **probabilità predittiva > 0.5** ai successi quando lasci fuori un successo; viceversa, penalizza di più l’unico zero lasciato fuori.
* La **differenza ELPD-LOO** $=\sum_i (\log p_A - \log p_B)$ può risultare **leggermente > 0** (cioè Modello A migliore), ma con **SE ampio** perché $n$ è piccolo: è normale in esempi didattici così minimali.

> Regola pratica: per un confronto più “convincente”, guarda se $|\Delta \text{ELPD}|$ è almeno *circa 2 $\times$ SE*. Con $n=5$ questo raramente accade: l’obiettivo qui è capire *come si calcola* e *cosa significa*.
:::


### ELPD-LOO e il problema dell'overfitting

Valutare un modello sugli stessi dati con cui è stato addestrato tende a *sovrastimare* la sua capacità predittiva, soprattutto se il modello è molto flessibile. È come giudicare le capacità di uno studente facendogli ripetere esattamente gli stessi esercizi che ha già svolto durante lo studio: otterrà sicuramente un buon punteggio, ma questo non ci dice nulla su quanto bene saprà risolvere problemi *nuovi*.

La *Leave-One-Out Cross-Validation (LOO-CV)* risolve questo problema:

* per ciascuna osservazione $i$, il modello viene valutato su $y_i$ usando *solo dati che non includono $y_i$*;
* il risultato è una misura *out-of-sample* della bontà predittiva, molto meno sensibile all’overfitting.

Grazie a metodi efficienti come il *Pareto-smoothed importance sampling (PSIS)*, oggi è possibile stimare l’ELPD-LOO *senza dover riadattare il modello $n$ volte*. In R, questo è implementato nella funzione `loo()` (pacchetto {**loo**}), compatibile con `brms` e `rstanarm`.

::: {.callout-note collapse=true title="Esempio: ELPD atteso vs LOO stimato"}

Immaginiamo una moneta con probabilità vera di testa $p=0.6$, ma un modello che assume $q=0.5$. Calcoliamo prima l’*ELPD teorico*: il log-score atteso se il modello $q$ predice dati generati da $p$.

```{r}
n <- 10
p <- 0.6  # probabilità vera
q <- 0.5  # probabilità assunta dal modello

y_vals   <- 0:n
p_y      <- dbinom(y_vals, size = n, prob = p)        # vera distribuzione
log_q_y  <- log(dbinom(y_vals, size = n, prob = q))   # log-prob modello
elpd     <- sum(p_y * log_q_y)

cat(sprintf("ELPD atteso (modello q = 0.5): %.4f\n", elpd))
```

Ora vediamo come stimare **in pratica** l’ELPD-LOO con `loo()` partendo da un set di dati simulato:

```{r}
library(brms)
library(loo)

# Simuliamo dati binomiali (10 lanci per 20 soggetti)
set.seed(123)
df <- data.frame(
  k = rbinom(20, size = n, prob = p),
  n = n
)

# Modello con q fisso = 0.5 (intercetta logit = 0)
fit <- brm(k | trials(n) ~ 1, data = df,
           family = binomial(),
           prior = prior(constant(0), class = "Intercept"),  # logit(0.5)=0
           iter = 2000, chains = 2)

# Calcoliamo LOO con PSIS
loo_res <- loo(fit)

print(loo_res)
```

Qui:

* `loo_res` fornisce **ELPD-LOO** (stima pratica di ELPD) e il suo errore standard;
* è possibile confrontare due modelli con `loo_compare()` usando la differenza di ELPD-LOO e il relativo SE;
* i valori di *Pareto k* aiutano a capire se la stima PSIS è affidabile.
:::

::: {.callout-tip}
**In pratica**

1. **Capire il concetto:** L’ELPD misura la capacità predittiva attesa su nuovi dati; LOO-CV fornisce una stima “fuori campione”.
2. **Usare PSIS-LOO:** evita di riadattare il modello $n$ volte, mantenendo buona accuratezza.
3. **Workflow tipico in R:**

   * Adattare il modello (`brm()` o `stan_glm()`);
   * Estrarre le log-likelihood per osservazione (`log_lik()`);
   * Calcolare `loo()` → ELPD-LOO, SE, e diagnostici Pareto k;
   * Confrontare modelli con `loo_compare()`.
4. **Decisione:** preferire il modello con ELPD-LOO più alto (differenza ≥ 2×SE è una soglia indicativa).
:::


## Criteri di informazione come approssimazioni della divergenza $D_{\text{KL}}$

Oltre alla *Leave-One-Out Cross-Validation*, esistono altri strumenti per stimare la qualità predittiva di un modello senza dover conoscere la distribuzione vera dei dati.
Molti di questi metodi derivano, in modo più o meno diretto, dalla *divergenza di Kullback–Leibler* $D_{\text{KL}}$, che — come visto — misura la distanza tra la distribuzione reale e quella stimata dal modello.

L’idea di base è sempre la stessa:

* valutare quanto bene il modello spiega i dati (*bontà di adattamento*);
* penalizzare la *complessità* del modello, per ridurre il rischio di *overfitting*.

Questa logica si traduce in *criteri di informazione* che combinano due componenti:

1. *termine di fit*: misura di quanto bene il modello si adatta ai dati osservati (es. log-verosimiglianza, MSE);
2. *termine di penalizzazione*: aumenta con il numero di parametri o con la flessibilità del modello.

Tra i criteri più usati troviamo:

* **MSE** (Mean Squared Error) – semplice e intuitivo, basato sugli errori di previsione;
* **AIC** (Akaike Information Criterion) – approssima $D_{\text{KL}}$ tra il modello e la verità, penalizzando il numero di parametri;
* **BIC** (Bayesian Information Criterion) – simile all’AIC, ma con penalizzazione più forte per modelli complessi, proporzionale al numero di osservazioni;
* **WAIC** (Widely Applicable Information Criterion) – versione pienamente bayesiana, basata sulle previsioni del modello integrate sull’intera distribuzione a posteriori.

Nelle sezioni seguenti vedremo come ciascun criterio si calcola, quali assunzioni richiede e in quali situazioni è preferibile rispetto agli altri.


### Errore Quadratico Medio (MSE)

L’*Errore Quadratico Medio* misura la media delle differenze al quadrato tra valori osservati e previsti:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
$$ {#eq-mse-def}

* Valori più bassi indicano previsioni più vicine ai dati osservati.
* Non tiene conto della complessità del modello, quindi può favorire modelli eccessivamente flessibili (*overfitting*).

Utile per valutare l’accuratezza, ma da solo non è adatto a scegliere tra modelli con diversa complessità.


### Akaike Information Criterion (AIC)

L’*AIC* è un’approssimazione della divergenza $D_{\text{KL}}$ e stima quanta informazione si perde usando un modello per descrivere i dati:

$$
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k,
$$ {#eq-aic-def}

dove:

* $\hat{\theta}_{\text{MLE}}$: stima dei parametri ottenuta massimizzando la verosimiglianza;
* $k$: numero di parametri del modello.

**Interpretazione**

* Il primo termine valuta l’adattamento del modello ai dati.
* Il secondo penalizza la complessità per evitare overfitting.
* Un AIC più basso indica un miglior equilibrio tra accuratezza e semplicità.

**Limiti**

* Basato su assunzioni asintotiche (funziona meglio con campioni grandi).
* Usa solo stime puntuali, ignorando l’incertezza dei parametri.
* Non è pienamente coerente con l’approccio bayesiano.


### Bayesian Information Criterion (BIC)

Il *BIC* valuta il compromesso tra *adattamento ai dati* e *complessità del modello*, applicando una penalizzazione più severa rispetto all’AIC — soprattutto quando il numero di osservazioni $n$ è grande.

$$
BIC = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k,
$$ {#eq-bic-def}

dove:

* $p(y \mid \hat{\theta})$: massima verosimiglianza del modello (o MAP con prior piatti);
* $n$: numero di osservazioni indipendenti;
* $k$: numero di parametri stimati.

**Interpretazione**

* Il primo termine misura l’adattamento ai dati.
* Il secondo penalizza la complessità in modo crescente con $n$ e $k$.
* Un BIC più basso indica un compromesso migliore tra accuratezza e parsimonia.

**Vantaggi**

* Tende a favorire modelli più semplici quando $n$ è elevato.
* Ha una giustificazione teorica bayesiana: in certe condizioni, approssima il log della *marginal likelihood*.

**Limiti**

* Si basa su assunzioni forti (indipendenza, modelli regolari, prior deboli).
* Può sottoselezionare modelli utili con campioni piccoli o strutture complesse.


### Widely Applicable Information Criterion (WAIC)

Il *WAIC* è una versione *pienamente bayesiana* dell’AIC:

* utilizza *tutta la distribuzione a posteriori* dei parametri;
* fornisce una stima diretta della *capacità predittiva* del modello.

$$
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \mathrm{Var}_{\theta^{(s)}} \big( \log p(y_i \mid \theta^{(s)}) \big)
\right],
$$ {#eq-waic-def}

dove:

* $S$ = numero di campioni dalla distribuzione a posteriori;
* $\theta^{(s)}$ = $s$-esimo campione;
* il secondo termine stima il *numero effettivo di parametri* basato sulla variabilità della log-verosimiglianza.

**Vantaggi**

* Adatto anche a modelli complessi o non regolari.
* Usa direttamente i campioni MCMC.
* Migliore dell’AIC per modelli bayesiani, perché incorpora l’incertezza dei parametri.

**Nota**
Il WAIC è strettamente collegato all’ELPD: è una sua stima approssimata ottenuta dalla posteriori, senza bisogno di eseguire la LOO-CV.


**Riepilogo comparativo.**

| Criterio | Tipo                   | Penalizza la complessità? | Usa stime puntuali? | Supporta Bayesian MCMC? |
| -------- | ---------------------- | ------------------------- | ------------------- | ----------------------- |
| MSE      | Frequentista           | ❌                         | ✅                   | ❌                       |
| AIC      | Frequentista           | ✅ (modesta)               | ✅                   | ❌                       |
| BIC      | Frequentista/Bayesiano | ✅ (forte)                 | ✅                   | ❌                       |
| WAIC     | Bayesiano              | ✅ (effettiva)             | ❌                   | ✅                       |
| LOO-CV   | Bayesiano              | ✅ (empirica)              | ❌                   | ✅                       |


## Riflessioni conclusive {.unnumbered .unlisted}

La selezione del modello, in ottica bayesiana, ruota attorno a una domanda essenziale: *quanto bene il modello predice dati che non ha mai visto?*

Il riferimento teorico è l’*Expected Log Predictive Density (ELPD)*, che misura quanto la distribuzione predittiva del modello si avvicina alla vera (e ignota) distribuzione dei dati. In termini matematici, massimizzare l’ELPD equivale a minimizzare la *divergenza di Kullback–Leibler* rispetto alla vera generatrice: due facce dello stesso obiettivo, rappresentare al meglio la realtà sottostante.

Poiché $p_{\text{vera}}(y)$ è sconosciuta, l’ELPD va stimato. Le principali approssimazioni sono:

* **LOO-CV** (Leave-One-Out Cross-Validation): oggi lo strumento più affidabile, valuta ogni osservazione come “nuova” e stima la capacità di generalizzazione del modello.
* **WAIC**: alternativa completamente bayesiana, calcolata direttamente dai campioni della posteriori.
* **AIC** e **BIC**: criteri frequenstisti più rapidi ma basati su stime puntuali; utili in contesti semplici.
* **MSE**: misura l’accuratezza sulle osservazioni note, ma non penalizza la complessità e quindi non è adatto alla selezione del modello.

Nel confronto tra modelli, la *differenza di ELPD* (stimata con LOO-CV o WAIC) andrebbe interpretata insieme al relativo *errore standard*: una regola pratica è considerare rilevante una differenza almeno doppia rispetto all’errore standard.

**In sintesi**

* La buona statistica non si limita a spiegare il passato: sa *anticipare il futuro*.
* La *divergenza KL* fornisce la misura teorica della distanza tra modello e realtà.
* L’*ELPD*, stimato via LOO-CV o WAIC, traduce questa misura in una valutazione pratica della capacità predittiva.
* La scelta del modello ottimale richiede un equilibrio tra accuratezza, generalizzazione e parsimonia.

Con questi strumenti possiamo individuare modelli che colgono i veri pattern nei dati, evitando di farsi ingannare dal rumore e garantendo previsioni solide anche in contesti complessi.


## Informazioni sull'Ambiente di Sviluppo {.unnumbered .unlisted}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered .unlisted}

