# Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia {#sec-gauss-grid}

::: callout-important
## In questo capitolo imparerai a

- utilizzare il metodo basato su griglia per approssimare la distribuzione a posteriori gaussiana.

:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Grid approximation* del testo di @Johnson2022bayesrules.

:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |>
    source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(reshape2)
```
:::


## Introduzione 

In questo capitolo, estenderemo la discussione precedente sul calcolo della distribuzione a posteriori utilizzando il metodo basato su griglia, applicandolo questa volta a un caso con verosimiglianza gaussiana. In particolare, ci concentreremo su come costruire un modello gaussiano per descrivere l'intelligenza.

Immaginiamo di condurre uno studio sulla plusdotazione, considerando l'approccio psicometrico. Secondo questo approccio, una persona è considerata plusdotata se ha un QI (Quoziente Intellettivo) di 130 o superiore (Robinson, Zigler, & Gallagher, 2000). Anche se l'uso di un QI di 130 come soglia è il criterio più comune, non è universalmente accettato. L'intelligenza nei bambini plusdotati non è solo superiore rispetto a quella dei loro pari, ma è qualitativamente diversa (Lubart & Zenasni, 2010). I bambini plusdotati tendono a mostrare caratteristiche come un vocabolario ampio, un linguaggio molto sviluppato, processi di ragionamento avanzati, eccellente memoria, vasti interessi, forte curiosità, empatia, capacità di leadership, abilità visive elevate, impegno in situazioni sfidanti e un forte senso di giustizia (Song & Porath, 2005).

Nella simulazione che seguirà, assumeremo che i dati provengano da una distribuzione normale. Per semplicità, considereremo che la deviazione standard sia nota e pari a 5. Il parametro della media sarà l'oggetto della nostra inferenza.

## Dati

Supponiamo di avere un campione di 10 osservazioni. I dati saranno generati casualmente da una distribuzione normale con media 130 e deviazione standard 5.

```{r}
set.seed(123) # Per la riproducibilità
vera_media <- 130 # Media vera
sigma_conosciuta <- 5 # Deviazione standard conosciuta
dimensione_campione <- 10 # Dimensione del campione

# Generare un campione
campione <- round(rnorm(n = dimensione_campione, mean = vera_media, sd = sigma_conosciuta))
campione
```

## Griglia

Creiamo ora una griglia di 100 valori compresi tra 110 e 150.

```{r}
mu_griglia <- seq(110, 150, length.out = 100)
mu_griglia
```

## Calcolo della Verosimiglianza

Per ogni valore della griglia, calcoliamo la verosimiglianza complessiva come prodotto delle densità di probabilità.

```{r}
likelihood <- sapply(mu_griglia, function(mu) {
    prod(dnorm(campione, mean = mu, sd = sigma_conosciuta))
})
likelihood
```

## Calcolo della Distribuzione a Posteriori

Impostiamo una prior uniforme e calcoliamo la distribuzione a posteriori normalizzata.

```{r}
prior <- rep(1, length(mu_griglia)) # Prior uniforme
posterior_non_norm <- likelihood * prior
posterior <- posterior_non_norm / sum(posterior_non_norm) # Normalizzazione
```

Visualizzazione:

```{r}
# Creazione del dataframe per il plot
dat <- tibble(
  mu_griglia = mu_griglia, # Ascissa
  posterior = posterior    # Ordinata
)

# Grafico con ggplot2
dat |>
  ggplot(aes(x = mu_griglia, y = posterior)) +
  geom_line(size = 1.2, color = MetBrewer::met.brewer("Hiroshige", 10)[1]) + 
  labs(
    title = "Distribuzione a Posteriori della Media",
    x = "Media",
    y = "Probabilità"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5), # Centra il titolo
    legend.position = "none" # Rimuove la legenda (non necessaria per una linea singola)
  )
```

### Aggiunta di una Prior Informativa

Usiamo una prior gaussiana con media 140 e deviazione standard 3.

```{r}
# Calcolo prior, posterior non normalizzato e posterior
prior <- dnorm(mu_griglia, mean = 140, sd = 3)
posterior_non_norm <- likelihood * prior
posterior <- posterior_non_norm / sum(posterior_non_norm)

# Creazione del dataframe per il grafico
dat <- tibble(
  mu_griglia = mu_griglia,
  prior = prior / sum(prior),  # Normalizzazione del prior
  posterior = posterior        # Posterior già normalizzato
)

# Preparazione dei dati in formato lungo per ggplot2
long_data <- dat |>
  pivot_longer(
    cols = c(prior, posterior),
    names_to = "distribution",
    values_to = "density"
  )

# Grafico con ggplot2
long_data |>
  ggplot(aes(x = mu_griglia, y = density, color = distribution, linetype = distribution)) +
  geom_line(size = 1.2) +
  scale_colour_manual(
    values = MetBrewer::met.brewer("Hiroshige", 10)[1:2], # Usa i primi due colori della palette
    labels = c("Prior", "Posterior"),
    breaks = c("prior", "posterior")
  ) +
  scale_linetype_manual(
    values = c("prior" = 2, "posterior" = 1), # Linea tratteggiata per il prior
    labels = c("Prior", "Posterior"),
    breaks = c("prior", "posterior")
  ) +
  labs(
    title = "Distribuzione a Posteriori e Prior della Media",
    x = "Media",
    y = "Densità",
    color = "Distribuzione",
    linetype = "Distribuzione"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

## Campionamento dalla Posterior

Generiamo un campione dalla distribuzione a posteriori.

```{r}
# Set seed for reproducibility
set.seed(123)

# Sampling from the posterior distribution
indice_campionato <- sample(1:length(mu_griglia), size = 1000, replace = TRUE, prob = posterior)
media_campionata <- mu_griglia[indice_campionato]

# Create a dataframe for ggplot2
sample_df <- tibble(media_campionata = media_campionata)

# Histogram using ggplot2
ggplot(sample_df, aes(x = media_campionata)) +
  geom_histogram(
    bins = 20, 
    fill = MetBrewer::met.brewer("Hiroshige", 10)[3], # Use a color from MetBrewer
    color = "white", 
    alpha = 0.8
  ) +
  labs(
    title = "Campionamento dalla Posterior",
    x = "Media",
    y = "Frequenza"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```

```{r}
# Media e intervallo di credibilità
mean(media_campionata)
quantile(media_campionata, c(0.03, 0.97))
```

## Calcolo della Log-Verosimiglianza

Utilizziamo i logaritmi per migliorare la stabilità numerica.

```{r}
# Calcolo log-likelihood, log-prior e posterior
log_likelihood <- sapply(mu_griglia, function(mu) {
  sum(dnorm(campione, mean = mu, sd = sigma_conosciuta, log = TRUE))
})
log_prior <- dnorm(mu_griglia, mean = 140, sd = 3, log = TRUE)
log_posterior_non_norm <- log_likelihood + log_prior
log_posterior <- log_posterior_non_norm - max(log_posterior_non_norm) # Stabilizzazione numerica
posterior <- exp(log_posterior) / sum(exp(log_posterior))

# Creazione del dataframe per il grafico
dat <- tibble(
  mu_griglia = mu_griglia,
  posterior = posterior
)

# Grafico con ggplot2
dat |>
  ggplot(aes(x = mu_griglia, y = posterior)) +
  geom_line(size = 1.2, color = MetBrewer::met.brewer("Hiroshige", 10)[3]) + # Usa il terzo colore della palette
  labs(
    title = "Distribuzione a Posteriori con Log-Verosimiglianza",
    x = "Media",
    y = "Probabilità"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5), # Centra il titolo
    legend.position = "none"               # Rimuove la legenda per una linea singola
  )
```

## Estensione alla Deviazione Standard Ignota

Per una griglia bidimensionale di valori di $\mu$ e $\sigma$:

```{r}
# Define the grid for mu and sigma
mu_griglia <- seq(110, 150, length.out = 100)
sigma_griglia <- seq(1, 10, length.out = 50)

# Create combinations of mu and sigma using expand.grid
grid <- expand.grid(mu = mu_griglia, sigma = sigma_griglia)

# Compute the log-likelihood for each combination of mu and sigma
log_likelihood <- apply(grid, 1, function(params) {
    mu <- params[1]
    sigma <- params[2]
    sum(dnorm(campione, mean = mu, sd = sigma, log = TRUE))
})

# Reshape log-likelihood into a matrix
log_likelihood_2d <- matrix(log_likelihood, nrow = length(mu_griglia), ncol = length(sigma_griglia))

# Compute priors for mu and sigma
log_prior_mu <- dnorm(mu_griglia, mean = 140, sd = 5, log = TRUE)
log_prior_sigma <- dnorm(sigma_griglia, mean = 5, sd = 2, log = TRUE)

# Combine priors into a grid
log_prior_2d <- outer(log_prior_mu, log_prior_sigma, "+")

# Compute log-posterior
log_posterior_2d <- log_likelihood_2d + log_prior_2d
log_posterior_2d <- log_posterior_2d - max(log_posterior_2d) # Stabilize
posterior_2d <- exp(log_posterior_2d)
posterior_2d <- posterior_2d / sum(posterior_2d) # Normalize

# Convert posterior_2d to a data frame for visualization
posterior_df <- melt(posterior_2d)
names(posterior_df) <- c("mu_idx", "sigma_idx", "posterior")
posterior_df$mu <- mu_griglia[posterior_df$mu_idx]
posterior_df$sigma <- sigma_griglia[posterior_df$sigma_idx]

# Plot the posterior distribution
ggplot(posterior_df, aes(x = mu, y = sigma, fill = posterior)) +
  geom_tile() +
  scale_fill_gradientn(
    colours = MetBrewer::met.brewer("Hiroshige", 10), # Use MetBrewer palette
    name = "Posterior" # Legend title
  ) +
  labs(
    title = "Distribuzione a Posteriori Bidimensionale",
    x = expression(mu), 
    y = expression(sigma)
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "right"
  )
```

## Riflessioni Conclusive

Quando si passa alla stima simultanea di più parametri, come la media ($\mu$) e la deviazione standard ($\sigma$), l'analisi diventa notevolmente più complessa. Questo perché occorre considerare un numero molto maggiore di combinazioni di parametri rispetto alla stima di un solo parametro, aumentando così il carico computazionale. Inoltre, la scelta delle priors per ciascun parametro richiede particolare attenzione, poiché queste influenzeranno in modo diretto le stime a posteriori.

In scenari dove lo spazio dei parametri è multidimensionale o quando l'esplorazione della griglia diventa impraticabile, l'uso di metodi avanzati come il campionamento di Markov Chain Monte Carlo (MCMC) diventa indispensabile. Questi metodi permettono di campionare in modo efficiente dalla distribuzione a posteriori, senza la necessità di esplorare esplicitamente ogni combinazione possibile di parametri, rendendo l'analisi più gestibile anche in contesti complessi.

In conclusione, l'estensione dell'approccio bayesiano a problemi con più parametri sconosciuti richiede un'attenzione ancora maggiore nella definizione dello spazio dei parametri, nella selezione delle priors appropriate e nel calcolo delle distribuzioni a posteriori. L'adozione di tecniche come l'MCMC può facilitare questo processo, permettendo di affrontare in modo efficiente problemi che altrimenti sarebbero proibitivi dal punto di vista computazionale.

## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

