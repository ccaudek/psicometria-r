<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>74&nbsp; Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-3e8fa383bad517095c2b42029d2b9125.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-453635e726936aaf7fcc673d3ddfe9d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelli cognitivi</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori (Prior Predictive Check)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#distribuzione-predittiva-posteriore" id="toc-distribuzione-predittiva-posteriore" class="nav-link active" data-scroll-target="#distribuzione-predittiva-posteriore"><span class="header-section-number">74.1</span> Distribuzione Predittiva Posteriore</a></li>
  <li><a href="#la-sfida-della-valutazione-predittiva" id="toc-la-sfida-della-valutazione-predittiva" class="nav-link" data-scroll-target="#la-sfida-della-valutazione-predittiva"><span class="header-section-number">74.2</span> La sfida della valutazione predittiva</a></li>
  <li><a href="#expected-log-predictive-density-elpd" id="toc-expected-log-predictive-density-elpd" class="nav-link" data-scroll-target="#expected-log-predictive-density-elpd"><span class="header-section-number">74.3</span> Expected Log Predictive Density (ELPD)</a></li>
  <li><a href="#lppd-vs-elpd" id="toc-lppd-vs-elpd" class="nav-link" data-scroll-target="#lppd-vs-elpd"><span class="header-section-number">74.4</span> LPPD vs ELPD</a></li>
  <li><a href="#legame-con-la-divergenza-kl" id="toc-legame-con-la-divergenza-kl" class="nav-link" data-scroll-target="#legame-con-la-divergenza-kl"><span class="header-section-number">74.5</span> Legame con la Divergenza KL</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv" id="toc-leave-one-out-cross-validation-loo-cv" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv"><span class="header-section-number">74.6</span> Leave-One-Out Cross-Validation (LOO-CV)</a></li>
  <li><a href="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" id="toc-criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="nav-link" data-scroll-target="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl"><span class="header-section-number">74.7</span> Criteri di Informazione come Approssimazioni della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span></a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">74.8</span> Riflessioni Conclusive</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: LPPD, ELPD e il Log-Score</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi di apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alla fine di questo capitolo, sarai in grado di:</p>
<ul>
<li>comprendere in modo dettagliato la distribuzione predittiva posteriore, la divergenza <span class="math inline">\(D_{\text{KL}}\)</span>, il Log-Pointwise-Predictive-Density (LPPD) e la Densità Predittiva Logaritmica Attesa (ELPD).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>In questo capitolo esploreremo gli strumenti essenziali per valutare e confrontare i modelli statistici in un contesto bayesiano. Comprenderemo come questi strumenti ci aiutino a misurare l’efficacia di un modello nel rappresentare i dati osservati e, soprattutto, la sua capacità di generalizzare a nuovi dati.</p>
<p>Approfondiremo i seguenti concetti fondamentali:</p>
<ul>
<li>La <em>distribuzione predittiva posteriore</em>: come utilizzare il modello per fare previsioni su nuove osservazioni, tenendo conto dell’incertezza dei parametri.</li>
<li>Il <em>log-score</em> e la LPPD (<em>Log Pointwise Predictive Density</em>): strumenti per quantificare quanto bene il modello “spiega” ogni singolo punto dato.</li>
<li>L’ELPD (<em>Expected Log Predictive Density</em>): una misura cruciale della capacità del modello di fare previsioni accurate su dati futuri e sconosciuti.</li>
<li>Il ruolo implicito della <em>divergenza di Kullback-Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>). Come questa misura di “distanza” tra distribuzioni è intrinsecamente legata alla valutazione del modello.</li>
</ul>
<p>Infine, concluderemo il capitolo discutendo il concetto di <em>Leave-One-Out Cross-Validation</em> (LOO-CV), una tecnica robusta per stimare l’ELPD e valutare in modo efficiente la generalizzabilità del modello.</p>
<section id="distribuzione-predittiva-posteriore" class="level2" data-number="74.1"><h2 data-number="74.1" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore">
<span class="header-section-number">74.1</span> Distribuzione Predittiva Posteriore</h2>
<p>Nel contesto bayesiano, ogni previsione si basa non su un singolo valore dei parametri <span class="math inline">\(\theta\)</span>, ma sull’intera distribuzione posteriore <span class="math inline">\(p(\theta \mid y)\)</span>. Per questo, la distribuzione predittiva posteriore per nuovi dati <span class="math inline">\(\tilde{y}\)</span> è definita come:</p>
<p><span class="math display">\[
q(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta.
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Spiegazione intuitiva">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Spiegazione intuitiva
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Immagina di voler fare una previsione su un evento futuro, <span class="math inline">\(\tilde{y}\)</span> (i tuoi nuovi dati), dopo aver osservato dei dati passati, <span class="math inline">\(y\)</span>. Nel mondo bayesiano, non ci accontentiamo di un unico “miglior” valore per i parametri del nostro modello (come faresti con una stima puntuale classica), perché sappiamo che c’è sempre una certa incertezza su quali siano i “veri” parametri che governano il fenomeno.</p>
<p>Ecco come funziona la <em>distribuzione predittiva posteriore</em> <span class="math inline">\(q(\tilde{y} \mid y)\)</span> in modo più intuitivo:</p>
<ol type="1">
<li><p><strong>Non crediamo a un solo valore dei parametri:</strong> Dopo aver osservato i dati <span class="math inline">\(y\)</span>, il nostro modello bayesiano non ci dice “il parametro è esattamente X”. Ci fornisce invece una <em>distribuzione di probabilità sui parametri, <span class="math inline">\(p(\theta \mid y)\)</span></em>, che riflette la nostra conoscenza aggiornata. Questa distribuzione ci dice quanto è probabile ciascun possibile valore di <span class="math inline">\(\theta\)</span> (o combinazione di valori, se <span class="math inline">\(\theta\)</span> è un vettore) alla luce dei dati che abbiamo visto.</p></li>
<li><p><strong>Ogni “mondo” possibile genera previsioni diverse:</strong> Per ogni singolo valore (o combinazione di valori) che <span class="math inline">\(\theta\)</span> potrebbe assumere, il nostro modello genera una <strong>previsione su <span class="math inline">\(\tilde{y}\)</span></strong>, ovvero una distribuzione <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span>. Questo è il “meccanismo generatore” dei dati: se sapessimo con certezza <span class="math inline">\(\theta\)</span>, sapremmo come <span class="math inline">\(\tilde{y}\)</span> si comporterebbe.</p></li>
<li><p><strong>Facciamo una “media ponderata” delle previsioni:</strong> Poiché non sappiamo con certezza quale sia il vero <span class="math inline">\(\theta\)</span>, la distribuzione predittiva posteriore <span class="math inline">\(q(\tilde{y} \mid y)\)</span> combina tutte le possibili previsioni di <span class="math inline">\(\tilde{y}\)</span> (quelle generate da ogni possibile <span class="math inline">\(\theta\)</span>) in un’unica previsione complessiva. Questa combinazione non è una semplice media aritmetica. È una <em>media ponderata</em>, dove il “peso” assegnato a ciascuna previsione <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span> è dato dalla probabilità <span class="math inline">\(p(\theta \mid y)\)</span> che quel particolare <span class="math inline">\(\theta\)</span> sia il vero valore, in base ai dati che abbiamo già osservato.</p></li>
</ol>
<p>In altre parole, stiamo dicendo:</p>
<blockquote class="blockquote">
<p>“Prevedo <span class="math inline">\(\tilde{y}\)</span> considerando ogni singolo scenario possibile per i miei parametri <span class="math inline">\(\theta\)</span>. Se uno scenario <span class="math inline">\(\theta_1\)</span> è molto probabile (ha un alto <span class="math inline">\(p(\theta_1 \mid y)\)</span>), allora la previsione che deriva da quello scenario <span class="math inline">\(p(\tilde{y} \mid \theta_1)\)</span> avrà un grande impatto sulla mia previsione finale <span class="math inline">\(q(\tilde{y} \mid y)\)</span>. Se invece uno scenario <span class="math inline">\(\theta_2\)</span> è molto improbabile (ha un basso <span class="math inline">\(p(\theta_2 \mid y)\)</span>), la sua previsione <span class="math inline">\(p(\tilde{y} \mid \theta_2)\)</span> avrà un impatto minimo.”</p>
</blockquote>
<p>L’integrale matematico <span class="math inline">\(\int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta\)</span> rappresenta proprio questa <em>somma continua di previsioni ponderate</em>. Il risultato è una distribuzione <span class="math inline">\(q(\tilde{y} \mid y)\)</span> che non solo predice i valori più probabili per <span class="math inline">\(\tilde{y}\)</span>, ma incorpora anche tutta l’incertezza derivante dalla nostra ignoranza sui veri parametri del modello. È la nostra “migliore ipotesi” su come si comporteranno i nuovi dati, tenendo conto di tutto ciò che abbiamo imparato dai dati passati e di tutta l’incertezza residua.</p>
</div>
</div>
</div>
<p>In questo capitolo useremo a volte <span class="math inline">\(q(\cdot \mid y)\)</span> per indicare genericamente la distribuzione predittiva posteriore del modello, ma nella maggior parte dei casi adotteremo la notazione più esplicita <span class="math inline">\(p(y_i \mid y)\)</span>, per evidenziare che si tratta di una previsione marginale, ottenuta integrando la distribuzione dei dati condizionata ai parametri (<span class="math inline">\(p(y_i \mid \theta)\)</span>) sulla distribuzione posteriore dei parametri (<span class="math inline">\(p(\theta \mid y)\)</span>).</p>
</section><section id="la-sfida-della-valutazione-predittiva" class="level2" data-number="74.2"><h2 data-number="74.2" class="anchored" data-anchor-id="la-sfida-della-valutazione-predittiva">
<span class="header-section-number">74.2</span> La sfida della valutazione predittiva</h2>
<p>Idealmente, vorremmo sapere quanto bene questa distribuzione predittiva <span class="math inline">\(q(\tilde{y} \mid y)\)</span> si avvicina alla distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, ovvero la distribuzione che avrebbe generato i nuovi dati futuri <span class="math inline">\(\tilde{y}\)</span>, se la conoscessimo. Questo confronto teorico si basa sulla <em>divergenza di Kullback-Leibler</em>, che abbiamo studiato nel <a href="02_kl.html" class="quarto-xref"><span>Capitolo 73</span></a>.</p>
<p>Tuttavia, <em>non conosciamo <span class="math inline">\(p(\tilde{y})\)</span></em>. Non possiamo quindi calcolare direttamente <span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span>. Per superare questo limite, ci serviamo di misure che stimano <em>indirettamente</em> la bontà del modello. Le due più importanti sono il <em>log-score</em> e le sue versioni bayesiane: <em>LPPD</em> ed <em>ELPD</em>.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled" title="Notazione utilizzata">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notazione utilizzata
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nel capitolo usiamo la seguente notazione:</p>
<ul>
<li>
<span class="math inline">\(y\)</span>: dati osservati;</li>
<li>
<span class="math inline">\(\tilde{y}\)</span>: dati futuri (da predire);</li>
<li>
<span class="math inline">\(y_i\)</span>: singola osservazione nei dati;</li>
<li>
<span class="math inline">\(p(y_i \mid \theta)\)</span>: distribuzione dei dati condizionata ai parametri;</li>
<li>
<span class="math inline">\(p(\theta \mid y)\)</span>: distribuzione posteriore dei parametri;</li>
<li>
<span class="math inline">\(p(y_i \mid y)\)</span>: distribuzione predittiva posteriore per <span class="math inline">\(y_i\)</span> (media su <span class="math inline">\(p(y_i \mid \theta)\)</span>);</li>
<li>
<span class="math inline">\(q(\cdot \mid y)\)</span>: notazione alternativa per il modello predittivo bayesiano, usata nei confronti tra modelli.</li>
</ul>
</div>
</div>
<section id="il-log-score" class="level3" data-number="74.2.1"><h3 data-number="74.2.1" class="anchored" data-anchor-id="il-log-score">
<span class="header-section-number">74.2.1</span> Il Log-Score</h3>
<p>Il <em>log-probability score</em> (o <em>log-score</em>) è una misura pratica che possiamo utilizzare per valutare i modelli in assenza della conoscenza della distribuzione vera <span class="math inline">\(p(y)\)</span>. Per ogni osservazione <span class="math inline">\(y_i\)</span>, si calcola il logaritmo della probabilità che il modello le assegna:</p>
<p><span class="math display">\[
S = \sum_{i=1}^n \log p(y_i \mid y).
\]</span></p>
<p>dove <span class="math inline">\(p(y_i \mid y)\)</span> è la distribuzione predittiva posteriore per l’osservazione <span class="math inline">\(y_i\)</span>, ottenuta integrando su <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<p>Più la probabilità assegnata alle osservazioni è alta, meno negativo sarà il log-score. Un <em>log-score più alto (meno negativo)</em> indica un modello che fa previsioni migliori, poiché il modello assegnerebbe una probabilità maggiore agli eventi che si verificano effettivamente.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.1</strong></span> Consideriamo un semplice esempio numerico per illustrare il calcolo del <em>log-score</em> per un modello <span class="math inline">\(q\)</span>. Immaginiamo di avere un piccolo dataset con 3 osservazioni, e che il modello <span class="math inline">\(q\)</span> predica le probabilità per ciascuna osservazione come segue:</p>
<ul>
<li>Osservazione 1: <span class="math inline">\(q_1 = 0.8\)</span>,</li>
<li>Osservazione 2: <span class="math inline">\(q_2 = 0.6\)</span>,</li>
<li>Osservazione 3: <span class="math inline">\(q_3 = 0.7\)</span>.</li>
</ul>
<p>Il <em>log-score</em> è:</p>
<p><span class="math display">\[
\begin{align}
S(q) &amp;= \log(0.8) + \log(0.6) + \log(0.7) \approx -0.2231 + (-0.5108) + (-0.3567) \notag\\
&amp;= -1.0906.
\end{align}
\]</span></p>
<p>Il log-score totale per questo modello <span class="math inline">\(q\)</span> è <span class="math inline">\(-1.0906\)</span>. Poiché un log-score più alto (meno negativo) indica una migliore accuratezza predittiva, questo risultato suggerisce che <span class="math inline">\(q\)</span> ha una discreta accuratezza per le osservazioni date. Un altro modello con log-score meno negativo sarebbe da preferire.</p>
</div>
</section><section id="come-calcolare-il-log-score-in-pratica-con-mcmc" class="level3" data-number="74.2.2"><h3 data-number="74.2.2" class="anchored" data-anchor-id="come-calcolare-il-log-score-in-pratica-con-mcmc">
<span class="header-section-number">74.2.2</span> Come calcolare il log-score in pratica (con MCMC)</h3>
<p>Nel calcolo teorico, il log-score richiede la <em>probabilità predittiva</em> per ogni osservazione <span class="math inline">\(y_i\)</span>, ovvero:</p>
<p><span class="math display">\[
p(y_i \mid y) = \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Spiegazione intuitiva">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Spiegazione intuitiva
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>L’espressione</p>
<p><span class="math display">\[
  p(y_i \mid y) = \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta
\]</span></p>
<p>definisce la <em>distribuzione predittiva bayesiana</em> per l’osservazione <span class="math inline">\(y_i\)</span> <em>dopo aver osservato i dati</em> <span class="math inline">\(y\)</span>.</p>
<p>Per comprenderla, possiamo analizzarne i due componenti:</p>
<ul>
<li><p><span class="math inline">\(p(y_i \mid \theta)\)</span> è la <em>distribuzione dei dati futuri</em> (o di una nuova osservazione) <em>condizionata a un valore specifico dei parametri <span class="math inline">\(\theta\)</span></em>. → Descrive <em>cosa si aspetta il modello</em> che accada, <em>se</em> i parametri fossero proprio <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(p(\theta \mid y)\)</span> è la <em>distribuzione a posteriori dei parametri del modello</em>, ottenuta <em>dopo aver osservato i dati</em> <span class="math inline">\(y\)</span>. → Rappresenta la nostra <em>incertezza residua</em> su quali siano i veri valori dei parametri.</p></li>
</ul>
<p>L’integrale esprime il fatto che:</p>
<blockquote class="blockquote">
<p>Per fare una previsione su <span class="math inline">\(y_i\)</span>, <em>non fissiamo un singolo valore di <span class="math inline">\(\theta\)</span></em>, ma consideriamo <em>tutti i valori plausibili</em> (secondo la posteriore) e <em>facciamo una media ponderata</em> delle previsioni condizionate <span class="math inline">\(p(y_i \mid \theta)\)</span>.</p>
</blockquote>
<p>In altre parole:</p>
<blockquote class="blockquote">
<p>La distribuzione predittiva <span class="math inline">\(p(y_i \mid y)\)</span> è una <em>media (o somma continua)</em> delle distribuzioni <span class="math inline">\(p(y_i \mid \theta)\)</span>, <em>pesate</em> in base a quanto crediamo che ogni <span class="math inline">\(\theta\)</span> sia plausibile <em>dopo aver visto i dati</em>, cioè secondo <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
</blockquote>
</div>
</div>
</div>
<p>Ma questo integrale non ha una soluzione analitica, quindi in pratica <em>lo approssimiamo usando i campioni <span class="math inline">\(\theta^{(s)}\)</span> dalla distribuzione posteriore</em>, ottenuti con MCMC.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Spiegazione intuitiva">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Spiegazione intuitiva
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Quando <span class="math inline">\(p(\theta \mid y)\)</span> è troppo complessa per essere integrata analiticamente, possiamo <em>approssimare l’integrale predittivo</em> con una media sui campioni MCMC:</p>
<p><span class="math display">\[
p(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}), \quad \theta^{(s)} \sim p(\theta \mid y)
\]</span></p>
<p>In questo caso, ogni <span class="math inline">\(\theta^{(s)}\)</span> rappresenta una <em>possibile configurazione dei parametri</em> coerente con i dati osservati. Calcoliamo la probabilità di <span class="math inline">\(y_i\)</span> per ciascun campione, e poi facciamo la <em>media semplice</em> di queste previsioni — trattando i campioni come <em>equamente probabili</em>.</p>
<p><strong>Procedura passo-passo.</strong></p>
<p>Supponiamo di avere <span class="math inline">\(S\)</span> campioni posteriori <span class="math inline">\(\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(S)}\)</span>.<br>
Per ogni osservazione <span class="math inline">\(y_i\)</span>:</p>
<ol type="1">
<li><p><em>Calcola la probabilità condizionata</em> di <span class="math inline">\(y_i\)</span> dato ciascun campione: <span class="math display">\[
p(y_i \mid \theta^{(s)}) \quad \text{per } s = 1, \dots, S
\]</span></p></li>
<li><p><em>Fai la media</em> su tutti i campioni: <span class="math display">\[
\widehat{p}(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)})
\]</span></p></li>
<li><p><em>Applica il logaritmo</em> alla media: <span class="math display">\[
\log \widehat{p}(y_i \mid y)
\]</span></p></li>
<li><p><em>Somma su tutte le osservazioni</em> per ottenere il log-score: <span class="math display">\[
\text{Log-score} \approx \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right)
\]</span></p></li>
</ol>
<p>Questa quantità misura <em>quanto bene il modello predice i dati osservati</em>, tenendo conto dell’incertezza sui parametri stimata dalla distribuzione posteriore.</p>
</div>
</div>
</div>
<p>Ci stiamo chiedendo: “Quanto è probabile che il modello, in media sulla posteriore, consideri <span class="math inline">\(y_i\)</span>?”</p>
<ul>
<li>Se il modello <em>assegna alta probabilità</em> a ciò che è stato effettivamente osservato (<span class="math inline">\(y_i\)</span>), allora il logaritmo sarà <em>meno negativo</em>.</li>
<li>Se il modello è incerto o predice male <span class="math inline">\(y_i\)</span>, il valore sarà molto negativo.</li>
</ul>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.2</strong></span> Per fare un esempio numerico, calcoliamo il log-score con un modello binomiale e posteriore discretizzato. Vogliamo valutare <em>quanto bene</em> un modello binomiale prevede i dati osservati <span class="math inline">\(y = (y_1, y_2, \dots, y_n)\)</span>, usando il <em>log-score</em>:</p>
<p><span class="math display">\[
\text{Log-score} = \sum_{i=1}^n \log p(y_i \mid y)
\]</span></p>
<p>Poiché <span class="math inline">\(p(y_i \mid y)\)</span> è una media su <span class="math inline">\(p(y_i \mid \theta)\)</span> pesata dalla posteriore <span class="math inline">\(p(\theta \mid y)\)</span>, useremo un’approssimazione <em>discreta</em> per semplificare.</p>
<p>Scenario:</p>
<ul>
<li>osserviamo 3 successi su 5 tentativi: <span class="math inline">\(y = 3\)</span>, <span class="math inline">\(n = 5\)</span>;</li>
<li>usiamo un modello binomiale: <span class="math inline">\(y \sim \text{Binom}(n = 5, \theta)\)</span>;</li>
<li>scegliamo un prior uniforme su <span class="math inline">\(\theta\)</span> tra 0 e 1;</li>
<li>per semplicità, <em>discretizziamo</em> la distribuzione a posteriori su soli <em>3 valori di <span class="math inline">\(\theta\)</span></em>:</li>
</ul>
<table class="caption-top table">
<thead><tr class="header">
<th>Valore <span class="math inline">\(\theta^{(s)}\)</span>
</th>
<th>Peso <span class="math inline">\(p(\theta^{(s)} \mid y)\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>0.7</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>Questi rappresentano un’approssimazione <em>semplificata</em> della posteriore dopo aver osservato 3 successi su 5.</p>
<p>Supponiamo di voler calcolare il log-score per una nuova osservazione <span class="math inline">\(y\_{\text{new}} = 3\)</span> (cioè 3 successi su 5, come nei dati osservati). Vogliamo calcolare:</p>
<p><span class="math display">\[
\log \left[ \sum_{s=1}^3 p(y_{\text{new}} = 3 \mid \theta^{(s)}) \cdot p(\theta^{(s)} \mid y) \right]
\]</span></p>
<p><em>Passaggio 1:</em> calcolo delle verosimiglianze. Usiamo la formula binomiale:</p>
<p><span class="math display">\[
p(y = 3 \mid \theta) = \binom{5}{3} \cdot \theta^3 (1 - \theta)^2
\]</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 81%">
</colgroup>
<thead><tr class="header">
<th><span class="math inline">\(\theta^{(s)}\)</span></th>
<th><span class="math inline">\(p(y = 3 \mid \theta^{(s)})\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0.3</td>
<td>
<span class="math inline">\(\binom{5}{3} \cdot 0.3^3 \cdot 0.7^2\)</span> = 10 · 0.027 · 0.49 ≈ 0.1323</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>10 · 0.125 · 0.25 = 0.3125</td>
</tr>
<tr class="odd">
<td>0.7</td>
<td>10 · 0.343 · 0.09 = 0.3087</td>
</tr>
</tbody>
</table>
<p><em>Passaggio 2:</em> moltiplichiamo per i pesi posteriori:</p>
<p><span class="math display">\[
\begin{align*}
p(y = 3 \mid y) &amp;\approx 0.2 \cdot 0.1323 + 0.5 \cdot 0.3125 + 0.3 \cdot 0.3087 \\
&amp;= 0.0265 + 0.1563 + 0.0926 \\
&amp;= 0.2754
\end{align*}
\]</span></p>
<p><em>Passaggio 3:</em> calcoliamo il logaritmo:</p>
<p><span class="math display">\[
\log p(y = 3 \mid y) \approx \log(0.2754) \approx -1.289.
\]</span></p>
<p><em>Risultato:</em> il <em>log-score</em> per questa osservazione è circa <em>–1.29</em>. Se lo ripetessimo per più osservazioni (<span class="math inline">\(y_1, y_2, \dots, y_n\)</span>), sommeremmo i logaritmi ottenuti:</p>
<p><span class="math display">\[
\text{Log-score} = \sum_{i=1}^n \log \left( \sum_{s=1}^3 p(y_i \mid \theta^{(s)}) \cdot p(\theta^{(s)} \mid y) \right).
\]</span></p>
<p>Intuizione finale:</p>
<ul>
<li>il <em>log-score</em> misura <em>quanto il modello (in media sulla posteriore) “si aspettava” i dati osservati</em>;</li>
<li>un valore <em>più alto</em> (cioè meno negativo) indica che il modello <em>assegna più probabilità</em> all’evento osservato → <em>previsione migliore</em>;</li>
<li>snche se l’integrale è complesso, possiamo <em>approssimarlo con pochi punti posteriori</em> per capirne il significato.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati osservati</span></span>
<span><span class="va">y_obs</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">n_trials</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span></span>
<span><span class="co"># Tre valori discreti di theta (approssimazione della posteriore)</span></span>
<span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Probabilità posteriori associate (devono sommare a 1)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcola la verosimiglianza p(y | theta) per ciascun theta</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_obs</span>, size <span class="op">=</span> <span class="va">n_trials</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcola la previsione marginale p(y | y)</span></span>
<span><span class="va">p_y_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">posterior_weights</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcola il log-score</span></span>
<span><span class="va">log_score</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_y_given_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Output</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Valori di theta:"</span>, <span class="va">theta_vals</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Valori di theta: 0.3 0.5 0.7</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Pesi posteriori:"</span>, <span class="va">posterior_weights</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Pesi posteriori: 0.2 0.5 0.3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Verosimiglianze:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">likelihoods</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Verosimiglianze: 0.1323 0.3125 0.3087</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Predizione marginale:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_y_given_y</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Predizione marginale: 0.2753</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Log-score:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">log_score</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Log-score: -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section></section><section id="expected-log-predictive-density-elpd" class="level2" data-number="74.3"><h2 data-number="74.3" class="anchored" data-anchor-id="expected-log-predictive-density-elpd">
<span class="header-section-number">74.3</span> Expected Log Predictive Density (ELPD)</h2>
<p>Il <em>Log Pointwise Predictive Density</em> (LPPD) misura quanto bene il modello si adatta ai dati <em>osservati</em>. Ma come possiamo sapere se il modello è in grado di <em>generalizzare</em> bene a nuovi dati?</p>
<p>Qui entra in gioco l’<em>Expected Log Predictive Density (ELPD)</em>, che valuta la capacità del modello di predire dati <em>non visti</em>. Si calcola tramite la tecnica della <em>Leave-One-Out Cross-Validation (LOO-CV)</em>: si esclude a turno ciascuna osservazione, si adatta il modello ai dati rimanenti e si calcola la densità predittiva dell’osservazione esclusa.</p>
<p>La formula è:</p>
<p><span class="math display">\[
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i}),
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span> è l’<span class="math inline">\(i\)</span>-esima osservazione,</li>
<li>
<span class="math inline">\(\mathbf{y}_{-i}\)</span> rappresenta tutte le osservazioni <em>eccetto</em> <span class="math inline">\(y_i\)</span>.</li>
</ul>
<p>Un <em>ELPD più alto</em> indica che il modello riesce a predire accuratamente anche osservazioni non utilizzate per stimare i parametri, suggerendo una buona capacità di generalizzazione.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Confronto con il log-score">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confronto con il log-score
</div>
</div>
<div class="callout-body-container callout-body">
<p>L’ELPD può essere interpretato come una <em>versione più cauta del log-score</em>. Mentre il log-score si basa sulla probabilità assegnata a ciascuna osservazione <em>utilizzando l’intero campione</em>, l’ELPD calcola la stessa quantità <em>escludendo</em> l’osservazione da predire.</p>
<p>In altre parole:</p>
<blockquote class="blockquote">
<p><em>l’ELPD è un log-score calcolato in modo out-of-sample</em>.</p>
</blockquote>
<p>Questa strategia riduce il rischio di overfitting e fornisce una stima più realistica della qualità predittiva del modello.</p>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.3</strong></span> Per illustrare il calcolo dell’ELPD, vediamo un esempio semplice con un set di dati molto piccolo. Supponiamo di avere un dataset di tre osservazioni: <span class="math inline">\(y_1, y_2, y_3\)</span>. Supponiamo che il nostro modello stimi le probabilità per ciascuna osservazione in base a tutte le altre osservazioni, cioè utilizziamo la leave-one-out cross-validation (LOO-CV) per calcolare <span class="math inline">\(p(y_i \mid \mathbf{y}_{-i})\)</span>.</p>
<p>Immaginiamo che il modello produca le seguenti probabilità condizionali per ogni osservazione <span class="math inline">\(y_i\)</span>:</p>
<ul>
<li>
<span class="math inline">\(p(y_1 \mid y_2, y_3) = 0.6\)</span>,</li>
<li>
<span class="math inline">\(p(y_2 \mid y_1, y_3) = 0.7\)</span>,</li>
<li>
<span class="math inline">\(p(y_3 \mid y_1, y_2) = 0.5\)</span>.</li>
</ul>
<p>L’ELPD si calcola sommando i logaritmi di queste probabilità:</p>
<p><span class="math display">\[
\text{ELPD} = \log p(y_1 \mid y_2, y_3) + \log p(y_2 \mid y_1, y_3) + \log p(y_3 \mid y_1, y_2).
\]</span></p>
<p>Calcoliamo i logaritmi naturali di ciascuna probabilità:</p>
<ul>
<li>
<span class="math inline">\(\log p(y_1 \mid y_2, y_3) = \log 0.6 \approx -0.5108\)</span>,</li>
<li>
<span class="math inline">\(\log p(y_2 \mid y_1, y_3) = \log 0.7 \approx -0.3567\)</span>,</li>
<li>
<span class="math inline">\(\log p(y_3 \mid y_1, y_2) = \log 0.5 \approx -0.6931\)</span>.</li>
</ul>
<p>Sommiamo i logaritmi per ottenere l’ELPD:</p>
<p><span class="math display">\[
\text{ELPD} = -0.5108 + (-0.3567) + (-0.6931) = -1.5606.
\]</span></p>
<p>L’ELPD ottenuto è <span class="math inline">\(-1.5606\)</span>. In generale, valori più vicini a 0 o positivi indicano una migliore capacità predittiva del modello, poiché suggeriscono che le probabilità condizionali assegnate dal modello alle osservazioni lasciate fuori non sono troppo basse. Valori molto negativi indicherebbero che il modello ha assegnato probabilità molto basse alle osservazioni effettivamente osservate, suggerendo una scarsa capacità predittiva. L’ELPD è un modo efficace per valutare quanto bene un modello generalizza a nuovi dati, evitando l’overfitting.</p>
<p>Un altro modello con ELPD meno negativo sarebbe preferibile.</p>
</div>
</section><section id="lppd-vs-elpd" class="level2" data-number="74.4"><h2 data-number="74.4" class="anchored" data-anchor-id="lppd-vs-elpd">
<span class="header-section-number">74.4</span> LPPD vs ELPD</h2>
<ul>
<li>
<em>LPPD</em> valuta quanto bene il modello predice i dati <em>osservati</em>.</li>
<li>
<em>ELPD</em> valuta quanto bene il modello predice <em>nuovi dati</em>.</li>
</ul>
<p>LPPD tende a premiare modelli più complessi (rischio di overfitting). L’ELPD, grazie alla LOO-CV, penalizza l’overfitting e favorisce modelli che generalizzano meglio.</p>
</section><section id="legame-con-la-divergenza-kl" class="level2" data-number="74.5"><h2 data-number="74.5" class="anchored" data-anchor-id="legame-con-la-divergenza-kl">
<span class="header-section-number">74.5</span> Legame con la Divergenza KL</h2>
<p>Sebbene <em>non possiamo calcolare <span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span> direttamente</em>, differenze in LPPD o ELPD <em>approssimano la differenza tra divergenze KL</em> dei modelli. Se il modello <span class="math inline">\(A\)</span> ha un ELPD più alto di quello di <span class="math inline">\(B\)</span>, possiamo concludere che <span class="math inline">\(A\)</span> è “più vicino” alla distribuzione vera dei dati.</p>
<p>In altre parole:</p>
<blockquote class="blockquote">
<p><em>Massimizzare l’ELPD <span class="math inline">\(\approx\)</span> Minimizzare la divergenza KL</em>.</p>
</blockquote>
<p>In sintesi, la distribuzione predittiva posteriori, la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e l’ELPD sono strumenti matematici che ci permettono di valutare la bontà di un modello statistico. La divergenza KL fornisce una misura teoricamente ideale di quanto un modello approssima la vera distribuzione dei dati, mentre l’ELPD offre un’alternativa praticabile che valuta la capacità del modello di fare previsioni su nuovi dati.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.4</strong></span> Consideriamo un secondo esempio per illustrare il concetto di ELPD utilizzando la distribuzione binomiale. Immaginiamo di condurre un esperimento in cui lanciamo una moneta 10 volte e contiamo il numero di volte in cui otteniamo testa. Supponiamo che la vera probabilità di ottenere testa sia 0.6.</p>
<ol type="1">
<li>
<em>Distribuzione reale dei dati:</em> Segue una distribuzione binomiale con 10 lanci e probabilità di successo pari a 0.6: <span class="math inline">\(y \sim \text{Binomial}(10, 0.6).\)</span>
</li>
<li>
<em>Distribuzione stimata dal modello:</em> Il nostro modello ipotizza che la probabilità di ottenere testa sia 0.5, cioè considera la moneta come equa: <span class="math inline">\(p(y \mid \theta) = \text{Binomial}(10, 0.5).\)</span>
</li>
</ol>
<p>Ora procediamo al calcolo dell’ELPD. Nel codice R qui sotto, useremo <code>p</code> per rappresentare la <em>vera</em> probabilità di successo (ad esempio, <span class="math inline">\(p = 0.6\)</span>) e <code>q</code> per rappresentare la probabilità assunta dal <em>modello candidato</em> (es. <span class="math inline">\(q = 0.5\)</span>). Per evitare confusione con la notazione matematica <span class="math inline">\(p(y)\)</span> vista nel testo, teniamo presente che in questo contesto <code>p_y</code> indica la vera distribuzione dei dati, mentre <code>dbinom(..., prob = q)</code> rappresenta le previsioni del modello.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># vera probabilità di testa</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità stimata dal modello</span></span>
<span></span>
<span><span class="co"># Calcolo dell'ELPD per il modello con q = 0.5</span></span>
<span><span class="va">elpd</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">y</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>         <span class="co"># probabilità vera</span></span>
<span>  <span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span>  <span class="co"># log-probabilità del modello</span></span>
<span>  <span class="va">elpd</span> <span class="op">&lt;-</span> <span class="va">elpd</span> <span class="op">+</span> <span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD del modello che stima p = 0.5: %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD del modello che stima p = 0.5: -2.0549</span></span>
<span></span>
<span><span class="co"># Calcolo dell'ELPD per il modello vero (q = p)</span></span>
<span><span class="va">elpd_true</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">y</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span>  <span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">elpd_true</span> <span class="op">&lt;-</span> <span class="va">elpd_true</span> <span class="op">+</span> <span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD del modello vero (p = 0.6): %.4f\n"</span>, <span class="va">elpd_true</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD del modello vero (p = 0.6): -1.8536</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’output mostra che l’ELPD del modello vero è più alto (meno negativo), confermando che <span class="math inline">\(q = 0.6\)</span> è più predittivo di <span class="math inline">\(q = 0.5\)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 42%">
<col style="width: 35%">
</colgroup>
<thead><tr class="header">
<th>Concetto</th>
<th>Cosa misura</th>
<th>Vantaggio</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Divergenza KL</strong></td>
<td>Discrepanza tra modello e realtà</td>
<td>Fondamento teorico</td>
</tr>
<tr class="even">
<td><strong>Log-score</strong></td>
<td>Accuratezza su dati osservati</td>
<td>Intuitivo e semplice</td>
</tr>
<tr class="odd">
<td><strong>LPPD</strong></td>
<td>Log-score bayesiano</td>
<td>Tiene conto dell’incertezza</td>
</tr>
<tr class="even">
<td><strong>ELPD</strong></td>
<td>Log-score su dati non visti</td>
<td>Stima la generalizzabilità</td>
</tr>
</tbody>
</table>
<p>In pratica, <em>ELPD è il criterio più utile</em> per scegliere tra modelli bayesiani, perché stima la performance predittiva su nuovi dati, anche in assenza della distribuzione vera.</p>
</div>
</section><section id="leave-one-out-cross-validation-loo-cv" class="level2" data-number="74.6"><h2 data-number="74.6" class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv">
<span class="header-section-number">74.6</span> Leave-One-Out Cross-Validation (LOO-CV)</h2>
<p>Uno dei principali obiettivi nella valutazione di un modello è stimare <em>quanto bene il modello generalizza a nuovi dati</em>. A questo scopo, una delle tecniche più efficaci e ampiamente utilizzate è la <em>Leave-One-Out Cross-Validation (LOO-CV)</em>.</p>
<section id="cosè-la-loo-cv" class="level3" data-number="74.6.1"><h3 data-number="74.6.1" class="anchored" data-anchor-id="cosè-la-loo-cv">
<span class="header-section-number">74.6.1</span> Cos’è la LOO-CV?</h3>
<p>La <em>LOO-CV</em> è un metodo di validazione predittiva che consiste nel:</p>
<ol type="1">
<li>
<em>Escludere</em> una sola osservazione dal dataset;</li>
<li>
<em>Adattare</em> il modello sui dati rimanenti;</li>
<li>
<em>Valutare</em> quanto bene il modello predice l’osservazione esclusa (cioè calcolare la densità predittiva per quell’osservazione);</li>
<li>
<em>Ripetere</em> il processo per ogni osservazione nel dataset;</li>
<li>
<em>Sommare</em> i logaritmi delle densità predittive ottenute, per stimare l’<em>Expected Log Predictive Density</em> (ELPD):</li>
</ol>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i})
\]</span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset senza l’osservazione <span class="math inline">\(i\)</span>.</p>
</section><section id="perché-la-loo-cv-è-importante" class="level3" data-number="74.6.2"><h3 data-number="74.6.2" class="anchored" data-anchor-id="perché-la-loo-cv-è-importante">
<span class="header-section-number">74.6.2</span> Perché la LOO-CV è importante</h3>
<p>Nel contesto bayesiano, la LOO-CV fornisce una <em>stima empirica dell’ELPD</em> senza richiedere la conoscenza della distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<p>A differenza del semplice log-score o della LPPD, che valutano il modello sugli stessi dati usati per l’adattamento, la LOO-CV valuta il modello <em>su dati non visti</em>. In questo modo:</p>
<ul>
<li>Offre una <em>stima più realistica della performance predittiva</em>;</li>
<li>Penalizza modelli che si adattano troppo bene ai dati osservati (overfitting);</li>
<li>Premia modelli che <em>generalizzano meglio</em>.</li>
</ul></section><section id="loo-cv-e-divergenza-di-kullback-leibler" class="level3" data-number="74.6.3"><h3 data-number="74.6.3" class="anchored" data-anchor-id="loo-cv-e-divergenza-di-kullback-leibler">
<span class="header-section-number">74.6.3</span> LOO-CV e Divergenza di Kullback-Leibler</h3>
<p>Nel confronto tra modelli, il nostro obiettivo è scegliere quello che si avvicina di più alla distribuzione vera dei dati, che indichiamo con <span class="math inline">\(p(\tilde{y})\)</span>. Tuttavia, questa distribuzione <em>è sconosciuta</em> nella pratica.</p>
<p>Una misura naturale della distanza tra il modello predittivo <span class="math inline">\(q(\tilde{y} \mid y)\)</span> e la distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span> è la divergenza di Kullback-Leibler (KL):</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_{p} \left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Poiché <span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span> <em>non dipende dal modello</em>, minimizzare la KL-divergence equivale a <em>massimizzare</em> il secondo termine:</p>
<p><span class="math display">\[
\mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Questa quantità rappresenta l’<em>accuratezza predittiva del modello</em>, valutata in media rispetto alla distribuzione vera dei dati. Ma, ancora una volta, <span class="math inline">\(p(\tilde{y})\)</span> è ignota.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Approfondimento: perché l’ELPD è legato alla divergenza KL">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Approfondimento: perché l’ELPD è legato alla divergenza KL
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Per capire il senso dell’ELPD, ricordiamo come è definita la divergenza di Kullback-Leibler tra la distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, e il modello predittivo <span class="math inline">\(q(\tilde{y} \mid y)\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Questa formula confronta <em>quanto bene il modello <span class="math inline">\(q\)</span> approssima la distribuzione vera</em>. I due termini hanno significati diversi:</p>
<ul>
<li>
<span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span> è l’<em>entropia</em> della distribuzione vera dei dati: dipende solo da <span class="math inline">\(p\)</span>, non dal modello.</li>
<li>
<span class="math inline">\(\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]\)</span> valuta <em>quanto bene il modello predice nuovi dati</em>, in media, se i dati futuri fossero generati da <span class="math inline">\(p\)</span>.</li>
</ul>
<p><strong>Perché possiamo ignorare il primo termine?</strong></p>
<p>Supponiamo di confrontare due modelli, <span class="math inline">\(q_1\)</span> e <span class="math inline">\(q_2\)</span>. Vogliamo sapere quale ha una KL-divergence più bassa rispetto alla distribuzione vera. Consideriamo la <em>differenza</em> tra le due:</p>
<p><span class="math display">\[
\begin{align}
\Delta &amp;= D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) \notag\\
&amp;= \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)] \right]
  - \left[ \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] \right] .
\end{align}
\]</span></p>
<p>Il primo termine si elide:</p>
<p><span class="math display">\[
\Delta = \mathbb{E}_{p}[\log q_2(\tilde{y} \mid y)] - \mathbb{E}_{p}[\log q_1(\tilde{y} \mid y)] .
\]</span></p>
<p><strong>Conclusione</strong>: per confrontare i modelli, ci basta guardare a <span class="math inline">\(\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]\)</span>, perché l’altro termine è uguale per tutti i modelli ed è quindi irrilevante nel confronto.</p>
<p><strong>Ma come stimare questa quantità?</strong></p>
<p>Purtroppo, non conosciamo la distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span>, quindi non possiamo calcolare direttamente il valore atteso <span class="math inline">\(\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]\)</span>. Ma possiamo usare una <em>strategia empirica</em>:</p>
<ul>
<li>se assumiamo che le osservazioni nel nostro dataset siano state generate da <span class="math inline">\(p(\tilde{y})\)</span>,</li>
<li>allora possiamo <em>approssimare il valore atteso</em> come una <em>media sui dati osservati</em>.</li>
</ul>
<p>Questa idea è alla base della <em>Leave-One-Out Cross-Validation</em> (LOO-CV):</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i}) \approx \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Cioè:</p>
<blockquote class="blockquote">
<p>Ogni osservazione <span class="math inline">\(y_i\)</span> è trattata come se fosse “nuova”, e il modello è valutato sulla sua capacità di predire <span class="math inline">\(y_i\)</span> <em>senza averla vista durante l’adattamento</em>.</p>
</blockquote>
<p><strong>Perché l’ELPD riflette la KL-divergence?</strong></p>
<ul>
<li>L’ELPD-LOO è una <em>stima empirica</em> di <span class="math inline">\(\mathbb{E}_{p}[\log q(\tilde{y} \mid y)]\)</span>.</li>
<li>E questo è esattamente il <em>termine che ci interessa minimizzare</em> per ridurre la divergenza KL.</li>
</ul>
<blockquote class="blockquote">
<p><em>Massimizzare l’ELPD è equivalente a minimizzare la distanza KL</em> tra il modello e la distribuzione vera, almeno nella parte che possiamo stimare dai dati.</p>
</blockquote>
<p>In sintesi,</p>
<ul>
<li>non possiamo calcolare direttamente la divergenza KL, ma possiamo stimarne una parte (quella che dipende dal modello);</li>
<li>confrontare i modelli sulla base dell’ELPD equivale a scegliere <em>quello più vicino alla distribuzione vera</em>, in termini di capacità predittiva.</li>
</ul>
</div>
</div>
</div>
<section id="entra-in-gioco-loo-cv" class="level4" data-number="74.6.3.1"><h4 data-number="74.6.3.1" class="anchored" data-anchor-id="entra-in-gioco-loo-cv">
<span class="header-section-number">74.6.3.1</span> Entra in gioco LOO-CV</h4>
<p>La <em>Leave-One-Out Cross-Validation</em> (LOO-CV) ci fornisce una <em>stima empirica</em> di questa aspettativa. In pratica, stimiamo:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid \mathbf{y}_{-i})
\approx \mathbb{E}_{p}[\log q(\tilde{y} \mid y)] ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span> è una delle osservazioni effettivamente raccolte,</li>
<li>
<span class="math inline">\(\mathbf{y}_{-i}\)</span> sono tutti i dati tranne <span class="math inline">\(y_i\)</span>,</li>
<li>
<span class="math inline">\(p(y_i \mid \mathbf{y}_{-i})\)</span> è la densità predittiva lasciando fuori <span class="math inline">\(y_i\)</span> durante l’addestramento.</li>
</ul>
<p>In questo modo, <em>LOO-CV simula la previsione su dati nuovi</em>, permettendoci di stimare quanto bene il modello generalizza.</p>
<p>In conclusione, anche se non possiamo calcolare direttamente la KL-divergence (perché <span class="math inline">\(p\)</span> è ignota), possiamo <em>stimare il secondo termine dell’espressione</em> tramite LOO-CV. In altre parole:</p>
<blockquote class="blockquote">
<p><em>Massimizzare <span class="math inline">\(\text{ELPD}_{\text{LOO}}\)</span> equivale, in pratica, a minimizzare la KL-divergence tra la distribuzione vera e la predizione del modello.</em></p>
</blockquote>
<p>Questo spiega perché <em>ELPD-LOO è una metrica centrale nel confronto tra modelli bayesiani</em>: ci avvicina, il più possibile, al comportamento che avremmo se conoscessimo la distribuzione vera dei dati.</p>
</section></section><section id="intuizione-come-riduce-loverfitting" class="level3" data-number="74.6.4"><h3 data-number="74.6.4" class="anchored" data-anchor-id="intuizione-come-riduce-loverfitting">
<span class="header-section-number">74.6.4</span> Intuizione: Come riduce l’overfitting</h3>
<p>Quando valutiamo un modello sui <em>dati con cui è stato addestrato</em>, rischiamo di <em>sovrastimare</em> la sua capacità predittiva. Un modello molto complesso (con molti parametri) può adattarsi anche al rumore dei dati, ottenendo un log-score elevato, ma comportandosi male su dati nuovi.</p>
<p>Con la LOO-CV, ogni osservazione viene esclusa a turno, e il modello <em>non ha mai “visto” il dato che sta per predire</em>. Questo rende la valutazione:</p>
<ul>
<li>più onesta;</li>
<li>meno influenzata dalla complessità del modello;</li>
<li>più simile a una vera previsione futura.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 38%">
</colgroup>
<thead><tr class="header">
<th>Metodo</th>
<th>Usa dati visti?</th>
<th>Penalizza overfitting?</th>
<th>Misura la capacità predittiva</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>LPPD</td>
<td>✅</td>
<td>❌</td>
<td>sui dati osservati</td>
</tr>
<tr class="even">
<td>ELPD (LOO)</td>
<td>❌</td>
<td>✅</td>
<td>su dati nuovi (non visti)</td>
</tr>
</tbody>
</table>
<p>In pratica, <em>LOO-CV è lo standard per confrontare modelli bayesiani</em>, perché fornisce una stima affidabile della performance predittiva fuori campione. Grazie a metodi efficienti come <code>loo()</code> in R (<code>brms</code>, <code>rstanarm</code>, <code>loo</code>), possiamo stimare l’ELPD_LOO anche per modelli complessi, senza dover riadattare il modello <span class="math inline">\(n\)</span> volte.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 74.5</strong></span> Immaginiamo di avere una distribuzione binomiale vera con <span class="math inline">\(p = 0.6\)</span> e un modello che ipotizza <span class="math inline">\(q = 0.5\)</span>. Possiamo calcolare l’ELPD come:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>  <span class="co"># vera probabilità</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># modello</span></span>
<span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># elpd è l'implementazione computazionale di ELPD</span></span>
<span><span class="va">elpd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD stimato (modello q = 0.5): %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD stimato (modello q = 0.5): -2.0549</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Questo valore è un’approssimazione dell’<em>expected log score</em> del modello su dati generati da <span class="math inline">\(p\)</span>, usando <span class="math inline">\(q\)</span> per la predizione.</p>
</div>
</section></section><section id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="level2" data-number="74.7"><h2 data-number="74.7" class="anchored" data-anchor-id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl">
<span class="header-section-number">74.7</span> Criteri di Informazione come Approssimazioni della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span>
</h2>
<p>Oltre alla Leave-One-Out Cross-Validation, esistono altri approcci per stimare la qualità predittiva di un modello. Molti di questi derivano, direttamente o indirettamente, dalla <em>divergenza di Kullback-Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che come abbiamo visto rappresenta la distanza tra la distribuzione vera e quella stimata dal modello.</p>
<p>Poiché la distribuzione vera è generalmente ignota, sono stati proposti diversi <em>criteri di informazione</em> che mirano a bilanciare due obiettivi opposti:</p>
<ol type="1">
<li>
<em>Bontà di adattamento</em> del modello ai dati osservati;</li>
<li>
<em>Penalizzazione della complessità</em> del modello, per evitare l’overfitting.</li>
</ol>
<p>Tra i più noti troviamo: <em>MSE</em>, <em>AIC</em>, <em>BIC</em>, e <em>WAIC</em>. Vediamoli uno per uno.</p>
<section id="errore-quadratico-medio-mse" class="level3" data-number="74.7.1"><h3 data-number="74.7.1" class="anchored" data-anchor-id="errore-quadratico-medio-mse">
<span class="header-section-number">74.7.1</span> Errore Quadratico Medio (MSE)</h3>
<p>L’<em>Errore Quadratico Medio</em> è un criterio semplice e intuitivo che misura la media delle differenze al quadrato tra valori osservati e valori previsti:</p>
<p><span class="math display">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span></p>
<ul>
<li>Minore è l’MSE, migliore è la capacità del modello di prevedere i dati osservati.</li>
<li>Tuttavia, l’MSE <em>non penalizza la complessità del modello</em> e può favorire modelli troppo flessibili.</li>
</ul>
<p>È quindi utile come indicatore di accuratezza, ma <em>non è sufficiente</em> per selezionare tra modelli con diversa complessità.</p>
</section><section id="akaike-information-criterion-aic" class="level3" data-number="74.7.2"><h3 data-number="74.7.2" class="anchored" data-anchor-id="akaike-information-criterion-aic">
<span class="header-section-number">74.7.2</span> Akaike Information Criterion (AIC)</h3>
<p>L’<em>AIC</em> nasce da un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e stima quanto informazione si perde usando un modello per descrivere i dati:</p>
<p><span class="math display">\[
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k
\]</span></p>
<ul>
<li>
<span class="math inline">\(\hat{\theta}\_{\text{MLE}}\)</span>: stima massima di verosimiglianza dei parametri;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri del modello.</li>
</ul>
<p>Interpretazione:</p>
<ul>
<li>il primo termine misura quanto bene il modello si adatta ai dati;</li>
<li>il secondo termine penalizza la complessità del modello.</li>
</ul>
<p>Un <em>AIC più basso</em> indica un compromesso migliore tra accuratezza e semplicità.</p>
<p>Limitazioni:</p>
<ul>
<li>basato su assunzioni asintotiche (funziona meglio con campioni grandi);</li>
<li>utilizza solo <em>stime puntuali</em>, ignorando l’incertezza dei parametri;</li>
<li>non è pienamente bayesiano.</li>
</ul></section><section id="bayesian-information-criterion-bic" class="level3" data-number="74.7.3"><h3 data-number="74.7.3" class="anchored" data-anchor-id="bayesian-information-criterion-bic">
<span class="header-section-number">74.7.3</span> Bayesian Information Criterion (BIC)</h3>
<p>Il <em>Bayesian Information Criterion</em> (BIC) è un criterio di selezione del modello che, come l’AIC, cerca un compromesso tra <strong>bontà di adattamento ai dati</strong> e <strong>complessità del modello</strong>. Tuttavia, rispetto all’AIC, il BIC applica una <strong>penalizzazione più severa</strong> alla complessità, rendendolo particolarmente adatto a situazioni con grandi quantità di dati.</p>
<p>La formula del BIC è:</p>
<p><span class="math display">\[
\text{BIC} = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(y \mid \hat{\theta})\)</span> è la <em>massima verosimiglianza</em> del modello, ovvero la probabilità dei dati osservati valutata nel punto <span class="math inline">\(\hat{\theta}\)</span> che massimizza la funzione di verosimiglianza (massimo a posteriori in caso di priori piatti);</li>
<li>
<span class="math inline">\(n\)</span> è il numero di osservazioni indipendenti;</li>
<li>
<span class="math inline">\(k\)</span> è il numero di parametri stimati nel modello.</li>
</ul>
<p>Il BIC può anche essere scritto come:</p>
<p><span class="math display">\[
\text{BIC} = \ln(n) \cdot k - 2 \ln L
\]</span></p>
<p>dove <span class="math inline">\(L = p(y \mid \hat{\theta})\)</span> è, appunto, la massima verosimiglianza.</p>
<p>Interpretazione:</p>
<ul>
<li>il primo termine, <span class="math inline">\(-2 \log p(y \mid \hat{\theta})\)</span>, misura quanto bene il modello si adatta ai dati;</li>
<li>il secondo termine, <span class="math inline">\(\log(n) \cdot k\)</span>, è una penalizzazione che aumenta con il numero di parametri e con la dimensione del campione.</li>
</ul>
<p>Un <em>valore più basso del BIC</em> indica un modello preferibile, cioè un miglior compromesso tra accuratezza e parsimonia.</p>
<p>Vantaggi:</p>
<ul>
<li>favorisce <em>modelli più semplici</em>, specialmente quando il numero di osservazioni <span class="math inline">\(n\)</span> è elevato;</li>
<li>ha una <em>giustificazione teorica bayesiana</em>: sotto ipotesi regolari e prior non informativi, il BIC approssima il logaritmo della <em>marginal likelihood</em> del modello (e quindi del modello bayesiano integrato).</li>
</ul>
<p>Limiti:</p>
<ul>
<li>si basa su <em>assunzioni forti</em>, tra cui l’indipendenza delle osservazioni, modelli regolari (es. parametri identificabili) e <em>prior</em> deboli o non informativi;</li>
<li>può <em>sottoselezionare</em> modelli utili in presenza di piccoli campioni, dati rumorosi e strutture complesse (es. modelli gerarchici, a posteriori multimodali).</li>
</ul></section><section id="widely-applicable-information-criterion-waic" class="level3" data-number="74.7.4"><h3 data-number="74.7.4" class="anchored" data-anchor-id="widely-applicable-information-criterion-waic">
<span class="header-section-number">74.7.4</span> Widely Applicable Information Criterion (WAIC)</h3>
<p>Il <em>WAIC</em> è una generalizzazione bayesiana dell’AIC, ed è progettato per:</p>
<ul>
<li>tenere conto dell’intera distribuzione a posteriori dei parametri;</li>
<li>fornire una <em>stima fully Bayesian</em> dell’accuratezza predittiva.</li>
</ul>
<p><span class="math display">\[
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \text{Var}_{\theta^{(s)}} \left( \log p(y_i \mid \theta^{(s)}) \right)
\right]
\]</span></p>
<ul>
<li>
<span class="math inline">\(S\)</span>: numero di campioni dalla distribuzione a posteriori;</li>
<li>
<span class="math inline">\(\theta^{(s)}\)</span>: s-esimo campione dalla posteriori;</li>
<li>la seconda somma rappresenta il numero <em>effettivo</em> di parametri, basato sulla variabilità della log-verosimiglianza.</li>
</ul>
<p>Caratteristiche principali:</p>
<ul>
<li>utilizza campioni MCMC → <em>adatto anche a modelli non regolari</em>;</li>
<li>fornisce un’<em>approssimazione del log score</em> su dati nuovi;</li>
<li>migliore dell’AIC per modelli bayesiani complessi.</li>
</ul>
<p>Riepilogo Comparativo</p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead><tr class="header">
<th>Criterio</th>
<th>Tipo</th>
<th>Penalizza la complessità?</th>
<th>Usa stime puntuali?</th>
<th>Supporta Bayesian MCMC?</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSE</td>
<td>Frequentista</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>AIC</td>
<td>Frequentista</td>
<td>✅ (modesta)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="odd">
<td>BIC</td>
<td>Frequentista/Bayesiano</td>
<td>✅ (forte)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>WAIC</td>
<td>Bayesiano</td>
<td>✅ (effettiva)</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr class="odd">
<td>LOO-CV</td>
<td>Bayesiano</td>
<td>✅ (empirica)</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table></section></section><section id="riflessioni-conclusive" class="level2" data-number="74.8"><h2 data-number="74.8" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">74.8</span> Riflessioni Conclusive</h2>
<p>Il cuore della selezione del modello, in statistica bayesiana, è il concetto di <em>accuratezza predittiva</em>. Più precisamente, ci interessa sapere quanto bene un modello possa prevedere <em>nuovi dati</em>, non solo spiegare quelli già osservati.</p>
<p>A questo scopo, il criterio più solido è l’<em>Expected Log Predictive Density</em> (ELPD), che valuta quanto la distribuzione predittiva del modello si avvicina alla (sconosciuta) distribuzione vera dei dati. Sebbene la <em>divergenza di Kullback-Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>) rappresenti una misura ideale per confrontare distribuzioni, il suo uso diretto è raramente possibile perché <span class="math inline">\(p_{\text{vera}}(y)\)</span> è ignota. Tuttavia, massimizzare l’ELPD equivale a minimizzare la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> rispetto alla vera generatrice: entrambi puntano a rappresentare accuratamente la realtà sottostante.</p>
<p>Poiché l’ELPD non è calcolabile in forma esatta, esistono <em>approssimazioni pratiche</em>:</p>
<ul>
<li>
<em>LOO-CV</em> (Leave-One-Out Cross-Validation) è oggi lo strumento più robusto per stimare l’ELPD. Valuta iterativamente ogni osservazione come “nuova” e fornisce una stima attendibile della capacità del modello di generalizzare.</li>
<li>
<em>WAIC</em> offre una stima simile, ma basata interamente sulla distribuzione a posteriori, senza riadattare il modello.</li>
<li>
<em>AIC</em> e <em>BIC</em>, pur derivando da un framework frequentista e basandosi su stime puntuali, offrono soluzioni rapide e utili in contesti semplici.</li>
<li>
<em>MSE</em>, infine, misura solo la distanza tra le previsioni e i valori osservati, ma <em>non penalizza la complessità</em>, e quindi è inadatto alla selezione del modello.</li>
</ul>
<p>Nel confronto tra modelli, la <em>differenza tra valori di ELPD</em> (stimata tramite LOO-CV o WAIC) può essere accompagnata da un <em>errore standard</em> che aiuta a quantificare l’incertezza della differenza. Una regola pratica: se la differenza tra modelli è almeno <em>due volte maggiore</em> dell’errore standard, è probabile che uno dei due modelli sia davvero superiore, in termini predittivi.</p>
<p>Conclusione sintetica:</p>
<ul>
<li>
<em>la buona statistica non è quella che spiega il passato, ma quella che anticipa il futuro</em>;</li>
<li>la <em>divergenza KL</em> ci dà una misura teorica della distanza tra modello e realtà;</li>
<li>l’<em>ELPD</em>, stimato via <em>LOO-CV</em> o <em>WAIC</em>, fornisce una misura pratica della capacità del modello di prevedere nuovi dati;</li>
<li>la <em>selezione del modello ottimale</em> richiede equilibrio tra accuratezza, generalizzazione e parsimonia.</li>
</ul>
<p>Con questi strumenti, possiamo scegliere modelli che <em>catturano i pattern reali nei dati senza farsi ingannare dal rumore</em>, garantendo affidabilità e interpretabilità anche in contesti complessi.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] thematic_0.1.7   MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.11.0      </span></span>
<span><span class="co">#&gt;  [5] gridExtra_2.3    patchwork_1.3.1  bayesplot_1.13.0 psych_2.5.6     </span></span>
<span><span class="co">#&gt;  [9] scales_1.4.0     markdown_2.0     knitr_1.50       lubridate_1.9.4 </span></span>
<span><span class="co">#&gt; [13] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.1.0     </span></span>
<span><span class="co">#&gt; [17] readr_2.1.5      tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2   </span></span>
<span><span class="co">#&gt; [21] tidyverse_2.0.0  rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] generics_0.1.4     stringi_1.8.7      lattice_0.22-7    </span></span>
<span><span class="co">#&gt;  [4] hms_1.1.3          digest_0.6.37      magrittr_2.0.3    </span></span>
<span><span class="co">#&gt;  [7] evaluate_1.0.4     grid_4.5.1         timechange_0.3.0  </span></span>
<span><span class="co">#&gt; [10] RColorBrewer_1.1-3 fastmap_1.2.0      rprojroot_2.1.0   </span></span>
<span><span class="co">#&gt; [13] jsonlite_2.0.0     mnormt_2.1.1       cli_3.6.5         </span></span>
<span><span class="co">#&gt; [16] rlang_1.1.6        withr_3.0.2        tools_4.5.1       </span></span>
<span><span class="co">#&gt; [19] parallel_4.5.1     tzdb_0.5.0         pacman_0.5.1      </span></span>
<span><span class="co">#&gt; [22] vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   </span></span>
<span><span class="co">#&gt; [25] htmlwidgets_1.6.4  pkgconfig_2.0.3    pillar_1.11.0     </span></span>
<span><span class="co">#&gt; [28] gtable_0.3.6       glue_1.8.0         xfun_0.52         </span></span>
<span><span class="co">#&gt; [31] tidyselect_1.2.1   rstudioapi_0.17.1  farver_2.1.2      </span></span>
<span><span class="co">#&gt; [34] htmltools_0.5.8.1  nlme_3.1-168       rmarkdown_2.29    </span></span>
<span><span class="co">#&gt; [37] compiler_4.5.1</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="pagination-link" aria-label="Introduzione">
        <span class="nav-page-text">Introduzione</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>