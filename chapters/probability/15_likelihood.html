<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>40&nbsp; La verosimiglianza – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/probability/16_likelihood_gauss.html" rel="next">
<link href="../../chapters/probability/14_gauss.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9ac0fa60fcd2ae55f2ddd9f231fc218f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-24909b4b034f8b88bf2a7d082dd17e2c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/15_likelihood.html"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La riforma metodologica in psicologia: dalla crisi alla rivoluzione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelli cognitivi</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/16_likelihood_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">La verosimiglianza gaussiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/17_likelihood_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Il rapporto di verosimiglianze</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/18_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Simulazioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Calcolo della distribuzione a posteriori gaussiana tramite metodo a griglia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata: un approccio frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_prediction_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Predizione e inferenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Disegno della ricerca e potere statistico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione" id="toc-il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="nav-link" data-scroll-target="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione"><span class="header-section-number">40.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</a></li>
  <li><a href="#sequenza-di-lanci-di-una-moneta" id="toc-sequenza-di-lanci-di-una-moneta" class="nav-link" data-scroll-target="#sequenza-di-lanci-di-una-moneta"><span class="header-section-number">40.2</span> Sequenza di Lanci di una Moneta</a></li>
  <li><a href="#esempi-di-verosimiglianza-senza-la-binomiale" id="toc-esempi-di-verosimiglianza-senza-la-binomiale" class="nav-link" data-scroll-target="#esempi-di-verosimiglianza-senza-la-binomiale"><span class="header-section-number">40.3</span> Esempi di Verosimiglianza senza la Binomiale</a></li>
  <li><a href="#esempio-di-verosimiglianza-con-la-binomiale" id="toc-esempio-di-verosimiglianza-con-la-binomiale" class="nav-link" data-scroll-target="#esempio-di-verosimiglianza-con-la-binomiale"><span class="header-section-number">40.4</span> Esempio di Verosimiglianza con la Binomiale</a></li>
  <li><a href="#la-stima-di-massima-verosimiglianza" id="toc-la-stima-di-massima-verosimiglianza" class="nav-link" data-scroll-target="#la-stima-di-massima-verosimiglianza"><span class="header-section-number">40.5</span> La Stima di Massima Verosimiglianza</a></li>
  <li><a href="#stima-di-massima-verosimiglianza-mle-con-r" id="toc-stima-di-massima-verosimiglianza-mle-con-r" class="nav-link" data-scroll-target="#stima-di-massima-verosimiglianza-mle-con-r"><span class="header-section-number">40.6</span> Stima di Massima Verosimiglianza (MLE) con R</a></li>
  <li><a href="#verosimiglianza-congiunta-estensione-del-concetto-di-verosimiglianza" id="toc-verosimiglianza-congiunta-estensione-del-concetto-di-verosimiglianza" class="nav-link" data-scroll-target="#verosimiglianza-congiunta-estensione-del-concetto-di-verosimiglianza"><span class="header-section-number">40.7</span> Verosimiglianza Congiunta: Estensione del Concetto di Verosimiglianza</a></li>
  <li><a href="#perch%C3%A9-%C3%A8-importante-la-verosimiglianza-congiunta" id="toc-perché-è-importante-la-verosimiglianza-congiunta" class="nav-link" data-scroll-target="#perch%C3%A9-%C3%A8-importante-la-verosimiglianza-congiunta"><span class="header-section-number">40.8</span> Perché è Importante la Verosimiglianza Congiunta?</a></li>
  <li><a href="#esempio-osservazioni-raggruppate" id="toc-esempio-osservazioni-raggruppate" class="nav-link" data-scroll-target="#esempio-osservazioni-raggruppate"><span class="header-section-number">40.9</span> Esempio: Osservazioni Raggruppate</a></li>
  <li><a href="#la-verosimiglianza-marginale" id="toc-la-verosimiglianza-marginale" class="nav-link" data-scroll-target="#la-verosimiglianza-marginale"><span class="header-section-number">40.10</span> La Verosimiglianza Marginale</a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">40.11</span> Riflessioni Conclusive</a></li>
  <li><a href="#esercizi" id="toc-esercizi" class="nav-link" data-scroll-target="#esercizi">Esercizi</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/15_likelihood.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/15_likelihood.html"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-prob-likelihood" class="quarto-section-identifier"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In questo capitolo, imparerai a:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Comprendere il concetto di verosimiglianza</strong>: Scoprirai il ruolo fondamentale che la verosimiglianza svolge nella stima dei parametri statistici.</li>
<li>
<strong>Generare grafici della funzione di verosimiglianza</strong>:
<ul>
<li>Implementare grafici per la funzione di verosimiglianza nel caso binomiale.</li>
</ul>
</li>
<li>
<strong>Interpretare i grafici della funzione di verosimiglianza</strong>: Sviluppare le competenze necessarie per analizzare e trarre conclusioni dai grafici generati.</li>
<li>
<strong>Capire il principio di stima di massima verosimiglianza (MLE)</strong>: Approfondiremo il metodo di stima di massima verosimiglianza.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Leggere il capitolo <em>Estimation</em> <span class="citation" data-cites="schervish2014probability">(<a href="#ref-schervish2014probability" role="doc-biblioref">Schervish &amp; DeGroot, 2014</a>)</span>.</li>
<li>Leggere il capitolo <em>Bayes’ rule</em> <span class="citation" data-cites="Johnson2022bayesrules">(<a href="#ref-Johnson2022bayesrules" role="doc-biblioref">Johnson et al., 2022</a>)</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<section id="introduzione" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>I ricercatori utilizzano diversi modelli matematici per descrivere e prevedere il comportamento dei dati osservati. Questi modelli si distinguono tra loro per la struttura funzionale, ovvero il modo in cui collegano le variabili osservate con parametri teorici. La scelta del modello migliore avviene confrontando le previsioni teoriche generate dal modello con i dati effettivamente osservati. Il modello che produce previsioni più vicine ai dati reali viene considerato il più adeguato per descrivere il fenomeno studiato.</p>
<p>In questo processo di confronto, la funzione di verosimiglianza gioca un ruolo fondamentale. Essa quantifica la probabilità che i dati osservati siano stati generati da un particolare modello con determinati valori dei suoi parametri. In altre parole, la verosimiglianza misura quanto i dati siano compatibili con il modello ipotizzato.</p>
</section><section id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="level2" data-number="40.1"><h2 data-number="40.1" class="anchored" data-anchor-id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione">
<span class="header-section-number">40.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</h2>
<p>La verosimiglianza quantifica quanto i dati osservati siano compatibili con diversi valori dei parametri di un modello. In termini più semplici, la verosimiglianza indica quanto ciascun valore possibile dei parametri sia plausibile nel descrivere il fenomeno osservato.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definizione 40.1</strong></span> Consideriamo un vettore aleatorio <span class="math inline">\(X\)</span>, la cui distribuzione è descritta da una funzione di densità di probabilità (nel caso di variabili continue) o da una funzione di massa di probabilità (nel caso di variabili discrete), indicata con <span class="math inline">\(f(x; \theta)\)</span>. Qui, <span class="math inline">\(\theta\)</span> rappresenta un vettore di parametri che appartiene a uno spazio parametrico <span class="math inline">\(\Theta\)</span>. Una volta osservato un valore specifico <span class="math inline">\(x\)</span> del vettore <span class="math inline">\(X\)</span>, possiamo definire la funzione di verosimiglianza per i parametri <span class="math inline">\(\theta\)</span> come:</p>
<p><span class="math display">\[
L(\theta; x) = f(x; \theta).
\]</span></p>
<p>Questa definizione evidenzia che la funzione di verosimiglianza si concentra sui parametri, mentre i dati sono fissati.</p>
</div>
<section id="relazione-tra-verosimiglianza-e-funzione-di-probabilità" class="level3" data-number="40.1.1"><h3 data-number="40.1.1" class="anchored" data-anchor-id="relazione-tra-verosimiglianza-e-funzione-di-probabilità">
<span class="header-section-number">40.1.1</span> Relazione tra Verosimiglianza e Funzione di Probabilità</h3>
<p>La formula matematica che collega i dati ai parametri è la stessa sia per la funzione di verosimiglianza che per la funzione di densità (o massa) di probabilità. Ciò che cambia è l’interpretazione e il modo in cui questa formula viene utilizzata nei due contesti.</p>
<ul>
<li><p><strong>Funzione di densità (o massa) di probabilità</strong>:<br>
Questa funzione descrive il processo generativo dei dati, indicando la probabilità (o densità di probabilità, nel caso continuo) che un determinato valore dei dati venga osservato, supponendo che i parametri del modello siano già noti e fissati. Qui, quindi, i parametri sono fissi e i dati variano.</p></li>
<li><p><strong>Funzione di verosimiglianza</strong>:<br>
La funzione di verosimiglianza inverte questo punto di vista. In essa, i dati osservati sono fissati, mentre i parametri <span class="math inline">\(\theta\)</span> variano. La verosimiglianza misura la plausibilità di ciascun valore possibile dei parametri nel descrivere i dati che sono stati effettivamente osservati. In sostanza, valuta la compatibilità di ogni valore dei parametri rispetto ai dati raccolti.</p></li>
</ul>
<p>Formalmente, la relazione tra queste due funzioni può essere espressa come:</p>
<p><span class="math display">\[
L(\theta \mid y) \propto p(y \mid \theta),
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(L(\theta \mid y)\)</span> rappresenta la funzione di verosimiglianza, ossia la plausibilità dei parametri <span class="math inline">\(\theta\)</span> alla luce dei dati osservati <span class="math inline">\(y\)</span>;</li>
<li>
<span class="math inline">\(p(y \mid \theta)\)</span> indica la funzione di densità (o massa) di probabilità, cioè la probabilità di osservare i dati <span class="math inline">\(y\)</span> assumendo che i parametri <span class="math inline">\(\theta\)</span> siano già noti e fissi.</li>
</ul>
<p>In sintesi, la funzione di probabilità risponde alla domanda “dato un certo insieme di parametri, quanto è probabile osservare questi dati?”, mentre la funzione di verosimiglianza si pone la domanda opposta: “dati questi dati osservati, quali valori dei parametri sono più plausibili?” Questa distinzione è fondamentale per l’inferenza statistica e per determinare i parametri più adatti a descrivere i fenomeni osservati.</p>
</section></section><section id="sequenza-di-lanci-di-una-moneta" class="level2" data-number="40.2"><h2 data-number="40.2" class="anchored" data-anchor-id="sequenza-di-lanci-di-una-moneta">
<span class="header-section-number">40.2</span> Sequenza di Lanci di una Moneta</h2>
<p>Per comprendere meglio il concetto di verosimiglianza, partiamo da un esempio semplice. Supponiamo di voler stimare la probabilità di ottenere testa in un lancio di moneta, indicata con <span class="math inline">\(p_H\)</span>. L’obiettivo è capire quali valori di <span class="math inline">\(p_H\)</span> rendano più plausibili i dati osservati. Per farlo, calcoliamo la probabilità di osservare <strong>sequenze specifiche di lanci</strong> di una moneta assumendo che ogni lancio sia indipendente dagli altri.</p>
<section id="perché-moltiplichiamo-le-probabilità" class="level3" data-number="40.2.1"><h3 data-number="40.2.1" class="anchored" data-anchor-id="perché-moltiplichiamo-le-probabilità">
<span class="header-section-number">40.2.1</span> Perché moltiplichiamo le probabilità?</h3>
<p>Quando lanciamo una moneta più volte, ogni lancio rappresenta un evento indipendente: il risultato di un lancio <strong>non influenza</strong> il successivo. Questo significa che la probabilità di osservare una specifica sequenza di successi (teste) e insuccessi (croci) si calcola moltiplicando le probabilità dei singoli eventi.</p>
<p>Ad esempio, consideriamo una moneta con probabilità <span class="math inline">\(p_H\)</span> di ottenere testa e <span class="math inline">\(1 - p_H\)</span> di ottenere croce. Se osserviamo la sequenza <strong>HTHT</strong>, la probabilità di ottenere questa specifica sequenza è:</p>
<p><span class="math display">\[
P(HTHT \mid p_H) = p_H \cdot (1 - p_H) \cdot p_H \cdot (1 - p_H) = p_H^2 (1 - p_H)^2.
\]</span></p>
<p>Se invece osserviamo una sequenza diversa, come <strong>THTH</strong> o <strong>HHTT</strong>, la probabilità sarà sempre <span class="math inline">\(p_H^2 (1 - p_H)^2\)</span>. Questo avviene perché <strong>l’ordine specifico dei risultati non influisce sulla probabilità complessiva della sequenza</strong>, finché il numero totale di successi e insuccessi rimane lo stesso.</p>
</section><section id="generalizzazione-della-probabilità-di-una-sequenza" class="level3" data-number="40.2.2"><h3 data-number="40.2.2" class="anchored" data-anchor-id="generalizzazione-della-probabilità-di-una-sequenza">
<span class="header-section-number">40.2.2</span> Generalizzazione della Probabilità di una Sequenza</h3>
<p>In generale, se osserviamo <strong><span class="math inline">\(n\)</span></strong> lanci di una moneta e registriamo <strong><span class="math inline">\(y\)</span></strong> successi (teste) e <strong><span class="math inline">\(n - y\)</span></strong> insuccessi (croci), la probabilità di ottenere una qualsiasi sequenza con esattamente <span class="math inline">\(y\)</span> teste e <span class="math inline">\(n - y\)</span> croci è:</p>
<p><span id="eq-like-seq-tossess-coin"><span class="math display">\[
P(Y = y \mid p_H) = p_H^y (1 - p_H)^{n - y}.
\tag{40.1}\]</span></span></p>
<p>Questa formula rappresenta la <strong>funzione di verosimiglianza</strong>, che misura la compatibilità tra i dati osservati e un dato valore di <span class="math inline">\(p_H\)</span>. In altre parole, ci dice <strong>quanto sia plausibile</strong> che il parametro <span class="math inline">\(p_H\)</span> abbia un determinato valore, dati i risultati osservati.</p>
</section><section id="connessione-con-la-distribuzione-binomiale" class="level3" data-number="40.2.3"><h3 data-number="40.2.3" class="anchored" data-anchor-id="connessione-con-la-distribuzione-binomiale">
<span class="header-section-number">40.2.3</span> Connessione con la Distribuzione Binomiale</h3>
<p>A questo punto, dovrebbe risultare evidente che l’equazione <a href="#eq-like-seq-tossess-coin" class="quarto-xref">Equazione&nbsp;<span>40.1</span></a> è il <strong>nucleo</strong> della distribuzione binomiale. Infatti, la funzione di probabilità della distribuzione binomiale completa è:</p>
<p><span class="math display">\[
P(Y = y \mid p_H) = \binom{n}{y} p_H^y (1 - p_H)^{n - y}.
\]</span></p>
<p>La differenza tra questa formula e la funzione di verosimiglianza è il <strong>coefficiente binomiale</strong> <span class="math inline">\(\binom{n}{y}\)</span>, che rappresenta il numero di modi in cui possiamo ottenere esattamente <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> lanci.</p>
<p>Possiamo <strong>rimuovere questa costante</strong> nella funzione di verosimiglianza perché non dipende dal parametro <span class="math inline">\(p_H\)</span> che vogliamo stimare. Poiché il nostro obiettivo è trovare il valore di <span class="math inline">\(p_H\)</span> che <strong>massimizza la funzione di verosimiglianza</strong>, qualsiasi termine costante rispetto a <span class="math inline">\(p_H\)</span> non influenzerà il risultato della massimizzazione.</p>
<p><strong>Nota:</strong> il coefficiente binomiale modifica l’altezza della verosimiglianza ma non la posizione del massimo, permettendo di ignorarlo nei calcoli di stima.</p>
<p>Per questo motivo, nella ricerca della <strong>stima di massima verosimiglianza</strong>, possiamo considerare solo il <strong>nucleo</strong> della funzione binomiale:</p>
<p><span id="eq-like-binom-nucleo"><span class="math display">\[
L(p_H \mid y) = p_H^y (1 - p_H)^{n - y}.
\tag{40.2}\]</span></span></p>
<p>Questa semplificazione ci permette di concentrarci solo sui valori di <span class="math inline">\(p_H\)</span> che rendono la funzione più alta, ignorando costanti moltiplicative che non influiscono sulla posizione del massimo.</p>
</section></section><section id="esempi-di-verosimiglianza-senza-la-binomiale" class="level2" data-number="40.3"><h2 data-number="40.3" class="anchored" data-anchor-id="esempi-di-verosimiglianza-senza-la-binomiale">
<span class="header-section-number">40.3</span> Esempi di Verosimiglianza senza la Binomiale</h2>
<section id="caso-1-due-lanci" class="level3" data-number="40.3.1"><h3 data-number="40.3.1" class="anchored" data-anchor-id="caso-1-due-lanci">
<span class="header-section-number">40.3.1</span> Caso 1: Due Lanci</h3>
<p>Supponiamo di lanciare una moneta due volte e di osservare una testa e una croce. Per stimare <span class="math inline">\(p_H\)</span>, calcoliamo la probabilità di osservare questa specifica sequenza per diversi valori di <span class="math inline">\(p_H\)</span>.</p>
<ul>
<li>
<p>Se la moneta è equa (<span class="math inline">\(p_H = 0.5\)</span>):</p>
<p><span class="math display">\[
P(H, T \mid p_H = 0.5) = 0.5 \times 0.5 = 0.25.
\]</span></p>
</li>
<li>
<p>Se invece <span class="math inline">\(p_H = 0.4\)</span>:</p>
<p><span class="math display">\[
P(H, T \mid p_H = 0.4) = 0.4 \times 0.6 = 0.24.
\]</span></p>
</li>
</ul>
<p>In generale, la <strong>funzione di verosimiglianza</strong> per questo esperimento è:</p>
<p><span class="math display">\[
L(p_H) = p_H^1 (1 - p_H)^1.
\]</span></p>
<p>Possiamo rappresentarla graficamente con il seguente codice R:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definizione dei parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">2</span>  <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span>  <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Sequenza di valori possibili per p_H</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">p_H</span><span class="op">^</span><span class="va">y</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p_H</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Creazione del dataframe</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Grafico della funzione di verosimiglianza</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">data</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Verosimiglianza per 2 Lanci di Moneta (1 Testa, 1 Croce)"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="15_likelihood_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
</section><section id="caso-2-tre-lanci" class="level3" data-number="40.3.2"><h3 data-number="40.3.2" class="anchored" data-anchor-id="caso-2-tre-lanci">
<span class="header-section-number">40.3.2</span> Caso 2: Tre Lanci</h3>
<p>Ora immaginiamo di aver lanciato la moneta tre volte e di aver osservato una testa e due croci. La probabilità di ottenere questa specifica sequenza per diversi valori di <span class="math inline">\(p_H\)</span> è:</p>
<ul>
<li>
<p>Se <span class="math inline">\(p_H = 0.5\)</span>:</p>
<p><span class="math display">\[
P(H, T, T \mid p_H = 0.5) = 0.5 \times 0.5 \times 0.5 = 0.125.
\]</span></p>
</li>
<li>
<p>Se <span class="math inline">\(p_H = 0.4\)</span>:</p>
<p><span class="math display">\[
P(H, T, T \mid p_H = 0.4) = 0.4 \times 0.6 \times 0.6 = 0.144.
\]</span></p>
</li>
</ul>
<p>La funzione di verosimiglianza per questo esperimento è:</p>
<p><span class="math display">\[
L(p_H) = p_H^1 (1 - p_H)^2.
\]</span></p>
<p>Possiamo rappresentarla graficamente con il seguente codice R:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definizione dei parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">3</span>  <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span>  <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Sequenza di valori possibili per p_H</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">p_H</span><span class="op">^</span><span class="va">y</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p_H</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Creazione del dataframe</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Grafico della funzione di verosimiglianza</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">data</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Verosimiglianza per 3 Lanci di Moneta (1 Testa, 2 Croci)"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="15_likelihood_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
</section><section id="interpretazione-dei-risultati" class="level3" data-number="40.3.3"><h3 data-number="40.3.3" class="anchored" data-anchor-id="interpretazione-dei-risultati">
<span class="header-section-number">40.3.3</span> Interpretazione dei Risultati</h3>
<p>Osservando i grafici, possiamo notare che:</p>
<ul>
<li>La <strong>funzione di verosimiglianza</strong> è massima per valori di <span class="math inline">\(p_H\)</span> vicini alla proporzione osservata nel campione. Ad esempio, se abbiamo ottenuto <strong>1 testa su 2 lanci</strong>, la verosimiglianza è massima intorno a <span class="math inline">\(p_H = 0.5\)</span>.</li>
<li>Quando <strong>aumentiamo il numero di lanci</strong>, la funzione di verosimiglianza diventa più stretta, indicando che più dati permettono una stima più precisa di <span class="math inline">\(p_H\)</span>.</li>
<li>Per valori estremi di <span class="math inline">\(p_H\)</span> (molto vicini a 0 o 1), la verosimiglianza è bassa, poiché questi valori non spiegano bene i dati osservati. Ad esempio, se abbiamo osservato <strong>1 testa e 2 croci</strong>, un valore di <span class="math inline">\(p_H = 0.9\)</span> avrebbe una verosimiglianza molto bassa.</li>
</ul>
<p>Questa sezione ha mostrato come calcolare la verosimiglianza basandoci solo sul prodotto delle probabilità dei singoli lanci (senza utilizzare la distribuzione binomiale). Nella prossima sezione, useremo la formula completa della distribuzione binomiale per formalizzare ulteriormente il concetto.</p>
</section></section><section id="esempio-di-verosimiglianza-con-la-binomiale" class="level2" data-number="40.4"><h2 data-number="40.4" class="anchored" data-anchor-id="esempio-di-verosimiglianza-con-la-binomiale">
<span class="header-section-number">40.4</span> Esempio di Verosimiglianza con la Binomiale</h2>
<p>Consideriamo ora un esperimento più esteso: lanciamo una moneta <strong><span class="math inline">\(n = 30\)</span></strong> volte e otteniamo <strong><span class="math inline">\(y = 23\)</span></strong> teste. Per modellare il numero di successi, utilizziamo la <strong>distribuzione binomiale</strong>, che descrive la probabilità di osservare esattamente <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove indipendenti:</p>
<p><span class="math display">\[
P(Y = y \mid \theta) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p>Poiché il coefficiente binomiale <span class="math inline">\(\binom{n}{y}\)</span> è una costante rispetto a <span class="math inline">\(\theta\)</span>, possiamo considerare la <strong>funzione di verosimiglianza</strong>, che esprime la probabilità dei dati osservati come funzione del parametro <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
L(\theta \mid y) = \theta^{23} (1 - \theta)^7.
\]</span></p>
<section id="visualizzazione-della-funzione-di-verosimiglianza" class="level3" data-number="40.4.1"><h3 data-number="40.4.1" class="anchored" data-anchor-id="visualizzazione-della-funzione-di-verosimiglianza">
<span class="header-section-number">40.4.1</span> Visualizzazione della Funzione di Verosimiglianza</h3>
<p>Il seguente codice R genera il grafico della funzione di verosimiglianza per diversi valori di <span class="math inline">\(\theta\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definizione dei parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span>  <span class="co"># Numero totale di lanci</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">23</span>  <span class="co"># Numero di teste osservate</span></span>
<span></span>
<span><span class="co"># Sequenza di valori possibili per p_H (probabilità di ottenere testa)</span></span>
<span><span class="va">p_H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo della funzione di verosimiglianza binomiale</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p_H</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Creazione del dataframe per la visualizzazione</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">p_H</span>, <span class="va">likelihood</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Grafico della funzione di verosimiglianza</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">data</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p_H</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Verosimiglianza per 30 Lanci di Moneta"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="va">H</span><span class="op">]</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Verosimiglianza"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="15_likelihood_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p>Osservando il grafico, notiamo che la <strong>funzione di verosimiglianza raggiunge il suo massimo intorno a</strong> <span class="math inline">\(p_H \approx 0.77\)</span>. Questo significa che il valore di <span class="math inline">\(p_H\)</span> che rende i dati osservati più plausibili è circa 0.77. Questo valore corrisponde alla <strong>stima di massima verosimiglianza (MLE)</strong> per la probabilità di ottenere testa, dato che abbiamo osservato <strong>23 successi su 30 prove</strong>.</p>
<p>L’intuizione alla base di questo risultato è semplice: la funzione di verosimiglianza esprime quanto siano compatibili i dati osservati con ogni possibile valore di <span class="math inline">\(p_H\)</span>. Il valore che <strong>massimizza</strong> questa funzione rappresenta l’ipotesi più plausibile per il vero valore del parametro <span class="math inline">\(p_H\)</span>.</p>
</section></section><section id="la-stima-di-massima-verosimiglianza" class="level2" data-number="40.5"><h2 data-number="40.5" class="anchored" data-anchor-id="la-stima-di-massima-verosimiglianza">
<span class="header-section-number">40.5</span> La Stima di Massima Verosimiglianza</h2>
<p>Per trovare il valore di <span class="math inline">\(\theta\)</span> che massimizza la funzione di verosimilianza, possiamo calcolare la derivata della log-verosimiglianza e risolvere. Nel caso dell’esempio, abbiamo:</p>
<p><span class="math display">\[
\ell(\theta) = 23 \log \theta + 7 \log (1 - \theta).
\]</span></p>
<p>Derivando e ponendo uguale a zero:</p>
<p><span class="math display">\[
\frac{d \ell(\theta)}{d\theta} = \frac{23}{\theta} - \frac{7}{1 - \theta} = 0.
\]</span></p>
<p>Risolviamo per <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\theta_{MLE} = \frac{y}{n} = \frac{23}{30} \approx 0.767.
\]</span></p>
<p>Questo significa che la <strong>stima di massima verosimiglianza</strong> della proporzione <span class="math inline">\(p\)</span> è la proporzione osservata nel campione. Per comprenderlo intuitivamente, possiamo pensare alla funzione di verosimiglianza come a una curva che rappresenta la plausibilità dei diversi valori di <span class="math inline">\(p\)</span> nel descrivere i dati osservati. Il valore più plausibile sarà quello che corrisponde al massimo della funzione di verosimiglianza, ossia la sua <strong>moda</strong>.</p>
<section id="unintuizione-geometrica" class="level3" data-number="40.5.1"><h3 data-number="40.5.1" class="anchored" data-anchor-id="unintuizione-geometrica">
<span class="header-section-number">40.5.1</span> Un’Intuizione Geometrica</h3>
<p>Possiamo immaginare di analizzare la funzione di verosimiglianza come se fosse un percorso collinare. Il punto più alto della collina rappresenta il valore di <span class="math inline">\(p\)</span> che rende i dati più probabili. Se volessimo trovare il punto più alto in modo sistematico, potremmo <strong>misurare la pendenza della collina</strong> in ogni punto. Se la pendenza è positiva, significa che la funzione sta ancora crescendo; se è negativa, significa che sta diminuendo. Il massimo della funzione si trova nel punto in cui la pendenza è <strong>esattamente zero</strong>—cioè quando la tangente alla curva è una retta orizzontale.</p>
<p>Matematicamente, questa intuizione si traduce nel calcolo della <strong>derivata</strong> della funzione di verosimiglianza: il massimo della funzione si trova imponendo che la derivata sia uguale a zero. Vediamo questo procedimento formalmente.</p>
<section id="dimostrazione-formale-per-la-proporzione-campionaria" class="level4" data-number="40.5.1.1"><h4 data-number="40.5.1.1" class="anchored" data-anchor-id="dimostrazione-formale-per-la-proporzione-campionaria">
<span class="header-section-number">40.5.1.1</span> Dimostrazione Formale per la Proporzione Campionaria</h4>
<p>Supponiamo di avere un campione di <span class="math inline">\(n\)</span> osservazioni, in cui abbiamo osservato <span class="math inline">\(y\)</span> successi. Se i dati seguono una distribuzione binomiale, la funzione di verosimiglianza è:</p>
<p><span class="math display">\[
L(p) = p^y (1 - p)^{n - y}.
\]</span></p>
<p>Per trovare il valore di <span class="math inline">\(p\)</span> che massimizza questa funzione, calcoliamo la <strong>log-verosimiglianza</strong> (per semplificare i calcoli):</p>
<p><span class="math display">\[
\ell(p) = \log L(p) = y \log p + (n - y) \log (1 - p).
\]</span></p>
<p>Deriviamo rispetto a <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
\frac{d\ell(p)}{dp} = \frac{y}{p} - \frac{n - y}{1 - p}.
\]</span></p>
<div class="callout callout-style-simple callout-important callout-titled" title="Nota">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Nota
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consideriamo la funzione di <strong>log-verosimiglianza</strong> per una sequenza di <span class="math inline">\(n\)</span> lanci di una moneta, con <span class="math inline">\(y\)</span> successi (teste) e <span class="math inline">\(n - y\)</span> insuccessi (croci):</p>
<p><span class="math display">\[
\ell(p) = \log L(p) = y \log p + (n - y) \log (1 - p).
\]</span></p>
<p>Vogliamo ora <strong>derivare rispetto a <span class="math inline">\(p\)</span></strong> per determinare il valore che massimizza la funzione di verosimiglianza.</p>
<p>Per farlo, utilizziamo le regole di derivazione dei logaritmi:</p>
<ul>
<li><p>La derivata di <span class="math inline">\(\log p\)</span> è: <span class="math display">\[
\frac{d}{dp} \log p = \frac{1}{p}.
\]</span></p></li>
<li><p>Per la derivata di <span class="math inline">\(\log(1 - p)\)</span>, notiamo che l’argomento del logaritmo è <span class="math inline">\(1 - p\)</span>, quindi dobbiamo applicare la <strong>regola della catena</strong>.</p></li>
</ul>
<p>La regola della catena afferma che, se abbiamo una funzione composta <span class="math inline">\(f(g(p))\)</span>, la sua derivata si calcola come:</p>
<p><span class="math display">\[
\frac{d}{dp} \log g(p) = \frac{1}{g(p)} \cdot g'(p).
\]</span></p>
<p>Nel nostro caso, poniamo:</p>
<p><span class="math display">\[
g(p) = 1 - p.
\]</span></p>
<p>Derivandola rispetto a <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
g'(p) = \frac{d}{dp} (1 - p) = -1.
\]</span></p>
<p>Applicando la regola della catena:</p>
<p><span class="math display">\[
\frac{d}{dp} \log(1 - p) = \frac{1}{1 - p} \cdot (-1) = -\frac{1}{1 - p}.
\]</span></p>
<p>Ora possiamo calcolare la derivata della funzione di log-verosimiglianza:</p>
<p><span class="math display">\[
\frac{d\ell(p)}{dp} = y \cdot \frac{1}{p} + (n - y) \cdot \left(-\frac{1}{1 - p}\right).
\]</span></p>
<p>Semplificando:</p>
<p><span class="math display">\[
\frac{d\ell(p)}{dp} = \frac{y}{p} - \frac{n - y}{1 - p}.
\]</span></p>
</div>
</div>
</div>
<p>Imponiamo che la derivata sia zero:</p>
<p><span class="math display">\[
\frac{y}{p} - \frac{n - y}{1 - p} = 0.
\]</span></p>
<p>Risolviamo per <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
y(1 - p) = (n - y) p.
\]</span></p>
<p><span class="math display">\[
y - yp = np - yp.
\]</span></p>
<p><span class="math display">\[
y = np.
\]</span></p>
<p><span class="math display">\[
\hat{p} = \frac{y}{n}.
\]</span></p>
<p>Dunque, la <strong>stima di massima verosimiglianza</strong> della proporzione è la proporzione campionaria <span class="math inline">\(\hat{p} = \frac{y}{n}\)</span>, che corrisponde esattamente all’intuizione che il valore più plausibile per <span class="math inline">\(p\)</span> è quello che meglio rappresenta i dati osservati.</p>
</section></section></section><section id="stima-di-massima-verosimiglianza-mle-con-r" class="level2" data-number="40.6"><h2 data-number="40.6" class="anchored" data-anchor-id="stima-di-massima-verosimiglianza-mle-con-r">
<span class="header-section-number">40.6</span> Stima di Massima Verosimiglianza (MLE) con R</h2>
<p>La stima di massima verosimiglianza (MLE) corrisponde al valore del parametro <span class="math inline">\(\theta\)</span> che massimizza la funzione di verosimiglianza <span class="math inline">\(L(\theta; X)\)</span> associata al modello statistico. Mentre in precedenza abbiamo derivato analiticamente lo stimatore MLE risolvendo l’equazione di verosimiglianza, in molti casi pratici—specialmente con modelli complessi—è preferibile adottare un approccio numerico. Questo metodo prevede il calcolo diretto della verosimiglianza su un intervallo di valori plausibili per <span class="math inline">\(\theta\)</span> e l’identificazione del punto di massimo tramite algoritmi di ottimizzazione.</p>
<p>Nel contesto di un modello binomiale, possiamo implementare questa strategia in R seguendo due strade:</p>
<ol type="1">
<li>
<strong>Valutazione su griglia</strong>: generare una sequenza di valori per <span class="math inline">\(\theta\)</span> (es. da 0 a 1), calcolare <span class="math inline">\(L(\theta; X)\)</span> per ogni valore utilizzando <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code>, e individuare il massimo con <code><a href="https://rdrr.io/r/base/which.min.html">which.max()</a></code>.<br>
</li>
<li>
<strong>Ottimizzazione numerica</strong>: utilizzare funzioni come <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> o <code><a href="https://rdrr.io/r/stats/nlm.html">nlm()</a></code> per trovare iterativamente il <span class="math inline">\(\theta\)</span> che massimizza la log-verosimiglianza, garantendo efficienza anche in casi multidimensionali.</li>
</ol>
<p>Illustreremo entrambe le metodologie. Iniziamo con la valutazione su griglia:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">23</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">10000</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcolo delle probabilità binomiali</span></span>
<span><span class="va">probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Identificazione dell'indice del massimo</span></span>
<span><span class="va">max_index</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Recupero del valore di theta corrispondente</span></span>
<span><span class="va">optimal_theta</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="va">max_index</span><span class="op">]</span></span>
<span><span class="va">optimal_theta</span></span>
<span><span class="co">#&gt; [1] 0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Spiegazione del codice:</strong></p>
<ul>
<li>
<code>dbinom(y, size = n, prob = theta)</code> calcola la probabilità di osservare esattamente <span class="math inline">\(y\)</span> successi su <span class="math inline">\(n\)</span> tentativi, per ciascun valore di <span class="math inline">\(\theta\)</span>.</li>
<li>
<code>which.max(probabilities)</code> trova l’indice del valore massimo nella sequenza delle probabilità.</li>
<li>
<code>theta[max_index]</code> restituisce il valore di <span class="math inline">\(\theta\)</span> corrispondente al massimo trovato.</li>
</ul>
<p>Consideriamo ora il metodo di ottimizzazione numerica:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione di log-verosimiglianza (negativa per minimizzazione)</span></span>
<span><span class="va">neg_log_likelihood</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">-</span> <span class="op">(</span><span class="va">y</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Formula binomiale</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Ottimizzazione con metodo Brent (specifico per problemi 1D)</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">0.5</span>,               <span class="co"># Valore iniziale</span></span>
<span>  fn <span class="op">=</span> <span class="va">neg_log_likelihood</span>, </span>
<span>  method <span class="op">=</span> <span class="st">"Brent"</span>,        <span class="co"># Ottimizzazione con limiti</span></span>
<span>  lower <span class="op">=</span> <span class="fl">1e-6</span>,            <span class="co"># Evita theta = 0 (log(0) = -Inf)</span></span>
<span>  upper <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="fl">1e-6</span>         <span class="co"># Evita theta = 1</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">optimal_theta_numerical</span> <span class="op">&lt;-</span> <span class="va">result</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">optimal_theta_numerical</span></span>
<span><span class="co">#&gt; [1] 0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Spiegazione del codice:</strong></p>
<ul>
<li>
<code>neg_log_likelihood</code> calcola la <strong>log-verosimiglianza negativa</strong> (necessaria perché <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> minimizza per default).<br>
</li>
<li>
<code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> cerca il minimo della funzione tra <code>lower</code> e <code>upper</code> usando l’algoritmo Brent, ottimizzato per problemi unidimensionali.<br>
</li>
<li>I limiti <code>1e-6</code> e <code>1 - 1e-6</code> evitano valori di <span class="math inline">\(\theta\)</span> estremi che generano <code>NaN</code> nel logaritmo.</li>
</ul>
<p>Il risultato finale coincide esattamente con la soluzione analitica <span class="math inline">\(\hat{\theta} = y/n\)</span> (≈ 0.7667).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Confronto con la soluzione analitica (y/n)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Numerico"</span> <span class="op">=</span> <span class="va">optimal_theta_numerical</span>, <span class="st">"Analitico"</span> <span class="op">=</span> <span class="va">y</span><span class="op">/</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="co">#&gt;  Numerico Analitico </span></span>
<span><span class="co">#&gt;    0.7667    0.7667</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Confronto degli approcci</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 39%">
<col style="width: 37%">
</colgroup>
<thead><tr class="header">
<th>Metodo</th>
<th>Vantaggi</th>
<th>Limiti</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Valutazione su griglia</td>
<td>Intuitivo, visivo</td>
<td>Computazionalmente costoso</td>
</tr>
<tr class="even">
<td>Ottimizzazione numerica</td>
<td>Efficiente, scalabile a parametri multipli</td>
<td>Richiede scelta di algoritmo e parametri iniziali</td>
</tr>
</tbody>
</table></section><section id="verosimiglianza-congiunta-estensione-del-concetto-di-verosimiglianza" class="level2" data-number="40.7"><h2 data-number="40.7" class="anchored" data-anchor-id="verosimiglianza-congiunta-estensione-del-concetto-di-verosimiglianza">
<span class="header-section-number">40.7</span> Verosimiglianza Congiunta: Estensione del Concetto di Verosimiglianza</h2>
<p>Abbiamo visto che, nel caso di una sequenza di <span class="math inline">\(n\)</span> lanci di una moneta, la funzione di <strong>verosimiglianza</strong> si basa sulla distribuzione binomiale. In questo caso, trattiamo un esperimento Bernoulliano ripetuto <span class="math inline">\(n\)</span> volte, e la nostra osservazione è <strong>il numero totale di successi</strong> (teste). Il numero complessivo di successi segue una <strong>distribuzione binomiale</strong>, e la funzione di verosimiglianza assume la forma:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = P(Y = y \mid \theta) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p>Qui la verosimiglianza è espressa direttamente in termini del numero totale di successi e insuccessi, senza dover scrivere il contributo di ogni singolo lancio.</p>
<p>Tuttavia, possiamo affrontare la questione da una prospettiva diversa: <strong>invece di considerare il numero totale di successi, possiamo pensare alla verosimiglianza come il prodotto delle probabilità di ogni singolo lancio</strong>. Questo ci porta a una generalizzazione importante: la <strong>verosimiglianza congiunta</strong> di più osservazioni indipendenti.</p>
<section id="dal-caso-binomiale-alla-verosimiglianza-congiunta" class="level3" data-number="40.7.1"><h3 data-number="40.7.1" class="anchored" data-anchor-id="dal-caso-binomiale-alla-verosimiglianza-congiunta">
<span class="header-section-number">40.7.1</span> Dal Caso Binomiale alla Verosimiglianza Congiunta</h3>
<p>Nel caso dei lanci della moneta, le singole osservazioni sono <strong>prove Bernoulliane indipendenti</strong>, ovvero ogni singolo lancio è un’osservazione indipendente che segue una distribuzione Bernoulli con parametro <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
P(Y_i = 1 \mid \theta) = \theta, \quad P(Y_i = 0 \mid \theta) = 1 - \theta.
\]</span></p>
<p>Se trattiamo ogni prova individualmente, la funzione di verosimiglianza per una <strong>singola osservazione</strong> è:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y_i) = \theta^{y_i} (1 - \theta)^{1 - y_i}.
\]</span></p>
<p>Ora, per un campione di <span class="math inline">\(n\)</span> osservazioni indipendenti, la <strong>verosimiglianza congiunta</strong> è il prodotto delle verosimiglianze delle singole osservazioni:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y_1, y_2, \dots, y_n) = \prod_{i=1}^{n} \theta^{y_i} (1 - \theta)^{1 - y_i}.
\]</span></p>
<p>Riconosciamo che questa espressione è identica alla funzione di verosimiglianza della distribuzione binomiale, perché il numero totale di successi è:</p>
<p><span class="math display">\[
y = \sum_{i=1}^{n} y_i.
\]</span></p>
<p>Quindi, riscrivendo la verosimiglianza congiunta, otteniamo:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = \theta^{\sum y_i} (1 - \theta)^{n - \sum y_i} = \theta^y (1 - \theta)^{n - y}.
\]</span></p>
<p><strong>Questa è proprio la verosimiglianza della distribuzione binomiale!</strong> Questo mostra che il caso binomiale può essere visto come una forma compatta di verosimiglianza congiunta per prove Bernoulliane indipendenti.</p>
</section></section><section id="perché-è-importante-la-verosimiglianza-congiunta" class="level2" data-number="40.8"><h2 data-number="40.8" class="anchored" data-anchor-id="perché-è-importante-la-verosimiglianza-congiunta">
<span class="header-section-number">40.8</span> Perché è Importante la Verosimiglianza Congiunta?</h2>
<p>L’idea della <strong>verosimiglianza congiunta</strong> è fondamentale perché ci permette di estendere i concetti di verosimiglianza dal caso di una singola osservazione al caso di molte osservazioni indipendenti. Questo è utile in molti contesti statistici:</p>
<ol type="1">
<li>
<strong>Stimare parametri</strong> basandosi su un intero campione invece che su una singola osservazione.</li>
<li>
<strong>Definire modelli statistici più complessi</strong>, in cui le osservazioni sono indipendenti ma possono avere diverse distribuzioni.</li>
<li>
<strong>Applicare la log-verosimiglianza</strong> per rendere più agevole il calcolo e l’ottimizzazione.</li>
</ol>
<p>In sintesi, l’esempio binomiale mostra che la verosimiglianza congiunta di prove Bernoulliane indipendenti si riconduce alla verosimiglianza binomiale, rendendo il concetto meno evidente a prima vista. Tuttavia, la vera potenza della verosimiglianza congiunta si manifesta in distribuzioni continue come la normale, dove la produttoria delle densità di probabilità per singole osservazioni è chiaramente distinta dalla funzione di verosimiglianza per il campione intero.</p>
<p>La chiave per comprendere il concetto è rendersi conto che <strong>la verosimiglianza di un’intera sequenza di prove indipendenti è il prodotto delle singole verosimiglianze</strong>, e che nel caso binomiale questa proprietà si manifesta in modo più compatto grazie alla forma stessa della distribuzione.</p>
</section><section id="esempio-osservazioni-raggruppate" class="level2" data-number="40.9"><h2 data-number="40.9" class="anchored" data-anchor-id="esempio-osservazioni-raggruppate">
<span class="header-section-number">40.9</span> Esempio: Osservazioni Raggruppate</h2>
<p>Per illustrare il concetto di <strong>verosimiglianza congiunta</strong> nel caso della distribuzione binomiale, consideriamo quattro gruppi distinti di osservazioni binomiali indipendenti:</p>
<ul>
<li>
<strong>Gruppo 1</strong>: 23 successi su 30 prove.</li>
<li>
<strong>Gruppo 2</strong>: 20 successi su 28 prove.</li>
<li>
<strong>Gruppo 3</strong>: 29 successi su 40 prove.</li>
<li>
<strong>Gruppo 4</strong>: 29 successi su 36 prove.</li>
</ul>
<p>Poiché ogni gruppo segue una distribuzione binomiale indipendente con lo stesso parametro <span class="math inline">\(\theta\)</span>, la log-verosimiglianza congiunta si ottiene <strong>sommando</strong> le log-verosimiglianze di ciascun gruppo:</p>
<p><span class="math display">\[
\log \mathcal{L}(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(n_i\)</span> è il numero totale di prove nel gruppo <span class="math inline">\(i\)</span>,</li>
<li>
<span class="math inline">\(y_i\)</span> è il numero di successi nel gruppo <span class="math inline">\(i\)</span>.</li>
</ul>
<p>Sostituendo i valori specifici:</p>
<p><span class="math display">\[
\begin{aligned}
\log \mathcal{L}(\theta) &amp;= [23\log(\theta) + (30-23)\log(1 - \theta)] +\\
&amp;\quad [20\log(\theta) + (28-20)\log(1 - \theta)] +\\
&amp;\quad [29\log(\theta) + (40-29)\log(1 - \theta)] +\\
&amp;\quad [29\log(\theta) + (36-29)\log(1 - \theta)].
\end{aligned}
\]</span></p>
<p>Questa formula ci permette di calcolare <strong>quanto è plausibile</strong> il valore del parametro <span class="math inline">\(\theta\)</span>, tenendo conto simultaneamente di tutte le osservazioni nei quattro gruppi.</p>
<section id="implementazione-in-r" class="level3" data-number="40.9.1"><h3 data-number="40.9.1" class="anchored" data-anchor-id="implementazione-in-r">
<span class="header-section-number">40.9.1</span> Implementazione in R</h3>
<p>Consideriamo ora un’<strong>implementazione pratica in R</strong>, che ci consenta di stimare <span class="math inline">\(\theta\)</span> massimizzando la log-verosimiglianza congiunta.</p>
<p>Definiamo una funzione che calcola la log-verosimiglianza congiunta per un dato valore di <span class="math inline">\(\theta\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">log_verosimiglianza_congiunta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>, <span class="va">dati</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Per evitare problemi numerici, limitiamo theta tra (0,1)</span></span>
<span>  <span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">pmax</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">pmin</a></span><span class="op">(</span><span class="va">theta</span>, <span class="fl">1</span> <span class="op">-</span> <span class="fl">1e-10</span><span class="op">)</span>, <span class="fl">1e-10</span><span class="op">)</span> </span>
<span>  </span>
<span>  <span class="va">log_likelihood</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">gruppo</span> <span class="kw">in</span> <span class="va">dati</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">n</span> <span class="op">&lt;-</span> <span class="va">gruppo</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>  <span class="co"># Numero totale di prove nel gruppo</span></span>
<span>    <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">gruppo</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>  <span class="co"># Numero di successi nel gruppo</span></span>
<span>    <span class="va">log_likelihood</span> <span class="op">&lt;-</span> <span class="va">log_likelihood</span> <span class="op">+</span> <span class="va">y</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">log_likelihood</span><span class="op">)</span>  <span class="co"># Restituiamo il valore negativo per la minimizzazione</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Inseriamo i dati dei quattro gruppi in una lista:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dati_gruppi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">23</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">20</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">40</span>, <span class="fl">29</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">36</span>, <span class="fl">29</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per trovare il valore di <span class="math inline">\(\theta\)</span> che <strong>massimizza</strong> la log-verosimiglianza congiunta, utilizziamo la funzione <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">0.5</span>,                          <span class="co"># Valore iniziale di theta</span></span>
<span>  fn <span class="op">=</span> <span class="va">log_verosimiglianza_congiunta</span>, <span class="co"># Funzione da minimizzare</span></span>
<span>  dati <span class="op">=</span> <span class="va">dati_gruppi</span>,                 <span class="co"># Dati dei gruppi</span></span>
<span>  method <span class="op">=</span> <span class="st">"L-BFGS-B"</span>,                <span class="co"># Metodo numerico con limiti</span></span>
<span>  lower <span class="op">=</span> <span class="fl">0</span>,                          <span class="co"># Theta deve rimanere tra 0 e 1</span></span>
<span>  upper <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stima ottimale di theta</span></span>
<span><span class="va">result</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="co">#&gt; [1] 0.7537</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La funzione <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code> minimizza la funzione data, perciò passiamo <strong>il negativo della log-verosimiglianza</strong> per ottenere il massimo della verosimiglianza.</p>
<p>Vediamo ora come varia la log-verosimiglianza per diversi valori di <span class="math inline">\(\theta\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">theta_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.01</span>, <span class="fl">0.99</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">log_likelihood_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">theta_values</span>, <span class="kw">function</span><span class="op">(</span><span class="va">t</span><span class="op">)</span> <span class="fu">log_verosimiglianza_congiunta</span><span class="op">(</span><span class="va">t</span>, <span class="va">dati_gruppi</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_values</span>, log_likelihood <span class="op">=</span> <span class="va">log_likelihood_values</span><span class="op">)</span>, </span>
<span>  <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="va">log_likelihood</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Funzione di Log-Verosimiglianza Congiunta"</span>,</span>
<span>    x <span class="op">=</span> <span class="st">"Theta"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Log-verosimiglianza negativa"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="15_likelihood_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p>Questa curva mostra <strong>il comportamento della funzione di log-verosimiglianza</strong>, consentendoci di individuare visivamente il valore di <span class="math inline">\(\theta\)</span> che la massimizza.</p>
<p>In sintesi, la log-verosimiglianza congiunta ci permette di considerare <strong>tutti i dati disponibili simultaneamente</strong> per ottenere una stima più affidabile del parametro <span class="math inline">\(\theta\)</span>. In questo esempio, abbiamo visto come:</p>
<ol type="1">
<li>La log-verosimiglianza di più gruppi indipendenti <strong>si somma</strong>.</li>
<li>Possiamo stimare <span class="math inline">\(\theta\)</span> numericamente utilizzando <code><a href="https://rdrr.io/r/stats/optim.html">optim()</a></code>.</li>
<li>È utile visualizzare la log-verosimiglianza per comprendere come varia rispetto a <span class="math inline">\(\theta\)</span>.</li>
</ol>
<p>Questo approccio è fondamentale in inferenza statistica e può essere esteso a molti altri contesti in cui abbiamo più gruppi di dati indipendenti.</p>
</section></section><section id="la-verosimiglianza-marginale" class="level2" data-number="40.10"><h2 data-number="40.10" class="anchored" data-anchor-id="la-verosimiglianza-marginale">
<span class="header-section-number">40.10</span> La Verosimiglianza Marginale</h2>
<p>La <strong>verosimiglianza marginale</strong> è un concetto fondamentale nell’inferenza bayesiana, utilizzato per valutare <strong>quanto un modello sia compatibile con i dati osservati</strong>, tenendo conto dell’incertezza sui parametri.</p>
<p>A differenza della verosimiglianza standard, che misura la plausibilità dei dati per un valore <strong>fisso</strong> del parametro, la <strong>verosimiglianza marginale</strong> considera <strong>tutti i possibili valori del parametro</strong>, pesandoli in base alla loro probabilità a priori. Questo approccio permette di integrare l’incertezza nella valutazione del modello.</p>
<section id="caso-con-parametri-discreti" class="level3" data-number="40.10.1"><h3 data-number="40.10.1" class="anchored" data-anchor-id="caso-con-parametri-discreti">
<span class="header-section-number">40.10.1</span> Caso con Parametri Discreti</h3>
<p>Per comprendere meglio il concetto, consideriamo un esperimento in cui eseguiamo <strong>10 tentativi</strong> e otteniamo <strong>7 successi</strong>. Supponiamo che la probabilità di successo <span class="math inline">\(\theta\)</span> possa assumere solo tre valori discreti:</p>
<p><span class="math display">\[
\theta \in \{0.1, 0.5, 0.9\}.
\]</span></p>
<p>Per calcolare la <strong>verosimiglianza marginale</strong>, dobbiamo:</p>
<ol type="1">
<li>
<p><strong>Assegnare una probabilità a priori</strong> a ciascun valore di <span class="math inline">\(\theta\)</span>, ad esempio:</p>
<ul>
<li>Distribuzione uniforme: <span class="math display">\[
p(\theta = 0.1) = p(\theta = 0.5) = p(\theta = 0.9) = \frac{1}{3}.
\]</span>
</li>
<li>Distribuzione non uniforme (ad esempio, dando più peso a <span class="math inline">\(\theta = 0.5\)</span>): <span class="math display">\[
p(\theta = 0.1) = \frac{1}{4}, \quad p(\theta = 0.5) = \frac{1}{2}, \quad p(\theta = 0.9) = \frac{1}{4}.
\]</span>
</li>
</ul>
</li>
<li>
<p><strong>Calcolare la probabilità di osservare 7 successi su 10 prove per ogni valore di <span class="math inline">\(\theta\)</span></strong>:</p>
<p><span class="math display">\[
p(k=7 \mid \theta) = \binom{10}{7} \theta^7 (1 - \theta)^3.
\]</span></p>
</li>
<li>
<p><strong>Moltiplicare ciascuna di queste probabilità per la corrispondente probabilità a priori e sommare i risultati</strong>:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \sum_{i} p(k=7 \mid \theta_i) p(\theta_i).
\]</span></p>
</li>
</ol>
<p>Sostituendo i valori per la distribuzione uniforme:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \binom{10}{7} 0.1^7 (0.9)^3 \cdot \frac{1}{3} +
\binom{10}{7} 0.5^7 (0.5)^3 \cdot \frac{1}{3} +
\binom{10}{7} 0.9^7 (0.1)^3 \cdot \frac{1}{3}.
\]</span></p>
<p>Questa somma rappresenta la <strong>verosimiglianza marginale</strong>, ossia la probabilità di ottenere 7 successi su 10, considerando tutte le possibili incertezze su <span class="math inline">\(\theta\)</span>.</p>
</section><section id="caso-con-parametri-continui" class="level3" data-number="40.10.2"><h3 data-number="40.10.2" class="anchored" data-anchor-id="caso-con-parametri-continui">
<span class="header-section-number">40.10.2</span> Caso con Parametri Continui</h3>
<p>Nella maggior parte delle situazioni, il parametro <span class="math inline">\(\theta\)</span> non assume solo pochi valori discreti, ma può variare <strong>continuamente</strong> in un intervallo (ad esempio, tra 0 e 1). In questo caso, invece di sommare, dobbiamo <strong>integrare</strong>:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 p(\theta) \, d\theta.
\]</span></p>
<p>Qui:</p>
<ul>
<li>
<span class="math inline">\(p(\theta)\)</span> è la <strong>distribuzione a priori</strong> di <span class="math inline">\(\theta\)</span>.</li>
<li>L’integrale rappresenta una media ponderata della probabilità di ottenere i dati, considerando tutti i valori di <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Ad esempio, se <span class="math inline">\(\theta \sim \text{Beta}(2,2)\)</span>, la verosimiglianza marginale diventa:</p>
<p><span class="math display">\[
p(k=7 \mid n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 \frac{\theta (1-\theta)}{B(2,2)} \, d\theta.
\]</span></p>
<p>Questo tipo di calcolo viene spesso risolto numericamente.</p>
</section><section id="calcolo-numerico-della-verosimiglianza-marginale-in-r" class="level3" data-number="40.10.3"><h3 data-number="40.10.3" class="anchored" data-anchor-id="calcolo-numerico-della-verosimiglianza-marginale-in-r">
<span class="header-section-number">40.10.3</span> Calcolo Numerico della Verosimiglianza Marginale in R</h3>
<p>Se vogliamo calcolare la verosimiglianza marginale numericamente, possiamo usare l’integrazione numerica in R.</p>
<p><strong>Caso con Parametri Discreti.</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definiamo i valori possibili di theta e le probabilità a priori</span></span>
<span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span><span class="op">)</span></span>
<span><span class="va">prior_probs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span>  <span class="co"># Distribuzione uniforme</span></span>
<span></span>
<span><span class="co"># Calcoliamo la verosimiglianza per ciascun valore di theta</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">7</span>, size <span class="op">=</span> <span class="fl">10</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcoliamo la verosimiglianza marginale sommando i contributi ponderati</span></span>
<span><span class="va">marginal_likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">prior_probs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">marginal_likelihood</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0582</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Caso con Parametri Continui.</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definiamo la funzione di verosimiglianza pesata dalla prior</span></span>
<span><span class="va">integrand</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">7</span>, size <span class="op">=</span> <span class="fl">10</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, shape1 <span class="op">=</span> <span class="fl">2</span>, shape2 <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Eseguiamo l'integrazione numerica</span></span>
<span><span class="va">marginal_likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">integrand</span>, lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">marginal_likelihood</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1119</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interpretazione-della-verosimiglianza-marginale" class="level3" data-number="40.10.4"><h3 data-number="40.10.4" class="anchored" data-anchor-id="interpretazione-della-verosimiglianza-marginale">
<span class="header-section-number">40.10.4</span> Interpretazione della Verosimiglianza Marginale</h3>
<p>La <strong>verosimiglianza marginale</strong> rappresenta la probabilità complessiva dei dati, <strong>tenendo conto di tutte le possibili incertezze sul parametro <span class="math inline">\(\theta\)</span></strong>.</p>
<ul>
<li>Se la verosimiglianza marginale è <strong>alta</strong>, significa che il modello nel suo insieme è compatibile con i dati osservati.</li>
<li>Se la verosimiglianza marginale è <strong>bassa</strong>, significa che, indipendentemente dal valore di <span class="math inline">\(\theta\)</span>, il modello non spiega bene i dati.</li>
</ul>
<p>A differenza della verosimiglianza classica, che valuta quanto siano probabili i dati per un <strong>singolo valore</strong> di <span class="math inline">\(\theta\)</span>, la verosimiglianza marginale <strong>considera tutte le possibili ipotesi sul parametro</strong>.</p>
</section><section id="ruolo-nella-statistica-bayesiana" class="level3" data-number="40.10.5"><h3 data-number="40.10.5" class="anchored" data-anchor-id="ruolo-nella-statistica-bayesiana">
<span class="header-section-number">40.10.5</span> Ruolo nella Statistica Bayesiana</h3>
<p>La verosimiglianza marginale svolge un ruolo cruciale nell’inferenza bayesiana perché appare nella <strong>formula di Bayes</strong>:</p>
<p><span class="math display">\[
p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},
\]</span></p>
<p>dove <span class="math inline">\(p(D)\)</span> è la <strong>verosimiglianza marginale</strong>. Questa quantità: 1. <strong>Serve da fattore di normalizzazione</strong> per la distribuzione a posteriori <span class="math inline">\(p(\theta \mid D)\)</span>. 2. <strong>Permette di confrontare modelli diversi</strong>, attraverso il <strong>fattore di Bayes</strong>:</p>
<p><span class="math display">\[
   BF = \frac{p(D \mid M_1)}{p(D \mid M_2)},
   \]</span></p>
<p>dove <span class="math inline">\(M_1\)</span> e <span class="math inline">\(M_2\)</span> sono due modelli diversi.</p>
<p>In conclusione, la <strong>verosimiglianza marginale</strong> è un concetto chiave nell’inferenza bayesiana, che ci permette di valutare quanto un modello sia coerente con i dati, tenendo conto di tutte le possibili incertezze sui parametri.</p>
<ul>
<li>Per parametri <strong>discreti</strong>, si calcola come una somma ponderata.</li>
<li>Per parametri <strong>continui</strong>, si calcola con un <strong>integrale</strong>.</li>
<li>È essenziale per il calcolo della distribuzione <strong>a posteriori</strong> e per il <strong>confronto tra modelli</strong>.</li>
</ul></section></section><section id="riflessioni-conclusive" class="level2" data-number="40.11"><h2 data-number="40.11" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">40.11</span> Riflessioni Conclusive</h2>
<p>La funzione di verosimiglianza rappresenta un ponte fondamentale tra i dati osservati e i parametri di un modello statistico, offrendo una misura della plausibilità dei dati rispetto a diversi valori possibili dei parametri. La sua costruzione richiede l’integrazione di tre elementi chiave: il modello statistico ipotizzato come generatore dei dati, lo spazio dei parametri associato al modello e le osservazioni empiriche disponibili.</p>
<p>Nell’ambito dell’inferenza statistica, la funzione di verosimiglianza svolge un ruolo centrale. Essa consente di valutare quanto bene diversi valori dei parametri siano in grado di spiegare i dati osservati, diventando così uno strumento essenziale per la stima dei parametri e la selezione del modello. La sua corretta applicazione è determinante per garantire analisi dati rigorose e interpretazioni affidabili dei risultati.</p>
<p>In sintesi, padroneggiare il concetto di verosimiglianza e saperlo applicare in modo appropriato sono competenze indispensabili per chi si occupa di ricerca empirica e analisi di dati complessi. La verosimiglianza non è solo uno strumento tecnico, ma un pilastro metodologico che supporta la comprensione e l’interpretazione dei fenomeni osservati attraverso modelli statistici.</p>
</section><section id="esercizi" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="esercizi">Esercizi</h2>
<div class="callout callout-style-simple callout-important callout-titled" title="Problemi">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Problemi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Spiega ciascuno dei concetti seguenti con una frase:</p>
<ul>
<li>probabilità.</li>
<li>funzione di massa di probabilità.</li>
<li>funzione di densità di probabilità.</li>
<li>distribuzione di probabilità.</li>
<li>distribuzione di probabilità discreta.</li>
<li>distribuzione di probabilità continua.</li>
<li>funzione di distribuzione cumulativa (cdf).</li>
<li>verosimiglianza</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-important callout-titled" title="Problemi">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Problemi
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>All’esame ti verrà chiesto di:</p>
<ul>
<li>Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\theta\)</span>.</li>
<li>Calcolare la stima di massima verosimiglianza.</li>
<li>Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.</li>
</ul>
</div>
</div>
</div>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.4.2 (2024-10-31)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.3.2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] thematic_0.1.6   MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.10.0      </span></span>
<span><span class="co">#&gt;  [5] gridExtra_2.3    patchwork_1.3.0  bayesplot_1.11.1 psych_2.4.12    </span></span>
<span><span class="co">#&gt;  [9] scales_1.3.0     markdown_1.13    knitr_1.49       lubridate_1.9.4 </span></span>
<span><span class="co">#&gt; [13] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.0.4     </span></span>
<span><span class="co">#&gt; [17] readr_2.1.5      tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.1   </span></span>
<span><span class="co">#&gt; [21] tidyverse_2.0.0  rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] generics_0.1.3    stringi_1.8.4     lattice_0.22-6    hms_1.1.3        </span></span>
<span><span class="co">#&gt;  [5] digest_0.6.37     magrittr_2.0.3    evaluate_1.0.3    grid_4.4.2       </span></span>
<span><span class="co">#&gt;  [9] timechange_0.3.0  fastmap_1.2.0     rprojroot_2.0.4   jsonlite_1.9.1   </span></span>
<span><span class="co">#&gt; [13] mnormt_2.1.1      cli_3.6.4         rlang_1.1.5       munsell_0.5.1    </span></span>
<span><span class="co">#&gt; [17] withr_3.0.2       tools_4.4.2       parallel_4.4.2    tzdb_0.4.0       </span></span>
<span><span class="co">#&gt; [21] colorspace_2.1-1  pacman_0.5.1      vctrs_0.6.5       R6_2.6.1         </span></span>
<span><span class="co">#&gt; [25] lifecycle_1.0.4   htmlwidgets_1.6.4 pkgconfig_2.0.3   pillar_1.10.1    </span></span>
<span><span class="co">#&gt; [29] gtable_0.3.6      glue_1.8.0        xfun_0.51         tidyselect_1.2.1 </span></span>
<span><span class="co">#&gt; [33] rstudioapi_0.17.1 farver_2.1.2      htmltools_0.5.8.1 nlme_3.1-167     </span></span>
<span><span class="co">#&gt; [37] labeling_0.4.3    rmarkdown_2.29    compiler_4.4.2</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Johnson2022bayesrules" class="csl-entry" role="listitem">
Johnson, A. A., Ott, M., &amp; Dogucu, M. (2022). <em><span>Bayes Rules! An Introduction to Bayesian Modeling with R</span></em>. CRC Press.
</div>
<div id="ref-schervish2014probability" class="csl-entry" role="listitem">
Schervish, M. J., &amp; DeGroot, M. H. (2014). <em>Probability and statistics</em> (Vol. 563). Pearson Education London, UK:
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/probability/14_gauss.html" class="pagination-link" aria-label="Assunzione di gaussianità e trasformazioni dei dati">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/probability/16_likelihood_gauss.html" class="pagination-link" aria-label="La verosimiglianza gaussiana">
        <span class="nav-page-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">La verosimiglianza gaussiana</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/15_likelihood.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>