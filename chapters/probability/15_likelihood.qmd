# La verosimiglianza {#sec-prob-likelihood}

::: callout-important
## In questo capitolo, imparerai a:

- **Comprendere il concetto di verosimiglianza**: Scoprirai il ruolo fondamentale che la verosimiglianza svolge nella stima dei parametri statistici.
- **Generare grafici della funzione di verosimiglianza**: 
  - Implementare grafici per la funzione di verosimiglianza nel caso binomiale.
- **Interpretare i grafici della funzione di verosimiglianza**: Sviluppare le competenze necessarie per analizzare e trarre conclusioni dai grafici generati.
- **Capire il principio di stima di massima verosimiglianza (MLE)**: Approfondiremo il metodo di stima di massima verosimiglianza.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Estimation* [@schervish2014probability].
- Leggere il capitolo *Bayes' rule* [@Johnson2022bayesrules].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


## Introduzione {.unnumbered}

I ricercatori utilizzano diversi modelli matematici per descrivere e prevedere il comportamento dei dati osservati. Questi modelli si distinguono tra loro per la struttura funzionale, ovvero il modo in cui collegano le variabili osservate con parametri teorici. La scelta del modello migliore avviene confrontando le previsioni teoriche generate dal modello con i dati effettivamente osservati. Il modello che produce previsioni più vicine ai dati reali viene considerato il più adeguato per descrivere il fenomeno studiato.

In questo processo di confronto, la funzione di verosimiglianza gioca un ruolo fondamentale. Essa quantifica la probabilità che i dati osservati siano stati generati da un particolare modello con determinati valori dei suoi parametri. In altre parole, la verosimiglianza misura quanto i dati siano compatibili con il modello ipotizzato.

È importante sottolineare che la funzione di verosimiglianza, da sola, non costituisce un modello scientifico completo. Un modello scientifico completo, infatti, comprende altri elementi essenziali, tra cui:

- **I priori (approccio bayesiano)**: rappresentano le ipotesi iniziali sui parametri del modello prima che i dati vengano osservati. Queste ipotesi preliminari riflettono la conoscenza pregressa o le aspettative teoriche.

- **La modellazione dell'errore di misurazione**: tiene conto delle imperfezioni inevitabili nelle misurazioni. Considerare l'errore consente al modello di spiegare non solo il processo teorico ma anche le discrepanze tra dati osservati e realtà.

Nell'approccio bayesiano, i priori e la funzione di verosimiglianza vengono combinati per ottenere la distribuzione a posteriori, che rappresenta una sintesi aggiornata delle conoscenze sui parametri alla luce dei dati osservati. Questo passaggio è cruciale perché la scelta dei priori può influenzare notevolmente le conclusioni dell'analisi.

In conclusione, mentre la funzione di verosimiglianza descrive come potrebbero essere generati i dati da un modello specifico, un modello scientifico completo include ulteriori componenti, come le ipotesi iniziali (priori) e la gestione degli errori di misurazione, per rappresentare in modo più preciso e realistico il fenomeno oggetto di studio. Questo capitolo approfondirà proprio il concetto di verosimiglianza e il suo ruolo centrale nel processo di inferenza statistica.

## Il Principio della Verosimiglianza e la sua Formalizzazione

La verosimiglianza quantifica quanto i dati osservati siano compatibili con diversi valori dei parametri di un modello. In termini più semplici, la verosimiglianza indica quanto ciascun valore possibile dei parametri sia plausibile nel descrivere il fenomeno osservato.

::: {#def-}
Consideriamo un vettore aleatorio $X$, la cui distribuzione è descritta da una funzione di densità di probabilità (nel caso di variabili continue) o da una funzione di massa di probabilità (nel caso di variabili discrete), indicata con $f(x; \theta)$. Qui, $\theta$ rappresenta un vettore di parametri che appartiene a uno spazio parametrico $\Theta$. Una volta osservato un valore specifico $x$ del vettore $X$, possiamo definire la **funzione di verosimiglianza** per i parametri $\theta$ come:

$$
L(\theta; x) = f(x; \theta).
$$

È importante sottolineare che nella definizione della verosimiglianza, la variabile osservata $x$ è fissa, mentre la funzione viene considerata in relazione ai parametri $\theta$. Al contrario, nella definizione originale della funzione $f$, i parametri sono fissi e la funzione è valutata rispetto ai possibili valori della variabile aleatoria $X$.
::: 

### Relazione tra Verosimiglianza e Funzione di Probabilità

La formula matematica che collega i dati ai parametri è la stessa sia per la funzione di verosimiglianza che per la funzione di densità (o massa) di probabilità. Ciò che cambia è l'interpretazione e il modo in cui questa formula viene utilizzata nei due contesti.

- **Funzione di densità (o massa) di probabilità**:  
  Questa funzione descrive il processo generativo dei dati, indicando la probabilità (o densità di probabilità, nel caso continuo) che un determinato valore dei dati venga osservato, supponendo che i parametri del modello siano già noti e fissati. Qui, quindi, i parametri sono fissi e i dati variano.

- **Funzione di verosimiglianza**:  
  La funzione di verosimiglianza inverte questo punto di vista. In essa, i dati osservati sono fissati, mentre i parametri $\theta$ variano. La verosimiglianza misura la plausibilità di ciascun valore possibile dei parametri nel descrivere i dati che sono stati effettivamente osservati. In sostanza, valuta la compatibilità di ogni valore dei parametri rispetto ai dati raccolti.

Formalmente, la relazione tra queste due funzioni può essere espressa come:

$$
L(\theta \mid y) \propto p(y \mid \theta),
$$

dove:

- $L(\theta \mid y)$ rappresenta la funzione di verosimiglianza, ossia la plausibilità dei parametri $\theta$ alla luce dei dati osservati $y$;
- $p(y \mid \theta)$ indica la funzione di densità (o massa) di probabilità, cioè la probabilità di osservare i dati $y$ assumendo che i parametri $\theta$ siano già noti e fissi.

In sintesi, la funzione di probabilità risponde alla domanda "dato un certo insieme di parametri, quanto è probabile osservare questi dati?", mentre la funzione di verosimiglianza si pone la domanda opposta: "dati questi dati osservati, quali valori dei parametri sono più plausibili?" Questa distinzione è fondamentale per l’inferenza statistica e per determinare i parametri più adatti a descrivere i fenomeni osservati.

## Verosimiglianza Binomiale

Consideriamo un esempio pratico per chiarire il concetto: il lancio di una moneta. Supponiamo di osservare 23 teste su 30 lanci. La probabilità di osservare esattamente questo risultato, dato un certo valore della probabilità di successo $\theta$, può essere calcolata usando la funzione di massa della distribuzione binomiale:

$$
P(Y = y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y},
$$

dove:

- $n$ è il numero totale di lanci,
- $y$ è il numero di successi osservati (in questo caso, teste),
- $\theta$ è la probabilità di ottenere successo (testa) in ogni singolo lancio.

La **funzione di verosimiglianza**, invece, pone attenzione sui valori dei parametri $\theta$ che meglio spiegano i dati osservati. Per una distribuzione binomiale, questa funzione è definita come:

$$
\mathcal{L}(\theta \mid y) = \theta^y (1 - \theta)^{n - y}.
$$

Qui il coefficiente binomiale $\binom{n}{y}$ viene omesso, perché non dipende dal parametro $\theta$, quindi non influisce nella valutazione della plausibilità dei valori dei parametri.

### Esempio: Verosimiglianza per il Lancio di una Moneta

Assumendo:

- $n = 30$ (numero totale di lanci),
- $y = 23$ (numero di teste osservate),

la funzione di verosimiglianza diventa:

$$
\mathcal{L}(\theta \mid y) = \theta^{23}(1 - \theta)^7.
$$

Questa funzione ci consente di valutare la plausibilità di diversi valori di $\theta$ rispetto ai dati osservati. Ad esempio, possiamo calcolare e visualizzare la funzione di verosimiglianza per una serie di valori di $\theta$ compresi nell'intervallo [0, 1].

In R, possiamo farlo con il seguente codice:

```{r}
# Parametri
n <- 30
y <- 23

# Definizione della griglia di valori per theta
theta <- seq(0, 1, length.out = 100)

# Calcolo della verosimiglianza
likelihood <- theta^y * (1 - theta)^(n - y)

# Visualizzazione della funzione di verosimiglianza
library(ggplot2)

ggplot(
  data.frame(theta, likelihood),
  aes(x = theta, y = likelihood)
) +
  geom_line(color = "blue") +
  labs(
    title = "Funzione di Verosimiglianza",
    x = expression(theta),
    y = "Verosimiglianza"
  )
```

### Interpretazione della Verosimiglianza

- **Valore di $\theta$**: La funzione di verosimiglianza evidenzia quali valori di $\theta$ sono più plausibili alla luce dei dati osservati.
- **Stima di Massima Verosimiglianza (MLE)**: Il valore di $\theta$ che massimizza la funzione di verosimiglianza è detto stima di massima verosimiglianza. Per trovarlo, possiamo usare un approccio numerico, calcolando direttamente la probabilità binomiale e identificando il valore che rende massima la verosimiglianza:

```{r}
# Calcolo delle probabilità binomiali
probabilities <- dbinom(y, size = n, prob = theta)

# Identificazione dell'indice del massimo
max_index <- which.max(probabilities)

# Recupero del valore di theta corrispondente
optimal_theta <- theta[max_index]
optimal_theta
```

**Spiegazione del codice:**

- `dbinom(y, size = n, prob = theta)` calcola la probabilità di osservare esattamente $y$ successi su $n$ tentativi, per ciascun valore di $\theta$.
- `which.max(probabilities)` trova l’indice del valore massimo nella sequenza delle probabilità.
- `theta[max_index]` restituisce il valore di $\theta$ corrispondente al massimo trovato.

Questo esempio illustra come la funzione di verosimiglianza sia uno strumento per identificare e valutare la plausibilità dei parametri che meglio descrivono i dati empirici raccolti.

## La Funzione di Log-Verosimiglianza

La **log-verosimiglianza** è definita come il logaritmo naturale della funzione di verosimiglianza:

$$
\ell(\theta) = \log \mathcal{L}(\theta \mid y).
$$

Questa trasformazione è utile perché rende più semplici i calcoli e garantisce una maggiore stabilità numerica, soprattutto quando si lavora con grandi insiemi di dati.

### Esempio grafico della log-verosimiglianza

Utilizzando l’esempio precedente del lancio di una moneta, dove abbiamo osservato 23 successi su 30 lanci, possiamo calcolare e visualizzare la log-verosimiglianza nel seguente modo:

```{r}
# Parametri
n <- 30  # numero totale di lanci
y <- 23  # numero di teste osservate

# Sequenza di valori possibili per theta
theta <- seq(0, 1, length.out = 1000)

# Calcolo della log-verosimiglianza
log_likelihood <- dbinom(y, size = n, prob = theta, log = TRUE)

# Preparazione dei dati per la visualizzazione
data <- data.frame(theta = theta, log_likelihood = log_likelihood)

# Grafico con ggplot2
ggplot(data, aes(x = theta, y = log_likelihood)) +
  geom_line(color = "blue", linewidth = 1) +
  labs(
    title = "Funzione di Log-Verosimiglianza",
    x = expression(theta),
    y = "Log-verosimiglianza"
  )
```

### Individuazione del massimo della log-verosimiglianza

Il valore massimo della log-verosimiglianza corrisponde alla stima di massima verosimiglianza (MLE) già calcolata precedentemente. Possiamo identificarlo facilmente tramite il seguente codice:

```{r}
# Individuazione del massimo della log-verosimiglianza
max_index <- which.max(log_likelihood)

# Valore ottimale di theta
optimal_theta <- theta[max_index]
optimal_theta
```

### Interpretazione

Il valore $\theta$ che massimizza la log-verosimiglianza rappresenta la stima più plausibile del parametro alla luce dei dati osservati. La log-verosimiglianza semplifica questa valutazione grazie alla trasformazione logaritmica, rendendo più agevole identificare il valore ottimale dei parametri.

## Verosimiglianza Congiunta

Nell'inferenza statistica capita spesso di avere più osservazioni indipendenti provenienti dallo stesso processo probabilistico. Ad esempio, possiamo raccogliere diverse misurazioni (o osservazioni), ciascuna generata indipendentemente e che segue la stessa distribuzione statistica. Questo scenario viene chiamato condizione di indipendenza e identica distribuzione (spesso abbreviata con IID, dall'inglese "Independent and Identically Distributed").

### Come calcolare la Verosimiglianza Congiunta

Quando abbiamo più osservazioni indipendenti, vogliamo valutare la plausibilità complessiva di un parametro (ad esempio, la probabilità di successo $\theta$) per tutte le osservazioni insieme. Questa plausibilità complessiva è chiamata **verosimiglianza congiunta** e si ottiene moltiplicando le probabilità individuali di ogni osservazione:

$$
p(y_1, y_2, \ldots, y_n \mid \theta) = \prod_{i=1}^{n} p(y_i \mid \theta),
$$

ovvero

$$
p(y_1, y_2, \ldots, y_n \mid \theta) = p(y_1 \mid \theta) \times p(y_2 \mid \theta) \times \ldots \times p(y_n \mid \theta).
$$

Quindi, la funzione di verosimiglianza congiunta è:

$$
\mathcal{L}(\theta \mid Y) = \prod_{i=1}^{n} \mathcal{L}(\theta \mid y_i) = \prod_{i=1}^{n} p(y_i \mid \theta),
$$

ovvero

$$
\mathcal{L}(\theta \mid Y) = p(y_1 \mid \theta) \times p(y_2 \mid \theta) \times \ldots \times p(y_n \mid \theta).
$$

Il valore del parametro $\theta$ che rende massima questa funzione (ossia rende più plausibili i dati osservati) è chiamato **stimatore di massima verosimiglianza (MLE)**.

### Log-Verosimiglianza Congiunta

Moltiplicare molte probabilità può causare problemi numerici. Per questo motivo, spesso si usa la **log-verosimiglianza**, che trasforma le moltiplicazioni in somme:

$$
\log \mathcal{L}(\theta \mid Y) = \log p(y_1 \mid \theta) + \log p(y_2 \mid \theta) + \ldots + \log p(y_n \mid \theta).
$$

### Esempio pratico con osservazioni raggruppate

Consideriamo un esempio pratico con quattro gruppi distinti di osservazioni binomiali indipendenti:

- **Gruppo 1**: 23 successi su 30 prove
- **Gruppo 2**: 20 successi su 28 prove
- **Gruppo 3**: 29 successi su 40 prove
- **Gruppo 4**: 29 successi su 36 prove

In questo caso, la log-verosimiglianza congiunta si calcola sommando le log-verosimiglianze di ciascun gruppo:

$$
\log \mathcal{L}(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
$$

dove $n_i$ e $y_i$ indicano rispettivamente il numero di prove totali e il numero di successi nel gruppo $i$-esimo. In termini espliciti:

$$
\begin{align}
\log \mathcal{L}(\theta) &= [23\log(\theta) + (30-23)\log(1 - \theta)] +\notag\\
& \quad [20\log(\theta) + (28-20)\log(1 - \theta)] + \notag\\
& \quad [29\log(\theta) + (40-29)\log(1 - \theta)] + \notag\\
&\quad [29\log(\theta) + (36-29)\log(1 - \theta)].
\end{align}
$$

Questa formula ci dice quanto è plausibile il parametro $\theta$ considerando contemporaneamente tutti i gruppi.

### Implementazione in R

In R, definiamo così la funzione che calcola la log-verosimiglianza congiunta:

```{r}
log_verosimiglianza_congiunta <- function(theta, dati) {
  # Evitiamo problemi numerici: theta deve essere tra 0 e 1, ma non esattamente 0 o 1
  theta <- pmax(pmin(theta, 1 - 1e-10), 1e-10) # Impedisce a theta di essere 0 o 1 esattamente
  
  log_likelihood <- 0
  for (gruppo in dati) {
    n <- gruppo[1]  # numero totale di prove nel gruppo
    y <- gruppo[2]  # numero di successi nel gruppo
    log_likelihood <- log_likelihood + y * log(theta) + (n - y) * log(1 - theta)
  }
  
  return(-log_likelihood) # restituiamo il negativo per semplificare la ricerca del massimo
}
```

```{r}
# Dati dei gruppi
dati_gruppi <- list(c(30, 23), c(28, 20), c(40, 29), c(36, 29))
```

### Come trovare il valore ottimale di $\theta$

Per trovare numericamente il valore di $\theta$ che rende massima la log-verosimiglianza, usiamo la funzione `optim()`:

- `optim()` è una funzione che cerca automaticamente il valore di un parametro (in questo caso $\theta$) che minimizza o massimizza un’altra funzione data. Qui vogliamo minimizzare il negativo della log-verosimiglianza, che equivale a massimizzare la verosimiglianza stessa.

```{r}
result <- optim(
  par = 0.5,                          # Valore iniziale di theta
  fn = log_verosimiglianza_congiunta, # Funzione da minimizzare
  dati = dati_gruppi,                 # Dati dei gruppi
  method = "L-BFGS-B",                # Metodo numerico con limiti
  lower = 0,                          # Valore minimo che theta può assumere
  upper = 1                           # Valore massimo che theta può assumere
)

# Stima ottimale di theta
result$par
```

### Grafico della log-verosimiglianza

Vediamo come cambia la log-verosimiglianza per vari valori di $\theta$:

```{r}
theta_values <- seq(0.01, 0.99, length.out = 100)

log_likelihood_values <- numeric(length(theta_values))

# Usiamo un ciclo for()
for (i in 1:length(theta_values)) {
  log_likelihood_values[i] <- 
    log_verosimiglianza_congiunta(theta_values[i], dati_gruppi)
}

ggplot(
  data.frame(
    theta = theta_values, 
    log_likelihood = log_likelihood_values
  ), 
  aes(x = theta, y = log_likelihood)) +
  geom_line(color = "blue") +
  labs(
    title = "Funzione di Log-Verosimiglianza Congiunta",
    x = "Theta",
    y = "Log-verosimiglianza negativa"
  )
```

In conclusione, la log-verosimiglianza congiunta permette di considerare tutti i dati insieme per ottenere una stima affidabile del parametro $\theta$. 

## La Verosimiglianza Marginale

La **verosimiglianza marginale** è un concetto importante nell'inferenza bayesiana che ci aiuta a valutare quanto un modello statistico sia in grado di spiegare i dati raccolti, considerando tutte le possibili incertezze sui parametri del modello. A differenza della verosimiglianza standard, che valuta la plausibilità dei dati rispetto a un valore fisso del parametro, la verosimiglianza marginale tiene conto di tutti i valori possibili che il parametro può assumere, ciascuno pesato in base a quanto riteniamo inizialmente probabile quel valore (probabilità a priori).

### Caso con Parametri Discreti

Vediamo un esempio pratico per capire meglio. Immaginiamo di avere un esperimento con 10 tentativi in cui otteniamo esattamente 7 successi. Supponiamo che il parametro $\theta$ (la probabilità di successo) possa assumere soltanto i valori discreti $0.1, 0.5, 0.9$. Per valutare correttamente la verosimiglianza marginale, dobbiamo assegnare una probabilità a priori a ciascuno di questi valori, assicurandoci che la loro somma sia esattamente 1.

Ad esempio, potremmo assegnare probabilità uniformi:

- $p(0.1) = 1/3$
- $p(0.5) = 1/3$
- $p(0.9) = 1/3$

Oppure potremmo considerare che il valore centrale sia due volte più probabile degli altri:

- $p(0.1) = 1/4$
- $p(0.5) = 1/2$
- $p(0.9) = 1/4$

Per calcolare la verosimiglianza marginale, è necessario calcolare la probabilità di osservare i dati (7 successi su 10 tentativi) per ciascun valore di θ, moltiplicare per la probabilità a priori di quel valore di θ, e poi sommare tutti i risultati:

$$
\begin{align}
p(k=7 \mid n=10) &= p(k=7 \mid \theta=0.1)·p(θ=0.1) + \notag\\
& \quad p(k=7 \mid \theta=0.5)·p(\theta=0.5) + \notag\\
& \quad p(k=7 \mid \theta=0.9)·p(\theta=0.9),
\end{align}
$$

dove:

- $p(k=7 \mid \theta =0.1)$ = probabilità di osservare 7 successi su 10 tentativi quando $\theta =0.1 = \binom{10}{7}0.1^7(1-0.1)^3$;
- $p(\theta=0.1)$ = probabilità a priori che $\theta$ sia 0.1 (ad esempio 1/3 o 1/4 a seconda della scelta).

Calcoliamo ad esempio usando le probabilità a priori uniformi:

$$
p(k=7 \mid n=10) = \binom{10}{7}0.1^7(0.9)^3·\frac{1}{3} + \binom{10}{7}0.5^7(0.5)^3·\frac{1}{3} + \binom{10}{7}0.9^7(0.1)^3·\frac{1}{3}.
$$

### Caso con Parametri Continui

Nella maggior parte delle situazioni reali, però, il parametro $\theta$ può assumere qualsiasi valore in un intervallo continuo (ad esempio, tra 0 e 1). In questo caso, invece di sommare, dobbiamo integrare su tutti i valori possibili di $\theta$:

$$
p(k = 7 \mid n = 10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 p(\theta) \, d\theta.
$$

Anche in questo caso, $p(\theta)$ rappresenta la densità di probabilità iniziale per ogni valore di $\theta$.

### Calcolo Numerico della Verosimiglianza Marginale

Per calcolare la verosimiglianza marginale con un parametro continuo in modo pratico, possiamo usare tecniche di integrazione numerica. Vediamo come fare usando il linguaggio R:

```{r}
# Funzione che descrive la probabilità dei dati per ogni valore di theta
likelihood <- function(theta) {
  dbinom(x = 7, size = 10, prob = theta)
}

# Calcolo numerico della verosimiglianza marginale
marginal_likelihood <- integrate(likelihood, lower = 0, upper = 1)$value

# Risultato finale
cat("La verosimiglianza marginale è:", marginal_likelihood, "\n")
```

### Interpretazione della Verosimiglianza Marginale

La verosimiglianza marginale ci dice quanto, in generale, il nostro modello sia compatibile con i dati osservati, tenendo conto di tutte le incertezze possibili sul parametro $\theta$. Possiamo immaginarla come la "media" delle probabilità che i dati siano stati generati dal modello, calcolata su tutte le possibilità per $\theta$.

È importante notare che la verosimiglianza marginale non valuta i dati rispetto a un singolo valore preciso di $\theta$, ma integra le informazioni provenienti da tutte le possibili scelte del parametro.

### Ruolo nell'Inferenza Bayesiana

La verosimiglianza marginale ha un ruolo centrale nella statistica bayesiana, poiché serve come fattore di normalizzazione nella formula di Bayes:

$$
p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},
$$

dove $p(D)$ è proprio la verosimiglianza marginale. Questa quantità garantisce che la distribuzione a posteriori $p(\theta \mid D)$ sia una vera distribuzione di probabilità (ossia, che la sua area totale sia 1).

In conclusione, la verosimiglianza marginale:

- ci permette di capire quanto un modello sia plausibile considerando tutte le possibili incertezze sui parametri;
- può essere calcolata sommando o integrando su tutti i valori possibili dei parametri, pesati dalla probabilità iniziale;
- è fondamentale per confrontare modelli diversi e garantire l'accuratezza delle conclusioni statistiche.

## Riflessioni Conclusive

La funzione di verosimiglianza rappresenta un ponte fondamentale tra i dati osservati e i parametri di un modello statistico, offrendo una misura della plausibilità dei dati rispetto a diversi valori possibili dei parametri. La sua costruzione richiede l’integrazione di tre elementi chiave: il modello statistico ipotizzato come generatore dei dati, lo spazio dei parametri associato al modello e le osservazioni empiriche disponibili.

Nell’ambito dell’inferenza statistica, la funzione di verosimiglianza svolge un ruolo centrale. Essa consente di valutare quanto bene diversi valori dei parametri siano in grado di spiegare i dati osservati, diventando così uno strumento essenziale per la stima dei parametri e la selezione del modello. La sua corretta applicazione è determinante per garantire analisi dati rigorose e interpretazioni affidabili dei risultati.

In sintesi, padroneggiare il concetto di verosimiglianza e saperlo applicare in modo appropriato sono competenze indispensabili per chi si occupa di ricerca empirica e analisi di dati complessi. La verosimiglianza non è solo uno strumento tecnico, ma un pilastro metodologico che supporta la comprensione e l’interpretazione dei fenomeni osservati attraverso modelli statistici.

## Esercizi {.unnumbered}

::: {.callout-important title="Problemi" collapse="true"}

Spiega ciascuno dei concetti seguenti con una frase:

- probabilità.
- funzione di massa di probabilità.
- funzione di densità di probabilità.
- distribuzione di probabilità.
- distribuzione di probabilità discreta.
- distribuzione di probabilità continua.
- funzione di distribuzione cumulativa (cdf).
- verosimiglianza

:::

::: {.callout-important title="Problemi" collapse="true"}
All'esame ti verrà chiesto di:

- Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori $\theta$.
- Calcolare la stima di massima verosimiglianza.
- Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

