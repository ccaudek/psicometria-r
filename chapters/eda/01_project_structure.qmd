# Le fasi del progetto di analisi dei dati {#sec-eda-proj-structure}

::: callout-important 
## In questo capitolo imparerai a

- gestire il ciclo di vita di un progetto di analisi: dalla definizione della domanda di ricerca alla comunicazione dei risultati
- organizzare i file del progetto per garantirne portabilità e condivisione
:::

::: callout-tip 
## Prerequisiti

- Leggere [Veridical Data Science](https://vdsbook.com) [@yu2024veridical] focalizzandoti sul primo capitolo, che introduce le problematiche della data science, e sul quarto capitolo, che fornisce le linee guida dettagliate sull'organizzazione di un progetto di analisi dei dati.
:::

::: callout-caution 
## Preparazione del Notebook

```{r}
# Carica il file _common.R per impostazioni di pacchetti e opzioni
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione

Una gestione accurata ed efficace dei dati è fondamentale in molte discipline, inclusa la psicologia, dove l’analisi di dataset complessi rappresenta un aspetto centrale della ricerca. Garantire che i dati siano raccolti con precisione, organizzati in modo chiaro e facilmente accessibili per analisi e verifiche è essenziale per preservare l’integrità del lavoro scientifico e promuoverne la riproducibilità. Una gestione rigorosa dei dati assicura qualità e affidabilità in tutte le fasi di un progetto, dalla raccolta alla documentazione dei processi di elaborazione e delle eventuali modifiche apportate.

Dati ben organizzati e documentati non solo semplificano e rendono più efficiente il processo di analisi, ma riducono anche il rischio di errori, migliorando l’utilizzabilità e l’interpretazione delle informazioni. Questo è particolarmente rilevante quando si lavora con dataset provenienti da fonti eterogenee o con strutture complesse. Inoltre, la trasparenza e la completezza nella gestione dei dati rappresentano una condizione imprescindibile per garantire la riproducibilità della ricerca, un pilastro fondamentale della scienza. La possibilità per altri ricercatori di replicare i risultati utilizzando gli stessi dati e metodi rafforza la credibilità delle conclusioni e contribuisce a costruire un progresso scientifico condiviso e solido. Di conseguenza, una gestione dei dati responsabile non è solo una buona pratica, ma una necessità per la produzione di conoscenze affidabili e sostenibili.

::: {.callout-tip collapse="true"}
## Domande Iniziali

Prima di immergerti nella lettura di questo capitolo, prenditi un momento per riflettere sulle seguenti domande. Quali risposte daresti prima di leggere il materiale?

- Perché è importante organizzare e documentare accuratamente i dati in un progetto di ricerca?
- Quali problemi possono emergere se si inizia a scrivere codice senza una pianificazione adeguata?
- Quali vantaggi offre R per la gestione dei dati rispetto ad altri strumenti?
- Quali strategie potrebbero migliorare la riproducibilità del tuo lavoro?
- Come struttureresti un progetto di analisi dati per mantenerlo chiaro e facilmente replicabile?

Ora, mentre leggi il capitolo, confronta le tue risposte con i concetti discussi.
:::

## Pianificazione

Quando ci si appresta ad affrontare un progetto complesso, è raro che i programmatori esperti inizino a scrivere codice senza una pianificazione adeguata [@gillespie2016efficient]. Al contrario, dedicano tempo a definire con cura i passaggi necessari per portare a termine il compito nel modo più efficiente possibile. Come sottolinea @berkun2005art, “una preparazione intelligente minimizza il lavoro”. Sebbene i motori di ricerca possano essere utili per identificare strategie appropriate, approcci basati su tentativi ed errori – come scrivere codice in modo casuale e cercare soluzioni ai messaggi di errore che ne derivano – risultano generalmente inefficienti e dispendiosi in termini di tempo.

Il pensiero strategico nelle fasi iniziali di un progetto è cruciale. Le decisioni prese in questo momento possono avere ripercussioni a cascata sull’intero ciclo di vita del progetto. Scelte sbagliate o affrettate possono generare problemi difficili da correggere, con conseguenze negative sull’efficacia, la sostenibilità e la qualità del lavoro. Nel contesto dello sviluppo software, questo fenomeno è stato definito “debito tecnico” [@kruchten2012technical], ovvero “codice non del tutto corretto che rimandiamo di sistemare”. Accumulare debito tecnico significa aumentare la probabilità di dover intervenire costantemente in futuro, con un impatto significativo su costi, tempi e qualità del risultato finale.

Per minimizzare il debito tecnico, il punto di partenza ideale è spesso rappresentato da strumenti semplici, come carta e penna, uniti a una mente aperta e creativa. Abbozzare idee e definire con precisione gli obiettivi, senza farsi imbrigliare dai vincoli di una specifica tecnologia, è un esercizio estremamente utile prima di iniziare a scrivere codice. In questa fase, la definizione della “visione” del progetto è un processo che richiede creatività e riflessione, indipendenti dai limiti del software scelto per l’implementazione. Una volta chiariti gli obiettivi, i requisiti e le sfide, diventa più agevole selezionare tecnologie e strumenti adeguati, valutando aspetti quali scalabilità, manutenibilità e compatibilità con le esigenze future.

Solo una pianificazione oculata può ridurre il rischio di errori e assicurare che il progetto sia sostenibile nel lungo termine, prevenendo l’accumulo di debiti tecnici e aumentando l’efficienza complessiva del lavoro.

### Cinque suggerimenti per un workflow efficiente

@gillespie2016efficient presentano cinque suggerimenti per un workflow efficiente:

1. **Inizia senza scrivere codice, ma con una mente chiara e possibilmente carta e penna**. Questo ti aiuterà a mantenere gli obiettivi al centro dell’attenzione, senza farti distrarre dalla tecnologia.  
2. **Fai un piano**. Le dimensioni e la natura del piano dipenderanno dal progetto, ma definire tempistiche, risorse e suddividere il lavoro in “blocchi” ti renderà più efficace quando inizierai.  
3. **Seleziona i pacchetti che utilizzerai per implementare il piano in anticipo**. Minuti spesi nella ricerca e selezione delle opzioni disponibili potrebbero farti risparmiare ore in futuro.  
4. **Documenta il tuo lavoro in ogni fase**. Il lavoro può essere efficace solo se comunicato chiaramente, e il codice può essere compreso in modo efficiente solo se commentato.  
5. **Rendi l’intero workflow il più riproducibile possibile**. Strumenti come Quarto possono aiutarti in questa fase di documentazione.

## Capacità di Gestione dei Dati in R

Una volta compresi i benefici di una pianificazione strategica, è importante disporre di strumenti adeguati per gestire, analizzare e documentare i dati. In questo capitolo ci concentreremo sul ruolo di R in un workflow efficiente di analisi dei dati. R è uno strumento potente e versatile, pensato per supportare tutte le fasi del ciclo di vita dei dati, dalla raccolta alla documentazione, risultando così un alleato indispensabile per chiunque lavori con dataset complessi. Le sue principali funzionalità includono:

- **Importazione ed esportazione dei dati**: Pacchetti come `readr` e `rio` semplificano l’importazione di dati da fonti diverse (file CSV, database, API web) e l’esportazione in formati adatti a svariati utilizzi.  
- **Pulizia e preparazione dei dati**: Grazie a pacchetti come `dplyr`, `tidyr` e `stringr`, R offre strumenti intuitivi per manipolare, trasformare e preparare i dati in modo efficiente, rendendoli pronti per l’analisi.  
- **Esplorazione e sintesi**: Con pacchetti come `dplyr` e `ggplot2`, R permette di calcolare statistiche descrittive, individuare pattern significativi e visualizzare distribuzioni e relazioni in modo chiaro e informativo.  
- **Documentazione dinamica**: Strumenti come R Markdown e Quarto consentono di integrare in un unico documento codice, analisi, testo esplicativo e risultati, facilitando la riproducibilità e la trasparenza del lavoro.  
- **Controllo delle versioni**: L’integrazione di Git in RStudio offre un sistema di gestione delle versioni che consente di monitorare modifiche, collaborare con altri e garantire la tracciabilità del processo analitico.

La combinazione di pianificazione strategica e strumenti avanzati come R rappresenta un approccio vincente per affrontare progetti complessi. Le funzionalità di R, se utilizzate in modo appropriato, riducono al minimo il rischio di errori e massimizzano l’efficienza e l’affidabilità dei risultati.

## Configurare l’Ambiente R

Per sfruttare al meglio le potenzialità di R è essenziale configurare correttamente RStudio e integrare i pacchetti fondamentali per la gestione dei dati. Una configurazione adeguata favorisce la riproducibilità e l’organizzazione del lavoro.

### Workspace e Cronologia

Accedi a **Tools > Global Options > General** e modifica le seguenti impostazioni:

- Disabilita l’opzione *Restore .RData into workspace at startup*.  
- Imposta *Save workspace to .RData on exit* su **Never**.

Queste scelte incoraggiano una gestione basata sugli script, rendendo il lavoro più trasparente e riducendo possibili conflitti tra sessioni diverse. Ogni analisi sarà così chiaramente documentata nel codice, evitando dipendenze da file temporanei o precedenti sessioni di lavoro.

### Pacchetti Essenziali

R è composto da un modulo base che fornisce le funzionalità fondamentali del linguaggio, ma il suo potere deriva dall’ampio ecosistema di pacchetti. Per questo corso (e in generale per un workflow efficiente), utilizzeremo regolarmente i seguenti pacchetti:

- **`here`**: per la gestione ordinata dei percorsi relativi, evitando problemi con i percorsi assoluti.  
- **`tidyverse`**: una raccolta di pacchetti (tra cui `dplyr`, `tidyr`, `ggplot2`) essenziali per la manipolazione, l’analisi e la visualizzazione dei dati.

Assicurati di installarli e caricarli all’inizio di ogni script con i comandi:

```r
# install.packages(c("here", "tidyverse"))
# installali solo la prima volta
library(here)
library(tidyverse)
```

### Gestione dei Progetti

I progetti in RStudio rappresentano uno strumento chiave per mantenere il lavoro ben organizzato. Sfruttare i progetti consente di creare ambienti separati, in cui ogni progetto ha la propria directory dedicata.

Per creare un nuovo progetto:

1. Vai su **File > New Project**.  
2. Seleziona la directory dove verrà creato il tuo progetto.  
3. Salva tutti i file correlati (script, dati, risultati) all’interno di questa directory.

Questo approccio previene confusione e facilita la navigazione, in quanto ogni progetto diventa un’unità autonoma, ideale per mantenere ordine e coerenza nel lavoro.

## Il Ciclo di Vita di un Progetto di Data Science

Una volta configurato un ambiente stabile, organizzato e pronto per l’analisi, è possibile affrontare in modo sistematico le diverse fasi che caratterizzano tipicamente un progetto di Data Science. Secondo la proposta di [@yu2024veridical], queste fasi sono:

1. **Formulazione del problema e raccolta dei dati**: Definizione delle domande di ricerca e acquisizione dei dataset.  
2. **Pulizia, preprocessing e analisi esplorativa**: Preparazione dei dati per l’analisi attraverso trasformazioni e sintesi.  
3. **Analisi predittiva e/o inferenziale** (opzionale): Modelli statistici o predittivi per rispondere alle domande di ricerca.  
4. **Valutazione dei risultati**: Interpretazione e verifica delle conclusioni tratte.  
5. **Comunicazione dei risultati**: Presentazione dei risultati in forma visiva e narrativa.

Non tutti i progetti prevedono la fase di analisi predittiva, ma quasi tutti attraversano le altre fasi. Mantenere un approccio organizzato e ben strutturato lungo tutto il ciclo di vita riduce il rischio di errori e favorisce la riproducibilità.

### Fase 1: Formulazione del Problema e Raccolta dei Dati

La definizione di una domanda di ricerca chiara e precisa rappresenta la base di qualunque progetto di data science. È cruciale stabilire con attenzione l’ambito dell’analisi:

1. **Ambito Applicativo**  
   In un contesto applicativo, l’obiettivo principale è spesso la realizzazione di un report descrittivo su un intervento o un’attività specifica (ad esempio, valutare l’efficacia di un programma di educazione sessuale rivolto agli adolescenti). In questo caso, l’attenzione si focalizza sulla descrizione dei risultati ottenuti, senza necessariamente approfondire i fondamenti teorici.

2. **Ambito della Ricerca Psicologica**  
   In un contesto di ricerca, invece, diventa essenziale giustificare le scelte metodologiche, la selezione degli strumenti di misura e la formulazione delle domande di ricerca sulla base della letteratura esistente e di teorie consolidate. (Questo è oggetto di approfondimento nel @sec-measurement.)

#### Caratteristiche di una Buona Domanda di Ricerca

La domanda di ricerca deve essere formulata in modo che sia possibile rispondervi analizzando i dati a disposizione. Talvolta, la domanda iniziale può risultare troppo ampia o irrealistica rispetto ai dati disponibili, rendendo necessario un processo di revisione. L’obiettivo è assicurare che i dati raccolti (o da raccogliere) siano appropriati e sufficienti a fornire una risposta solida al quesito di partenza.

#### Raccolta dei Dati

In questa fase, è altrettanto cruciale identificare chiaramente i dati da utilizzare e comprenderne la provenienza. Esistono principalmente due scenari:

1. **Utilizzo di Dati Esistenti**  
   Alcuni progetti si basano su dataset già disponibili, reperiti da repository pubblici, database aziendali o esperimenti precedenti.

2. **Nuova Raccolta di Dati**  
   Altri progetti richiedono la raccolta di dati ex novo. In entrambi i casi, è importante pianificare con cura le analisi prima di avviare la raccolta, onde evitare di ritrovarsi con dataset incompleti o non adatti agli obiettivi.

È inoltre essenziale documentare accuratamente le tecniche e procedure utilizzate per la raccolta dati, nonché valutare e dichiarare eventuali limitazioni che possano inficiare l’interpretazione dei risultati.

### Fase 2: Pulizia dei Dati e Analisi Esplorativa

#### Importazione ed Esportazione dei Dati in R

Una volta individuati i dati, occorre importarli in R in un formato adatto all’analisi, tipicamente in un **data frame**. Il pacchetto `rio` fornisce la funzione universale `import()`, che rende superfluo l’uso di funzioni specifiche per ciascun formato (es. `.csv`, `.xlsx`, `.json`, ecc.). Per organizzare al meglio i file, si consiglia di creare una struttura di cartelle chiara (es. `data/raw` per i dati grezzi e `data/processed` per i dati elaborati) e di usare percorsi relativi attraverso il pacchetto **here**.

Esempio di importazione:

```r
library(here)
library(rio)

df <- import(here("data", "raw", "my_data.csv"))
```

Per esportare i dati elaborati:

```r
export(df, here("data", "processed", "my_data_processed.csv"))
```

Questa strategia migliora la portabilità (il progetto può essere facilmente trasferito su altri computer) e la riproducibilità (poiché non è necessario modificare manualmente i percorsi dei file).

#### Pulizia dei Dati

Dopo avere definito la domanda di ricerca e recuperato i dati, il passo successivo è la pulizia. Un dataset “pulito” è ordinato, coerente e privo di ambiguità, così che sia comprensibile sia per il computer sia per l’analista. La pulizia dei dati implica l’individuazione di valori anomali, formattazioni errate, duplicati, incongruenze e valori mancanti, per poi sistemarli in modo appropriato (con codice, senza alterare i file dei dati grezzi originali).

La pulizia dei dati è una fase cruciale: aiuta a sviluppare una comprensione più approfondita del dataset e delle sue eventuali limitazioni. L’obiettivo è creare una versione dei dati che rifletta il più fedelmente possibile la realtà e che sia interpretata correttamente dal computer.

#### Preprocessing

Il preprocessing consiste nell’effettuare quelle trasformazioni sui dati puliti che sono richieste da specifici algoritmi di analisi o di modellazione. Ad esempio, alcuni modelli possono richiedere variabili su scale comparabili oppure l’assenza di valori mancanti. Durante il preprocessing, si possono anche generare nuove variabili ritenute utili all’analisi.

Non esiste una procedura di preprocessing “universale”: ogni scelta va documentata e motivata, poiché implica decisioni soggettive che influiscono sull’interpretazione finale dei risultati.

#### Analisi Esplorativa dei Dati

Dopo l’importazione, la pulizia e il preprocessing, è il momento di esplorare il dataset. L’Analisi Esplorativa dei Dati (Exploratory Data Analysis, EDA) comprende:

- Calcolo di statistiche descrittive (misure di tendenza centrale, dispersione, etc.)  
- Identificazione di pattern e distribuzioni insolite  
- Creazione di tabelle di contingenza e diagrammi (istogrammi, boxplot, scatterplot, ecc.)

L’obiettivo principale è far familiarizzare il ricercatore con la struttura dei dati e fornire ipotesi da verificare nelle fasi successive.

### Fase 3: Analisi Predittiva e Inferenziale

A seconda degli obiettivi del progetto, l’analisi può includere modelli statistici o algoritmi di machine learning orientati all’inferenza e/o alla previsione. In psicologia, ciò può tradursi nell’uso di modelli di regressione, test statistici, modelli misti o metodi di classificazione e clustering. L’obiettivo è rispondere a domande di ricerca specifiche o fare inferenze su una popolazione più ampia. Nei contesti applicativi, può riguardare la costruzione di modelli predittivi per stimare il comportamento di variabili di interesse sulla base dei dati storici.

### Fase 4: Valutazione dei Risultati

In questa fase si valuta la bontà dei risultati ottenuti, sia dal punto di vista quantitativo (applicando metriche appropriate o test statistici) sia qualitativo (interpretazione teorica o pratica dei risultati). È il momento di ricollegarsi alla domanda di ricerca iniziale, per verificare se gli obiettivi sono stati raggiunti e quali implicazioni emergono.

### Fase 5: Comunicazione dei Risultati

L’ultima fase consiste nella comunicazione dei risultati a un pubblico più ampio, che può essere composto da ricercatori, committenti, stakeholder o utenti finali. È essenziale presentare i risultati in maniera chiara e accessibile, evitando di dare per scontata una profonda conoscenza del progetto o del gergo tecnico. Si può optare per:

- **Articoli scientifici**: per condividere le scoperte con la comunità accademica.  
- **Report interni**: per informare un team di lavoro o un ente finanziatore.  
- **Presentazioni**: per trasmettere rapidamente i risultati a un pubblico non specialistico.

Una comunicazione efficace richiede di spiegare in modo comprensibile le analisi e le figure mostrate, fornendo una guida interpretativa. L’uso di grafici e diagrammi ben curati può facilitare molto la fruizione delle informazioni.

## Organizzazione del Progetto

Un aspetto fondamentale per il successo e la riproducibilità di un progetto di analisi dati è l’organizzazione efficiente dei file, che include dati, codice e documentazione. Questi file dovrebbero essere riuniti all’interno di una singola cartella (o directory) dedicata al progetto.

### Home Directory

In RStudio, è possibile creare un file `nome_del_progetto.Rproj`, che definisce la cartella principale (home directory) da cui R avvia il lavoro. Basta aprire RStudio cliccando su questo file per lavorare automaticamente all’interno dell’ambiente del progetto, rendendo più agevole l’uso di percorsi relativi e la gestione di script, dati e output.

### Struttura di un Progetto

@yu2024veridical propone un template per organizzare in modo chiaro un progetto di analisi dati:

```
nome_progetto/
├── nome_progetto.Rproj
├── data/
│   ├── raw/
│   │   └── my_data.csv
│   ├── processed/
├── dslc_documentation/
│   ├── 01_data_cleaning.qmd
│   ├── 02_analysis.qmd
│   └── functions/
└── README.md
```

1. **`data/`:**  
   - **`raw/`**: contiene i dati grezzi, mai modificati;  
   - **`processed/`**: ospita i dati puliti ed elaborati, pronti per l’analisi;  
   - Importante inserire un **codebook** per documentare il significato delle variabili e le unità di misura.

2. **`dslc_documentation/`:**  
   - Contiene i file necessari per l’analisi (in Quarto `.qmd`, Jupyter Notebook `.ipynb`, o altro).  
   - Può includere una sottocartella **`functions/`** con script `.R` o `.py` contenenti funzioni personalizzate.

3. **`README.md`:**  
   - Descrive la struttura del progetto, gli obiettivi e i passaggi per replicare le analisi.

#### Vantaggi di questa struttura

1. **Organizzazione Chiara**: separare i dati grezzi da quelli elaborati riduce la confusione e il rischio di modifiche involontarie ai dati originali.  
2. **Riproducibilità**: la presenza di documentazione completa (codebook, file di analisi, README) consente di comprendere e replicare i risultati anche a distanza di tempo o da parte di terzi.  
3. **Portabilità**: lavorando con percorsi relativi (es. grazie al pacchetto **here** in R), l’intero progetto può essere trasferito da un computer all’altro senza bisogno di modificare il codice.  
4. **Efficienza**: la sottocartella `functions/` permette di riutilizzare codice e funzioni personalizzate, evitando duplicazioni negli script principali.

## Riflessioni Conclusive

La forza e la bellezza del codice risiedono nella sua riusabilità: una volta scritto, può essere eseguito infinite volte per ottenere risultati coerenti. Se l’ambiente di lavoro è configurato correttamente, lo stesso codice applicato agli stessi dati fornirà sempre gli stessi output. Questo principio, chiamato *riproducibilità computazionale*, è essenziale per garantire trasparenza e affidabilità nel lavoro scientifico.

La riproducibilità offre numerosi benefici:

- **Monitorare le modifiche del progetto**  
  La capacità di riprodurre il lavoro facilita il monitoraggio dell’evoluzione del progetto e delle scelte implementative nel tempo.  

- **Riprodurre il proprio lavoro**  
  Il primo vero vantaggio della riproducibilità è per chi ha sviluppato il codice: tornare a distanza di mesi (o anni) su un progetto e ritrovare un flusso di lavoro chiaro e replicabile agevola enormemente la revisione o l’estensione delle analisi.  

- **Condivisione e crescita collettiva**  
  Mettere a disposizione il proprio lavoro in forma riproducibile consente ad altri ricercatori di validare e ampliare i risultati, stimolando un progresso scientifico collettivo e robusto.

Rendere il proprio lavoro riproducibile richiede attenzione alla documentazione, un’organizzazione solida del progetto e l’uso di strumenti adeguati per garantire la stabilità dell’ambiente. I metodi e le tecniche esplorate in questo capitolo – dalla pianificazione strategica alla pulizia dei dati, dalla struttura delle cartelle al controllo di versione – sono tutti tasselli fondamentali per realizzare analisi rigorose, verificabili e di valore duraturo.

**In sintesi**, un progetto di Data Science di successo richiede sia un’accurata pianificazione strategica – per evitare debiti tecnici e massimizzare l’efficacia del proprio lavoro – sia l’uso di strumenti adeguati, come R e i relativi pacchetti, in un ambiente ben configurato. Seguire un ciclo di vita del progetto ben definito e mantenere un’organizzazione logica di dati, codice e documentazione consente di promuovere l’efficienza, ridurre i rischi di errore e favorire la riproducibilità, un valore fondante della scienza moderna.

::: {.callout-note}
Un problema cruciale nella psicologia contemporanea è la *crisi di replicabilità*, che evidenzia come molti risultati di ricerca non siano replicabili [@open2015estimating]. La *riproducibilità computazionale*, pur avendo un obiettivo più ristretto, si concentra sulla possibilità di ottenere gli stessi risultati applicando lo stesso codice agli stessi dati. Questo approccio, sebbene non risolva interamente la crisi, rappresenta un passo fondamentale verso una scienza più trasparente e rigorosa.
:::

::: {.callout-tip collapse="true"}
## Risposte alle Domande Iniziali

Ora che hai completato il capitolo, confrontiamo le risposte alle domande iniziali con quanto appreso:

1. **Perché è importante organizzare e documentare accuratamente i dati in un progetto di ricerca?**
   - Una gestione strutturata dei dati riduce il rischio di errori, facilita l’analisi e migliora la riproducibilità, rendendo il lavoro scientifico più affidabile e trasparente.

2. **Quali problemi possono emergere se si inizia a scrivere codice senza una pianificazione adeguata?**
   - Senza una pianificazione strategica si rischia di incorrere nel “debito tecnico”, accumulando codice disordinato che richiede correzioni costose in termini di tempo e risorse. Inoltre, si potrebbero fare scelte subottimali che compromettono la scalabilità e manutenibilità del progetto.

3. **Quali vantaggi offre R per la gestione dei dati rispetto ad altri strumenti?**
   - R fornisce strumenti avanzati per importazione, pulizia, analisi e visualizzazione dei dati, oltre a supportare la documentazione dinamica e il controllo delle versioni con Git. La sua ampia comunità e la disponibilità di pacchetti specializzati lo rendono particolarmente adatto per l’analisi statistica e la ricerca.

4. **Quali strategie potrebbero migliorare la riproducibilità del tuo lavoro?**
   - Utilizzare strumenti come Quarto per documentare il codice e le analisi, adottare percorsi relativi con il pacchetto `here`, strutturare i dati in cartelle ben organizzate e integrare il controllo delle versioni con Git.

5. **Come struttureresti un progetto di analisi dati per mantenerlo chiaro e facilmente replicabile?**
   - Adottando una struttura chiara, ad esempio:
     ```
     nome_progetto/
     ├── nome_progetto.Rproj
     ├── data/
     │   ├── raw/
     │   │   └── my_data.csv
     │   ├── processed/
     ├── dslc_documentation/
     │   ├── 01_data_cleaning.qmd
     │   ├── 02_analysis.qmd
     │   └── functions/
     └── README.md
     ```
     - Questa organizzazione separa i dati grezzi da quelli elaborati, include documentazione chiara e facilita la riproducibilità.

**Conclusione**: Riflettere in anticipo sui problemi e sulle strategie di gestione dei dati aiuta a costruire workflow più efficienti e affidabili. Se le tue risposte iniziali differivano da queste, quali nuovi spunti hai appreso da questo capitolo?
:::

## Esercizi

::: {.callout-tip title="Esercizio" collapse="true"}
L'obiettivo di questo esercizio è comprendere il ciclo di vita di un progetto di analisi dei dati, l'organizzazione del progetto e la gestione della riproducibilità.

1. **Gestione del progetto di analisi**  

   - Quali sono le fasi principali di un progetto di analisi dei dati secondo Yu (2024)?
   - Spiega il ruolo della fase di formulazione del problema e raccolta dei dati.

2. **Organizzazione del workspace in R**  

   - Quali impostazioni devono essere modificate in RStudio per favorire la riproducibilità?
   - Perché è importante usare percorsi relativi nei progetti in RStudio?
   - Descrivi il ruolo del pacchetto `here` nella gestione dei percorsi dei file.

3. **Struttura dei progetti in R**  

   - Quali sono i vantaggi dell'utilizzo dei progetti in RStudio?
   - Quali sono le cartelle principali in una struttura organizzata di un progetto?
   - Perché è utile separare i dati grezzi dai dati processati?

4. **Importazione ed esportazione dei dati**  

   - Quali pacchetti di R possono essere utilizzati per importare ed esportare dati?  
   - Scrivi un esempio di codice per importare un file CSV usando `rio` e il pacchetto `here`.  
   - Come puoi esportare un dataset modificato in una cartella dedicata ai dati processati?

5. **Pulizia e preprocessing dei dati**  

   - Qual è la differenza tra pulizia e preprocessing dei dati?  
   - Quali strumenti di `dplyr` sono comunemente usati per pulire e trasformare i dati?

6. **Analisi esplorativa dei dati (EDA)**  

   - Quali sono alcuni strumenti utilizzati in R per effettuare un'analisi esplorativa dei dati?  
   - Scrivi un breve esempio di codice in R per calcolare statistiche descrittive di base su un dataset.  

7. **Riproducibilità e comunicazione dei risultati**  

   - Perché la riproducibilità è un elemento chiave nella scienza dei dati?  
   - Quali strumenti offre Quarto per la documentazione e la condivisione dei risultati di un’analisi?
:::

::: {.callout-tip title="Soluzione" collapse="true"}
**1. Gestione del progetto di analisi**

- **Fasi principali del progetto di analisi dei dati (Yu, 2024):**  
  1. Formulazione del problema e raccolta dei dati  
  2. Pulizia, preprocessing e analisi esplorativa  
  3. Analisi predittiva e/o inferenziale (se applicabile)  
  4. Valutazione dei risultati  
  5. Comunicazione dei risultati  

- **Ruolo della fase di formulazione del problema:**  
  Aiuta a definire gli obiettivi dell'analisi e a selezionare le fonti di dati adeguate. Una domanda di ricerca ben definita garantisce che i dati siano pertinenti e che le analisi siano mirate.

**2. Organizzazione del workspace in R**

- **Impostazioni da modificare in RStudio:**  
  - Disabilitare *Restore .RData into workspace at startup*  
  - Impostare *Save workspace to .RData on exit* su "Never"  

- **Importanza dei percorsi relativi:**  
  Permettono di rendere il progetto portabile e riproducibile, evitando problemi di percorsi assoluti specifici per un computer.

- **Ruolo del pacchetto `here`**  
  Aiuta a gestire i percorsi relativi all'interno del progetto senza dover specificare percorsi assoluti.

**3. Struttura dei progetti in R**

- **Vantaggi dell'uso dei progetti in RStudio:**  
  Mantengono ambienti separati, organizzano i file e facilitano la riproducibilità.

- **Cartelle principali in una struttura organizzata:**  
  - `data/raw/` → Dati grezzi  
  - `data/processed/` → Dati elaborati  
  - `dslc_documentation/` → Documentazione e script  
  - `functions/` → Funzioni personalizzate  

- **Separare dati grezzi da dati processati:**  
  Evita di modificare accidentalmente i dati originali, garantendo riproducibilità.

**4. Importazione ed esportazione dei dati**

- **Pacchetti per importazione/esportazione:**  
  - `rio`: unifica funzioni di import/export  
  - `readr`: specifico per CSV e altri formati di testo  
  - `here`: gestisce percorsi relativi  

- **Esempio di codice per importare dati CSV:**  

  ```r
  library(here)
  library(rio)
  df <- rio::import(here("data", "raw", "my_data.csv"))
  ```

- **Esportare dati modificati:**  

  ```r
  rio::export(df, here("data", "processed", "my_data_processed.csv"))
  ```

**5. Pulizia e preprocessing dei dati**

- **Differenza tra pulizia e preprocessing:**  
  - Pulizia: rimozione di errori, gestione dei dati mancanti, formattazione  
  - Preprocessing: trasformazione dei dati per adattarli a modelli specifici  

- **Strumenti di `dplyr` per pulizia e trasformazione:**  
  - `mutate()`, `filter()`, `select()`, `rename()`, `relocate()`

**6. Analisi esplorativa dei dati (EDA)**

- **Strumenti comuni:**  
  - `summary()`, `str()`, `glimpse()`, `ggplot2` per visualizzazione  

- **Esempio di codice per statistiche descrittive:**  
  ```r
  summary(df)
  ```

**7. Riproducibilità e comunicazione dei risultati**

- **Importanza della riproducibilità:**  
  - Facilita la verifica e il miglioramento degli studi  
  - Previene errori accidentali  
  - Consente a terzi di replicare e costruire su ricerche precedenti  

- **Strumenti di Quarto:**  
  - Permette di combinare testo, codice e output in documenti riproducibili  
  - Supporta citazioni automatiche e gestione delle bibliografie  
:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}
