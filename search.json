[
  {
    "objectID": "chapters/key_notions/introduction_key_notions.html",
    "href": "chapters/key_notions/introduction_key_notions.html",
    "title": "Introduzione",
    "section": "",
    "text": "La data science è un campo che si sviluppa all’intersezione tra la statistica e l’informatica. La statistica fornisce una serie di metodologie per analizzare i dati e ottenere informazioni significative, mentre l’informatica si occupa dello sviluppo di software e strumenti per implementare tali metodologie. In questa sezione della dispensa, approfondiremo alcuni concetti fondamentali della statistica e della misurazione psicologica. Considereremo anche in termini generali quali sono gli obiettivi e i limiti dell’analisi dei dati psicologici.",
    "crumbs": [
      "Fondamenti",
      "Introduzione"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html",
    "href": "chapters/eda/introduction_eda.html",
    "title": "Introduzione",
    "section": "",
    "text": "Dopo aver acquisito un dataset, è fondamentale comprendere a fondo le caratteristiche dei dati in esso contenuti. Sebbene le statistiche descrittive e altre misure numeriche siano spesso efficaci per ottenere una visione d’insieme, talvolta è un’immagine a valere più di mille parole.\nIn questa sezione della dispensa, esploreremo alcuni concetti chiave della statistica descrittiva. Oltre a fornire definizioni teoriche, presenteremo istruzioni pratiche in Python per condurre analisi statistiche su dati reali. Il capitolo si concluderà con una riflessione critica sui limiti di un approccio epistemologico basato esclusivamente sull’analisi delle associazioni tra variabili, evidenziando l’importanza di indagare le cause sottostanti ai fenomeni osservati.",
    "crumbs": [
      "EDA",
      "Introduzione"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psicometria",
    "section": "",
    "text": "Benvenuti\nBenvenuti nel sito web dell’insegnamento di Psicometria, parte del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#descrizione",
    "href": "index.html#descrizione",
    "title": "Psicometria",
    "section": "Descrizione",
    "text": "Descrizione\nL’insegnamento offre una formazione teorico-pratica nell’ambito dell’inferenza statistica, con un focus particolare sulle applicazioni in campo psicologico. Attraverso esercitazioni pratiche in Python e R, gli studenti acquisiranno competenze nell’analisi di dati e nell’uso di modelli statistici avanzati.\n\nAnno Accademico: 2024-2025\nCodice Insegnamento: B000286\nOrario e Luogo: Lunedì e Martedì (8:30-10:30), Giovedì (11:30-13:30), Plesso didattico La Torretta.\n\n\n\n\n\n\n\nQuesto sito web è la fonte ufficiale per il programma dell’insegnamento B000286 - Psicometria e le modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#struttura-dellinsegnamento",
    "href": "index.html#struttura-dellinsegnamento",
    "title": "Psicometria",
    "section": "Struttura dell’Insegnamento",
    "text": "Struttura dell’Insegnamento\nL’insegnamento è articolato in diverse sezioni, ognuna delle quali si concentra su un argomento chiave per fornire una comprensione approfondita delle metodologie e degli strumenti necessari per l’analisi statistica e la gestione dei dati.\n\nFondamenti di Statistica: Introduzione ai concetti statistici di base, con un focus su misure descrittive, distribuzioni e visualizzazione dei dati.\nTeoria della Probabilità: Principi fondamentali della probabilità, inclusi eventi, distribuzioni di probabilità e applicazioni pratiche.\nInferenza Frequentista: Approfondimento sull’inferenza statistica basata sulla frequenza, con applicazioni per test di ipotesi e stime puntuali e intervallari.\nInferenza Bayesiana: Introduzione e applicazioni avanzate dell’inferenza bayesiana, con un confronto con l’approccio frequentista.\nProgrammazione in R: Esercitazioni pratiche sull’utilizzo di R per l’analisi dei dati, con particolare attenzione a script replicabili e workflow efficienti.\nGestione di un Progetto di Data Analysis: Approccio strutturato per interpretare, documentare e comunicare i risultati di un’analisi, con attenzione alla riproducibilità.\nComunicazione dei Risultati: Tecniche per rappresentare in modo efficace i risultati statistici, inclusa la creazione di report, visualizzazioni e presentazioni.\n\nQuesta struttura mira a combinare teoria e pratica, offrendo agli studenti gli strumenti necessari per affrontare le sfide dell’analisi statistica in contesti reali.\nConsulta il syllabus completo per ulteriori dettagli.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#materiale-didattico",
    "href": "index.html#materiale-didattico",
    "title": "Psicometria",
    "section": "Materiale Didattico",
    "text": "Materiale Didattico\nIl presente sito web ospita la dispensa ufficiale del corso, contenente tutte le note e i materiali relativi alle lezioni. Per quanto riguarda le esercitazioni pratiche e gli esempi applicativi, è possibile accedere al sito dedicato, disponibile al seguente indirizzo: Psicometria Esercizi. Entrambi i materiali sono forniti gratuitamente agli studenti, senza necessità di ulteriori acquisti.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "Prefazione",
    "section": "",
    "text": "Bibliografia\nCome possiamo migliorare l’analisi dei dati psicologici per renderla più affidabile e robusta? È possibile affrontare questa sfida semplicemente applicando una serie di algoritmi o procedure standard? L’analisi dei dati in psicologia può davvero essere ridotta a un insieme di “ricette” preconfezionate (McElreath, 2020)?\nQueste domande ci portano a riflettere sulla natura stessa dell’analisi dei dati psicologici. A differenza di ciò che suggerisce l’approccio frequentista del test dell’ipotesi nulla, l’analisi dei dati non è una disciplina che si esaurisce con l’applicazione meccanica di metodi predefiniti. Anzi, considerare l’analisi dei dati come un insieme di procedure automatiche contribuisce a uno dei problemi più gravi della psicologia contemporanea: la crisi della replicabilità dei risultati (Korbmacher et al., 2023).\nMa perché la replicabilità è così cruciale? Se i risultati delle ricerche psicologiche non sono replicabili, significa che la nostra comprensione dei fenomeni psicologici è superficiale e inaffidabile. Questo non è solo un problema teorico o accademico; ha implicazioni dirette sulle applicazioni pratiche della psicologia. Se le basi scientifiche sono incerte, anche le strategie di intervento psicologico rischiano di essere inefficaci o addirittura dannose (Funder et al., 2014; Ioannidis, 2019; Shrout & Rodgers, 2018; Tackett et al., 2019).\nPerché le pratiche di analisi dei dati derivanti dal frequentismo potrebbero contribuire a questa crisi? In che modo gli incentivi accademici influenzano la qualità della ricerca psicologica? E, soprattutto, quali alternative abbiamo per migliorare l’affidabilità e la validità delle nostre conclusioni?\nL’analisi bayesiana emerge come una delle proposte per superare i limiti dell’approccio frequentista (Gelman et al., 1995). Tuttavia, è sufficiente abbandonare l’inferenza frequentista per risolvere i problemi della psicologia? Come possiamo integrare metodi robusti e flessibili, come quelli bayesiani, con una comprensione più approfondita e trasparente dei fenomeni psicologici?\nIn questo corso, esploreremo queste domande, cercando di identificare le “buone pratiche” dell’analisi dei dati psicologici. Discuteremo i limiti delle metodologie attuali, esamineremo le cause sottostanti della crisi della replicabilità e valuteremo come l’adozione di metodi avanzati, come l’inferenza bayesiana e la modellazione causale, possa offrire soluzioni efficaci (Oberauer & Lewandowsky, 2019; Wagenmakers et al., 2018; Yarkoni, 2022). Il nostro obiettivo è fornire una visione critica e costruttiva, che non solo identifichi le sfide della ricerca psicologica, ma proponga anche percorsi concreti per migliorare la qualità e l’affidabilità della scienza psicologica.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "programmazione2024.html",
    "href": "programmazione2024.html",
    "title": "1  Calendario delle lezioni",
    "section": "",
    "text": "1.1 Calendario delle relazioni in itinere\nIl calendario didattico prevede 32 incontri, con un esame parziale a metà corso e gli ultimi tre incontri riservati a un secondo esame parziale e alle presentazioni degli studenti.\nPausa di Pasqua\nEsame parziale e presentazioni finali\nLe relazioni di avanzamento del progetto di gruppo dovranno essere consegnate entro le scadenze stabilite. Ogni gruppo dovrà presentare un unico elaborato.\nOgni relazione rappresenta una tappa del progetto di gruppo, che culminerà nella presentazione finale durante gli ultimi incontri del corso.",
    "crumbs": [
      "Programmazione",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Calendario delle lezioni</span>"
    ]
  },
  {
    "objectID": "programmazione2024.html#calendario-delle-relazioni-in-itinere",
    "href": "programmazione2024.html#calendario-delle-relazioni-in-itinere",
    "title": "1  Calendario delle lezioni",
    "section": "",
    "text": "Data di Scadenza\nContenuto della Relazione\n\n\n\n\n18 marzo\nRelazione 1: Importazione dei dati, data wrangling, data tidying, dizionario dei dati, statistiche descrittive\n\n\n25 marzo\nRelazione 2: Priori coniugati e metodo basato su griglia\n\n\n31 marzo\nRelazione 3: Metodi Monte Carlo e algoritmi MCMC\n\n\n7 aprile\nRelazione 4: Regressione lineare\n\n\n8 maggio\nRelazione 5: Confronto di modelli\n\n\n18 maggio\nRelazione 6: Analisi frequentista; limiti dell’approccio frequentista",
    "crumbs": [
      "Programmazione",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Calendario delle lezioni</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html",
    "href": "chapters/key_notions/00_uncertainty.html",
    "title": "2  Abbracciare l’incertezza",
    "section": "",
    "text": "2.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nL’espressione “abbracciare l’incertezza” è tra le più emblematiche nel panorama della statistica bayesiana. In questo capitolo, approfondiremo il significato di questa affermazione, seguendo la trattazione introduttiva proposta nel primo capitolo di Understanding Uncertainty di Lindley (Lindley, 2013).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#lincertezza-nella-ricerca-psicologica",
    "href": "chapters/key_notions/00_uncertainty.html#lincertezza-nella-ricerca-psicologica",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.2 L’incertezza nella ricerca psicologica",
    "text": "2.2 L’incertezza nella ricerca psicologica\nL’incertezza rappresenta un elemento cruciale non solo nella statistica, ma in tutte le discipline scientifiche, con particolare rilievo per la psicologia, che affronta fenomeni complessi e difficili da misurare. Nell’indagare processi cognitivi, emozioni e comportamenti, i ricercatori si confrontano con dati complessi, spesso ambigui e suscettibili di interpretazioni molteplici. Sebbene alcune affermazioni possano essere sostenute con elevata confidenza o confutate con certezza, la maggior parte delle ipotesi scientifiche si colloca in una zona grigia dominata dall’incertezza.\nL’obiettivo di questo insegnamento è guidare gli studenti nella comprensione e nella gestione dell’incertezza nella ricerca psicologica, adottando l’approccio bayesiano all’analisi dei dati. Questo metodo, basato sulla quantificazione e sull’aggiornamento delle credenze alla luce di nuove evidenze, fornirà agli studenti gli strumenti per affrontare l’incertezza in modo rigoroso e sistematico, sia nella carriera accademica sia nella pratica clinica.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#la-natura-soggettiva-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#la-natura-soggettiva-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.3 La natura soggettiva dell’incertezza",
    "text": "2.3 La natura soggettiva dell’incertezza\nUn elemento cruciale dell’incertezza, spesso trascurato, è la sua dimensione soggettiva. De Finetti (Finetti, 1970) ha evidenziato come l’incertezza sia, almeno in parte, una questione personale: ciò che è incerto per uno psicologo può non esserlo per un altro, in funzione delle loro esperienze, conoscenze pregresse e interpretazioni dei dati disponibili. Anche di fronte a una stessa questione, due ricercatori possono condividere un’incertezza comune, ma con gradi di intensità diversi.\nQuesta componente soggettiva è particolarmente significativa in psicologia, dove le differenze individuali e culturali influenzano la percezione e l’interpretazione dei fenomeni. L’approccio bayesiano offre un potente strumento per affrontare questa soggettività, consentendo di quantificare le differenze tra credenze individuali e di aggiornarle in modo coerente sulla base di nuove evidenze oggettive.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#lonnipresenza-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#lonnipresenza-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.4 L’onnipresenza dell’incertezza",
    "text": "2.4 L’onnipresenza dell’incertezza\nL’incertezza pervade ogni aspetto della ricerca psicologica. Ogni esperimento, misurazione o interpretazione dei dati comporta un margine di incertezza. Questa condizione è particolarmente evidente nello studio di fenomeni complessi come il comportamento umano o i processi mentali, dove innumerevoli variabili interagiscono, molte delle quali difficili da misurare o controllare con precisione.\nTuttavia, l’incertezza non deve essere vista come un ostacolo insormontabile. Al contrario, riconoscerla e quantificarla può favorire una comprensione più profonda e realistica dei fenomeni psicologici. Attraverso l’approccio bayesiano, diventa possibile integrare l’incertezza nel processo di indagine scientifica, trattandola non come un limite, ma come una risorsa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#superare-la-soppressione-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#superare-la-soppressione-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.5 Superare la soppressione dell’incertezza",
    "text": "2.5 Superare la soppressione dell’incertezza\nNonostante la sua onnipresenza, l’incertezza è spesso ignorata o minimizzata nella comunicazione scientifica. Questo può avvenire attraverso interpretazioni eccessivamente ottimistiche dei risultati, la presentazione di conclusioni come fatti certi, o una riluttanza a riconoscere i limiti degli studi condotti. Tale atteggiamento, sebbene comprensibile, può condurre a conclusioni errate e a una visione distorta della realtà.\nL’approccio bayesiano permette di affrontare l’incertezza in modo esplicito e costruttivo. Fornendo un quadro rigoroso per quantificarla, analizzarla e comunicarla chiaramente, migliora la trasparenza della ricerca e promuove conclusioni più oneste e accurate.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#i-benefici-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#i-benefici-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.6 I benefici dell’incertezza",
    "text": "2.6 I benefici dell’incertezza\nContrariamente a quanto si possa pensare, l’incertezza offre numerosi vantaggi per la ricerca psicologica:\n\nStimola l’esplorazione scientifica: La consapevolezza dell’incertezza incoraggia i ricercatori a formulare nuove ipotesi e a migliorare i metodi di studio.\nPromuove l’onestà intellettuale: Accettare l’incertezza rende i ricercatori più cauti e aperti a prospettive alternative.\nMigliora la qualità delle analisi: Integrare l’incertezza porta a disegni sperimentali più robusti e interpretazioni più accurate.\nFacilita la collaborazione interdisciplinare: Riconoscere i limiti delle proprie conoscenze stimola la ricerca di input da altri esperti.\nRiflette la complessità dei fenomeni psicologici: L’incertezza è intrinseca ai processi mentali e riconoscerla consente di rappresentarli in modo più realistico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#tipi-di-incertezza",
    "href": "chapters/key_notions/00_uncertainty.html#tipi-di-incertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.7 Tipi di incertezza",
    "text": "2.7 Tipi di incertezza\nL’incertezza nella ricerca può essere classificata in tre categorie principali, in base alla sua origine: aleatoria, epistemica e ontologica (Gansch & Adee, 2020).\n\n2.7.1 Incertezza Aleatoria\nL’incertezza aleatoria è intrinseca alla natura casuale di un processo e non può essere eliminata per un dato modello probabilistico. Essa è considerata irreducibile e viene quantificata tramite distribuzioni probabilistiche. Ad esempio, nella misurazione della risposta di un individuo a uno stimolo, la variabilità intrinseca nel comportamento umano, dovuta a fattori imprevedibili, rappresenta un caso di incertezza aleatoria. Questo tipo di incertezza è una caratteristica fondamentale di molti fenomeni psicologici e biologici.\n\n\n2.7.2 Incertezza Epistemica\nL’incertezza epistemica deriva dalla conoscenza limitata o incompleta di un fenomeno. Essa rappresenta il “noto-ignoto”, cioè ciò che sappiamo di non sapere, ed è legata alle semplificazioni insite in ogni modello scientifico. Ad esempio, un modello psicologico che non consideri le influenze culturali o ambientali potrebbe risultare incompleto, introducendo incertezza epistemica. Diversamente dall’incertezza aleatoria, l’incertezza epistemica può essere ridotta attraverso il miglioramento dei modelli, l’inclusione di variabili rilevanti o la raccolta di ulteriori dati.\n\n\n2.7.3 Incertezza Ontologica\nL’incertezza ontologica riguarda l’“ignoto-ignoto”, ovvero aspetti di un sistema che non sono ancora stati identificati. In psicologia, questo potrebbe riferirsi a variabili o processi non ancora scoperti che influenzano un comportamento. Ad esempio, studiando i disturbi mentali, potrebbero emergere nuovi fattori di rischio precedentemente sconosciuti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#il-calcolo-dellincertezza-nellapproccio-bayesiano",
    "href": "chapters/key_notions/00_uncertainty.html#il-calcolo-dellincertezza-nellapproccio-bayesiano",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.8 Il calcolo dell’incertezza nell’approccio bayesiano",
    "text": "2.8 Il calcolo dell’incertezza nell’approccio bayesiano\nL’insegnamento si propone di fornire agli studenti strumenti per affrontare e quantificare l’incertezza attraverso l’approccio bayesiano (Gelman et al., 1995). Fondato sul teorema di Bayes, questo metodo rappresenta un quadro teorico rigoroso e sistematico per aggiornare le credenze alla luce di nuove evidenze, configurandosi come una componente centrale della metodologia scientifica.\nIl processo si basa su quattro passaggi essenziali. In primo luogo, si parte dalla quantificazione delle credenze iniziali, note come prior, che rappresentano le conoscenze pregresse o le ipotesi relative a un determinato fenomeno psicologico. Successivamente, si analizza la forza delle evidenze empiriche fornite dai dati raccolti, formalizzata nella likelihood. Queste due informazioni vengono combinate per generare le credenze aggiornate, chiamate posterior, che sintetizzano la conoscenza disponibile integrando i dati empirici e le ipotesi iniziali. Infine, le credenze aggiornate possono essere utilizzate per prendere decisioni più informate, pianificare ricerche future e orientare interventi.\n\n2.8.1 Il ruolo delle credenze e delle decisioni nella ricerca psicologica\nLe credenze rivestono un ruolo fondamentale nella ricerca psicologica, influenzando tutte le fasi del processo scientifico, dalla progettazione degli esperimenti all’interpretazione dei risultati, fino alla scelta di interventi clinici. L’approccio bayesiano si distingue per la sua capacità di esplicitare e formalizzare queste credenze, consentendo di aggiornare il loro contenuto in modo coerente e trasparente man mano che emergono nuove evidenze.\nQuesto metodo permette non solo di ottimizzare le decisioni basandosi su informazioni aggiornate, ma anche di comunicare chiaramente l’incertezza associata alle conclusioni, evidenziandone i limiti e garantendo maggiore trasparenza scientifica. Affrontare l’incertezza come una componente intrinseca della ricerca non solo migliora la qualità dell’analisi, ma consente anche di promuovere un approccio più realistico e rigoroso nello studio dei fenomeni psicologici.\nIn sintesi, l’approccio bayesiano offre un modello operativo per integrare l’incertezza nel processo decisionale, trattandola non come un ostacolo, ma come un elemento essenziale per una comprensione più sfumata e accurata della realtà.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#riflessioni-conclusive",
    "href": "chapters/key_notions/00_uncertainty.html#riflessioni-conclusive",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.9 Riflessioni Conclusive",
    "text": "2.9 Riflessioni Conclusive\nQuesto insegnamento fornisce gli strumenti per applicare l’analisi bayesiana nell’ambito dei dati psicologici, insegnando a considerare l’incertezza come una parte integrante e preziosa del processo scientifico. Attraverso questo approccio, gli studenti potranno acquisire una comprensione più raffinata e strutturata dei fenomeni psicologici, integrando l’incertezza come elemento fondamentale per interpretare i dati e formulare inferenze rigorose.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#bibliografia",
    "href": "chapters/key_notions/00_uncertainty.html#bibliografia",
    "title": "2  Abbracciare l’incertezza",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nFinetti, B. de. (1970). Teoria delle probabilità: sintesi introduttiva con appendice critica. Einaudi.\n\n\nGansch, R., & Adee, A. (2020). System theoretic view on uncertainties. 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), 1345–1350.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman; Hall/CRC.\n\n\nLindley, D. V. (2013). Understanding uncertainty. John Wiley & Sons.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html",
    "href": "chapters/key_notions/01_key_notions.html",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nQuesto capitolo introduce il contesto e i principi base dell’analisi dei dati, con un focus su come le tecniche statistiche, combinate con una solida teoria dei fenomeni, siano strumentali all’avanzamento delle conoscenze scientifiche.\nL’analisi dei dati consente di sintetizzare grandi quantità di informazioni e di verificare le previsioni avanzate dalle teorie. Tuttavia, senza una teoria che dia significato ai dati, le osservazioni rimangono mere descrizioni prive di un contesto esplicativo. È attraverso l’integrazione tra dati e teoria che si raggiunge una comprensione profonda dei fenomeni e si favorisce l’avanzamento scientifico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#introduzione",
    "href": "chapters/key_notions/01_key_notions.html#introduzione",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Most of the fundamental ideas of science are essentially simple, and may, as a rule, be expressed in a language comprehensible to everyone.\n(Einstein A and Infeld L, 1938)\n\n\n\n\n\n\n\n\nStatistica\n\n\n\nIl termine “statistica” può assumere diversi significati, a seconda del contesto in cui viene utilizzato.\n\nNel primo senso, la statistica è una scienza e una disciplina che si occupa dello studio e dell’applicazione di metodi e tecniche per la raccolta, l’organizzazione, l’analisi, l’interpretazione e la presentazione di dati.\nNel secondo senso, il termine “statistica” si riferisce a una singola misura o un valore numerico che è stato calcolato a partire da un campione di dati. Questo tipo di statistica rappresenta una caratteristica specifica del campione. Esempi comuni di statistiche in questo senso includono la media campionaria, la deviazione standard campionaria o il coefficiente di correlazione campionario.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#la-spiegazione-scientifica",
    "href": "chapters/key_notions/01_key_notions.html#la-spiegazione-scientifica",
    "title": "3  Concetti chiave",
    "section": "3.1 La Spiegazione Scientifica",
    "text": "3.1 La Spiegazione Scientifica\nLa scienza non si limita a descrivere o prevedere i fenomeni; essa mira a spiegare il perché degli eventi, fornendo una comprensione delle cause e dei meccanismi che governano il mondo. La spiegazione scientifica è quindi uno strumento essenziale per costruire teorie che non solo descrivono e prevedono, ma anche chiariscono le dinamiche causali e le connessioni tra fenomeni, aiutando così a sviluppare un controllo informato sugli stessi.\nSe prendiamo l’esempio del successo accademico in psicologia dell’educazione, possiamo osservare che i dati rivelano una forte associazione tra il livello di istruzione dei genitori e il successo scolastico dei figli. Tuttavia, una semplice previsione basata su questa associazione – “provenendo da una famiglia con basso livello d’istruzione, è improbabile che tu ottenga un titolo universitario” – non risponde alle domande fondamentali per migliorare il sistema educativo: perché esiste questa disparità? Quali interventi potrebbero ridurre questa disuguaglianza?\nPer andare oltre la previsione, la scienza deve individuare i fattori causali che contribuiscono al fenomeno, esplorare il modo in cui agire su questi fattori potrebbe alterare l’outcome, e stimare le incertezze e le dinamiche temporali di questi effetti. Ad esempio, per ridurre la disuguaglianza educativa, è necessario comprendere se e come aumentare il sostegno finanziario agli studenti possa realmente facilitare il percorso scolastico di chi proviene da contesti meno favoriti, e prevedere gli effetti di lungo termine di tali politiche. Questo approccio permette non solo di prevedere ma anche di controllare e migliorare i fenomeni studiati.\n\n3.1.1 Elementi Fondamentali della Spiegazione Scientifica\nLa filosofia della scienza ha individuato tre elementi chiave di una spiegazione scientifica:\n\nExplanandum: il fenomeno da spiegare. Ad esempio, “si è verificata una crisi petrolifera nel 1973.”\nExplanans: un insieme di affermazioni che spiegano il fenomeno. Per esempio, “gli stati membri dell’OAPEC hanno imposto un embargo sul petrolio in risposta al sostegno degli Stati Uniti a Israele nella guerra del Kippur.”\nLegame esplicativo: i principi o le leggi che descrivono il meccanismo sottostante, ossia il modo in cui l’explanans causa l’explanandum. Nel caso dell’embargo, il legame potrebbe essere: “gli stati dell’OAPEC usarono il petrolio come strumento politico per influenzare la politica estera degli Stati Uniti.”\n\nI modelli scientifici incorporano questi elementi, rappresentando una metodologia per ottenere spiegazioni scientifiche. Essi includono il fenomeno da spiegare, i fattori causali rilevanti e i meccanismi che collegano i fattori all’esito. A differenza dei modelli puramente descrittivi o predittivi, i modelli scientifici in psicologia sono progettati per rispondere a domande causali, facilitando la comprensione e il controllo dei fenomeni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#modelli-psicologici",
    "href": "chapters/key_notions/01_key_notions.html#modelli-psicologici",
    "title": "3  Concetti chiave",
    "section": "3.2 Modelli Psicologici",
    "text": "3.2 Modelli Psicologici\nUn modello è una rappresentazione matematica semplificata di un fenomeno reale. È composto da un insieme di equazioni e ipotesi che definiscono la struttura probabilistica e le relazioni tra le variabili, cercando di cogliere gli aspetti essenziali del fenomeno senza includerne ogni dettaglio. Esistono spesso diversi modelli applicabili a uno stesso problema, e il compito della scienza dei dati è identificare quello che meglio si adatta ai dati, soddisfacendo criteri di validità e accuratezza.\nI modelli psicologici sono strumenti concettuali per descrivere, spiegare e prevedere il comportamento umano e i processi mentali. Un buon modello psicologico dovrebbe avere alcune caratteristiche fondamentali:\n\nCoerenza descrittiva: Il modello deve rappresentare in modo logico e coerente il fenomeno studiato, catturando gli aspetti chiave del processo psicologico e organizzando le osservazioni in una struttura comprensibile.\nCapacità predittiva: Un modello efficace deve essere in grado di fare previsioni accurate sui futuri sviluppi del fenomeno. Questa capacità non solo ne aumenta l’utilità, ma permette anche di testarne la validità.\nSupporto empirico: Le ipotesi e le previsioni del modello devono essere confermate da dati raccolti attraverso ricerche sistematiche e rigorose.\nFalsificabilità: Un modello scientifico deve poter essere testato e, se necessario, confutato con l’osservazione e l’esperimento. Questo principio assicura che il modello rimanga aperto alla revisione e al miglioramento in base a nuove evidenze.\nParsimonia: Il modello dovrebbe spiegare il fenomeno nel modo più semplice possibile, evitando complessità inutili.\nGeneralizzabilità: Deve essere applicabile a una vasta gamma di situazioni e contesti, non limitandosi a casi specifici o condizioni sperimentali particolari.\nUtilità pratica: Un modello efficace dovrebbe fornire spunti utili per interventi, terapie o applicazioni nel mondo reale.\n\nLa modellazione in psicologia affronta sfide uniche dovute alla natura soggettiva e variabile dell’esperienza umana. I ricercatori devono bilanciare la precisione scientifica con la flessibilità necessaria per cogliere la complessità dei fenomeni psicologici, considerando al contempo i limiti etici della sperimentazione e le potenziali implicazioni sociali dei loro modelli.\nL’analisi dei dati, attraverso tecniche statistiche, è il mezzo per valutare un modello psicologico. Oltre a stabilire se il modello riesce a spiegare i dati osservati, l’analisi verifica la capacità del modello di fare previsioni su dati non ancora raccolti. In questo modo, la modellazione non solo consente di comprendere i fenomeni psicologici ma permette anche di prevedere e, in certi casi, influenzare il comportamento e i processi mentali.\n\n3.2.1 Rappresentare i Fenomeni per Ragionare e Comunicare\nLa spiegazione scientifica, oltre a chiarire i meccanismi causali, serve anche a fornire un linguaggio per ragionare sui fenomeni e per condividere la conoscenza. In psicologia, la costruzione di modelli scientifici permette di rappresentare i fenomeni attraverso variabili, funzioni e parametri, fornendo un vocabolario per descrivere componenti, dipendenze e proprietà dei fenomeni. Un modello semplice e chiaro consente di emulare il comportamento del fenomeno senza necessità di simulazioni complesse, facilitando la comunicazione e l’intuizione.\nUn aspetto importante della spiegazione scientifica è la possibilità di utilizzare i modelli per stimolare l’intuizione e generare nuove domande. La comprensione dei fenomeni attraverso una rappresentazione scientifica accessibile permette di formulare ipotesi, collegare concetti, e trasferire conoscenze da un campo all’altro.\nIn sintesi, la spiegazione scientifica va oltre la mera previsione: mira a fornire una comprensione completa dei fenomeni, basata su nessi causali e su un linguaggio formale per ragionare e comunicare. I modelli scientifici non solo predicono eventi, ma spiegano come e perché questi eventi si verificano, offrendo una struttura con cui intervenire e influenzare i fenomeni stessi.\nNell’analisi dei dati bayesiana, questa attenzione alle cause e agli effetti trova un’applicazione naturale. La possibilità di aggiornare le proprie credenze alla luce di nuove informazioni consente di costruire modelli che non si limitano alla descrizione o alla previsione, ma che forniscono spiegazioni coerenti e profonde dei fenomeni, aiutando a sviluppare teorie sempre più raffinate e applicabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#ruolo-dellanalisi-dei-dati",
    "href": "chapters/key_notions/01_key_notions.html#ruolo-dellanalisi-dei-dati",
    "title": "3  Concetti chiave",
    "section": "3.3 Ruolo dell’Analisi dei Dati",
    "text": "3.3 Ruolo dell’Analisi dei Dati\nL’analisi dei dati riveste un ruolo centrale nelle scienze, specialmente in psicologia, per due ragioni principali:\n\nRiassumere grandi quantità di informazioni: consente di sintetizzare dati complessi in statistiche descrittive, grafici e altre rappresentazioni che rendono i dati accessibili e comprensibili. Questo processo evidenzia tendenze generali, variazioni e anomalie, facilitando l’identificazione di schemi comportamentali e differenze tra gruppi.\nVerificare le predizioni di un modello scientifico: permette di confrontare le aspettative teoriche con i dati osservati, valutando la validità delle ipotesi sottostanti. Questa verifica contribuisce direttamente all’avanzamento della conoscenza scientifica, sostenendo, modificando o confutando una teoria.\n\nSebbene l’analisi dei dati possa portare alla scoperta di correlazioni o schemi interessanti, questi risultati, senza una teoria, offrono solo una comprensione limitata. Per esempio, rilevare che due variabili psicologiche sono correlate non fornisce informazioni sulla natura di questa relazione o sul motivo per cui esiste. Per interpretare e attribuire un significato a queste osservazioni, è necessario un quadro teorico che le contestualizzi e proponga meccanismi causali o esplicativi.\n\n3.3.1 Carattere Multidisciplinare dell’Analisi dei Dati\nL’analisi dei dati si situa all’intersezione di tre discipline principali: statistica, teoria della probabilità e informatica. Ciascuna contribuisce con strumenti e approcci specifici essenziali per comprendere i dati, estrarre conoscenza e generare nuove ipotesi scientifiche.\n\nStatistica: offre tecniche per raccogliere, analizzare e interpretare i dati, fornendo strumenti descrittivi e inferenziali utili per trarre conclusioni e prendere decisioni.\nTeoria della probabilità: fornisce la base matematica della statistica, consentendo di modellare e quantificare l’incertezza e di comprendere i fenomeni aleatori che caratterizzano molte osservazioni in psicologia.\nInformatica: supporta l’analisi attraverso strumenti per la gestione, l’elaborazione e la visualizzazione di grandi quantità di dati. La programmazione consente di sviluppare modelli avanzati e gestire dataset complessi.\n\nQuesta natura multidisciplinare riflette la complessità dell’analisi dei dati e la necessità di integrare diverse competenze per affrontare le sfide scientifiche contemporanee.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "href": "chapters/key_notions/01_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "title": "3  Concetti chiave",
    "section": "3.4 Concetti Chiave nell’Analisi dei Dati",
    "text": "3.4 Concetti Chiave nell’Analisi dei Dati\nPer condurre un’analisi dei dati efficace, è fondamentale comprendere alcuni concetti chiave.\n\n3.4.1 Popolazioni e Campioni\nIn ogni analisi dei dati, è fondamentale identificare la popolazione di interesse, l’insieme completo di entità o individui che rappresentano il fenomeno studiato. In psicologia, ad esempio, si può voler studiare il benessere in una popolazione generale o in una sotto-popolazione specifica, come gli individui che hanno subito un evento stressante.\nPer ottenere informazioni dettagliate su una popolazione, si utilizzano campioni: sottoinsiemi rappresentativi dai quali si possono fare inferenze sull’intera popolazione. La rappresentatività del campione è cruciale, poiché un campione non rappresentativo può portare a conclusioni errate e limitare la generalizzabilità dei risultati.\n\n\n3.4.2 Bias nella Raccolta Dati\nI bias nella raccolta e interpretazione dei dati possono influenzare profondamente i risultati di uno studio. Capire chi ha raccolto i dati, come e con quali finalità, è fondamentale per garantire una corretta interpretazione. I dati non sono mai neutri e le intenzioni che ne guidano la raccolta spesso ne influenzano l’interpretazione (Murray & Carr, 2024; Nobles, 2000)\n\n\n\nTabella creata da Ellie Murray.\n\n\n\n\n3.4.3 Variabili e Costanti\nNell’analisi statistica, le variabili rappresentano le caratteristiche osservate che possono assumere diversi valori (numerici o categorici). Al contrario, le costanti sono valori che rimangono fissi in un dato contesto. Si distinguono poi le variabili indipendenti (o predittive), che influenzano le variabili dipendenti, e le variabili dipendenti, che rappresentano gli esiti di interesse.\n\n\n3.4.4 Effetti\nIn statistica, un effetto misura il cambiamento osservato nelle variabili dipendenti in relazione alle variabili indipendenti. Ad esempio, l’efficacia di una terapia può essere valutata misurando la differenza nei sintomi prima e dopo il trattamento (Huntington-Klein, 2021).\n\n\n3.4.5 Stima e Inferenza\n\n3.4.5.1 Stima\nLa stima statistica consente di ottenere informazioni su una popolazione a partire da un campione. Si utilizzano statistiche campionarie (come la media campionaria) per stimare i parametri della popolazione (come la media vera della popolazione).\nGli stimatori devono possedere proprietà come:\n\nconsistenza: la stima converge al vero valore del parametro all’aumentare della dimensione del campione;\nnon distorsione: il valore atteso dello stimatore è uguale al vero valore del parametro;\nefficienza: lo stimatore ha la minor varianza possibile.\n\nL’accuratezza della stima dipende da vari fattori, tra cui la dimensione e la rappresentatività del campione, la variabilità nella popolazione e il metodo di campionamento utilizzato.\n\n\n\n3.4.6 Inferenza Statistica\nDopo aver ottenuto le stime, l’inferenza statistica permette di trarre conclusioni più generali sulla popolazione. Essa consente di valutare ipotesi specifiche o rispondere a domande di ricerca basate sui dati raccolti.\nAd esempio, se abbiamo stimato la media del rendimento accademico in un campione di studenti, l’inferenza statistica ci consente di quantificare l’incertezza riguardo alla differenza di rendimento tra maschi e femmine all’interno della popolazione più ampia. In questo modo, l’inferenza statistica ci fornisce gli strumenti per fare previsioni e trarre conclusioni su fenomeni che riguardano l’intera popolazione.\nEsistono due approcci principali.\nL’inferenza bayesiana:\n\nSi basa sul teorema di Bayes;\nUtilizza probabilità a priori, che riflettono conoscenze o credenze iniziali su un fenomeno;\nAggiorna queste probabilità con nuovi dati per ottenere probabilità a posteriori;\nFornisce una interpretazione delle probabilità come gradi di credenza soggettivi.\n\nL’approccio frequentista:\n\nSi fonda sulla frequenza relativa di eventi osservati in esperimenti ripetuti;\nUtilizza strumenti come il test di ipotesi nulla e gli intervalli di confidenza per trarre conclusioni;\nNon fa uso di probabilità a priori, concentrandosi esclusivamente sui dati osservati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "href": "chapters/key_notions/01_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "title": "3  Concetti chiave",
    "section": "3.5 Le Sfide dell’Inferenza Statistica in Psicologia",
    "text": "3.5 Le Sfide dell’Inferenza Statistica in Psicologia\nSecondo Gelman et al. (2020), l’inferenza statistica in psicologia affronta tre sfide principali:\n\nGeneralizzare dai campioni alla popolazione: Questa sfida è strettamente legata al problema del campionamento di comodo, spesso usato in psicologia, ma presente in quasi tutte le applicazioni dell’inferenza statistica. La difficoltà risiede nel trarre conclusioni affidabili su una popolazione più ampia partendo da un campione limitato e, a volte, non rappresentativo.\nGeneralizzare dal gruppo trattato al gruppo di controllo: Questa sfida riguarda l’inferenza causale, un aspetto centrale per determinare l’efficacia dei trattamenti psicologici. L’obiettivo è stabilire se i risultati osservati nel gruppo trattato possano essere applicati al gruppo di controllo o ad altre popolazioni, permettendo una valutazione valida dell’effetto del trattamento.\nGeneralizzare dalle misurazioni osservate ai costrutti sottostanti: In psicologia, i dati raccolti non corrispondono mai perfettamente ai costrutti teorici di interesse. La sfida è inferire questi costrutti latenti dai dati osservati, che rappresentano spesso solo un’approssimazione imperfetta.\n\nQueste sfide evidenziano la complessità dell’inferenza in psicologia e la necessità di metodologie robuste per affrontarle.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#riflessioni-conclusive",
    "href": "chapters/key_notions/01_key_notions.html#riflessioni-conclusive",
    "title": "3  Concetti chiave",
    "section": "3.6 Riflessioni Conclusive",
    "text": "3.6 Riflessioni Conclusive\nIn psicologia, le teorie forniscono ipotesi testabili che spiegano il “come” e il “perché” di determinati fenomeni mentali e comportamentali. Una teoria robusta permette di formulare previsioni chiare e specifiche, che possono essere verificate empiricamente attraverso l’analisi dei dati. Ad esempio, una teoria sull’ansia potrebbe prevedere che, in un compito di esposizione graduale a stimoli ansiogeni, il livello di ansia diminuisca progressivamente. Senza una teoria che spieghi perché questo dovrebbe accadere, tale osservazione rimane solo un dato descrittivo, privo di valore esplicativo o predittivo.\nL’analisi dei dati diventa davvero potente quando è integrata a una teoria. Senza teoria, i dati possono descrivere fenomeni ma non spiegare i meccanismi sottostanti. La teoria fornisce il contesto interpretativo, orientando la raccolta e l’analisi dei dati, e permettendo una comprensione profonda dei fenomeni psicologici.\nUn esempio è l’uso della data science per analizzare l’efficacia di un trattamento psicoterapeutico. I dati possono mostrarci una diminuzione dei sintomi in seguito alla terapia, ma è solo la teoria alla base del trattamento che fornisce un quadro interpretativo per questo miglioramento, proponendo i meccanismi per cui il trattamento riduce i sintomi. La teoria orienta quindi l’analisi e permette di interpretare i dati in un contesto scientifico.\nSviluppare una teoria in psicologia è complesso a causa della notevole variabilità umana. Un buon modello psicologico deve prevedere con precisione i comportamenti osservabili e rappresentare i processi mentali latenti. Queste previsioni devono essere testabili e falsificabili (Eronen & Bringmann, 2021).\nLa relazione tra teoria e analisi dei dati è dinamica e iterativa. I modelli e le teorie si evolvono grazie alla verifica empirica. Se i dati non supportano le previsioni di una teoria, essa viene modificata o sostituita, favorendo l’avanzamento scientifico.\nIn conclusione, la teoria e l’analisi dei dati sono complementari e interdipendenti. L’analisi dei dati offre gli strumenti per testare e affinare le teorie psicologiche, mentre la teoria dà significato e contesto ai dati, rendendo possibile una comprensione profonda e utile dei fenomeni psicologici.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#bibliografia",
    "href": "chapters/key_notions/01_key_notions.html#bibliografia",
    "title": "3  Concetti chiave",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nEronen, M. I., & Bringmann, L. F. (2021). The theory crisis in psychology: How to move forward. Perspectives on Psychological Science, 16(4), 779–788.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press.\n\n\nHuntington-Klein, N. (2021). The effect: An introduction to research design and causality. Chapman; Hall/CRC.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMurray, E. J., & Carr, K. C. (2024). Measuring Racial Sentiment Using Social Media Is Harder Than It Seems. Epidemiology, 35(1), 60–63.\n\n\nNobles, M. (2000). Shades of citizenship: Race and the census in modern politics. Stanford University Press.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html",
    "href": "chapters/key_notions/02_measurement.html",
    "title": "4  La misurazione in psicologia",
    "section": "",
    "text": "4.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nLa scienza si avvale di modelli per interpretare i dati, ma opera sempre con teorie incomplete e misurazioni soggette a errori. Di conseguenza, è fondamentale riconoscere le incertezze quando si cerca di estrarre informazioni dalle misurazioni utilizzando i nostri modelli. Nessuna misurazione, spiegazione o previsione è perfettamente accurata e precisa, e non possiamo mai conoscere con esattezza l’entità dei loro errori.\nQuesta incertezza viene catturata in tre equazioni fondamentali. La prima è l’Equazione di Misurazione, che riconosce l’errore osservativo: \\(y = z + ϵ_y\\), dove \\(y\\) rappresenta il valore misurato, \\(z\\) il valore reale e \\(ϵ_y\\) l’errore di misurazione. La seconda è l’Equazione di Modellazione, che esprime la presenza di un diverso tipo di errore: \\(z = f(x,θ) + ϵ_\\text{model}\\), dove \\(f\\) è il modello, \\(x\\) sono le condizioni ambientali per cui eseguiamo il modello, θ sono i valori dei parametri del modello e \\(ϵ_\\text{model}\\) rappresenta l’errore del modello, che sorge perché \\(f\\), \\(x\\) e θ saranno tutti in qualche misura imprecisi.\nCombinando queste due equazioni, otteniamo l’Equazione della Scienza: \\(y = f(x,θ) + ϵ_\\text{model} + ϵ_y\\). La scienza è il tentativo di spiegare le osservazioni \\(y\\) utilizzando un modello \\(f\\), cercando di minimizzare l’errore di misurazione \\(ϵ_y\\) e l’errore del modello \\(ϵ_\\text{model}\\), in modo che il modello possa essere utilizzato per fare previsioni sul mondo reale (\\(z\\)). L’approccio bayesiano alla scienza riconosce e quantifica le incertezze su tutti e sei gli elementi dell’Equazione della Scienza: \\(y\\), \\(f\\), \\(x\\), θ, \\(ϵ_\\text{model}\\) e \\(ϵ_y\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#la-teoria-della-misurazione",
    "href": "chapters/key_notions/02_measurement.html#la-teoria-della-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.2 La teoria della Misurazione",
    "text": "4.2 La teoria della Misurazione\nLa teoria della misurazione, oggetto di questo capitolo, si concentra sull’errore di misurazione e sull’equazione fondamentale \\(y = z + ϵ_y\\). Questa equazione può essere esaminata da tre prospettive distinte. La prima concerne l’affidabilità della misura, rappresentata dal termine \\(ϵ_y\\). La psicometria, branca dedicata alla teoria della misurazione psicologica, si occupa di quantificare l’affidabilità delle misure psicologiche attraverso metodi come la Teoria Classica dei Test e la Teoria di Risposta all’Item.\nLa seconda prospettiva riguarda la validità delle misure psicologiche, ovvero quanto adeguatamente la misura \\(y\\) rappresenti il costrutto \\(z\\). Questo aspetto, più complesso dell’affidabilità, non può essere risolto meramente con metodi statistici, ma richiede una profonda comprensione delle teorie psicologiche e della loro capacità di descrivere e prevedere i fenomeni psicologici.\nLa terza prospettiva si concentra sulle procedure di assegnazione dei valori a \\(y\\), esplorando quali metodi (questionari, interviste, esperimenti) siano più appropriati e come valutarne l’adeguatezza.\n\n4.2.1 Costrutti Psicologici\nLa teoria della misurazione sottolinea l’importanza di distinguere tra la procedura di misurazione e il costrutto che si intende misurare. Ad esempio, mentre la temperatura è un costrutto, il termometro è lo strumento di misurazione. Analogamente, l’abilità matematica è un costrutto, mentre un test di matematica è la procedura per misurarla.\nNelle scienze psicologiche e sociali, la misurazione presenta sfide uniche rispetto alle scienze fisiche, poiché i costrutti in esame sono spesso astratti e non direttamente osservabili. Ciò richiede una particolare attenzione alla validità e all’affidabilità degli strumenti di misurazione, nonché una costante riflessione sulle limitazioni e le potenziali fonti di errore.\nIl capitolo introduce concetti fondamentali relativi alla misurazione quantitativa delle caratteristiche psicologiche, con un focus sulla teoria delle scale di misura di Stevens (1946). Questa teoria fornisce un quadro concettuale per comprendere i diversi tipi di scale di misurazione e le operazioni matematiche appropriate per ciascuna. Inoltre, vengono esplorate alcune procedure di scaling psicologico, ovvero l’assegnazione di numeri all’intensità di fenomeni psicologici.\n\n\n4.2.2 Scaling Psicologico\nLo scaling psicologico si occupa della trasformazione dei dati empirici raccolti durante uno studio psicologico in misure o punteggi che rappresentino accuratamente le caratteristiche psicologiche oggetto di indagine.\nScaling di Guttman. Uno dei metodi di scaling più noti è lo «Scaling di Guttman», che viene utilizzato per rappresentare relazioni ordinate tra gli elementi di una scala. Ad esempio, in un questionario sui sintomi dell’ansia, le domande possono essere disposte in ordine di intensità crescente dei sintomi. Secondo il modello di Guttman, se un partecipante risponde “sì” a una domanda che riflette un sintomo più intenso, ci si aspetta che abbia risposto “sì” anche a tutte le domande precedenti, che rappresentano sintomi di intensità minore. Questo approccio consente di costruire una scala che riflette in modo sistematico e coerente la gravità dei sintomi.\nScaling Thurstoniano. Lo «Scaling Thurstoniano» è un metodo utilizzato per misurare preferenze o giudizi soggettivi. Ad esempio, per valutare la preferenza tra diversi tipi di cibi, i partecipanti confrontano due cibi alla volta ed esprimono una preferenza. Le risposte vengono poi utilizzate per assegnare punteggi che riflettono la preferenza media per ciascun cibo.\nQuestionari Likert. I questionari Likert richiedono ai partecipanti di esprimere il loro grado di accordo con una serie di affermazioni su una scala a più livelli, che va da «fortemente in disaccordo» a «fortemente d’accordo». I punteggi ottenuti vengono sommati per rappresentare la posizione complessiva dell’individuo rispetto all’oggetto di studio.\n\n\n4.2.3 Metodi di Valutazione delle Scale Psicologiche\nPer valutare le proprietà delle scale psicologiche, vengono utilizzati vari metodi. Ad esempio, l’affidabilità delle misure può essere analizzata utilizzando il coefficiente alpha di Cronbach o il coefficiente Omega di McDonald, entrambi utilizzati per misurare la coerenza interna delle risposte ai diversi item di un questionario. Inoltre, la validità delle scale può essere esaminata confrontando i risultati ottenuti con misure simili o attraverso analisi statistiche che verificano se la scala cattura accuratamente il costrutto psicologico che si intende misurare. La validità di costrutto è particolarmente cruciale, poiché riguarda la capacità della scala di misurare effettivamente il concetto psicologico che si intende esplorare.\n\n\n4.2.4 Prospettive Moderne\nNegli ultimi anni, il dibattito sulla misurazione psicologica si è arricchito di nuove prospettive, grazie all’avvento di tecnologie avanzate e all’integrazione di approcci interdisciplinari. Ecco alcune delle tendenze più rilevanti.\nTeoria della Risposta agli Item. La Teoria della Risposta agli Item (IRT) ha guadagnato popolarità per la sua capacità di fornire stime più precise delle abilità latenti rispetto ai modelli classici. La IRT considera la probabilità che un individuo risponda correttamente a un item in funzione della sua abilità e delle caratteristiche dell’item stesso, offrendo una visione più dettagliata delle proprietà psicometriche degli strumenti di misurazione.\nApprocci Bayesiani. Gli approcci bayesiani stanno rivoluzionando il campo della psicometria, permettendo di incorporare informazioni a priori nelle stime e di aggiornare le credenze sulla base di nuovi dati. Questi metodi sono particolarmente utili per affrontare la complessità e l’incertezza inerenti alla misurazione psicologica.\nAnalisi di Rete. L’analisi di rete è un’altra metodologia emergente che vede i costrutti psicologici non come variabili latenti indipendenti, ma come reti di sintomi interconnessi. Questo approccio può offrire nuove intuizioni sulla struttura delle psicopatologie e sulla dinamica dei sintomi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#le-scale-di-misurazione",
    "href": "chapters/key_notions/02_measurement.html#le-scale-di-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.3 Le scale di misurazione",
    "text": "4.3 Le scale di misurazione\nLe scale di misurazione sono strumenti fondamentali per assegnare numeri ai dati osservati, rappresentando le proprietà psicologiche. La teoria delle scale di Stevens Stevens (1946) identifica quattro tipi di scale di misurazione: nominali, ordinali, a intervalli e di rapporti. Ognuna di queste scale consente di effettuare operazioni aritmetiche diverse, poiché ciascuna di esse è in grado di “catturare” solo alcune delle proprietà dei fenomeni psicologici che si intende misurare.\n\n\n\nScale di misurazione.\n\n\n\n4.3.1 Scala nominale\nILa scala nominale è il livello di misurazione più semplice e corrisponde ad una tassonomia o classificazione delle categorie che utilizziamo per descrivere i fenomeni psicologici. I simboli o numeri che costituiscono questa scala rappresentano i nomi delle categorie e non hanno alcun valore numerico intrinseco. Con la scala nominale possiamo solo distinguere se una caratteristica psicologica è uguale o diversa da un’altra.\nI dati raccolti con la scala nominale sono suddivisi in categorie qualitative e mutuamente esclusive, in cui ogni dato appartiene ad una sola categoria. In questa scala, esiste solo la relazione di equivalenza tra le misure delle unità di studio: gli elementi del campione appartenenti a classi diverse sono differenti, mentre tutti quelli della stessa classe sono tra loro equivalenti.\nL’unica operazione algebrica consentita dalla scala nominale è quella di contare le unità di studio che appartengono ad ogni categoria e il numero totale di categorie. Di conseguenza, la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.\nDalla scala nominale è possibile costruire altre scale nominali equivalenti alla prima, trasformando i valori della scala di partenza in modo tale da cambiare i nomi delle categorie, ma lasciando inalterata la suddivisione delle unità di studio nelle medesime classi di equivalenza. In altre parole, cambiando i nomi delle categorie di una variabile misurata su scala nominale, si ottiene una nuova variabile esattamente equivalente alla prima.\n\n\n4.3.2 Scala ordinale\nLa scala ordinale mantiene la caratteristica della scala nominale di classificare ogni unità di misura all’interno di una singola categoria, ma introduce la relazione di ordinamento tra le categorie. In quanto basata su una relazione di ordine, una scala ordinale descrive solo il rango di ordine tra le categorie e non fornisce informazioni sulla distanza tra di esse. Non ci dice, ad esempio, se la distanza tra le categorie \\(a\\) e \\(b\\) è uguale, maggiore o minore della distanza tra le categorie \\(b\\) e \\(c\\).\nUn esempio classico di scala ordinale è quello della scala Mohs per la determinazione della durezza dei minerali. Per stabilire la durezza dei minerali si usa il criterio empirico della scalfittura. Vengono stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo, topazio, corindone e diamante. Un minerale appartenente ad uno di questi livelli se scalfisce quello di livello inferiore ed è scalfito da quello di livello superiore.\n\n\n\nLa scala di durezza dei minerali di Mohs. Un oggetto è considerato più duro di X se graffia X. Sono incluse anche misure di durezza relativa utilizzando uno sclerometro, da cui emerge la non linearità della scala di Mohs (Burchard, 2004).\n\n\n\n\n4.3.3 Scala ad intervalli\nLa scala ad intervalli di misurazione include le proprietà della scala nominale e della scala ordinale e permette di misurare le distanze tra le coppie di unità statistiche in termini di un intervallo costante, chiamato “unità di misura”, a cui viene attribuito il valore “1”. L’origine della scala, ovvero il punto zero, è scelta arbitrariamente e non indica l’assenza della proprietà che si sta misurando. Ciò significa che la scala ad intervalli consente anche valori negativi e lo zero non viene attribuito all’unità statistica in cui la proprietà risulta assente.\nLa scala ad intervalli equivalenti consente l’esecuzione di operazioni algebriche basate sulla differenza tra i numeri associati ai diversi punti della scala, operazioni algebriche non possibili con le scale di misura nominale o ordinale. Tuttavia, il limite della scala ad intervalli è che non consente di calcolare il rapporto tra coppie di misure. È possibile affermare la differenza tra \\(a\\) e \\(b\\) come la metà della differenza tra \\(c\\) e \\(d\\) o che le due differenze sono uguali, ma non è possibile affermare che \\(a\\) abbia una proprietà misurata in quantità doppia rispetto a \\(b\\). In altre parole, non è possibile stabilire rapporti diretti tra le misure ottenute. Solo le differenze tra le modalità permettono tutte le operazioni aritmetiche, come la somma, l’elevazione a potenza o la divisione, che sono alla base della statistica inferenziale.\nNelle scale ad intervalli equivalenti, l’unità di misura è arbitraria e può essere cambiata attraverso una dilatazione, ovvero la moltiplicazione di tutti i valori della scala per una costante positiva. Inoltre, la traslazione, ovvero l’aggiunta di una costante a tutti i valori della scala, è ammessa poiché non altera le differenze tra i valori della scala. La scala rimane invariata rispetto a traslazioni e dilatazioni e dunque le uniche trasformazioni ammissibili sono le trasformazioni lineari:\n\\[\ny' = a + by, \\quad b &gt; 0.\n\\]\nInfatti, l’uguaglianza dei rapporti fra gli intervalli rimane invariata a seguito di una trasformazione lineare.\nEsempio di scala ad intervalli è la temperatura misurata in gradi Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è possibile stabilire se due modalità sono uguali o diverse: 30\\(^\\circ\\)C \\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale è possibile mettere due modalità in una relazione d’ordine: 30\\(^\\circ\\)C \\(&gt;\\) 20\\(^\\circ\\)C. In aggiunta ai casi precedenti, però, è possibile definire una unità di misura per cui è possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c’è una differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. I valori di temperatura, oltre a poter essere ordinati secondo l’intensità del fenomeno, godono della proprietà che le differenze tra loro sono direttamente confrontabili e quantificabili.\nIl limite della scala ad intervalli è quello di non consentire il calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura di 80\\(^\\circ\\)C non è il doppio di una di 40\\(^\\circ\\)C. Se infatti esprimiamo le stesse temperature nei termini della scala Fahrenheit, allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti, 20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa che la relazione “il doppio di” che avevamo individuato in precedenza si applicava ai numeri della scala centigrada, ma non alla proprietà misurata (cioè la temperatura). La decisione di che scala usare (Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non deve influenzare le inferenze che traiamo dai dati. Queste inferenze, infatti, devono dirci qualcosa a proposito della realtà empirica e non possono in nessun modo essere condizionate dalle nostre scelte arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che quella Fahrenheit.\nConsideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature: \\(20^\\circ C = 68^\\circ F\\), \\(15^\\circ C = 59^\\circ F\\), \\(10^\\circ C = 50 ^\\circ F\\).\nÈ facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:\n\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]\n\n\n4.3.4 Scala di rapporti\nNella scala a rapporti equivalenti, lo zero non è arbitrario e rappresenta l’elemento che ha intensità nulla rispetto alla proprietà misurata. Per costruire questa scala, si associa il numero 0 all’elemento con intensità nulla e si sceglie un’unità di misura \\(u\\). Ad ogni elemento si assegna un numero \\(a\\) definito come \\(a=d/u\\), dove \\(d\\) rappresenta la distanza dall’origine. In questo modo, i numeri assegnati riflettono le differenze e i rapporti tra le intensità della proprietà misurata.\nIn questa scala, è possibile effettuare operazioni aritmetiche non solo sulle differenze tra i valori della scala, ma anche sui valori stessi della scala. L’unica scelta arbitraria è l’unità di misura, ma lo zero deve sempre rappresentare l’intensità nulla della proprietà considerata.\nLe trasformazioni ammissibili in questa scala sono chiamate trasformazioni di similarità e sono del tipo \\(y' = by\\), dove \\(b&gt;0\\). In questa scala, i rapporti tra i valori rimangono invariati dopo le trasformazioni. In altre parole, se rapportiamo due valori originali e due valori trasformati, il rapporto rimane lo stesso: \\(\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "href": "chapters/key_notions/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.4 Gerarchia dei livelli delle scale di misurazione",
    "text": "4.4 Gerarchia dei livelli delle scale di misurazione\nSecondo Stevens (1946), esiste una gerarchia dei livelli delle scale di misurazione, denominati “livelli di scala”. Questi livelli sono organizzati in modo gerarchico, in cui la scala nominale rappresenta il livello più basso della misurazione, mentre la scala a rapporti equivalenti rappresenta il livello più alto.\n\nLa scala nominale è il livello più elementare, in cui le categorie o le etichette vengono assegnate agli oggetti o agli individui senza alcuna valutazione di grandezza o ordine.\nAl livello successivo si trova la scala ordinale, in cui le categorie sono ordinate in base a una qualche qualità o caratteristica. Qui, è possibile stabilire un ordine di preferenza o gerarchia tra le categorie, ma non è possibile quantificare la differenza tra di esse in modo preciso.\nLa scala intervallo rappresenta un livello successivo, in cui le categorie sono ordinate e la differenza tra di esse è quantificabile in modo preciso. In questa scala, è possibile effettuare operazioni matematiche come l’addizione e la sottrazione tra i valori, ma non è possibile stabilire un vero e proprio punto zero significativo.\nInfine, la scala a rapporti equivalenti rappresenta il livello più alto. In questa scala, le categorie sono ordinate, la differenza tra di esse è quantificabile in modo preciso e esiste un punto zero assoluto che rappresenta l’assenza totale della grandezza misurata. Questo livello di scala permette di effettuare tutte le operazioni matematiche, compresa la moltiplicazione e la divisione.\n\nPassando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.\n\n\n\nRelazioni tra i livelli di misurazione.\n\n\nPer ciò che riguarda le trasformazioni ammissibili, più il livello di scala è basso, più le funzioni sono generali (sono minori cioè i vincoli per passare da una rappresentazione numerica ad un’altra equivalente). Salendo la gerarchia, la natura delle funzioni di trasformazione si fa più restrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#variabili-discrete-o-continue",
    "href": "chapters/key_notions/02_measurement.html#variabili-discrete-o-continue",
    "title": "4  La misurazione in psicologia",
    "section": "4.5 Variabili discrete o continue",
    "text": "4.5 Variabili discrete o continue\nLe variabili possono essere classificate come variabili a livello di intervalli o di rapporti e possono essere sia discrete che continue.\n\nLe variabili discrete assumono valori specifici ma non possono assumere valori intermedi. Una volta che l’elenco dei valori accettabili è stato definito, non vi sono casi che si trovano tra questi valori. In genere, le variabili discrete assumono valori interi, come il numero di eventi, il numero di persone o il numero di oggetti.\nD’altra parte, le variabili continue possono assumere qualsiasi valore all’interno di un intervallo specificato. Teoricamente, ciò significa che è possibile utilizzare frazioni e decimali per ottenere qualsiasi grado di precisione.\n\n\n\n\nVariabili discrete e continue.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "href": "chapters/key_notions/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.6 Comprendere gli errori nella misurazione",
    "text": "4.6 Comprendere gli errori nella misurazione\nGli errori di misurazione possono essere casuali o sistematici. Gli errori casuali sono fluttuazioni aleatorie, mentre gli errori sistematici sono costanti e derivano da problemi nel metodo di misurazione o negli strumenti.\n\n4.6.1 Precisione e Accuratezza\nLa precisione indica la coerenza tra misurazioni ripetute, mentre l’accuratezza si riferisce alla vicinanza del valore misurato al valore reale. Entrambi i concetti sono cruciali per l’assessment psicometrico.\nUtilizzando l’analogia del tiro al bersaglio, si può avere una serie di colpi vicini tra loro ma lontani dal centro (precisione senza accuratezza) oppure colpi distribuiti in modo sparso ma in media vicini al centro (accuratezza senza precisione).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#assessment-psicometrico",
    "href": "chapters/key_notions/02_measurement.html#assessment-psicometrico",
    "title": "4  La misurazione in psicologia",
    "section": "4.7 Assessment psicometrico",
    "text": "4.7 Assessment psicometrico\nL’assessment psicometrico valuta la qualità delle misurazioni psicologiche, considerando la validità e l’affidabilità.\n\n4.7.1 Validità nella Misurazione Psicologica\nLa validità è una proprietà psicometrica fondamentale dei test psicologici. Secondo gli Standards for Educational and Psychological Testing (2014), la validità si riferisce al grado in cui evidenza e teoria supportano le interpretazioni dei punteggi dei test per gli usi proposti. Questo concetto evidenzia che la validità riguarda sia il significato dei punteggi sia il loro utilizzo, rendendola “la considerazione più fondamentale nello sviluppo e nella valutazione dei test”.\n\n\n4.7.2 Evoluzione del Concetto di Validità\nTradizionalmente, la validità era suddivisa in tre categorie:\n\nValidità di Contenuto: Si riferisce alla corrispondenza tra il contenuto degli item di un test e il dominio dell’attributo psicologico che il test intende misurare. È importante che gli item siano pertinenti e rappresentativi dell’attributo misurato.\nValidità di Criterio: Valuta il grado di concordanza tra i risultati ottenuti tramite lo strumento di misurazione e i risultati ottenuti da altri strumenti che misurano lo stesso costrutto o da un criterio esterno. Include validità concorrente e predittiva.\nValidità di Costrutto: Riguarda il grado in cui un test misura effettivamente il costrutto che si intende misurare. Si suddivide in validità convergente (accordo con strumenti che misurano lo stesso costrutto) e validità divergente (capacità di discriminare tra costrutti diversi).\n\nLa moderna teoria della validità non adotta più questa visione tripartita. Gli Standards del 2014 descrivono la validità come un concetto unitario, dove diverse forme di evidenza concorrono a supportare l’interpretazione dei punteggi del test per il loro utilizzo previsto.\n\n\n4.7.3 Tipologie di Prove di Validità\nGli Standards del 2014 identificano cinque categorie principali di prove di validità:\n\nProve Basate sul Contenuto del Test: Valutano quanto il contenuto del test rappresenti adeguatamente il dominio del costrutto da misurare.\nProve Basate sui Processi di Risposta: Analizzano se i processi cognitivi e comportamentali degli esaminandi riflettono il costrutto valutato.\nProve Basate sulla Struttura Interna: Esaminano la coerenza tra gli elementi del test e la struttura teorica del costrutto. L’analisi fattoriale è uno strumento chiave in questo contesto.\nProve Basate sulle Relazioni con Altre Variabili: Studiano la correlazione tra i punteggi del test e altre variabili teoricamente correlate, utilizzando metodi come la validità convergente e divergente.\nProve Basate sulle Conseguenze del Test: Considerano le implicazioni e gli effetti dell’uso del test, sia intenzionali che non intenzionali.\n\n\n\n4.7.4 Minacce alla Validità\nLa validità può essere compromessa quando un test non misura integralmente il costrutto di interesse (sotto-rappresentazione del costrutto) o quando include varianza estranea al costrutto. Inoltre, fattori esterni come l’ansia o la bassa motivazione degli esaminandi, e deviazioni nelle procedure di amministrazione e valutazione, possono influenzare negativamente la validità delle interpretazioni dei risultati.\n\n\n4.7.5 Integrazione delle Prove di Validità\nLa validità di un test si costruisce attraverso l’integrazione di diverse linee di evidenza. Ogni interpretazione o uso di un test deve essere validato specificamente, richiedendo una valutazione continua e accurata delle prove disponibili. Questo processo implica la costruzione di un argomento di validità che consideri attentamente la qualità tecnica del test e l’adeguatezza delle sue interpretazioni per gli scopi previsti.\nIn conclusione, la validità è un concetto complesso e integrato che richiede un’analisi continua e multidimensionale delle evidenze. La moderna teoria della validità enfatizza l’importanza di considerare diverse forme di evidenza per supportare le interpretazioni dei punteggi dei test, garantendo che siano utilizzati in modo appropriato e significativo. Gli sviluppatori e gli utilizzatori di test devono impegnarsi a valutare costantemente la validità per assicurare misurazioni psicologiche accurate e affidabili.\n\n\n4.7.6 Affidabilità\nL’affidabilità concerne la consistenza e stabilità delle misurazioni, verificata attraverso metodi come l’affidabilità test-retest, inter-rater, intra-rater e l’affidabilità interna.\n\nAffidabilità Test-Retest: Questa forma di affidabilità verifica la consistenza delle misurazioni nel tempo. Se un individuo viene testato in due momenti diversi, i risultati dovrebbero essere simili, assumendo che non ci siano stati cambiamenti significativi nel costrutto misurato.\nAffidabilità Inter-rater: In questo caso, l’affidabilità è determinata dalla concordanza tra le valutazioni di diversi esaminatori. Ad esempio, se più psicologi dovessero valutare un individuo utilizzando lo stesso strumento, le loro valutazioni dovrebbero essere simili.\nAffidabilità Intra-rater: Questa misura dell’affidabilità si riferisce alla consistenza delle valutazioni dello stesso esaminatore in momenti diversi.\nAffidabilità Interna: Si riferisce alla coerenza delle risposte all’interno dello stesso test. Ad esempio, se un test misura un costrutto come l’ansia, gli item che misurano l’ansia dovrebbero correlare positivamente l’uno con l’altro. Un modo comune per valutare l’affidabilità interna è utilizzare il coefficiente \\(\\omega\\) di McDonald.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#commenti-e-considerazioni-finali",
    "href": "chapters/key_notions/02_measurement.html#commenti-e-considerazioni-finali",
    "title": "4  La misurazione in psicologia",
    "section": "4.8 Commenti e considerazioni finali",
    "text": "4.8 Commenti e considerazioni finali\nLa teoria della misurazione è fondamentale nella ricerca empirica per valutare l’attendibilità e la validità delle misurazioni. È cruciale valutare l’errore nella misurazione per garantire la precisione e l’accuratezza delle misure. L’assessment psicometrico si occupa di valutare la qualità delle misurazioni psicologiche, considerando l’affidabilità e la validità per garantire misure accurate dei costrutti teorici. Le moderne tecnologie e metodologie stanno continuamente arricchendo questo campo, offrendo strumenti sempre più raffinati per la comprensione delle caratteristiche psicologiche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#bibliografia",
    "href": "chapters/key_notions/02_measurement.html#bibliografia",
    "title": "4  La misurazione in psicologia",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nLilienfeld, S. O., & Strother, A. N. (2020). Psychological measurement and the replication crisis: Four sacred cows. Canadian Psychology/Psychologie Canadienne, 61(4), 281–288.\n\n\nMaul, A., Irribarra, D. T., & Wilson, M. (2016). On the philosophical foundations of psychological measurement. Measurement, 79, 311–320.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677–680.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html",
    "href": "chapters/key_notions/03_data_analysis.html",
    "title": "5  L’analisi dei dati psicologici",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nNel panorama contemporaneo delle scienze sociali e della psicologia, gli ultimi due decenni hanno visto l’emergere di una profonda trasformazione metodologica ed epistemologica. Questo movimento, caratterizzato da concetti chiave quali “Credibility Revolution” (Angrist & Pischke, 2010), “Causal Revolution” (Pearl & Mackenzie, 2018) e “Replication Crisis” (Collaboration, 2015), ha determinato un cambiamento paradigmatico nelle pratiche delle scienze sociali e, in particolare, della psicologia (Korbmacher et al., 2023). Questa transizione verso quella che Munger (2023) definisce “Science versione 2” è stata motivata dalle lacune metodologiche precedenti e ha catalizzato l’adozione di approcci più rigorosi e replicabili.\nLa genesi di questa Riforma è radicata nella constatazione di problematiche metodologiche pervasive, tra cui la proliferazione di falsi positivi (Simmons et al., 2011), l’abuso dei “gradi di libertà dei ricercatori” (Gelman & Loken, 2013), e l’inadeguatezza delle pratiche statistiche tradizionali (Gelman & Loken, 2014). Fenomeni come il p-hacking, l’uso di campioni sottodimensionati (Button et al., 2013), e la mancanza di trasparenza nei metodi di ricerca hanno contribuito a minare la credibilità delle scoperte psicologiche (Ioannidis, 2005; Meehl, 1967), portando alla cosiddetta “Replication Crisis” (Baker, 2016; Bishop, 2019) – si veda il ?sec-crisis.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano",
    "href": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.1 L’Approccio Bayesiano",
    "text": "5.1 L’Approccio Bayesiano\nIn risposta a queste sfide, l’approccio bayesiano è emerso come un paradigma statistico fondamentale nella “Credibility Revolution”. Contrariamente all’inferenza frequentista basata sul Test dell’Ipotesi Nulla, la statistica bayesiana offre un framework più flessibile e intuitivo per l’analisi dei dati e l’inferenza causale. Il principio cardine dell’approccio bayesiano, l’aggiornamento delle distribuzioni di probabilità a priori (priors) alla luce di nuove evidenze, si allinea perfettamente con l’obiettivo di una scienza cumulativa e auto-correttiva.\nL’adozione di metodi bayesiani in psicologia comporta diversi vantaggi significativi:\n\nQuantificazione dell’incertezza: L’inferenza bayesiana fornisce distribuzioni di probabilità posteriori complete per i parametri di interesse, offrendo una rappresentazione più ricca e sfumata dell’incertezza rispetto agli intervalli di confidenza frequentisti.\nIncorporazione di conoscenze pregresse: Le priors bayesiane consentono l’integrazione formale di conoscenze precedenti nel processo inferenziale, promuovendo un approccio cumulativo alla ricerca.\nRobustezza alle pratiche di ricerca discutibili: I metodi bayesiani sono meno suscettibili a pratiche come il p-hacking, poiché l’inferenza si basa sull’intera distribuzione posteriore piuttosto che su soglie arbitrarie di significatività.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano-nella-ricerca",
    "href": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano-nella-ricerca",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.2 L’approccio bayesiano nella ricerca",
    "text": "5.2 L’approccio bayesiano nella ricerca\nL’impiego delle statistiche bayesiane nella ricerca psicologica presenta notevoli vantaggi rispetto ad altri metodi statistici tradizionali, come il test di significatività dell’ipotesi nulla. Un punto di forza importante risiede nella sua indipendenza dalla teoria dei grandi campioni, rendendolo particolarmente adatto per gli studi psicologici che spesso si basano su campioni di dimensioni ridotte (Larson et al., 2023).\nLa ricerca psicologica è frequentemente caratterizzata da campioni limitati, dovuti a diversi fattori quali la bassa prevalenza di determinate condizioni, le difficoltà nel reclutamento dei partecipanti e le complessità nelle procedure di valutazione. Questi campioni di piccole dimensioni sono intrinsecamente soggetti a una maggiore eterogeneità, che si manifesta nella variabilità del fenotipo comportamentale delle condizioni psicologiche esaminate e nella discrepanza tra le stime degli effetti in diversi studi. Tale eterogeneità può condurre a stime degli effetti distorte e scarsamente riproducibili.\nL’approccio bayesiano offre una soluzione efficace a queste problematiche. In primo luogo, consente di valutare l’adeguatezza della dimensione del campione attraverso un’analisi della sensibilità dei risultati rispetto alla specificazione delle distribuzioni a priori. In secondo luogo, permette di ottenere risultati precisi anche con campioni ridotti, a condizione che le conoscenze a priori siano accurate e ben definite.\nUn ulteriore vantaggio dell’approccio bayesiano è la sua capacità di ottimizzare l’uso dei campioni di partecipanti, favorendo un’inclusione equa delle popolazioni diversificate. Questo è particolarmente rilevante per gruppi spesso sottorappresentati, come le minoranze etniche. Le statistiche bayesiane aiutano a superare questa sfida evitando di esercitare una pressione eccessiva su questi gruppi per aumentarne la partecipazione, permettendo così una ricerca più equa e rappresentativa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#modellazione-formale",
    "href": "chapters/key_notions/03_data_analysis.html#modellazione-formale",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.3 Modellazione Formale",
    "text": "5.3 Modellazione Formale\nLa “Credibility Revolution” ha catalizzato l’integrazione della Data Science nelle pratiche di ricerca psicologica. L’adozione di pipeline di analisi dei dati riproducibili, l’uso di controllo di versione, e la condivisione di dati e codice sono diventati standard de facto nella comunità scientifica. Questi strumenti non solo migliorano la trasparenza e la replicabilità della ricerca, ma facilitano anche la collaborazione e l’accumulo di conoscenze nel campo.\nParallelamente, si è osservato un rinnovato interesse per la modellazione formale in psicologia, che consente non solo la verifica ma anche lo sviluppo di modelli dei meccanismi sottostanti ai fenomeni psicologici (Oberauer & Lewandowsky, 2019; Van Dongen et al., 2024). Questo approccio supera la mera descrizione delle associazioni tra variabili, che era tipica della pratica dominante dell’ANOVA nel contesto pre-riforma.\nLa modellazione bayesiana si presta particolarmente bene a questo approccio, offrendo un framework unificato per la specificazione di modelli formali, l’incorporazione di incertezza parametrica, e la valutazione dell’evidenza empirica. Attraverso tecniche come il confronto tra modelli bayesiano e l’analisi di sensibilità, i ricercatori possono valutare rigorosamente la plausibilità relativa di diverse teorie psicologiche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#riflessioni-epistemologiche",
    "href": "chapters/key_notions/03_data_analysis.html#riflessioni-epistemologiche",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.4 Riflessioni Epistemologiche",
    "text": "5.4 Riflessioni Epistemologiche\nL’adozione di metodi bayesiani e della Data Science in psicologia deve essere accompagnata da una profonda riflessione epistemologica. Come sottolineato da George Box\n\ntutti i modelli sono sbagliati, ma alcuni sono utili.\n\nQuesta massima risuona particolarmente nel contesto della ricerca psicologica, dove i fenomeni di interesse sono spesso complessi e multifattoriali.\nL’approccio bayesiano, con la sua enfasi sull’aggiornamento iterativo delle credenze alla luce di nuove evidenze, si allinea naturalmente con una visione della scienza come processo di apprendimento continuo piuttosto che come ricerca di verità assolute. Questa prospettiva riconosce i limiti intrinseci dei nostri modelli e delle nostre teorie, pur valorizzandone l’utilità euristica e predittiva (si veda la discussione nella ?sec-poetic-validity).\nIn particolare, McElreath (2020) sottolinea l’importanza di riconoscere la dualità tra il “mondo del modello” e il mondo reale più ampio che cerchiamo di comprendere. Questa consapevolezza è cruciale per evitare la reificazione dei nostri modelli statistici e per mantenere una prospettiva critica sulle nostre inferenze.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#conclusione",
    "href": "chapters/key_notions/03_data_analysis.html#conclusione",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.5 Conclusione",
    "text": "5.5 Conclusione\nL’integrazione dell’approccio bayesiano e della data science nella ricerca psicologica rappresenta una risposta promettente alle sfide poste dalla “Replication Crisis”. Offrendo un framework coerente per la modellazione formale, l’inferenza statistica e l’incorporazione di conoscenze pregresse, questi approcci promettono di elevare il rigore e la credibilità della ricerca psicologica. Tuttavia, è fondamentale che l’adozione di questi metodi sia accompagnata da una adeguata consapevolezza metodologica ed epistemologica – si veda, ad esempio, il ?sec-causal-inference-regr.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#bibliografia",
    "href": "chapters/key_notions/03_data_analysis.html#bibliografia",
    "title": "5  L’analisi dei dati psicologici",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of economic perspectives, 24(2), 3–30.\n\n\nBaker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604).\n\n\nBishop, D. (2019). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research.\n\n\nButton, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376.\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no «fishing expedition» or «p-hacking» and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 348(1-17), 3.\n\n\nGelman, A., & Loken, E. (2014). The statistical crisis in science. American scientist, 102(6), 460–465.\n\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124.\n\n\nKorbmacher, M., Azevedo, F., Pennington, C. R., Hartmann, H., Pownall, M., Schmidt, K., Elsherif, M., Breznau, N., Robertson, O., Kalandadze, T., et al. (2023). The replication crisis has led to positive structural, procedural, and community changes. Communications Psychology, 1(1), 3.\n\n\nLabatut, B. (2021). Quando abbiamo smesso di capire il mondo. Adelphi Edizioni spa.\n\n\nLarson, C., Kaplan, D., Girolamo, T., Kover, S. T., & Eigsti, I.-M. (2023). A Bayesian statistics tutorial for clinical research: Prior distributions and meaningful results for small clinical samples. Journal of Clinical Psychology, 79(11), 2602–2624.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of science, 34(2), 103–115.\n\n\nMunger, K. (2023). Temporal validity as meta-science. Research & Politics, 10(3), 20531680231187271.\n\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26, 1596–1618.\n\n\nPearl, J., & Mackenzie, D. (2018). The book of why: the new science of cause and effect. Basic books.\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science, 22(11), 1359–1366.\n\n\nVan Dongen, N., Bork, R. van, Finnemann, A., Haslbeck, J., Maas, H. L. van der, Robinaugh, D. J., Ron, J. de, Sprenger, J., & Borsboom, D. (2024). Productive explanation: A framework for evaluating explanations in psychological science. Psychological Review.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html",
    "href": "chapters/eda/01_project_structure.html",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "",
    "text": "7.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nSeguendo Yu & Barter (2024), in questo capitolo introdurremo l’analisi esplorativa dei dati situandola all’interno dell’intero ciclo di vita di un progetto di data science (DSLC). Secondo Yu & Barter (2024), ogni progetto di analisi dei dati segue una combinazione delle seguenti fasi:\nMentre quasi tutti i progetti di data science attraversano le fasi 1-2 e 4-5, non tutti includono la fase 3.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#introduzione",
    "href": "chapters/eda/01_project_structure.html#introduzione",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "",
    "text": "Formulazione del problema e raccolta dei dati.\nPulizia dei dati, preprocessing e analisi esplorativa.\nAnalisi predittiva e/o inferenziale.\nValutazione dei risultati.\nComunicazione dei risultati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-1-formulazione-del-problema-e-raccolta-dei-dati",
    "href": "chapters/eda/01_project_structure.html#fase-1-formulazione-del-problema-e-raccolta-dei-dati",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.2 Fase 1: Formulazione del Problema e Raccolta dei Dati",
    "text": "7.2 Fase 1: Formulazione del Problema e Raccolta dei Dati\nLa formulazione di una domanda di ricerca precisa è il punto di partenza di ogni progetto di data science. È cruciale che la domanda sia formulata in modo tale da poter essere risolta attraverso l’analisi dei dati disponibili. Alle volte la domanda iniziale è troppo vaga o non risolvibile. L’obiettivo è riformulare la domanda in modo tale che possa trovare una risposta utilizzando i dati a disposizione.\n\n7.2.1 Raccolta dei Dati\nAlcuni progetti utilizzano dati esistenti (da repository pubblici, database interni o esperimenti passati), mentre altri richiedono la raccolta di nuovi dati. Ogni volta che è possibile, è necessario avere ben chiaro quali analisi statistiche verranno svolte prima di raccogliere i dati. Se questo non viene fatto, può succedere che i dati raccolti non siano adeguati per rispondere alle domande di interesse, in quanto mancano informazioni cruciali, o vengono violate assunzioni richieste dai modelli statistici che si vogliono impiegare.\nÈ fondamentale sviluppare una comprensione approfondita dei processi di acquisizione dei dati e del significato delle misure ottenute. Parallelamente, è cruciale essere pienamente consapevoli degli strumenti e delle metodologie impiegate nella raccolta dei dati. In altri termini, è essenziale riconoscere e valutare i potenziali bias che possono emergere dalle tecniche e dalle procedure adottate durante il processo di raccolta dati.\n\n7.2.2 Terminologia dei Dati\nIn una matrice di dati (comunemente denominata “dataset”), ogni colonna rappresenta una diversa tipologia di misurazione, definita come variabile, carattere o attributo.\nGeneralmente, le variabili in un dataset si classificano in una delle seguenti categorie:\n\n\nQuantitative:\n\nContinue: Valori che possono assumere qualsiasi numero reale all’interno di un intervallo (es. importo di spesa, durata di permanenza su un sito web).\nDiscrete: Valori numerici interi, spesso risultato di conteggi (es. numero di visitatori di un sito web in un determinato periodo, numero di esemplari di una specie in una data località).\n\n\n\nQualitative (o Categoriche):\n\nNominali: Categorie senza un ordine intrinseco (es. partito politico, reparto ospedaliero, nazione).\nOrdinali: Categorie con un ordine naturale ma senza una metrica definita tra i livelli (es. livello di istruzione, grado di soddisfazione).\n\n\nTemporali: Date e orari in vari formati (es. “01/01/2020 23:00:05” o “1 gen 2020”).\n\nTestuali:\n\nStrutturate: Testo con formato predefinito (es. nominativo, indirizzo postale, email).\nNon strutturate: Corpo di testo esteso senza struttura predefinita (es. cartelle cliniche, recensioni, post sui social media).\n\n\n\nLa dimensionalità dei dati si riferisce al numero di variabili (colonne) presenti nel dataset. Si parla di “dati ad alta dimensionalità” quando il numero di variabili è elevato (generalmente superiore a 100, sebbene non esista una soglia universalmente accettata).\nOgni riga del dataset corrisponde a una singola unità statistica, anche detta caso o osservazione. Queste rappresentano le entità su cui vengono effettuate le misurazioni.\nQuesta struttura, in cui i dati sono organizzati in colonne (variabili) e righe (unità statistiche), viene definita come matrice dei dati o, in ambito informatico, come formato tabellare.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-2-pulizia-dei-dati-e-analisi-esplorativa",
    "href": "chapters/eda/01_project_structure.html#fase-2-pulizia-dei-dati-e-analisi-esplorativa",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.3 Fase 2: Pulizia dei Dati e Analisi Esplorativa",
    "text": "7.3 Fase 2: Pulizia dei Dati e Analisi Esplorativa\n\n7.3.1 Pulizia dei Dati\nDopo aver definito la domanda della ricerca e avere raccolto i dati rilevanti, è il momento di pulire i dati. Un dataset pulito è ordinato, formattato in modo appropriato e ha voci non ambigue. La fase iniziale di pulizia dei dati consiste nell’identificare problemi con i dati (come formattazioni anomale e valori non validi) e modificarli in modo che i valori siano validi e formattati in modo comprensibile sia per il computer che per noi. La pulizia dei dati è una fase estremamente importante di un progetto di data science perché non solo aiuta a garantire che i dati siano interpretati correttamente dal computer, ma aiuta anche a sviluppare una comprensione dettagliata delle informazioni contenute nei dati e delle loro limitazioni.\nL’obiettivo della pulizia dei dati è creare una versione dei dati che rifletta nella maniera più fedele possibile la realtà e che sia interpretata correttamente dal computer. Per garantire che il computer utilizzi fedelmente le informazioni contenute nei dati, è necessario modificare i dati (scrivendo codice, non modificando il file dati grezzo stesso) in modo che siano in linea con ciò che il computer “si aspetta”. Tuttavia, il processo di pulizia dei dati è necessariamente soggettivo e comporta fare assunzioni sulle quantità reali sottostanti misurate e decisioni su quali modifiche siano le più sensate.\n\n7.3.2 Preprocessing\nIl preprocessing si riferisce al processo di modifica dei dati puliti per soddisfare i requisiti di un algoritmo specifico che si desidera applicare. Ad esempio, se si utilizza un algoritmo che richiede che le variabili siano sulla stessa scala, potrebbe essere necessario trasformarle, oppure, se si utilizza un algoritmo che non consente valori mancanti, potrebbe essere necessario imputarli o rimuoverli. Durante il preprocessing, potrebbe essere utile anche definire nuove caratteristiche/variabili utilizzando le informazioni esistenti nei dati, se si ritiene che queste possano essere utili per l’analisi.\nCome per la pulizia dei dati, non esiste un unico modo corretto per pre-elaborare un dataset, e la procedura finale comporta tipicamente una serie di decisioni che dovrebbero essere documentate nel codice e nei file di documentazione.\n\n7.3.3 Analisi Esplorativa dei Dati\nDopo l’acquisizione dei dati, si procede con un’analisi approfondita che si articola in due fasi principali:\n\n\nAnalisi Esplorativa dei Dati (EDA - Exploratory Data Analysis):\nQuesta fase iniziale mira a far familiarizzare il ricercatore con il dataset e a scoprire pattern nascosti. Si realizza attraverso:\n\nLa costruzione di tabelle di frequenza e contingenza\nIl calcolo di statistiche descrittive (come indici di posizione, dispersione e forma della distribuzione)\nLa creazione di rappresentazioni grafiche preliminari\n\nL’EDA permette di generare ipotesi sui dati e di guidare le successive analisi statistiche.\n\n\nAnalisi Esplicativa:\nIn questa fase successiva, l’obiettivo è raffinare e perfezionare le analisi per comunicare efficacemente i risultati a un pubblico più ampio. Ciò comporta:\n\nL’ottimizzazione delle tabelle per una maggiore leggibilità\nIl perfezionamento delle visualizzazioni grafiche per una comunicazione chiara ed efficace\nLa selezione delle statistiche più rilevanti per supportare le conclusioni\n\nL’analisi esplicativa si concentra sulla presentazione chiara e convincente dei risultati, adattando il livello di dettaglio e il linguaggio al pubblico di riferimento.\n\n\nEntrambe le fasi sono cruciali: l’EDA consente di comprendere a fondo la struttura e le caratteristiche dei dati, mentre l’analisi esplicativa assicura che le scoperte siano comunicate in modo efficace e comprensibile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-3-analisi-predittiva-e-inferenziale",
    "href": "chapters/eda/01_project_structure.html#fase-3-analisi-predittiva-e-inferenziale",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.4 Fase 3: Analisi Predittiva e Inferenziale",
    "text": "7.4 Fase 3: Analisi Predittiva e Inferenziale\nMolte domande nella data science si presentano come problemi di inferenza e/o previsione, in cui l’obiettivo principale è utilizzare dati osservati, passati o presenti, per descrivere le caratteristiche di una popolazione più ampia o per fare previsioni su dati futuri non ancora disponibili. Questo tipo di analisi è spesso orientato a supportare decisioni nel mondo reale.\nNel corso, ci concentreremo principalmente sull’approccio bayesiano per affrontare questi problemi inferenziali, fornendo un’introduzione a come tale prospettiva possa essere applicata efficacemente in questo contesto.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-4-valutazione-dei-risultati",
    "href": "chapters/eda/01_project_structure.html#fase-4-valutazione-dei-risultati",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.5 Fase 4: Valutazione dei Risultati",
    "text": "7.5 Fase 4: Valutazione dei Risultati\nIn questa fase, i risultati ottenuti vengono analizzati alla luce della domanda di ricerca iniziale. Si procede a una valutazione sia quantitativa, attraverso l’applicazione di tecniche statistiche appropriate, sia qualitativa, attraverso un’attenta riflessione critica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-5-comunicazione-dei-risultati",
    "href": "chapters/eda/01_project_structure.html#fase-5-comunicazione-dei-risultati",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.6 Fase 5: Comunicazione dei Risultati",
    "text": "7.6 Fase 5: Comunicazione dei Risultati\nL’ultima fase di un progetto di analisi dei dati consiste nel condividere i risultati con un pubblico più ampio, il che richiede la preparazione di materiali comunicativi chiari e concisi. L’obiettivo è trasformare i risultati dell’analisi in informazioni utili per supportare il processo decisionale. Questo può includere la stesura di un articolo scientifico, la creazione di un report per un team di lavoro, o la preparazione di una presentazione con diapositive.\nLa comunicazione deve essere adattata al pubblico di riferimento. Non si deve dare per scontato che il pubblico abbia familiarità con il progetto: è fondamentale spiegare l’analisi e le visualizzazioni in modo chiaro e dettagliato. Anche se per il ricercatore il messaggio principale di una figura o diapositiva può sembrare ovvio, è sempre una buona pratica guidare il pubblico nella sua interpretazione, evitando l’uso di gergo tecnico complesso.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "href": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.7 Organizzazione del Progetto",
    "text": "7.7 Organizzazione del Progetto\nUn requisito fondamentale per un progetto di analisi dei dati è organizzare in modo efficiente i file sul proprio computer. Questo include i file dei dati, il codice e la documentazione del progetto. Tutti questi elementi dovrebbero essere raccolti all’interno di una singola cartella dedicata al progetto.\n\n7.7.1 Home Directory\nIn RStudio, è possibile creare un file chiamato nome_del_progetto.Rproj, che consente di configurare automaticamente la home directory del progetto, ovvero la cartella principale da cui R avvia il lavoro relativo al progetto. Per utilizzare questa funzionalità, è sufficiente aprire RStudio cliccando direttamente sul file nome_del_progetto.Rproj.\nLa home directory rappresenta il punto di riferimento principale per tutte le operazioni del progetto, come il caricamento di file, il salvataggio degli output e la gestione delle risorse.\nGrazie a questa configurazione, è possibile utilizzare percorsi relativi per accedere ai file all’interno del progetto. I percorsi relativi si basano sempre sulla cartella principale del progetto, il che rende il codice più portabile e adattabile. In pratica, chiunque scarichi il tuo progetto sarà in grado di eseguirlo senza dover modificare manualmente i percorsi dei file. Questo approccio migliora la condivisione e garantisce una maggiore riproducibilità del tuo lavoro.\n\n7.7.2 Struttura di un Progetto\nYu & Barter (2024) propone il seguente template per la struttura di un progetto:\n\nLe due cartelle principali sono:\n\n\ndata/: contiene il dataset grezzo (ad esempio, data.csv) e una sottocartella con documentazione relativa ai dati, come metadati e codebook.\n\ndslc_documentation/: raccoglie i file di documentazione e codice necessari per le varie fasi del progetto. Questi possono essere file .qmd (per Quarto, in R) o .ipynb (per Jupyter Notebook, in Python), utilizzati per condurre ed esplorare le analisi. I file sono prefissati da un numero per mantenerli in ordine cronologico. All’interno di questa cartella, è presente una sottocartella functions/, che contiene script .R (per R) o .py (per Python) con funzioni utili per le diverse analisi.\n\nUn file README.md descrive la struttura del progetto e riassume il contenuto di ogni file.\nUn’organizzazione come quella proposta da Yu & Barter (2024) offre un notevole vantaggio: permette di specificare i percorsi dei file in modo relativo, utilizzando come radice la cartella del progetto. Questo rende il progetto facilmente trasferibile e condivisibile tra diversi utenti o computer.\n\nEsempio 7.1 Per illustrare come gestire l’archiviazione dei dati sul computer e importarli in R, consideriamo i dati raccolti da Zetsche et al. (2019) in uno studio che ha esaminato le aspettative negative come meccanismo chiave nel mantenimento della depressione. In questo studio, i ricercatori hanno confrontato 30 soggetti con episodi depressivi a un gruppo di controllo di 37 individui sani, utilizzando il Beck Depression Inventory (BDI-II) per valutare i livelli di depressione.\nUtilizzo dei File e Percorsi Relativi nel Progetto\nIl file CSV contenente i dati, insieme a tutti gli altri file utilizzati in questa dispensa, è memorizzato nella cartella data, situata all’interno della cartella psicometria-r, che rappresenta la directory principale del progetto.\nPer specificare il percorso relativo di un file rispetto alla home directory del progetto, si utilizza la funzione here() fornita dal pacchetto here. Questa funzione genera automaticamente il percorso corretto a partire dalla directory principale del progetto. Per esempio, per verificare quale sia la directory principale del progetto in relazione alla directory personale, si può eseguire:\n\nhere::here()\n#&gt; [1] \"/Users/corradocaudek/_repositories/psicometria-r\"\n\nQuando RStudio viene aperto in modo da definire la directory principale del progetto (ad esempio cliccando sul file .Rproj del progetto), è possibile utilizzare percorsi relativi per accedere ai file. Questo approccio facilita la portabilità del codice e rende il progetto più facilmente condivisibile.\nAd esempio, per importare i dati dal file data.mood.csv situato nella cartella data, è sufficiente scrivere:\n\ndf &lt;- rio::import(\n  here::here(\"data\", \"data.mood.csv\")\n)\n\nSpiegazione della Sintassi di here()\nGli argomenti passati a here() rappresentano le sottocartelle o i file da raggiungere a partire dalla directory principale del progetto. Non è necessario specificare l’intero percorso assoluto, ma solo la sequenza di cartelle “nidificate” a partire dalla home directory.\nNel caso dell’esempio sopra, l’istruzione:\n\nhere::here(\"data\", \"data.mood.csv\")\n#&gt; [1] \"/Users/corradocaudek/_repositories/psicometria-r/data/data.mood.csv\"\n\nspecifica che, rispetto alla cartella del progetto (psicometria-r), si deve accedere alla sottocartella data e lì cercare il file data.mood.csv. Questo elimina la necessità di conoscere o modificare il percorso assoluto, semplificando notevolmente la gestione dei file.\nNota: Per l’utente, l’uso di percorsi relativi significa che il file data.mood.csv sarà sempre trovato correttamente purché si mantenga la struttura delle cartelle del progetto. Questo approccio rende il progetto più robusto e facilmente condivisibile.\n\n7.7.3 Esplorazione dei Dati\nPer conoscere le dimensioni del data frame utilizziamo l’istruzione dim():\n\ndim(df)\n#&gt; [1] 1188   44\n\nIl data frame ha 1188 righe e 44 colonne. Visualizziamo il nome delle colonne con la funzione names().\n\ndf |&gt;\n  names()\n#&gt;  [1] \"V1\"                  \"vpn_nr\"              \"esm_id\"             \n#&gt;  [4] \"group\"               \"bildung\"             \"bdi\"                \n#&gt;  [7] \"nr_of_episodes\"      \"nobs_mood\"           \"trigger_counter\"    \n#&gt; [10] \"form\"                \"traurig_re\"          \"niedergeschlagen_re\"\n#&gt; [13] \"unsicher_re\"         \"nervos_re\"           \"glucklich_re\"       \n#&gt; [16] \"frohlich_re\"         \"mood_sad.5\"          \"mood_fearful.5\"     \n#&gt; [19] \"mood_neg.5\"          \"mood_happy.5\"        \"cesd_sum\"           \n#&gt; [22] \"rrs_sum\"             \"rrs_brood\"           \"rrs_reflect\"        \n#&gt; [25] \"forecast_sad\"        \"forecast_fear\"       \"forecast_neg\"       \n#&gt; [28] \"forecast_happy\"      \"recall_sad\"          \"recall_fear\"        \n#&gt; [31] \"recall_neg\"          \"recall_happy\"        \"diff_neg.fore.5\"    \n#&gt; [34] \"diff_sad.fore.5\"     \"diff_fear.fore.5\"    \"diff_happy.fore.5\"  \n#&gt; [37] \"diff_neg.retro.5\"    \"diff_sad.retro.5\"    \"diff_fear.retro.5\"  \n#&gt; [40] \"diff_happy.retro.5\"  \"mood_sad5_tm1\"       \"mood_neg5_tm1\"      \n#&gt; [43] \"mood_fearful5_tm1\"   \"mood_happy5_tm1\"\n\nDiamo un’occhiata ai dati con la funzione glimpse():\n\ndf |&gt;\n  glimpse()\n#&gt; Rows: 1,188\n#&gt; Columns: 44\n#&gt; $ V1                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n#&gt; $ vpn_nr              &lt;int&gt; 101, 101, 101, 101, 101, 101, 101, 101, 101, 1…\n#&gt; $ esm_id              &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10…\n#&gt; $ group               &lt;chr&gt; \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd…\n#&gt; $ bildung             &lt;chr&gt; \"abitur\", \"abitur\", \"abitur\", \"abitur\", \"abitu…\n#&gt; $ bdi                 &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25…\n#&gt; $ nr_of_episodes      &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#&gt; $ nobs_mood           &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14…\n#&gt; $ trigger_counter     &lt;int&gt; 5, 6, 7, 8, 10, 11, 12, 15, 16, 18, 20, 21, 22…\n#&gt; $ form                &lt;chr&gt; \"Forecasting\", \"Forecasting\", \"Forecasting\", \"…\n#&gt; $ traurig_re          &lt;dbl&gt; 3.67, 3.67, 1.67, 3.00, 3.00, 2.33, 1.67, 2.33…\n#&gt; $ niedergeschlagen_re &lt;dbl&gt; 3.00, 3.67, 1.67, 3.00, 3.00, 2.33, 1.67, 2.33…\n#&gt; $ unsicher_re         &lt;dbl&gt; 2.33, 3.67, 2.33, 3.00, 2.33, 2.33, 1.67, 1.67…\n#&gt; $ nervos_re           &lt;dbl&gt; 3.00, 3.67, 2.33, 3.67, 3.00, 2.33, 1.67, 1.67…\n#&gt; $ glucklich_re        &lt;dbl&gt; 3.00, 2.33, 2.33, 2.33, 3.00, 2.33, 3.67, 3.67…\n#&gt; $ frohlich_re         &lt;dbl&gt; 3.00, 2.33, 3.00, 3.00, 3.00, 3.00, 3.67, 3.67…\n#&gt; $ mood_sad.5          &lt;dbl&gt; 3.33, 3.67, 1.67, 3.00, 3.00, 2.33, 1.67, 2.33…\n#&gt; $ mood_fearful.5      &lt;dbl&gt; 2.67, 3.67, 2.33, 3.33, 2.67, 2.33, 1.67, 1.67…\n#&gt; $ mood_neg.5          &lt;dbl&gt; 3.00, 3.67, 2.00, 3.17, 2.83, 2.33, 1.67, 2.00…\n#&gt; $ mood_happy.5        &lt;dbl&gt; 3.00, 2.33, 2.67, 2.67, 3.00, 2.67, 3.67, 3.67…\n#&gt; $ cesd_sum            &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25…\n#&gt; $ rrs_sum             &lt;int&gt; 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59…\n#&gt; $ rrs_brood           &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14…\n#&gt; $ rrs_reflect         &lt;int&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11…\n#&gt; $ forecast_sad        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4…\n#&gt; $ forecast_fear       &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n#&gt; $ forecast_neg        &lt;dbl&gt; 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2…\n#&gt; $ forecast_happy      &lt;dbl&gt; 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2…\n#&gt; $ recall_sad          &lt;dbl&gt; 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3…\n#&gt; $ recall_fear         &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4…\n#&gt; $ recall_neg          &lt;dbl&gt; 3.25, 3.25, 3.25, 3.25, 3.25, 3.25, 3.25, 3.25…\n#&gt; $ recall_happy        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n#&gt; $ diff_neg.fore.5     &lt;dbl&gt; -0.500, -1.167, 0.500, -0.667, -0.333, 0.167, …\n#&gt; $ diff_sad.fore.5     &lt;dbl&gt; -1.333, -1.667, 0.333, -1.000, -1.000, -0.333,…\n#&gt; $ diff_fear.fore.5    &lt;dbl&gt; 0.333, -0.667, 0.667, -0.333, 0.333, 0.667, 1.…\n#&gt; $ diff_happy.fore.5   &lt;dbl&gt; -1.000, -0.333, -0.667, -0.667, -1.000, -0.667…\n#&gt; $ diff_neg.retro.5    &lt;dbl&gt; 0.2500, -0.4167, 1.2500, 0.0833, 0.4167, 0.916…\n#&gt; $ diff_sad.retro.5    &lt;dbl&gt; 0.167, -0.167, 1.833, 0.500, 0.500, 1.167, 1.8…\n#&gt; $ diff_fear.retro.5   &lt;dbl&gt; 0.333, -0.667, 0.667, -0.333, 0.333, 0.667, 1.…\n#&gt; $ diff_happy.retro.5  &lt;dbl&gt; -1.000, -0.333, -0.667, -0.667, -1.000, -0.667…\n#&gt; $ mood_sad5_tm1       &lt;dbl&gt; NA, 3.33, 3.67, 1.67, 3.00, 3.00, 2.33, 1.67, …\n#&gt; $ mood_neg5_tm1       &lt;dbl&gt; NA, 3.00, 3.67, 2.00, 3.17, 2.83, 2.33, 1.67, …\n#&gt; $ mood_fearful5_tm1   &lt;dbl&gt; NA, 2.67, 3.67, 2.33, 3.33, 2.67, 2.33, 1.67, …\n#&gt; $ mood_happy5_tm1     &lt;dbl&gt; NA, 3.00, 2.33, 2.67, 2.67, 3.00, 2.67, 3.67, …",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "href": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.8 Riflessioni Conclusive",
    "text": "7.8 Riflessioni Conclusive\nLa bellezza del codice risiede nella sua riusabilità: una volta scritto, può essere utilizzato tutte le volte che si desidera. Se configurato correttamente, lo stesso codice applicato agli stessi dati produrrà sempre gli stessi risultati. Questo principio, noto come riproducibilità computazionale, offre numerosi vantaggi.\n\n\nTracciare le modifiche al progetto: La riproducibilità semplifica il monitoraggio delle evoluzioni e dei cambiamenti nel progetto, permettendo di vedere come si sviluppa nel tempo.\n\nRiprodurre il proprio lavoro: L’utente più interessato alla riproducibilità sei tu stesso. La capacità di replicare i risultati è una caratteristica essenziale, poiché in futuro potresti aver bisogno di riprendere in mano il lavoro e comprenderne i dettagli. La riproducibilità rende questo processo molto più semplice.\n\nCostruire su basi solide: Anche altri ricercatori possono utilizzare il tuo lavoro come punto di partenza, espandendo e approfondendo le conoscenze che hai contribuito a sviluppare.\n\nTuttavia, rendere il codice riproducibile è più difficile di quanto sembri. In questo capitolo abbiamo esplorato alcuni metodi che possono aiutare a raggiungere questo obiettivo.\n\n\n\n\n\n\nUno dei problemi più importanti nella psicologia contemporanea è la crisi di replicabilità: molti risultati di ricerca non sono replicabili (Collaboration, 2015). La riproducibilità computazionale si concentra su un obiettivo più ristretto: ottenere gli stessi risultati utilizzando lo stesso codice sugli stessi dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/01_project_structure.html#informazioni-sullambiente-di-sviluppo",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "\n7.9 Informazioni sull’Ambiente di Sviluppo",
    "text": "7.9 Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats4    stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] mokken_3.1.2         poLCA_1.6.0.1        scatterplot3d_0.3-44\n#&gt;  [4] mirt_1.43            lattice_0.22-6       MASS_7.3-61         \n#&gt;  [7] viridis_0.6.5        viridisLite_0.4.2    ggpubr_0.6.0        \n#&gt; [10] ggExtra_0.10.1       gridExtra_2.3        patchwork_1.3.0     \n#&gt; [13] bayesplot_1.11.1     psych_2.4.6.26       scales_1.3.0        \n#&gt; [16] markdown_1.13        knitr_1.49           lubridate_1.9.3     \n#&gt; [19] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4         \n#&gt; [22] purrr_1.0.2          readr_2.1.5          tidyr_1.3.1         \n#&gt; [25] tibble_3.2.1         ggplot2_3.5.1        tidyverse_2.0.0     \n#&gt; [28] rio_1.2.3            here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1         pbapply_1.7-2        testthat_3.2.1.1    \n#&gt;  [4] permute_0.9-7        airports_0.1.0       rlang_1.1.4         \n#&gt;  [7] magrittr_2.0.3       compiler_4.4.2       mgcv_1.9-1          \n#&gt; [10] vctrs_0.6.5          pkgconfig_2.0.3      fastmap_1.2.0       \n#&gt; [13] backports_1.5.0      utf8_1.2.4           promises_1.3.1      \n#&gt; [16] rmarkdown_2.29       sessioninfo_1.2.2    tzdb_0.4.0          \n#&gt; [19] openintro_2.5.0      xfun_0.49            jsonlite_1.8.9      \n#&gt; [22] later_1.4.0          styler_1.10.3        Deriv_4.1.6         \n#&gt; [25] broom_1.0.7          parallel_4.4.2       cluster_2.1.6       \n#&gt; [28] R6_2.5.1             stringi_1.8.4        parallelly_1.39.0   \n#&gt; [31] car_3.1-3            brio_1.1.5           Rcpp_1.0.13-1       \n#&gt; [34] future.apply_1.11.3  snow_0.4-4           audio_0.1-11        \n#&gt; [37] cherryblossom_0.1.0  pacman_0.5.1         R.utils_2.12.3      \n#&gt; [40] R.cache_0.16.0       httpuv_1.6.15        Matrix_1.7-1        \n#&gt; [43] splines_4.4.2        timechange_0.3.0     tidyselect_1.2.1    \n#&gt; [46] abind_1.4-8          yaml_2.3.10          vegan_2.6-8         \n#&gt; [49] codetools_0.2-20     miniUI_0.1.1.1       dcurver_0.9.2       \n#&gt; [52] curl_6.0.1           listenv_0.9.1        shiny_1.9.1         \n#&gt; [55] withr_3.0.2          evaluate_1.0.1       future_1.34.0       \n#&gt; [58] pillar_1.9.0         carData_3.0-5        generics_0.1.3      \n#&gt; [61] rprojroot_2.0.4      hms_1.1.3            munsell_0.5.1       \n#&gt; [64] globals_0.16.3       xtable_1.8-4         glue_1.8.0          \n#&gt; [67] RPushbullet_0.3.4    tools_4.4.2          data.table_1.16.2   \n#&gt; [70] beepr_2.0            SimDesign_2.17.1     ggsignif_0.6.4      \n#&gt; [73] grid_4.4.2           colorspace_2.1-1     nlme_3.1-166        \n#&gt; [76] Formula_1.2-5        usdata_0.3.1         cli_3.6.3           \n#&gt; [79] fansi_1.0.6          gtable_0.3.6         R.methodsS3_1.8.2   \n#&gt; [82] rstatix_0.7.2        digest_0.6.37        progressr_0.15.1    \n#&gt; [85] GPArotation_2024.3-1 htmlwidgets_1.6.4    farver_2.1.2        \n#&gt; [88] htmltools_0.5.8.1    R.oo_1.27.0          lifecycle_1.0.4     \n#&gt; [91] mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#bibliografia",
    "href": "chapters/eda/01_project_structure.html#bibliografia",
    "title": "7  Le fasi del progetto di analisi dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html",
    "href": "chapters/eda/02_data_cleaning.html",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "8.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nNonostante la fase più interessante di un progetto di analisi dei dati sia quella in cui si riesce a rispondere alla domanda che ha dato avvio all’indagine, gran parte del tempo di un analista è in realtà dedicata a una fase preliminare: la pulizia e il preprocessing dei dati, operazioni che vengono svolte ancor prima dell’analisi esplorativa.\nIn questo capitolo, esamineremo un caso concreto di data cleaning e preprocessing, seguendo il tutorial di Crystal Lewis. Il problema viene presentato come segue:\nCrystal Lewis elenca i seguenti passaggi da seguire nel processo di data cleaning:\nSebbene l’ordine di questi passaggi sia flessibile e possa essere adattato alle esigenze specifiche, c’è un passaggio che non dovrebbe mai essere saltato: il primo, ovvero la revisione dei dati. Senza una revisione preliminare, l’analista rischia di sprecare ore a pulire i dati per poi scoprire che mancano dei partecipanti, che i dati non sono organizzati come previsto o, peggio ancora, che sta lavorando con i dati sbagliati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#introduzione",
    "href": "chapters/eda/02_data_cleaning.html#introduzione",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "I am managing data for a longitudinal randomized controlled trial (RCT) study. For this RCT, schools are randomized to either a treatment or control group. Students who are in a treatment school receive a program to boost their math self-efficacy. Data is collected on all students in two waves (wave 1 is in the fall of a school year, and wave 2 is collected in the spring). At this point in time, we have collected wave 1 of our student survey on a paper form and we set up a data entry database for staff to enter the information into. Data has been double-entered, checked for entry errors, and has been exported in a csv format (“w1_mathproj_stu_svy_raw.csv”) to a folder (called “data”) where it is waiting to be cleaned.\n\n\n\nRevisione dei dati.\nRegolazione del numero di casi.\nDe-identificazione dei dati.\nEliminazione delle colonne irrilevanti.\nDivisione delle colonne, se necessario.\nRidenominazione delle variabili.\nTrasformazione/normalizzazione delle variabili.\nStandardizzazione delle variabili.\nAggiornamento dei tipi di variabili, se necessario.\nRicodifica delle variabili.\nCreazione di eventuali variabili necessarie.\nGestione dei valori mancanti, se necessario.\nAggiunta di metadati, se necessario.\nValidazione dei dati.\nFusione e/o unione dei dati, se necessario.\nTrasformazione dei dati, se necessario.\nSalvataggio dei dati puliti.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#importare-i-dati",
    "href": "chapters/eda/02_data_cleaning.html#importare-i-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.2 Importare i Dati",
    "text": "7.2 Importare i Dati\nI dati grezzi non dovrebbero mai essere modificati direttamente. È consigliabile organizzare i dati in una struttura di cartelle all’interno di una directory chiamata data, che contiene due sottocartelle: raw e processed. I dati originali, non ancora elaborati, devono essere conservati nella cartella raw e mantenuti inalterati. I dati ripuliti e preprocessati, invece, devono essere salvati nella cartella processed.\nPer fare un esempio, importiamo i dati dal file w1_mathproj_stu_svy_raw.csv e iniziamo il processo di pulizia. È importante notare che tutte le istruzioni sono formulate in modo relativo alla home directory del progetto. Prima di tutto, definiamo il percorso della home directory del progetto.\n\nEsaminare i Dati\n\nProcediamo con l’importazione dei dati.\n\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-0…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt;\n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nModifica i casi secondo necessità\n\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  dim()\n#&gt; [1] 5 7\n\n\nDe-identificazione dei Dati\n\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"\n\n\nRimuovere le Colonne non Necessarie\n\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\nDividere le Colonne Secondo Necessità\n\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\nRinominare le Colonne\n\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2\n\n\n\n\n\nTrasformare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n\nStandardizzare / Normalizzare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.\n\nAggiornare i Tipi delle Variabili\n\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.\n\nRicodificare le Variabili\n\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)\n\n\nAggiungere Nuove Variabili nel Data Frame\n\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3\n\n\nAffrontare il Problema dei Dati Mancanti\n\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale\noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.\n\nAggiungere i Metadati\n\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4\n\n\nValidazione dei Dati\n\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(\n    columns = c(stu_id),\n    left = 1300, right = 1400, na_pass = TRUE\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(grade_level),\n    set = c(9, 10, 11, 12, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(int),\n    set = c(0, 1, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(math1:math4),\n    set = c(1, 2, 3, 4, NA)\n  ) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-27|21:46:50]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-27 21:46:50 CET &lt; 1 s 2024-11-27 21:46:50 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.\n\nUnire e/o aggiungere dati se necessario\n\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.\n\nTrasformare i dati se necessario\n\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#esaminare-i-dati",
    "href": "chapters/eda/02_data_cleaning.html#esaminare-i-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.3 (1) Esaminare i Dati",
    "text": "7.3 (1) Esaminare i Dati\nProcediamo con l’importazione dei dati.\n\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-0…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt;\n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#modifica-i-casi-secondo-necessità",
    "href": "chapters/eda/02_data_cleaning.html#modifica-i-casi-secondo-necessità",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.4 (2) Modifica i casi secondo necessità",
    "text": "7.4 (2) Modifica i casi secondo necessità\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  dim()\n#&gt; [1] 5 7",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#de-identificazione-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#de-identificazione-dei-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.5 (3) De-identificazione dei Dati",
    "text": "7.5 (3) De-identificazione dei Dati\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#rimuovere-le-colonne-non-necessarie",
    "href": "chapters/eda/02_data_cleaning.html#rimuovere-le-colonne-non-necessarie",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.6 (4) Rimuovere le Colonne non Necessarie",
    "text": "7.6 (4) Rimuovere le Colonne non Necessarie\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#dividere-le-colonne-secondo-necessità",
    "href": "chapters/eda/02_data_cleaning.html#dividere-le-colonne-secondo-necessità",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.7 (5) Dividere le Colonne Secondo Necessità",
    "text": "7.7 (5) Dividere le Colonne Secondo Necessità\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#rinominare-le-colonne",
    "href": "chapters/eda/02_data_cleaning.html#rinominare-le-colonne",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.8 (6) Rinominare le Colonne",
    "text": "7.8 (6) Rinominare le Colonne\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#trasformare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#trasformare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.9 (7) Trasformare le Variabili",
    "text": "7.9 (7) Trasformare le Variabili\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#standardizzare-normalizzare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#standardizzare-normalizzare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.10 (8) Standardizzare / Normalizzare le Variabili",
    "text": "7.10 (8) Standardizzare / Normalizzare le Variabili\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiornare-i-tipi-delle-variabili",
    "href": "chapters/eda/02_data_cleaning.html#aggiornare-i-tipi-delle-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.11 (9) Aggiornare i Tipi delle Variabili",
    "text": "7.11 (9) Aggiornare i Tipi delle Variabili\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#ricodificare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#ricodificare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.12 (10) Ricodificare le Variabili",
    "text": "7.12 (10) Ricodificare le Variabili\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiungere-nuove-variabili-nel-data-frame",
    "href": "chapters/eda/02_data_cleaning.html#aggiungere-nuove-variabili-nel-data-frame",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.13 (11) Aggiungere Nuove Variabili nel Data Frame",
    "text": "7.13 (11) Aggiungere Nuove Variabili nel Data Frame\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#affrontare-il-problema-dei-dati-mancanti",
    "href": "chapters/eda/02_data_cleaning.html#affrontare-il-problema-dei-dati-mancanti",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.14 (12) Affrontare il Problema dei Dati Mancanti",
    "text": "7.14 (12) Affrontare il Problema dei Dati Mancanti\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale\noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiungere-i-metadati",
    "href": "chapters/eda/02_data_cleaning.html#aggiungere-i-metadati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.15 (13) Aggiungere i Metadati",
    "text": "7.15 (13) Aggiungere i Metadati\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#validazione-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#validazione-dei-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.16 (14) Validazione dei Dati",
    "text": "7.16 (14) Validazione dei Dati\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(\n    columns = c(stu_id),\n    left = 1300, right = 1400, na_pass = TRUE\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(grade_level),\n    set = c(9, 10, 11, 12, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(int),\n    set = c(0, 1, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(math1:math4),\n    set = c(1, 2, 3, 4, NA)\n  ) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-27|21:41:32]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-27 21:41:32 CET &lt; 1 s 2024-11-27 21:41:32 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#unire-eo-aggiungere-dati-se-necessario",
    "href": "chapters/eda/02_data_cleaning.html#unire-eo-aggiungere-dati-se-necessario",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.17 (15) Unire e/o aggiungere dati se necessario",
    "text": "7.17 (15) Unire e/o aggiungere dati se necessario\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#trasformare-i-dati-se-necessario",
    "href": "chapters/eda/02_data_cleaning.html#trasformare-i-dati-se-necessario",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.18 (16) Trasformare i dati se necessario",
    "text": "7.18 (16) Trasformare i dati se necessario\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#salvare-il-dataset-pulito-finale",
    "href": "chapters/eda/02_data_cleaning.html#salvare-il-dataset-pulito-finale",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.3 (17) Salvare il dataset pulito finale",
    "text": "7.3 (17) Salvare il dataset pulito finale\nL’ultimo passaggio del processo di pulizia consiste nell’esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file .csv e un file .parquet).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "href": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "\n8.3 Organizzazione dei file e informazioni aggiuntive",
    "text": "8.3 Organizzazione dei file e informazioni aggiuntive\nInfine, è essenziale includere una documentazione adeguata per garantire che le informazioni siano interpretate correttamente, sia da altri utenti che da te stesso, se dovessi tornare a lavorare su questo progetto in futuro. La documentazione minima da fornire dovrebbe includere:\n\n\nDocumentazione a livello di progetto: Questa sezione fornisce informazioni contestuali sul perché e come i dati sono stati raccolti. È utile per chiunque voglia comprendere lo scopo e la metodologia del progetto.\n\nMetadati a livello di progetto: Se condividi i dati in un repository pubblico o privato, è importante includere metadati a livello di progetto. Questi metadati forniscono informazioni dettagliate che facilitano la ricerca, la comprensione e la consultabilità dei dati. I metadati a livello di progetto possono includere descrizioni generali del progetto, parole chiave, e riferimenti bibliografici.\n\nDizionario dei dati: Un documento che descrive tutte le variabili presenti nel dataset, inclusi i loro nomi, tipi, range di valori, categorie e qualsiasi altra informazione rilevante. Questo strumento è fondamentale per chiunque voglia comprendere o analizzare i dati.\n\nREADME: Un file che fornisce una panoramica rapida dei file inclusi nel progetto, spiegando cosa contengono e come sono interconnessi. Il README è spesso il primo documento consultato e serve a orientare l’utente tra i vari file e risorse del progetto. Questa documentazione non solo aiuta a mantenere il progetto organizzato, ma è anche cruciale per facilitare la collaborazione e l’archiviazione a lungo termine.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "\n8.4 Dizionario dei Dati",
    "text": "8.4 Dizionario dei Dati\nApprofondiamo qui il problema della creazione del Dizionario dei dati.\nUn dizionario dei dati è un documento che descrive le caratteristiche di ciascuna variabile in un dataset. Include informazioni come il nome della variabile, il tipo di dato, il range di valori, le categorie (per le variabili categoriche), e altre informazioni rilevanti. Questo strumento è essenziale per comprendere e analizzare correttamente il dataset.\nSi presti particolare attenzione alle guide di stile per la denominazione delle variabili e la codifica dei valori delle risposte.\nEcco come tradurre i passi per creare un dizionario dei dati in R, utilizzando il pacchetto tibble per creare il dizionario e writexl o readr per esportarlo in formato .xlsx o .csv.\n\n8.4.1 Passi per Creare un Dizionario dei Dati\n\n\nIdentificare le variabili: Elencare tutte le variabili presenti nel dataset.\n\nDescrivere ogni variabile: Per ciascuna variabile, definire il tipo (ad esempio, integer, numeric, character), il range di valori accettabili o le categorie, e fornire una descrizione chiara.\n\nSalvare il dizionario dei dati: Il dizionario può essere salvato in un file .csv o .xlsx per una facile consultazione.\n\n8.4.2 Esempio in R\n\nCreeremo un dizionario dei dati per un dataset di esempio e lo salveremo sia in formato CSV che Excel.\n\nlibrary(tibble)\nlibrary(readr)\nlibrary(writexl)\n\n# Creazione del Dizionario dei Dati\ndata_dict &lt;- tibble(\n  `Variable Name` = c(\n    \"stu_id\",\n    \"svy_date\",\n    \"grade_level\",\n    \"math1\",\n    \"math2\",\n    \"math3\",\n    \"math4\"\n  ),\n  `Type` = c(\n    \"integer\",\n    \"datetime\",\n    \"integer\",\n    \"integer\",\n    \"integer\",\n    \"numeric\",\n    \"numeric\"\n  ),\n  `Description` = c(\n    \"Student ID\",\n    \"Survey Date\",\n    \"Grade Level\",\n    \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n  ),\n  `Range/Values` = c(\n    \"1347-1399\",\n    \"2023-02-13 to 2023-02-14\",\n    \"9-12\",\n    \"1-4\",\n    \"1-4\",\n    \"1.0-4.0 (NA allowed)\",\n    \"1.0-4.0 (NA allowed)\"\n  )\n)\n\n# Visualizza il Dizionario dei Dati\nprint(data_dict)\n\n# Salva il Dizionario dei Dati in un file CSV\nwrite_csv(data_dict, \"data_dictionary.csv\")\n\n# Salva il Dizionario dei Dati in un file Excel\nwrite_xlsx(data_dict, \"data_dictionary.xlsx\")\n\nOutput Atteso: file CSV (data_dictionary.csv).\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\nRange/Values\n\n\n\nstu_id\ninteger\nStudent ID\n1347-1399\n\n\nsvy_date\ndatetime\nSurvey Date\n2023-02-13 to 2023-02-14\n\n\ngrade_level\ninteger\nGrade Level\n9-12\n\n\nmath1\ninteger\nMath Response 1 (1: Strongly Disagree, 4: Strongly Agree)\n1-4\n\n\nmath2\ninteger\nMath Response 2 (1: Strongly Disagree, 4: Strongly Agree)\n1-4\n\n\nmath3\nnumeric\nMath Response 3 (1: Strongly Disagree, 4: Strongly Agree)\n1.0-4.0 (NA allowed)\n\n\nmath4\nnumeric\nMath Response 4 (1: Strongly Disagree, 4: Strongly Agree)\n1.0-4.0 (NA allowed)\n\n\n\n\n\nDocumentazione: Il dizionario dei dati offre una descrizione chiara e standardizzata, utile per analisi successive e per la condivisione del dataset.\n\nSalvataggio multiplo: I formati .csv e .xlsx garantiscono la massima compatibilità con altri software e sistemi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#guide-di-stile",
    "href": "chapters/eda/02_data_cleaning.html#guide-di-stile",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "\n8.5 Guide di Stile",
    "text": "8.5 Guide di Stile\nLe guide di stile possono essere applicate a diversi aspetti di un progetto di analisi dei dati, non soltanto al dizionario dei dati. Un’ottima introduzione alle regole di stile per un progetto di analisi dei dati è fornita in questo capitolo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "href": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "\n8.6 Riflessioni Conclusive",
    "text": "8.6 Riflessioni Conclusive\nNel processo di analisi dei dati, la fase di pulizia e pre-elaborazione è cruciale per garantire la qualità e l’integrità dei risultati finali. Sebbene questa fase possa sembrare meno interessante rispetto all’analisi vera e propria, essa costituisce la base su cui si costruiscono tutte le successive elaborazioni e interpretazioni. Attraverso una serie di passaggi strutturati, come quelli illustrati in questo capitolo, è possibile trasformare dati grezzi e disordinati in un dataset pulito, coerente e pronto per l’analisi. La cura nella gestione dei dati, dalla rimozione di duplicati alla creazione di un dizionario dei dati, è fondamentale per ottenere risultati affidabili e riproducibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/02_data_cleaning.html#informazioni-sullambiente-di-sviluppo",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pointblank_0.12.2 haven_2.5.4       labelled_2.13.0   mice_3.16.0      \n#&gt;  [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [25] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1      rlang_1.1.4       magrittr_2.0.3    compiler_4.4.2   \n#&gt;  [5] vctrs_0.6.5       pkgconfig_2.0.3   shape_1.4.6.1     fastmap_1.2.0    \n#&gt;  [9] backports_1.5.0   utf8_1.2.4        promises_1.3.1    blastula_0.3.5   \n#&gt; [13] rmarkdown_2.29    tzdb_0.4.0        nloptr_2.1.1      xfun_0.49        \n#&gt; [17] glmnet_4.1-8      jomo_2.7-6        jsonlite_1.8.9    later_1.4.0      \n#&gt; [21] pan_1.9           broom_1.0.7       parallel_4.4.2    R6_2.5.1         \n#&gt; [25] stringi_1.8.4     car_3.1-3         boot_1.3-31       rpart_4.1.23     \n#&gt; [29] Rcpp_1.0.13-1     iterators_1.0.14  base64enc_0.1-3   pacman_0.5.1     \n#&gt; [33] R.utils_2.12.3    httpuv_1.6.15     Matrix_1.7-1      splines_4.4.2    \n#&gt; [37] nnet_7.3-19       timechange_0.3.0  tidyselect_1.2.1  abind_1.4-8      \n#&gt; [41] yaml_2.3.10       codetools_0.2-20  miniUI_0.1.1.1    lattice_0.22-6   \n#&gt; [45] shiny_1.9.1       withr_3.0.2       evaluate_1.0.1    survival_3.7-0   \n#&gt; [49] xml2_1.3.6        pillar_1.9.0      carData_3.0-5     foreach_1.5.2    \n#&gt; [53] generics_0.1.3    rprojroot_2.0.4   hms_1.1.3         commonmark_1.9.2 \n#&gt; [57] munsell_0.5.1     minqa_1.2.8       xtable_1.8-4      glue_1.8.0       \n#&gt; [61] tools_4.4.2       data.table_1.16.2 lme4_1.1-35.5     ggsignif_0.6.4   \n#&gt; [65] grid_4.4.2        colorspace_2.1-1  nlme_3.1-166      Formula_1.2-5    \n#&gt; [69] cli_3.6.3         fansi_1.0.6       gt_0.11.1         gtable_0.3.6     \n#&gt; [73] R.methodsS3_1.8.2 rstatix_0.7.2     sass_0.4.9.9000   digest_0.6.37    \n#&gt; [77] htmlwidgets_1.6.4 farver_2.1.2      R.oo_1.27.0       htmltools_0.5.8.1\n#&gt; [81] lifecycle_1.0.4   mitml_0.4-5       mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#bibliografia",
    "href": "chapters/eda/02_data_cleaning.html#bibliografia",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBuchanan, E. M., Crain, S. E., Cunningham, A. L., Johnson, H. R., Stash, H., Papadatou-Pastou, M., Isager, P. M., Carlsson, R., & Aczel, B. (2021). Getting started creating data dictionaries: How to create a shareable data set. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920928007.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#tutorial",
    "href": "chapters/eda/02_data_cleaning.html#tutorial",
    "title": "8  Flusso di lavoro per la pulizia dei dati",
    "section": "\n8.2 Tutorial",
    "text": "8.2 Tutorial\nEsamineremo questi passaggi seguendo il tutorial di Crystal Lewis.\nImportare i Dati\nI dati grezzi non dovrebbero mai essere modificati direttamente. È consigliabile organizzare i dati in una struttura di cartelle all’interno di una directory chiamata data, che contiene due sottocartelle: raw e processed. I dati originali, non ancora elaborati, devono essere conservati nella cartella raw e mantenuti inalterati. I dati ripuliti e preprocessati, invece, devono essere salvati nella cartella processed.\nPer fare un esempio, importiamo i dati dal file w1_mathproj_stu_svy_raw.csv e iniziamo il processo di pulizia. È importante notare che tutte le istruzioni sono formulate in modo relativo alla home directory del progetto. Prima di tutto, definiamo il percorso della home directory del progetto.\n\nEsaminare i Dati\n\nProcediamo con l’importazione dei dati.\n\nsvy = rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-02…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt; \n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt; \n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nModificare i casi secondo necessità\n\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt; \n  dim()\n#&gt; [1] 5 7\n\n\nDe-identificazione dei Dati\n\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"\n\n\nRimuovere le Colonne non Necessarie\n\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\nDividere le Colonne Secondo Necessità\n\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\nRinominare le Colonne\n\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2\n\n\n\n\n\nTrasformare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n\nStandardizzare / Normalizzare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.\n\nAggiornare i Tipi delle Variabili\n\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.\n\nRicodificare le Variabili\n\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)\n\n\nAggiungere Nuove Variabili nel Data Frame\n\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3\n\n\nAffrontare il Problema dei Dati Mancanti\n\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale \noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n#&gt; Warning: Number of logged events: 2\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.\n\nAggiungere i Metadati\n\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4\n\n\nValidazione dei Dati\n\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(columns = c(stu_id), \n                   left = 1300, right = 1400, na_pass = TRUE) %&gt;%\n  col_vals_in_set(columns = c(grade_level), \n                  set = c(9, 10, 11, 12, NA)) %&gt;%\n  col_vals_in_set(columns = c(int),\n                  set = c(0, 1, NA)) %&gt;%\n  col_vals_in_set(columns = c(math1:math4),\n                  set = c(1, 2, 3, 4, NA)) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-30|08:12:45]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-30 08:12:45 CET &lt; 1 s 2024-11-30 08:12:45 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.\n\nUnire e/o aggiungere dati se necessario\n\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.\n\nTrasformare i dati se necessario\n\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.\n\nSalvare il dataset pulito finale\n\nL’ultimo passaggio del processo di pulizia consiste nell’esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file .csv e un file .parquet).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "prefazione.html#bibliografia",
    "href": "prefazione.html#bibliografia",
    "title": "Prefazione",
    "section": "",
    "text": "Funder, D. C., Levine, J. M., Mackie, D. M., Morf, C. C., Sansone, C., Vazire, S., & West, S. G. (2014). Improving the dependability of research in personality and social psychology: Recommendations for research and educational practice. Personality and Social Psychology Review, 18(1), 3–12.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman; Hall/CRC.\n\n\nIoannidis, J. P. (2019). What have we (not) learnt from millions of scientific papers with P values? The American Statistician, 73(sup1), 20–25.\n\n\nKorbmacher, M., Azevedo, F., Pennington, C. R., Hartmann, H., Pownall, M., Schmidt, K., Elsherif, M., Breznau, N., Robertson, O., Kalandadze, T., et al. (2023). The replication crisis has led to positive structural, procedural, and community changes. Communications Psychology, 1(1), 3.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26, 1596–1618.\n\n\nShrout, P. E., & Rodgers, J. L. (2018). Psychology, science, and knowledge construction: Broadening perspectives from the replication crisis. Annual Review of Psychology, 69(1), 487–510.\n\n\nTackett, J. L., Brandes, C. M., King, K. M., & Markon, K. E. (2019). Psychology’s replication crisis and clinical psychological science. Annual Review of Clinical Psychology, 15(1), 579–604.\n\n\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmı́ra, M., Epskamp, S., et al. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25, 35–57.\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "chapters/mcmc/introduction_mcmc.html",
    "href": "chapters/mcmc/introduction_mcmc.html",
    "title": "Introduzione",
    "section": "",
    "text": "In questa sezione della dispensa, ci concentreremo sulle procedure Monte Carlo a catena di Markov, con particolare attenzione all’algoritmo di Metropolis, che consente di approssimare la distribuzione a posteriori quando non è possibile ottenere una soluzione analitica. Esploreremo il concetto di predizione bayesiana, fondamentale per la costruzione della distribuzione predittiva a posteriori, e discuteremo anche la distribuzione predittiva a priori. Applicheremo l’inferenza bayesiana a diversi contesti, tra cui la stima di una proporzione, il confronto tra due proporzioni, la stima di una media da una distribuzione normale e il confronto tra due medie. Esamineremo inoltre il modello bayesiano di Poisson per l’analisi delle frequenze. Infine, introdurremo il modello gerarchico bayesiano, uno strumento potente per affrontare situazioni in cui le osservazioni sono strutturate su diversi livelli di incertezza.",
    "crumbs": [
      "MCMC",
      "Introduzione"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html",
    "href": "chapters/mcmc/08_stan_odds_ratio.html",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nIn questo capitolo, esploreremo l’applicazione degli strumenti statistici descritti nei capitoli precedenti all’analisi bayesiana di due proporzioni. Inizieremo definendo i concetti di odds, odds ratio e logit. Successivamente, mostreremo come effettuare l’analisi bayesiana per il confronto tra due proporzioni.\nUn rapporto di odds (OR) è una misura di associazione tra un’esposizione (o un certo gruppo o una certa conditione) e un risultato. L’OR rappresenta gli odds che si verifichi un risultato dato un’esposizione particolare, confrontate con gli odds del risultato che si verifica in assenza di tale esposizione.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#odds",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#odds",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.1 Odds",
    "text": "13.1 Odds\nIl termine “odds” rappresenta il rapporto tra la probabilità che un evento si verifichi e la probabilità che l’evento opposto si verifichi. Matematicamente, l’odds può essere calcolato come:\n\\[\n\\text{odds} = \\frac{\\pi}{1-\\pi},\n\\]\ndove \\(\\pi\\) rappresenta la probabilità dell’evento di interesse.\nMentre una probabilità \\(\\pi\\) è sempre compresa tra 0 e 1, gli odds possono variare da 0 a infinito. Per comprendere meglio gli odds lungo questo spettro, consideriamo tre diversi scenari in cui \\(\\pi\\) rappresenta la probabilità di un evento.\nSe la probabilità di un evento è \\(\\pi = \\frac{2}{3}\\), allora la probabilità che l’evento non si verifichi è \\(1 - \\pi = \\frac{1}{3}\\) e gli odds del verificarsi dell’evento sono:\n\\[\n\\text{odds} = \\frac{2/3}{1-2/3} = 2.\n\\]\nQuesto significa che la probabilità che l’evento si verifichi è il doppio della probabilità che non si verifichi.\nSe, invece, la probabilità dell’evento è \\(\\pi = \\frac{1}{3}\\), allora gli odds che l’evento si verifichi sono la metà rispetto agli odds che non si verifichi:\n\\[\n\\text{odds} = \\frac{1/3}{1-1/3} = \\frac{1}{2}.\n\\]\nInfine, se la probabilità dell’evento è \\(\\pi = \\frac{1}{2}\\), allora gli odds dell’evento sono pari a 1:\n\\[\n\\text{odds} = \\frac{1/2}{1-1/2} = 1.\n\\]\n\n13.1.1 Interpretazione\nGli odds possono essere interpretati nel modo seguente. Consideriamo un evento di interesse con probabilità \\(\\pi \\in [0, 1]\\) e gli odds corrispondenti \\(\\frac{\\pi}{1-\\pi} \\in [0, \\infty)\\). Confrontando gli odds con il valore 1, possiamo ottenere una prospettiva sull’incertezza dell’evento:\n\nGli odds di un evento sono inferiori a 1 se e solo se le probabilità dell’evento sono inferiori al 50-50, cioè \\(\\pi &lt; 0.5\\).\nGli odds di un evento sono uguali a 1 se e solo se le probabilità dell’evento sono del 50-50, cioè \\(\\pi = 0.5\\).\nGli odds di un evento sono superiori a 1 se e solo se le probabilità dell’evento sono superiori al 50-50, cioè \\(\\pi &gt; 0.5\\).\n\nUno dei motivi per preferire l’uso dell’odds rispetto alla probabilità, nonostante quest’ultima sia un concetto più intuitivo, risiede nel fatto che quando le probabilità si avvicinano ai valori estremi (cioè 0 o 1), è più facile rilevare e apprezzare le differenze tra gli odds piuttosto che le differenze tra le probabilità.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#odds-ratio",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#odds-ratio",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.2 Odds Ratio",
    "text": "13.2 Odds Ratio\nQuando abbiamo una variabile di interesse espressa come proporzione, possiamo confrontare i gruppi utilizzando l’odds ratio. L’odds ratio rappresenta il rapporto tra gli odds di un evento in un gruppo e gli odds dello stesso evento in un secondo gruppo:\n\\[\nOR = \\frac{odds_1}{odds_2} = \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}.\n\\]\nInterpretazione:\n\nOR = 1: l’appartenenza al gruppo non influenza il risultato;\nOR &gt; 1: l’appartenenza al gruppo specificato al numeratore dell’OR aumenta la probabilità dell’evento rispetto al gruppo specificato al denominatore;\nOR &lt; 1: l’appartenenza al gruppo specificato al numeratore dell’OR riduce la probabilità dell’evento rispetto al gruppo specificato al denominatore.\n\nL’odds ratio è particolarmente utile quando vogliamo confrontare due gruppi e vedere come l’appartenenza a uno di essi influenza la probabilità di un certo evento. Ad esempio, consideriamo uno studio psicologico in cui stiamo valutando l’efficacia di una terapia comportamentale per ridurre l’ansia. Possiamo suddividere i partecipanti allo studio in due gruppi: quelli che sono stati sottoposti al trattamento (gruppo di trattamento) e quelli che non sono stati sottoposti al trattamento (gruppo di controllo).\nCalcolando l’odds ratio tra il gruppo di trattamento e il gruppo di controllo, possiamo capire se la terapia ha aumentato o ridotto la probabilità di riduzione dell’ansia. Se l’odds ratio è maggiore di 1, significa che la terapia ha aumentato le probabilità di riduzione dell’ansia; se è inferiore a 1, significa che il trattamento ha ridotto le probabilità di riduzione dell’ansia. L’odds ratio ci fornisce quindi una misura dell’effetto della terapia rispetto al controllo.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#logaritmo-dellodds-ratio",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#logaritmo-dellodds-ratio",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.3 Logaritmo dell’Odds Ratio",
    "text": "13.3 Logaritmo dell’Odds Ratio\nIl logaritmo dell’odds ratio è una trasformazione matematica molto utilizzata nell’analisi statistica, specialmente nella regressione logistica. Essa permette di rendere l’odds ratio interpretabile su una scala lineare, semplificando l’analisi e l’interpretazione dei risultati.\nLa formula per calcolare il logaritmo dell’odds ratio è la seguente:\n\\[\n\\text{logit}(OR) = \\log(OR) = \\log\\left(\\frac{odds_1}{odds_2}\\right).\n\\]\nIn altre parole, il logaritmo dell’odds ratio è il logaritmo naturale del rapporto tra gli odds di un evento nel primo gruppo e gli odds dello stesso evento nel secondo gruppo.\n\n13.3.1 Interpretazione\nL’interpretazione del logaritmo dell’odds ratio è più intuitiva rispetto all’odds ratio stesso. Una variazione di una unità nel logaritmo dell’odds ratio corrisponde a un cambiamento costante nell’odds ratio stesso.\nSe il logaritmo dell’odds ratio è positivo, significa che l’odds dell’evento nel primo gruppo è maggiore rispetto al secondo gruppo. Più il valore del logaritmo dell’odds ratio si avvicina a zero, più l’odds dell’evento nei due gruppi si avvicina a essere simile.\nSe, invece, il logaritmo dell’odds ratio è negativo, l’odds dell’evento nel primo gruppo è inferiore rispetto al secondo gruppo. Un valore di logaritmo dell’odds ratio vicino a zero indica che l’odds dell’evento è simile nei due gruppi.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#analisi-bayesiana-delle-proporzioni",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#analisi-bayesiana-delle-proporzioni",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.4 Analisi bayesiana delle proporzioni",
    "text": "13.4 Analisi bayesiana delle proporzioni\nUna volta compresi i concetti di odds, odds ratio e logit, possiamo procedere all’analisi bayesiana delle proporzioni. Questo approccio consente di confrontare le proporzioni di due gruppi, ottenendo stime delle probabilità a posteriori e degli intervalli di credibilità.\nL’analisi bayesiana si basa sull’applicazione del teorema di Bayes per aggiornare le conoscenze a priori con l’evidenza fornita dai dati osservati. Questo permette di ottenere una distribuzione a posteriori delle quantità di interesse, come l’odds ratio.\nIn questo capitolo, analizzeremo un set di dati fittizio ispirato a un classico esperimento di etologia descritto da Hoffmann et al. (2022). Von Frisch (1914) ha voluto verificare se le api possiedono la visione dei colori confrontando il comportamento di due gruppi di api. L’esperimento si compone di una fase di addestramento e di una fase di test.\nNella fase di addestramento, le api del gruppo sperimentale vengono esposte a un disco blu e a un disco verde. Solo il disco blu è ricoperto di una soluzione zuccherina, molto appetita dalle api. Il gruppo di controllo, invece, non riceve alcun addestramento.\nNella fase di test, la soluzione zuccherina viene rimossa dal disco blu e si osserva il comportamento di entrambi i gruppi. Se le api del gruppo sperimentale hanno appreso che solo il disco blu contiene la soluzione zuccherina e sono in grado di distinguere tra il blu e il verde, dovrebbero preferire esplorare il disco blu piuttosto che quello verde durante la fase di test.\nIl ricercatore osserva che in 130 casi su 200, le api del gruppo sperimentale continuano ad avvicinarsi al disco blu dopo la rimozione della soluzione zuccherina. Le api del gruppo di controllo, che non sono state addestrate, si avvicinano al disco blu 100 volte su 200.\nPer confrontare il comportamento delle api nelle due condizioni, useremo l’odds ratio, così da confrontare le probabilità dell’evento critico (scelta del disco blu) tra i due gruppi.\nCalcoliamo la proporzione delle api che scelgono il disco blu nella condizione sperimentale:\n\\[\np_e = \\frac{130}{200} = 0.65\n\\]\nCalcoliamo gli odds nella condizione sperimentale:\n\\[\n\\text{odds}_e = \\frac{p_e}{1 - p_e} \\approx 1.86\n\\]\nQuesto ci indica che, nel gruppo sperimentale, ci sono circa 1.86 “successi” (ossia la scelta del disco blu) per ogni “insuccesso” (scelta del disco verde).\nProcediamo calcolando gli odds nella condizione di controllo:\n\\[\np_c = \\frac{100}{200} = 0.5\n\\]\n\\[\n\\text{odds}_c = \\frac{p_c}{1 - p_c} = 1.0\n\\]\nQuesto ci indica che, nel gruppo di controllo, il numero di “successi” e “insuccessi” è uguale.\nInfine, confrontiamo gli odds tra la condizione sperimentale e la condizione di controllo per calcolare l’odds ratio (OR):\n\\[\n\\text{OR} = \\frac{\\text{odds}_e}{\\text{odds}_c} = 1.86\n\\]\nGli odds di scelta del disco blu aumentano di circa 1.86 volte nel gruppo sperimentale rispetto al gruppo di controllo.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#analisi-bayesiana-dellodds-ratio",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#analisi-bayesiana-dellodds-ratio",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.5 Analisi Bayesiana dell’Odds Ratio",
    "text": "13.5 Analisi Bayesiana dell’Odds Ratio\nNella nostra analisi, ci focalizziamo sull’Odds Ratio (OR) per valutare la differenza nel comportamento di scelta delle api nelle due condizioni dell’esperimento discusso. L’OR fornisce una stima puntuale della differenza basata sul nostro campione specifico. Tuttavia, per realizzare un’inferenza statistica robusta, è essenziale considerare l’incertezza nelle nostre stime, caratterizzata attraverso la distribuzione a posteriori.\nL’analisi bayesiana si basa sull’applicazione del teorema di Bayes per aggiornare le nostre conoscenze a priori con l’evidenza fornita dai dati osservati. Questo ci permette di ottenere una distribuzione a posteriori delle quantità di interesse, come l’odds ratio.\nPer affrontare questa questione, adottiamo un approccio bayesiano, costruendo la distribuzione a posteriori dell’OR. A partire da questa distribuzione, determiniamo un intervallo di credibilità del 90%, che rappresenta l’intervallo entro il quale, con il 90% di confidenza, possiamo aspettarci che ricada il vero valore dell’OR della popolazione.\nSe questo intervallo non include il valore 1, disponiamo di una solida evidenza (con un livello di credibilità del 90%) che la differenza tra le due condizioni esaminate corrisponde a una differenza reale nella popolazione, il che suggerisce che non si tratta di un artefatto generato dalla nostra incertezza. In altre parole, possiamo affermare con ragionevole certezza che le api dispongono di una visione cromatica.\nD’altro canto, se l’intervallo di credibilità includesse il valore 1, ciò indicherebbe che la differenza osservata nel nostro campione potrebbe non riflettere una differenza significativa nella popolazione generale, suggerendo che potrebbe essere una peculiarità del nostro campione specifico.\nL’analisi bayesiana e il calcolo dell’intervallo di credibilità verranno condotti utilizzando cmdstanpy, che ci permette di implementare modelli bayesiani in modo efficiente e rigoroso. Utilizzeremo una distribuzione a priori debolmente informativa per l’OR, in modo da non influenzare eccessivamente i risultati con assunzioni preliminari.\nUna volta ottenuta la distribuzione a posteriori dell’OR, possiamo calcolare il nostro intervallo di credibilità del 90%. Questo intervallo fornirà una rappresentazione della nostra incertezza riguardo il vero valore dell’OR nella popolazione. Se il nostro intervallo di credibilità esclude il valore 1, possiamo concludere che esiste una differenza significativa tra i due gruppi, confermando che le api possono distinguere i colori e preferire il disco blu.\nIn sintesi, l’approccio bayesiano non solo ci permette di stimare l’OR, ma anche di quantificare la nostra incertezza e fare inferenze più solide e informative sulla capacità delle api di distinguere tra colori.\n\n13.5.1 Likelihood\nLa likelihood del modello descrive la probabilità di osservare i dati dati i parametri del modello. Nel nostro caso, abbiamo due gruppi con eventi binomiali.\nPer il gruppo 1:\n\\[\ny_1 \\sim \\text{Binomiale}(N_1, \\theta_1).\n\\]\nPer il gruppo 2:\n\\[\ny_2 \\sim \\text{Binomiale}(N_2, \\theta_2).\n\\]\n\n13.5.2 Priors\nI priors del modello descrivono le nostre convinzioni iniziali sui parametri prima di osservare i dati. Nel nostro caso, i parametri \\(\\theta_1\\) e \\(\\theta_2\\) seguono una distribuzione Beta(2, 2).\nPer \\(\\theta_1\\):\n\\[\n\\theta_1 \\sim \\text{Beta}(2, 2).\n\\]\nPer \\(\\theta_2\\):\n\\[\n\\theta_2 \\sim \\text{Beta}(2, 2).\n\\]\nCompiliamo e stampiamo il modello Stan.\n\n# Path to the Stan file\nstan_file &lt;- here::here(\"stan\", \"odds-ratio.stan\")\n\n# Create a CmdStanModel object\nmod &lt;- cmdstan_model(stan_file)\n\n\nmod$print()\n#&gt; //  Comparison of two groups with Binomial\n#&gt; data {\n#&gt;   int&lt;lower=0&gt; N1; // number of experiments in group 1\n#&gt;   int&lt;lower=0&gt; y1; // number of events in group 1\n#&gt;   int&lt;lower=0&gt; N2; // number of experiments in group 2\n#&gt;   int&lt;lower=0&gt; y2; // number of events in group 2\n#&gt; }\n#&gt; parameters {\n#&gt;   real&lt;lower=0, upper=1&gt; theta1; // probability of event in group 1\n#&gt;   real&lt;lower=0, upper=1&gt; theta2; // probability of event in group 2\n#&gt; }\n#&gt; model {\n#&gt;   // model block creates the log density to be sampled\n#&gt;   theta1 ~ beta(2, 2); // prior\n#&gt;   theta2 ~ beta(2, 2); // prior\n#&gt;   y1 ~ binomial(N1, theta1); // observation model / likelihood\n#&gt;   y2 ~ binomial(N2, theta2); // observation model / likelihood\n#&gt; }\n#&gt; generated quantities {\n#&gt;   real oddsratio = (theta1 / (1 - theta1)) / (theta2 / (1 - theta2));\n#&gt; }\n\nNel blocco generated quantities, calcoliamo l’odds ratio:\n\\[\n\\text{oddsratio} = \\frac{\\theta_1 / (1 - \\theta_1)}{\\theta_2 / (1 - \\theta_2)}.\n\\]\nQuesto rapporto delle odds ci dà una misura della forza dell’associazione tra l’evento e i gruppi.\nIn sintesi, il modello bayesiano utilizza i dati osservati per aggiornare le nostre convinzioni iniziali sui parametri \\(\\theta_1\\) e \\(\\theta_2\\), fornendo una distribuzione a posteriori che riflette sia le informazioni a priori sia le evidenze empiriche.\nCreiamo un dizionario che contiene i dati.\n\nn1 &lt;- 200\ny1 &lt;- 130\nn2 &lt;- 200\ny2 &lt;- 100\n\nstan_data &lt;- list(\n  N1 = n1,\n  y1 = y1,\n  N2 = n2,\n  y2 = y2\n)\n\nEseguiamo il campionamento.\n\nfit &lt;- mod$sample(\n  data = stan_data,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  iter_sampling = 5000, \n  iter_warmup = 2000,\n  adapt_delta = 0.99,\n  show_messages = FALSE\n)\n\nEstraiamo la distribuzione a posteriori dell’odds ratio e generiamo un istogramma.\n\n# Extract posterior draws for 'oddsratio'\nor_draws &lt;- fit$draws(variables = \"oddsratio\", format = \"array\")\n\n\n# Create a histogram of the posterior distribution\nmcmc_hist(\n  or_draws,\n  binwidth = NULL, # Automatically choose binwidth\n  freq = FALSE     # Plot density instead of frequencies\n) +\n  ggtitle(\"Istogramma della distribuzione a posteriori di OR\") +\n  xlab(\"Valori\") +\n  ylab(\"Frequenza\")\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nLa distribuzione posteriore del rapporto degli odds è il modo più semplice e accurato per descrivere la differenza tra i due gruppi. Nel caso presente, notiamo che vi è un’elevata probabilità che la differenza tra i due gruppi sia affidabile e relativamente grande.\nUn sommario della distribuzione a posteriori dell’odds ratio si ottine nel modo seguente.\n\nfit$summary()\n#&gt; # A tibble: 4 × 10\n#&gt;   variable      mean   median     sd    mad       q5      q95  rhat ess_bulk\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 lp__      -275.    -275.    0.976  0.710  -277.    -274.     1.00    7085.\n#&gt; 2 theta1       0.647    0.648 0.0334 0.0341    0.592    0.701  1.00   10382.\n#&gt; 3 theta2       0.500    0.500 0.0345 0.0349    0.443    0.556  1.00   10901.\n#&gt; 4 oddsratio    1.88     1.84  0.385  0.374     1.32     2.57   1.00   10385.\n#&gt; # ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nPossiamo determinare la probabilità che il rapporto di probabilità (odds ratio) superi 1. Per farlo, è sufficiente analizzare gli 8000 campioni della distribuzione posteriore dell’odds ratio\n\ndim(or_draws)\n#&gt; [1] 5000    4    1\n\ne calcolare la proporzione di questi che presenta un valore maggiore di 1:\n\n# Summarize the proportion of posterior draws for 'oddsratio' greater than 1.0\nfit$summary(\"oddsratio\", pr_gt_one = ~ mean(. &gt; 1.0))\n#&gt; # A tibble: 1 × 2\n#&gt;   variable  pr_gt_one\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 oddsratio     0.999",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#diagnostica-delle-catene-markoviane",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#diagnostica-delle-catene-markoviane",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.6 Diagnostica delle catene markoviane",
    "text": "13.6 Diagnostica delle catene markoviane\nPrima di esaminare i risultati, eseguiamo la diagnostica delle catene markoviane.\n\n13.6.1 Mixing\nIl trace plot precedente dimostra un buon mixing. Questo è evidenza che il campionamento MCMC ha raggiunto uno stato stazionario.\n\nmcmc_trace(or_draws) +\n  ggtitle(\"Trace Plot for 'oddsratio'\")\n\n\n\n\n\n\n\n\n13.6.2 Numerosità campionaria effettiva\nQuando si utilizzano metodi di campionamento MCMC, è ragionevole chiedersi se un particolare campione estratto dalla distribuzione a posteriori sia sufficientemente grande per calcolare con sicurezza le quantità di interesse, come una media o un HDI. Questo non è qualcosa che possiamo rispondere direttamente guardando solo il numero di punti della catena MCMC, e il motivo è che i campioni ottenuti dai metodi MCMC hanno un certo grado di autocorrelazione, quindi la quantità effettiva di informazioni contenute in quel campione sarà inferiore a quella che otterremmo da un campione iid della stessa dimensione. Possiamo pensare alla dimensione del campione effettivo (ESS) come a un stimatore che tiene conto dell’autocorrelazione e fornisce il numero di estrazioni che avremmo se il nostro campione fosse effettivamente iid.\nPer le catene buone, solitamente, il valore della dimensione del campione effettivo sarà inferiore al numero di campioni. Ma l’ESS può essere in realtà più grande del numero di campioni estratti. Quando si utilizza il campionatore NUTS, valori di ESS maggiori del numero totale di campioni possono verificarsi per parametri le cui distribuzioni posteriori sono vicine alla Gaussiana e che sono quasi indipendenti da altri parametri nel modello.\nNell’output di PyCM si considera ESS_BULK. Un euristica è che deve essere almeno uguale a 400. Nel caso presente questo si verifica, quindi il valore ESS_BULK non fornisce alcuna evidenza di cattivo mixing.\n\n13.6.3 R hat\nIn condizioni molto generali, i metodi di Markov chain Monte Carlo hanno garanzie teoriche che otterranno la risposta corretta indipendentemente dal punto di partenza. Sfortunatamente, tali garanzie sono valide solo per campioni infiniti. Quindi, nella pratica, abbiamo bisogno di modi per stimare la convergenza per campioni finiti. Un’idea diffusa è quella di generare più di una catena, partendo da punti molto diversi e quindi controllare le catene risultanti per vedere se sembrano simili tra loro. Questa nozione intuitiva può essere formalizzata in un indicatore numerico noto come R-hat. Esistono molte versioni di questo stimatore, poiché è stato perfezionato nel corso degli anni. In origine il R-hat veniva interpretato come la sovrastima della varianza dovuta al campionamento MCMC finito. Ciò significa che se si continua a campionare all’infinito si dovrebbe ottenere una riduzione della varianza della stima di un fattore R-hat. E quindi il nome “fattore di riduzione potenziale della scala” (potential scale reduction factor), con il valore target di 1 che significa che aumentare il numero di campioni non ridurrà ulteriormente la varianza della stima. Tuttavia, nella pratica è meglio pensarlo solo come uno strumento diagnostico senza cercare di sovra-interpretarlo.\nL’R-hat per il parametro theta viene calcolato come la deviazione standard di tutti i campioni di theta, ovvero includendo tutte le catene insieme, diviso per la radice quadratica media delle deviazioni standard separate all’interno della catena. Il calcolo effettivo è un po’ più complesso ma l’idea generale è questa. Idealmente dovremmo ottenere un valore di 1, poiché la varianza tra le catene dovrebbe essere la stessa della varianza all’interno della catena. Da un punto di vista pratico, valori di R-hat inferiori a 1.1 sono considerati sicuri.\nNel caso presente questo si verifica. Possiamo ottenere R hat nel modo seguente:\n\n# Extract the summary including R-hat\nsummary &lt;- fit$summary()\n\n# Extract R-hat values\nrhat_values &lt;- summary$rhat\n\n# Print R-hat values\nprint(rhat_values)\n#&gt; [1] 1.000647 1.000385 1.000449 1.000297\n\nIl valore di \\(\\hat{R}\\), al massimo, raggiunge il valore di 1.001. Essendo il valore molto simile a 1 nel caso presente, possiamo dire che non ci sono evidenza di assenza di convergenza.\n\n13.6.4 Errore standard di Monte Carlo\nQuando si utilizzano metodi MCMC introduciamo un ulteriore livello di incertezza poiché stiamo approssimando la posteriore con un numero finito di campioni. Possiamo stimare la quantità di errore introdotta utilizzando l’errore standard di Monte Carlo (MCSE). L’MCSE tiene conto del fatto che i campioni non sono veramente indipendenti l’uno dall’altro e sono in realtà calcolati dall’ESS. Mentre i valori di ESS e R-hat sono indipendenti dalla scala dei parametri, la statistica MCSE non lo è. Se vogliamo riportare il valore di un parametro stimato al secondo decimale, dobbiamo essere sicuri che MCSE sia al di sotto del secondo decimale altrimenti, finiremo, erroneamente, per riportare una precisione superiore a quella che abbiamo realmente. Dovremmo controllare MCSE solo una volta che siamo sicuri che ESS sia abbastanza alto e R-hat sia abbastanza basso; altrimenti, MCSE non è utile.\nNel nostro caso il MCSE è sufficientemente piccolo.\n\n# Extract posterior draws\ndraws &lt;- fit$draws()\n\n# Summarize the draws, including MCSE\nsummary &lt;- summarize_draws(draws, \"mean\", \"mcse_mean\")\n\n# Print the summary including MCSE\nprint(summary)\n#&gt; # A tibble: 4 × 3\n#&gt;   variable      mean mcse_mean\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 lp__      -275.     0.0120  \n#&gt; 2 theta1       0.647  0.000328\n#&gt; 3 theta2       0.500  0.000331\n#&gt; 4 oddsratio    1.88   0.00381\n\nL’errore standard di Monte Carlo ci informa della precisione della stima ottenuta usando il metodo MCMC. Non possiamo riportare una precisione dei risultati maggiore di quella indicata dalla MCSE. Pertanto, per il caso presente relativo all’Odds Ratio (OR), possiamo affermare che la precisione massima raggiungibile è limitata a due decimali.\n\n13.6.5 Autocorrelazione\nL’autocorrelazione riduce la quantità effettiva di informazioni contenute in un campione e quindi è qualcosa che vogliamo mantenere al minimo. Possiamo ispezionare direttamente l’autocorrelazione con az.plot_autocorr.\n\n# Extract posterior draws for 'oddsratio'\noddsratio_draws &lt;- fit$draws(variables = \"oddsratio\", format = \"matrix\")\n\n# Create an autocorrelation plot\nmcmc_acf(oddsratio_draws) +\n  ggtitle(\"Autocorrelation Plot for 'oddsratio'\")\n\n\n\n\n\n\n\n\n13.6.6 Rank Plots\nI grafici dei ranghi sono un altro strumento diagnostico visivo che possiamo utilizzare per confrontare il comportamento del campionamento sia all’interno che tra le catene. I grafici dei ranghi, in parole semplici, sono istogrammi dei campioni della distribuzione a posteriori espressi in termini di ranghi. Nei grafici dei ranghi, i ranghi sono calcolati combinando prima tutte le catene ma poi rappresentando i risultati separatamente per ogni catena. Se tutte le catene stimano la stessa distribuzione, ci aspettiamo che i ranghi abbiano una distribuzione uniforme. Inoltre, se i grafici dei ranghi di tutte le catene sembrano simili, ciò indica un buon mix delle catene.\nPossiamo ottenere i grafici dei ranghi con az.plot_rank.\n\nmcmc_rank_hist(oddsratio_draws) +\n  ggtitle(\"Rank Histogram for 'oddsratio'\")\n\n\n\n\n\n\n\nPossiamo vedere nella figura che i ranghi sono molto simili ad una distribuzione uniforme e che tutte le catene sono simili tra loro senza alcuno scostamento distintivo.\n\n13.6.7 Divergenza\nPer diagnosticare il funzionamento di un campionatore, abbiamo finora analizzato i campioni generati. Un altro approccio fondamentale consiste nel monitorare i meccanismi interni del metodo di campionamento. Una delle diagnosi più importanti in questo contesto è rappresentata dal concetto di divergenza, particolarmente rilevante nei metodi Hamiltonian Monte Carlo (HMC). Le divergenze (o transizioni divergenti) sono un segnale sensibile che indica potenziali problemi nella geometria del modello o nel campionamento. Queste diagnosi sono complementari agli altri controlli descritti in precedenza.\n\n13.6.7.1 Che cosa sono le transizioni divergenti?\nLe transizioni divergenti si verificano quando il metodo HMC non riesce a esplorare efficacemente la distribuzione a posteriori. Ciò accade, ad esempio, in presenza di geometrie complesse come regioni strette e allungate della distribuzione, dove il campionatore fatica a seguire il gradiente. Le transizioni divergenti sono quindi un’indicazione che il modello potrebbe necessitare di una riformulazione o di parametri di campionamento più adeguati (come un maggiore adapt_delta).\nCmdStan consente di rilevare queste transizioni e riporta il numero di divergenze per ciascuna catena durante il campionamento. Un risultato ottimale è zero divergenze, poiché ciò indica che il campionatore ha esplorato la distribuzione senza difficoltà.\n\n13.6.7.2 Diagnosi delle divergenze\nPer eseguire questa diagnosi, possiamo utilizzare il metodo fit$diagnostic_summary(), che restituisce un riepilogo dei principali indicatori diagnostici. Vediamo i valori restituiti nel caso specifico:\n\nfit$diagnostic_summary()\n#&gt; $num_divergent\n#&gt; [1] 0 0 0 0\n#&gt; \n#&gt; $num_max_treedepth\n#&gt; [1] 0 0 0 0\n#&gt; \n#&gt; $ebfmi\n#&gt; [1] 1.035259 1.051684 1.005021 1.068711\n\nI valori diagnostici restituiti sono i seguenti:\n\n\n$num_divergent:\n\nQuesto valore indica il numero di transizioni divergenti per ciascuna catena.\n\nCaso corrente: Tutte le catene riportano 0 transizioni divergenti. Ciò significa che il campionatore è stato in grado di esplorare la distribuzione a posteriori senza difficoltà, suggerendo che il modello è ben specificato e che non ci sono problemi nella geometria della distribuzione.\n\n\n\n$num_max_treedepth:\n\nQuesto valore rappresenta il numero di volte in cui una catena ha raggiunto la massima profondità dell’albero durante il campionamento NUTS (No-U-Turn Sampler).\nRaggiungere frequentemente la profondità massima potrebbe indicare inefficienza nel campionamento.\n\nCaso corrente: Nessuna catena ha raggiunto la massima profondità, il che significa che il campionatore è stato efficiente e non sono necessarie modifiche al parametro max_treedepth.\n\n\n\n$ebfmi (Energy Bayesian Fraction of Missing Information):\n\nL’E-BFMI misura l’efficienza del campionamento in relazione all’energia Hamiltoniana. Valori inferiori a 0.3 indicano problemi di esplorazione della distribuzione a posteriori.\n\nCaso corrente: Tutte le catene presentano valori di E-BFMI superiori a 1, indicando un’ottima esplorazione della distribuzione e l’assenza di problemi nella geometria del modello.\n\n\n\nIn sintesi, i risultati diagnostici indicano che il campionamento MCMC è stato eseguito correttamente e senza problemi:\n\n\n0 transizioni divergenti: Il modello è ben specificato e la distribuzione a posteriori è stata esplorata in modo efficace.\n\n0 superamenti della profondità dell’albero: Il campionatore è stato efficiente.\n\nE-BFMI &gt; 1: L’energia Hamiltoniana è stata esplorata in modo ottimale, senza segni di inefficienza.\n\nQuesti risultati ci consentono di procedere con fiducia nell’analisi dei risultati, poiché non vi sono evidenze di problematiche legate al campionamento o alla geometria del modello.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#riflessioni-conclusive",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#riflessioni-conclusive",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.8 Riflessioni Conclusive",
    "text": "13.8 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato come applicare un approccio bayesiano per analizzare e interpretare l’odds ratio tra due proporzioni. Attraverso l’uso del modello statistico, siamo stati in grado di stimare la distribuzione a posteriori dell’odds ratio e di calcolare l’intervallo di credibilità.\nI risultati ottenuti, supportati da un controllo diagnostico delle catene Markoviane, indicano che la differenza osservata tra i due gruppi è credibile e supportata dai dati. L’odds ratio stimato e il relativo intervallo di credibilità escludono il valore 1, suggerendo una differenza coerente tra i gruppi analizzati. L’approccio bayesiano si è dimostrato efficace, non solo per stimare i parametri di interesse, ma anche per quantificare l’incertezza associata a tali stime.\nIn sintesi, l’analisi bayesiana dell’odds ratio ha permesso di rispondere alla domanda di ricerca, confermando che le api mostrano comportamenti coerenti con una capacità di distinzione cromatica. L’approccio presentato in questo capitolo può essere esteso ad altre applicazioni, offrendo una struttura versatile per il confronto tra proporzioni in diversi contesti sperimentali.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#informazioni-sullambiente-di-sviluppo",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] posterior_1.6.0     cmdstanr_0.8.1.9000 MASS_7.3-61        \n#&gt;  [4] viridis_0.6.5       viridisLite_0.4.2   ggpubr_0.6.0       \n#&gt;  [7] ggExtra_0.10.1      gridExtra_2.3       patchwork_1.3.0    \n#&gt; [10] bayesplot_1.11.1    psych_2.4.6.26      scales_1.3.0       \n#&gt; [13] markdown_1.13       knitr_1.49          lubridate_1.9.3    \n#&gt; [16] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \n#&gt; [19] purrr_1.0.2         readr_2.1.5         tidyr_1.3.1        \n#&gt; [22] tibble_3.2.1        ggplot2_3.5.1       tidyverse_2.0.0    \n#&gt; [25] rio_1.2.3           here_1.0.1         \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] tidyselect_1.2.1     farver_2.1.2         fastmap_1.2.0       \n#&gt;  [4] tensorA_0.36.2.1     pacman_0.5.1         promises_1.3.1      \n#&gt;  [7] digest_0.6.37        timechange_0.3.0     mime_0.12           \n#&gt; [10] lifecycle_1.0.4      processx_3.8.4       magrittr_2.0.3      \n#&gt; [13] compiler_4.4.2       rlang_1.1.4          tools_4.4.2         \n#&gt; [16] utf8_1.2.4           yaml_2.3.10          data.table_1.16.2   \n#&gt; [19] ggsignif_0.6.4       labeling_0.4.3       htmlwidgets_1.6.4   \n#&gt; [22] mnormt_2.1.1         plyr_1.8.9           abind_1.4-8         \n#&gt; [25] miniUI_0.1.1.1       withr_3.0.2          grid_4.4.2          \n#&gt; [28] fansi_1.0.6          xtable_1.8-4         colorspace_2.1-1    \n#&gt; [31] cli_3.6.3            rmarkdown_2.29       generics_0.1.3      \n#&gt; [34] reshape2_1.4.4       tzdb_0.4.0           parallel_4.4.2      \n#&gt; [37] matrixStats_1.4.1    vctrs_0.6.5          jsonlite_1.8.9      \n#&gt; [40] carData_3.0-5        car_3.1-3            hms_1.1.3           \n#&gt; [43] rstatix_0.7.2        Formula_1.2-5        glue_1.8.0          \n#&gt; [46] ps_1.8.1             distributional_0.5.0 stringi_1.8.4       \n#&gt; [49] gtable_0.3.6         later_1.4.0          munsell_0.5.1       \n#&gt; [52] pillar_1.9.0         htmltools_0.5.8.1    R6_2.5.1            \n#&gt; [55] rprojroot_2.0.4      evaluate_1.0.1       shiny_1.9.1         \n#&gt; [58] lattice_0.22-6       backports_1.5.0      broom_1.0.7         \n#&gt; [61] httpuv_1.6.15        Rcpp_1.0.13-1        nlme_3.1-166        \n#&gt; [64] checkmate_2.3.2      xfun_0.49            pkgconfig_2.0.3",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#bibliografia",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#bibliografia",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nHoffmann, T., Hofman, A., & Wagenmakers, E.-J. (2022). Bayesian tests of two proportions: A tutorial with R and JASP. Methodology, 18(4), 239–277.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#interpretazione-dei-risultati",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#interpretazione-dei-risultati",
    "title": "13  Analisi bayesiana dell’odds-ratio",
    "section": "\n13.7 Interpretazione dei Risultati",
    "text": "13.7 Interpretazione dei Risultati\nI risultati della diagnosi delle catene Markoviane non evidenziano problematiche relative alla convergenza dell’algoritmo né discrepanze nel modello statistico adottato, permettendoci di procedere con l’analisi dei risultati ottenuti.\nL’analisi ha determinato un valore a posteriori per l’OR di 1.88, accompagnato da un intervallo di credibilità del 94% compreso tra 1.20 e 2.60. Poiché questo intervallo non include il valore 1, possiamo affermare, con un grado di certezza del 94%, che il comportamento delle api differisce nelle due condizioni sperimentali. Questo fornisce evidenza a supporto dell’ipotesi che le api dispongano di una visione cromatica.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/08_stan_odds_ratio.html#interpretazione-dei-risultati-1",
    "href": "chapters/mcmc/08_stan_odds_ratio.html#interpretazione-dei-risultati-1",
    "title": "8  Analisi bayesiana dell’odds-ratio",
    "section": "\n8.7 Interpretazione dei Risultati",
    "text": "8.7 Interpretazione dei Risultati\nI risultati della diagnosi delle catene Markoviane non evidenziano problematiche relative alla convergenza dell’algoritmo né discrepanze nel modello statistico adottato, permettendoci di procedere con l’analisi dei risultati ottenuti.\nL’analisi ha determinato un valore a posteriori per l’OR di 1.88, accompagnato da un intervallo di credibilità del 94% compreso tra 1.20 e 2.60. Poiché questo intervallo non include il valore 1, possiamo affermare, con un grado di certezza del 94%, che il comportamento delle api differisce nelle due condizioni sperimentali. Questo fornisce evidenza a supporto dell’ipotesi che le api dispongano di una visione cromatica.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analisi bayesiana dell'odds-ratio</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html",
    "href": "chapters/eda/03_dplyr.html",
    "title": "9  Data tidying",
    "section": "",
    "text": "9.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nL’obiettivo di questo capitolo è fornire un’introduzione alle funzioni principali del linguaggio R per le operazioni di data wrangling, cioè per il preprocessing e la pulizia dei dati. In R, queste operazioni sono strettamente legate al concetto di “data tidying”, che si riferisce all’organizzazione sistematica dei dati per facilitare l’analisi.\nPer comprendere meglio il concetto di “data tidying”, possiamo rifarci a una citazione tratta dal testo di riferimento R for Data Science (2e):\nL’essenza del “data tidying” è organizzare i dati in un formato che sia facile da gestire e analizzare. Anche se gli stessi dati possono essere rappresentati in vari modi, non tutte le rappresentazioni sono ugualmente efficienti o facili da usare. Un dataset “tidy” segue tre principi fondamentali che lo rendono particolarmente pratico:\nI pacchetti R come dplyr, ggplot2 e gli altri pacchetti del tidyverse sono progettati specificamente per lavorare con dati in formato “tidy”, permettendo agli utenti di eseguire operazioni di manipolazione e visualizzazione in modo più intuitivo ed efficiente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#introduzione",
    "href": "chapters/eda/03_dplyr.html#introduzione",
    "title": "9  Data tidying",
    "section": "",
    "text": "“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham\n\n\n\n\nOgni variabile è una colonna: ogni colonna nel dataset rappresenta una singola variabile.\n\nOgni osservazione è una riga: ogni riga nel dataset rappresenta un’unica osservazione.\n\nOgni valore è una cella: ogni cella del dataset contiene un singolo valore.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#pipe",
    "href": "chapters/eda/03_dplyr.html#pipe",
    "title": "9  Data tidying",
    "section": "\n9.2 Pipe",
    "text": "9.2 Pipe\nSia il pacchetto tidyr che il pacchetto dplyr utilizzano l’operatore pipe, che in R può essere rappresentato da due notazioni principali: |&gt; (introdotto nativamente in R a partire dalla versione 4.1.0) e %&gt;% (introdotto dal pacchetto magrittr, ampiamente utilizzato in tidyverse). Entrambi gli operatori permettono di concatenare in modo efficiente una serie di operazioni, ma presentano alcune differenze che meritano attenzione.\n\n9.2.1 Cosa Fa la Pipe?\nLa pipe è uno strumento potente che permette di collegare in modo diretto l’output di una funzione come input della funzione successiva. Questo approccio:\n\nRiduce la necessità di creare variabili intermedie.\nMigliora la leggibilità del codice.\nRende il flusso delle operazioni più chiaro e lineare.\n\nOgni funzione applicata con la pipe riceve automaticamente l’output della funzione precedente come suo primo argomento. Ciò consente di scrivere sequenze di operazioni in un formato compatto e intuitivo.\nEcco un esempio pratico:\n\n# Utilizzo della pipe per trasformare un dataset\nlibrary(dplyr)\n\ndf &lt;- data.frame(\n  id = 1:5,\n  value = c(10, 20, 30, 40, 50)\n)\n\n# Filtra i dati, seleziona colonne e calcola nuovi valori\ndf_clean &lt;- df |&gt;\n  dplyr::filter(value &gt; 20) |&gt;\n  dplyr::select(id, value) |&gt;\n  mutate(squared_value = value^2)\n\nIn questa sequenza, il dataset originale df viene filtrato, le colonne desiderate vengono selezionate e viene aggiunta una nuova colonna con il valore al quadrato.\n\nhead(df_clean)\n#&gt;   id value squared_value\n#&gt; 1  3    30           900\n#&gt; 2  4    40          1600\n#&gt; 3  5    50          2500\n\nIn sintesi, la pipe è uno strumento fondamentale per scrivere codice R moderno e leggibile, indipendentemente dal fatto che si utilizzi |&gt; o %&gt;%.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#verbi",
    "href": "chapters/eda/03_dplyr.html#verbi",
    "title": "9  Data tidying",
    "section": "\n9.3 Verbi",
    "text": "9.3 Verbi\nLe funzioni principali (“verbi) di dplyr sono le seguenti:\n\n\n\n\n\n\nVerbo dplyr\nDescrizione\n\n\n\nselect()\nSeleziona colonne\n\n\nfilter()\nFiltra righe\n\n\narrange()\nRiordina o organizza le righe\n\n\nmutate()\nCrea nuove colonne\n\n\nsummarise()\nRiassume i valori\n\n\ngroup_by()\nConsente di eseguire operazioni di gruppo\n\n\n\nI verbi di dplyr sono suddivisi in quattro gruppi, in base all’elemento su cui operano: righe, colonne, gruppi o tabelle.\nInoltre, le diverse funzioni bind_ e _joins permettono di combinare più tibbles (ovvero, data frame) in uno solo.\nPer introdurre il processo di “data tidying”, in questo tutorial utilizzeremo il dataset msleep.\n\ndata(msleep)\ndim(msleep)\n#&gt; [1] 83 11\n\nEsaminiamo i dati:\n\nglimpse(msleep)\n#&gt; Rows: 83\n#&gt; Columns: 11\n#&gt; $ name         &lt;chr&gt; \"Cheetah\", \"Owl monkey\", \"Mountain beaver\", \"Greater …\n#&gt; $ genus        &lt;chr&gt; \"Acinonyx\", \"Aotus\", \"Aplodontia\", \"Blarina\", \"Bos\", …\n#&gt; $ vore         &lt;chr&gt; \"carni\", \"omni\", \"herbi\", \"omni\", \"herbi\", \"herbi\", \"…\n#&gt; $ order        &lt;chr&gt; \"Carnivora\", \"Primates\", \"Rodentia\", \"Soricomorpha\", …\n#&gt; $ conservation &lt;chr&gt; \"lc\", NA, \"nt\", \"lc\", \"domesticated\", NA, \"vu\", NA, \"…\n#&gt; $ sleep_total  &lt;dbl&gt; 12.1, 17.0, 14.4, 14.9, 4.0, 14.4, 8.7, 7.0, 10.1, 3.…\n#&gt; $ sleep_rem    &lt;dbl&gt; NA, 1.8, 2.4, 2.3, 0.7, 2.2, 1.4, NA, 2.9, NA, 0.6, 0…\n#&gt; $ sleep_cycle  &lt;dbl&gt; NA, NA, NA, 0.133, 0.667, 0.767, 0.383, NA, 0.333, NA…\n#&gt; $ awake        &lt;dbl&gt; 11.9, 7.0, 9.6, 9.1, 20.0, 9.6, 15.3, 17.0, 13.9, 21.…\n#&gt; $ brainwt      &lt;dbl&gt; NA, 0.01550, NA, 0.00029, 0.42300, NA, NA, NA, 0.0700…\n#&gt; $ bodywt       &lt;dbl&gt; 50.000, 0.480, 1.350, 0.019, 600.000, 3.850, 20.490, …\n\nLe colonne, nell’ordine, corrispondono a quanto segue:\n\n\nNome colonna\nDescrizione\n\n\n\nname\nNome comune\n\n\ngenus\nRango tassonomico\n\n\nvore\nCarnivoro, onnivoro o erbivoro?\n\n\norder\nRango tassonomico\n\n\nconservation\nStato di conservazione del mammifero\n\n\nsleep_total\nQuantità totale di sonno, in ore\n\n\nsleep_rem\nSonno REM, in ore\n\n\nsleep_cycle\nDurata del ciclo di sonno, in ore\n\n\nawake\nQuantità di tempo trascorso sveglio, in ore\n\n\nbrainwt\nPeso del cervello, in chilogrammi\n\n\nbodywt\nPeso corporeo, in chilogrammi",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#righe",
    "href": "chapters/eda/03_dplyr.html#righe",
    "title": "9  Data tidying",
    "section": "\n9.4 Righe",
    "text": "9.4 Righe\nI verbi più importanti che operano sulle righe di un dataset sono filter(), che seleziona le righe da includere senza modificarne l’ordine, e arrange(), che cambia l’ordine delle righe senza alterare la selezione delle righe presenti.\n\nmsleep |&gt;\n  dplyr::filter(sleep_total &lt; 4) |&gt;\n  arrange(sleep_total)\n#&gt; # A tibble: 9 × 11\n#&gt;   name             genus         vore  order          conservation\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;       \n#&gt; 1 Giraffe          Giraffa       herbi Artiodactyla   cd          \n#&gt; 2 Pilot whale      Globicephalus carni Cetacea        cd          \n#&gt; 3 Horse            Equus         herbi Perissodactyla domesticated\n#&gt; 4 Roe deer         Capreolus     herbi Artiodactyla   lc          \n#&gt; 5 Donkey           Equus         herbi Perissodactyla domesticated\n#&gt; 6 African elephant Loxodonta     herbi Proboscidea    vu          \n#&gt; 7 Caspian seal     Phoca         carni Carnivora      vu          \n#&gt; 8 Sheep            Ovis          herbi Artiodactyla   domesticated\n#&gt; 9 Asian elephant   Elephas       herbi Proboscidea    en          \n#&gt; # ℹ 6 more variables: sleep_total &lt;dbl&gt;, sleep_rem &lt;dbl&gt;,\n#&gt; #   sleep_cycle &lt;dbl&gt;, awake &lt;dbl&gt;, brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\nPossiamo usare filter() speficicano più di una condizione logica.\n\nmsleep |&gt;\n  dplyr::filter((sleep_total &lt; 4 & bodywt &gt; 100) | brainwt &gt; 1) |&gt;\n  arrange(sleep_total)\n#&gt; # A tibble: 7 × 11\n#&gt;   name             genus         vore  order          conservation\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;       \n#&gt; 1 Giraffe          Giraffa       herbi Artiodactyla   cd          \n#&gt; 2 Pilot whale      Globicephalus carni Cetacea        cd          \n#&gt; 3 Horse            Equus         herbi Perissodactyla domesticated\n#&gt; 4 Donkey           Equus         herbi Perissodactyla domesticated\n#&gt; 5 African elephant Loxodonta     herbi Proboscidea    vu          \n#&gt; 6 Asian elephant   Elephas       herbi Proboscidea    en          \n#&gt; 7 Human            Homo          omni  Primates       &lt;NA&gt;        \n#&gt; # ℹ 6 more variables: sleep_total &lt;dbl&gt;, sleep_rem &lt;dbl&gt;,\n#&gt; #   sleep_cycle &lt;dbl&gt;, awake &lt;dbl&gt;, brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#colonne",
    "href": "chapters/eda/03_dplyr.html#colonne",
    "title": "9  Data tidying",
    "section": "\n9.5 Colonne",
    "text": "9.5 Colonne\nEsistono quattro verbi principali che modificano le colonne di un dataset senza cambiare le righe:\n\n\nrelocate() cambia la posizione delle colonne;\n\nrename() modifica i nomi delle colonne;\n\nselect() seleziona le colonne da includere o escludere;\n\nmutate() crea nuove colonne a partire da quelle esistenti.\n\n\nmsleep2 &lt;- msleep |&gt;\n  mutate(\n    rem_prop = sleep_rem / sleep_total * 100\n  ) |&gt;\n  dplyr::select(name, vore, rem_prop, sleep_total) |&gt;\n  arrange(desc(rem_prop))\n\nglimpse(msleep2)\n#&gt; Rows: 83\n#&gt; Columns: 4\n#&gt; $ name        &lt;chr&gt; \"European hedgehog\", \"Thick-tailed opposum\", \"Giant ar…\n#&gt; $ vore        &lt;chr&gt; \"omni\", \"carni\", \"insecti\", \"omni\", \"carni\", \"omni\", \"…\n#&gt; $ rem_prop    &lt;dbl&gt; 34.7, 34.0, 33.7, 29.2, 28.7, 27.2, 26.4, 26.2, 25.6, …\n#&gt; $ sleep_total &lt;dbl&gt; 10.1, 19.4, 18.1, 8.9, 10.1, 18.0, 9.1, 10.3, 12.5, 8.…\n\nIn questo esempio, utilizziamo mutate() per creare una nuova colonna rem_prop che rappresenta la percentuale di sonno REM sul totale del sonno. Successivamente, select() viene utilizzato per scegliere solo alcune colonne del dataset, e infine desc(rem_prop) ordina i valori di rem_prop in ordine decrescente, dal valore maggiore a quello minore.\nPer cambiare il nome di una colonna possiamo usare rename(). Inoltre, possiamo cambiare l’ordine delle variabili con relocate().\n\nmsleep2 |&gt;\n  rename(rem_perc = rem_prop) |&gt;\n  relocate(rem_perc, .before = name)\n#&gt; # A tibble: 83 × 4\n#&gt;    rem_perc name                   vore    sleep_total\n#&gt;       &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1     34.7 European hedgehog      omni           10.1\n#&gt;  2     34.0 Thick-tailed opposum   carni          19.4\n#&gt;  3     33.7 Giant armadillo        insecti        18.1\n#&gt;  4     29.2 Tree shrew             omni            8.9\n#&gt;  5     28.7 Dog                    carni          10.1\n#&gt;  6     27.2 North American Opossum omni           18  \n#&gt;  7     26.4 Pig                    omni            9.1\n#&gt;  8     26.2 Desert hedgehog        &lt;NA&gt;           10.3\n#&gt;  9     25.6 Domestic cat           carni          12.5\n#&gt; 10     25   Eastern american mole  insecti         8.4\n#&gt; # ℹ 73 more rows",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#gruppi",
    "href": "chapters/eda/03_dplyr.html#gruppi",
    "title": "9  Data tidying",
    "section": "\n9.6 Gruppi",
    "text": "9.6 Gruppi\nIl verbo group_by() viene utilizzato per suddividere un dataset in gruppi, in base a una o più variabili, che siano rilevanti per l’analisi. Questo permette di eseguire operazioni di sintesi su ciascun gruppo separatamente, ottenendo informazioni aggregate.\nAd esempio, nel codice seguente:\n\nmsleep |&gt;\n  group_by(order) |&gt;\n  summarise(\n    avg_sleep = mean(sleep_total),\n    min_sleep = min(sleep_total),\n    max_sleep = max(sleep_total),\n    total = n()\n  ) |&gt;\n  arrange(desc(avg_sleep))\n#&gt; # A tibble: 19 × 5\n#&gt;    order           avg_sleep min_sleep max_sleep total\n#&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt;  1 Chiroptera          19.8       19.7      19.9     2\n#&gt;  2 Didelphimorphia     18.7       18        19.4     2\n#&gt;  3 Cingulata           17.8       17.4      18.1     2\n#&gt;  4 Afrosoricida        15.6       15.6      15.6     1\n#&gt;  5 Pilosa              14.4       14.4      14.4     1\n#&gt;  6 Rodentia            12.5        7        16.6    22\n#&gt;  7 Diprotodontia       12.4       11.1      13.7     2\n#&gt;  8 Soricomorpha        11.1        8.4      14.9     5\n#&gt;  9 Primates            10.5        8        17      12\n#&gt; 10 Erinaceomorpha      10.2       10.1      10.3     2\n#&gt; 11 Carnivora           10.1        3.5      15.8    12\n#&gt; 12 Scandentia           8.9        8.9       8.9     1\n#&gt; 13 Monotremata          8.6        8.6       8.6     1\n#&gt; 14 Lagomorpha           8.4        8.4       8.4     1\n#&gt; 15 Hyracoidea           5.67       5.3       6.3     3\n#&gt; 16 Artiodactyla         4.52       1.9       9.1     6\n#&gt; 17 Cetacea              4.5        2.7       5.6     3\n#&gt; 18 Proboscidea          3.6        3.3       3.9     2\n#&gt; 19 Perissodactyla       3.47       2.9       4.4     3\n\n\ngroup_by(order) suddivide il dataset msleep in gruppi, ciascuno corrispondente a un valore distinto della variabile order.\n\nSuccessivamente, summarise() calcola diverse statistiche per ogni gruppo:\n\n\navg_sleep è la media del totale del sonno (sleep_total) all’interno di ciascun gruppo.\n\nmin_sleep è il valore minimo di sleep_total in ogni gruppo.\n\nmax_sleep è il valore massimo di sleep_total in ogni gruppo.\n\ntotal è il numero di osservazioni (o righe) per ciascun gruppo, calcolato con la funzione n().\n\n\nInfine, arrange(desc(avg_sleep)) ordina i risultati in ordine decrescente in base alla media del sonno totale (avg_sleep), mostrando prima i gruppi con la media di sonno più alta.\n\nQuesto tipo di approccio è utile quando si vuole analizzare come cambiano le caratteristiche dei dati a seconda dei gruppi specifici, fornendo una visione più dettagliata e significativa.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#dati-mancanti",
    "href": "chapters/eda/03_dplyr.html#dati-mancanti",
    "title": "9  Data tidying",
    "section": "\n9.7 Dati mancanti",
    "text": "9.7 Dati mancanti\nNel dataset ci sono celle che contengono valori mancanti, indicati come NA. Questi rappresentano misurazioni per le quali i dati non sono stati registrati.\nPer ottenere una panoramica dei dati, inclusi i valori mancanti, possiamo utilizzare il comando:\n\nsummary(msleep)\n#&gt;      name              genus               vore          \n#&gt;  Length:83          Length:83          Length:83         \n#&gt;  Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                          \n#&gt;                                                          \n#&gt;                                                          \n#&gt;                                                          \n#&gt;     order           conservation        sleep_total      sleep_rem   \n#&gt;  Length:83          Length:83          Min.   : 1.90   Min.   :0.10  \n#&gt;  Class :character   Class :character   1st Qu.: 7.85   1st Qu.:0.90  \n#&gt;  Mode  :character   Mode  :character   Median :10.10   Median :1.50  \n#&gt;                                        Mean   :10.43   Mean   :1.88  \n#&gt;                                        3rd Qu.:13.75   3rd Qu.:2.40  \n#&gt;                                        Max.   :19.90   Max.   :6.60  \n#&gt;                                                        NA's   :22    \n#&gt;   sleep_cycle      awake         brainwt         bodywt    \n#&gt;  Min.   :0.1   Min.   : 4.1   Min.   :0.00   Min.   :   0  \n#&gt;  1st Qu.:0.2   1st Qu.:10.2   1st Qu.:0.00   1st Qu.:   0  \n#&gt;  Median :0.3   Median :13.9   Median :0.01   Median :   2  \n#&gt;  Mean   :0.4   Mean   :13.6   Mean   :0.28   Mean   : 166  \n#&gt;  3rd Qu.:0.6   3rd Qu.:16.1   3rd Qu.:0.13   3rd Qu.:  42  \n#&gt;  Max.   :1.5   Max.   :22.1   Max.   :5.71   Max.   :6654  \n#&gt;  NA's   :51                   NA's   :27\n\nPer visualizzare il pattern di dati mancanti, ovvero come la mancanza di una variabile possa influenzare la mancanza di altre, si può usare:\n\nmd.pattern(msleep, rotate.names = TRUE)\n#&gt;    name genus order sleep_total awake bodywt vore sleep_rem brainwt\n#&gt; 20    1     1     1           1     1      1    1         1       1\n#&gt; 9     1     1     1           1     1      1    1         1       1\n#&gt; 9     1     1     1           1     1      1    1         1       1\n#&gt; 5     1     1     1           1     1      1    1         1       1\n#&gt; 1     1     1     1           1     1      1    1         1       0\n#&gt; 10    1     1     1           1     1      1    1         1       0\n#&gt; 1     1     1     1           1     1      1    1         1       0\n#&gt; 1     1     1     1           1     1      1    1         1       0\n#&gt; 5     1     1     1           1     1      1    1         0       1\n#&gt; 3     1     1     1           1     1      1    1         0       1\n#&gt; 7     1     1     1           1     1      1    1         0       0\n#&gt; 5     1     1     1           1     1      1    1         0       0\n#&gt; 2     1     1     1           1     1      1    0         1       1\n#&gt; 1     1     1     1           1     1      1    0         1       1\n#&gt; 2     1     1     1           1     1      1    0         1       1\n#&gt; 2     1     1     1           1     1      1    0         0       0\n#&gt;       0     0     0           0     0      0    7        22      27\n#&gt;    conservation sleep_cycle    \n#&gt; 20            1           1   0\n#&gt; 9             1           0   1\n#&gt; 9             0           1   1\n#&gt; 5             0           0   2\n#&gt; 1             1           1   1\n#&gt; 10            1           0   2\n#&gt; 1             0           1   2\n#&gt; 1             0           0   3\n#&gt; 5             1           0   2\n#&gt; 3             0           0   3\n#&gt; 7             1           0   3\n#&gt; 5             0           0   4\n#&gt; 2             1           0   2\n#&gt; 1             0           1   2\n#&gt; 2             0           0   3\n#&gt; 2             0           0   5\n#&gt;              29          51 136\n\n\n\n\n\n\n\nIl modo più semplice per gestire i valori mancanti è l’analisi dei casi completi (complete case analysis), che esclude dall’analisi le osservazioni con valori mancanti e utilizza solo quelle con tutte le variabili registrate. Questo approccio può essere implementato come segue:\n\nmsleep_comp &lt;- msleep |&gt;\n  drop_na()\ndim(msleep_comp)\n#&gt; [1] 20 11\n\nTuttavia, per il dataset in questione, questa strategia non è adeguata, poiché si passa da 83 osservazioni iniziali a solo 20 righe dopo aver eliminato i dati mancanti.\nUn approccio più utile è l’utilizzo di metodi di imputazione (imputation methods). Uno di questi è l’imputazione semplice (single imputation, SI), dove il valore mancante viene sostituito dalla media della variabile corrispondente. Questo tipo di imputazione può essere eseguito come segue:\n\nimp &lt;- mice(msleep, method = \"mean\", m = 1, maxit = 1, print = FALSE)\ncomplete(imp) |&gt;\n  summary()\n#&gt;      name              genus               vore          \n#&gt;  Length:83          Length:83          Length:83         \n#&gt;  Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                          \n#&gt;                                                          \n#&gt;                                                          \n#&gt;     order           conservation        sleep_total      sleep_rem   \n#&gt;  Length:83          Length:83          Min.   : 1.90   Min.   :0.10  \n#&gt;  Class :character   Class :character   1st Qu.: 7.85   1st Qu.:1.15  \n#&gt;  Mode  :character   Mode  :character   Median :10.10   Median :1.88  \n#&gt;                                        Mean   :10.43   Mean   :1.88  \n#&gt;                                        3rd Qu.:13.75   3rd Qu.:2.20  \n#&gt;                                        Max.   :19.90   Max.   :6.60  \n#&gt;   sleep_cycle        awake         brainwt         bodywt    \n#&gt;  Min.   :0.117   Min.   : 4.1   Min.   :0.00   Min.   :   0  \n#&gt;  1st Qu.:0.417   1st Qu.:10.2   1st Qu.:0.01   1st Qu.:   0  \n#&gt;  Median :0.440   Median :13.9   Median :0.12   Median :   2  \n#&gt;  Mean   :0.440   Mean   :13.6   Mean   :0.28   Mean   : 166  \n#&gt;  3rd Qu.:0.440   3rd Qu.:16.1   3rd Qu.:0.28   3rd Qu.:  42  \n#&gt;  Max.   :1.500   Max.   :22.1   Max.   :5.71   Max.   :6654\n\nTuttavia, uno dei problemi dell’imputazione media è che tende a ridurre la varianza e a rendere le stime dell’errore standard meno accurate, generando bias verso il basso.\nUn metodo più sofisticato è l’imputazione multipla (multiple imputation, MI). Questa tecnica genera più imputazioni, creando diversi dataset completi. Per ciascuno di questi dataset, è possibile effettuare l’analisi desiderata e, al termine, combinare i risultati ottenuti dai vari dataset imputati per ottenere un risultato finale più robusto. Un esempio di questa tecnica utilizza il metodo di predictive mean matching (metodo = “pmm”), che sfrutta i valori vicini nei dati come imputazioni:\n\nimp2 &lt;- mice(msleep, method = \"pmm\", m = 1, maxit = 100, print = FALSE)\ncomplete(imp2) |&gt;\n  summary()\n#&gt;      name              genus               vore          \n#&gt;  Length:83          Length:83          Length:83         \n#&gt;  Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                          \n#&gt;                                                          \n#&gt;                                                          \n#&gt;     order           conservation        sleep_total      sleep_rem   \n#&gt;  Length:83          Length:83          Min.   : 1.90   Min.   :0.10  \n#&gt;  Class :character   Class :character   1st Qu.: 7.85   1st Qu.:0.90  \n#&gt;  Mode  :character   Mode  :character   Median :10.10   Median :1.50  \n#&gt;                                        Mean   :10.43   Mean   :1.84  \n#&gt;                                        3rd Qu.:13.75   3rd Qu.:2.40  \n#&gt;                                        Max.   :19.90   Max.   :6.60  \n#&gt;   sleep_cycle        awake         brainwt         bodywt    \n#&gt;  Min.   :0.117   Min.   : 4.1   Min.   :0.00   Min.   :   0  \n#&gt;  1st Qu.:0.183   1st Qu.:10.2   1st Qu.:0.00   1st Qu.:   0  \n#&gt;  Median :0.333   Median :13.9   Median :0.01   Median :   2  \n#&gt;  Mean   :0.450   Mean   :13.6   Mean   :0.24   Mean   : 166  \n#&gt;  3rd Qu.:0.667   3rd Qu.:16.1   3rd Qu.:0.14   3rd Qu.:  42  \n#&gt;  Max.   :1.500   Max.   :22.1   Max.   :5.71   Max.   :6654\n\nL’imputazione multipla, grazie alla sua capacità di considerare la variabilità tra le diverse imputazioni, fornisce stime più accurate rispetto all’imputazione media semplice, riducendo il rischio di bias e fornendo risultati più affidabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#considerazioni-conclusive",
    "href": "chapters/eda/03_dplyr.html#considerazioni-conclusive",
    "title": "9  Data tidying",
    "section": "\n9.8 Considerazioni Conclusive",
    "text": "9.8 Considerazioni Conclusive\nIl data wrangling è una delle fasi più importanti in qualsiasi pipeline di analisi dei dati. In questo capitolo abbiamo introdotto l’uso del pacchetto tidyverse di R per la manipolazione dei dati e il suo utilizzo in scenari di base. Tuttavia, il tidyverse è un ecosistema ampio e qui abbiamo trattato solo gli elementi fondamentali. Per approfondire, si consiglia di consultare ulteriori risorse come quelle disponibili sul sito web del tidyverse e il libro R for Data Science (2e), di cui esiste anche una traduzione italiana.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/03_dplyr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/03_dplyr.html#informazioni-sullambiente-di-sviluppo",
    "title": "9  Data tidying",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] mice_3.16.0       MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt;  [9] bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n#&gt; [13] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n#&gt; [17] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n#&gt; [21] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3        \n#&gt; [25] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1        airports_0.1.0      rlang_1.1.4        \n#&gt;  [4] magrittr_2.0.3      compiler_4.4.2      vctrs_0.6.5        \n#&gt;  [7] pkgconfig_2.0.3     shape_1.4.6.1       fastmap_1.2.0      \n#&gt; [10] backports_1.5.0     utf8_1.2.4          promises_1.3.1     \n#&gt; [13] rmarkdown_2.29      tzdb_0.4.0          openintro_2.5.0    \n#&gt; [16] nloptr_2.1.1        xfun_0.49           glmnet_4.1-8       \n#&gt; [19] jomo_2.7-6          jsonlite_1.8.9      later_1.4.0        \n#&gt; [22] styler_1.10.3       pan_1.9             broom_1.0.7        \n#&gt; [25] parallel_4.4.2      R6_2.5.1            stringi_1.8.4      \n#&gt; [28] rpart_4.1.23        car_3.1-3           boot_1.3-31        \n#&gt; [31] Rcpp_1.0.13-1       iterators_1.0.14    R.utils_2.12.3     \n#&gt; [34] cherryblossom_0.1.0 pacman_0.5.1        R.cache_0.16.0     \n#&gt; [37] httpuv_1.6.15       Matrix_1.7-1        splines_4.4.2      \n#&gt; [40] nnet_7.3-19         timechange_0.3.0    tidyselect_1.2.1   \n#&gt; [43] abind_1.4-8         yaml_2.3.10         codetools_0.2-20   \n#&gt; [46] miniUI_0.1.1.1      lattice_0.22-6      shiny_1.9.1        \n#&gt; [49] withr_3.0.2         evaluate_1.0.1      survival_3.7-0     \n#&gt; [52] pillar_1.9.0        carData_3.0-5       foreach_1.5.2      \n#&gt; [55] generics_0.1.3      rprojroot_2.0.4     hms_1.1.3          \n#&gt; [58] munsell_0.5.1       minqa_1.2.8         xtable_1.8-4       \n#&gt; [61] glue_1.8.0          tools_4.4.2         lme4_1.1-35.5      \n#&gt; [64] ggsignif_0.6.4      grid_4.4.2          colorspace_2.1-1   \n#&gt; [67] nlme_3.1-166        Formula_1.2-5       usdata_0.3.1       \n#&gt; [70] cli_3.6.3           fansi_1.0.6         gtable_0.3.6       \n#&gt; [73] R.methodsS3_1.8.2   rstatix_0.7.2       digest_0.6.37      \n#&gt; [76] htmlwidgets_1.6.4   farver_2.1.2        R.oo_1.27.0        \n#&gt; [79] htmltools_0.5.8.1   lifecycle_1.0.4     mitml_0.4-5        \n#&gt; [82] mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html",
    "href": "chapters/eda/04_exploring_qualitative_data.html",
    "title": "10  Esplorare i dati qualitativi",
    "section": "",
    "text": "10.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nIn questo capitolo ci concentreremo sull’analisi dei dati qualitativi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#il-dataset-penguins",
    "href": "chapters/eda/04_exploring_qualitative_data.html#il-dataset-penguins",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.2 Il dataset penguins\n",
    "text": "10.2 Il dataset penguins\n\nPer fornire esempi pratici, in questo capitolo utilizzeremo il dataset palmerpenguins, messo a disposizione da Allison Horst. I dati sono stati raccolti e resi disponibili da Dr. Kristen Gorman e dalla Palmer Station, parte del programma di ricerca ecologica a lungo termine Long Term Ecological Research Network. Il dataset contiene informazioni su 344 pinguini, appartenenti a 3 diverse specie, raccolte su 3 isole dell’arcipelago di Palmer, in Antartide. Per semplicità, i dati sono organizzati nel file penguins.csv.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#importare-i-dati",
    "href": "chapters/eda/04_exploring_qualitative_data.html#importare-i-dati",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.3 Importare i Dati",
    "text": "10.3 Importare i Dati\nPossiamo caricare i dati grezzi dal file penguins.csv in un DataFrame con il seguente comando:\n\nd &lt;- rio::import(here::here(\"data\", \"penguins.csv\"))\n\nEsaminiamo i dati.\n\nglimpse(d)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\",…\n#&gt; $ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.…\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.…\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, …\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 347…\n#&gt; $ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\",…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2…\n\n\nd |&gt; \n  head()\n#&gt;   species    island bill_length_mm bill_depth_mm flipper_length_mm\n#&gt; 1  Adelie Torgersen           39.1          18.7               181\n#&gt; 2  Adelie Torgersen           39.5          17.4               186\n#&gt; 3  Adelie Torgersen           40.3          18.0               195\n#&gt; 4  Adelie Torgersen             NA            NA                NA\n#&gt; 5  Adelie Torgersen           36.7          19.3               193\n#&gt; 6  Adelie Torgersen           39.3          20.6               190\n#&gt;   body_mass_g    sex year\n#&gt; 1        3750   male 2007\n#&gt; 2        3800 female 2007\n#&gt; 3        3250 female 2007\n#&gt; 4          NA   &lt;NA&gt; 2007\n#&gt; 5        3450 female 2007\n#&gt; 6        3650   male 2007\n\nPer semplicità, rimuoviamo le righe con valori mancanti con la seguente istruzione:\n\ndf &lt;- d |&gt;\n  drop_na()\n\n\ndf |&gt; \n  summary()\n#&gt;    species             island          bill_length_mm  bill_depth_mm  \n#&gt;  Length:333         Length:333         Min.   :32.10   Min.   :13.10  \n#&gt;  Class :character   Class :character   1st Qu.:39.50   1st Qu.:15.60  \n#&gt;  Mode  :character   Mode  :character   Median :44.50   Median :17.30  \n#&gt;                                        Mean   :43.99   Mean   :17.16  \n#&gt;                                        3rd Qu.:48.60   3rd Qu.:18.70  \n#&gt;                                        Max.   :59.60   Max.   :21.50  \n#&gt;  flipper_length_mm  body_mass_g       sex                 year     \n#&gt;  Min.   :172       Min.   :2700   Length:333         Min.   :2007  \n#&gt;  1st Qu.:190       1st Qu.:3550   Class :character   1st Qu.:2007  \n#&gt;  Median :197       Median :4050   Mode  :character   Median :2008  \n#&gt;  Mean   :201       Mean   :4207                      Mean   :2008  \n#&gt;  3rd Qu.:213       3rd Qu.:4775                      3rd Qu.:2009  \n#&gt;  Max.   :231       Max.   :6300                      Max.   :2009",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#tabelle-di-contingenza",
    "href": "chapters/eda/04_exploring_qualitative_data.html#tabelle-di-contingenza",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.4 Tabelle di Contingenza",
    "text": "10.4 Tabelle di Contingenza\nUna tabella di contingenza è uno strumento utilizzato per riassumere i dati di due variabili categoriali, ovvero variabili qualitative che assumono valori all’interno di un insieme finito di categorie. In una tabella di contingenza, ogni cella mostra quante volte si è verificata una combinazione specifica di categorie per le due variabili considerate.\nPer esempio, se prendiamo in esame due variabili categoriali come “island” e “species” all’interno di un DataFrame df, ciascuna delle quali rappresenta rispettivamente l’isola di provenienza e la specie dei pinguini, possiamo costruire una tabella che mostra quante volte ciascuna combinazione di “island” e “species” appare nel nostro campione. In altre parole, la tabella di contingenza ci permette di vedere quante osservazioni ci sono per ogni combinazione di categorie tra queste due variabili.\n\ntable(df$island, df$species)\n#&gt;            \n#&gt;             Adelie Chinstrap Gentoo\n#&gt;   Biscoe        44         0    119\n#&gt;   Dream         55        68      0\n#&gt;   Torgersen     47         0      0\n\nQuesta tabella di contingenza mostra la distribuzione di tre specie di pinguini (Adelie, Chinstrap, Gentoo) rispetto a tre isole (Biscoe, Dream, Torgersen). Ogni cella rappresenta il numero di pinguini di una determinata specie presenti su ciascuna isola. Ecco un’interpretazione dettagliata:\n\n\nIsola Biscoe: Qui troviamo 44 pinguini della specie Adelie e 119 pinguini della specie Gentoo, mentre non sono presenti pinguini Chinstrap.\n\nIsola Dream: Questa isola ospita 55 pinguini Adelie e 68 pinguini Chinstrap, ma nessun pinguino della specie Gentoo.\n\nIsola Torgersen: Su quest’isola sono presenti solo 47 pinguini della specie Adelie, e nessun pinguino delle specie Chinstrap o Gentoo.\n\nPossiamo dunque commentare dicendo:\n\nLa specie Adelie è distribuita su tutte e tre le isole, con numeri notevoli sia su Biscoe (44), Dream (55), che Torgersen (47).\nLa specie Chinstrap si trova solo sull’isola Dream (68 esemplari) e non è presente sulle altre due isole.\nLa specie Gentoo si trova esclusivamente sull’isola Biscoe (119 esemplari), non essendo presente su Dream e Torgersen.\n\nQuesto suggerisce una distribuzione geografica specifica delle diverse specie di pinguini, con alcune specie limitate a determinate isole e altre distribuite più ampiamente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#grafico-a-barre",
    "href": "chapters/eda/04_exploring_qualitative_data.html#grafico-a-barre",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.5 Grafico a barre",
    "text": "10.5 Grafico a barre\n\n10.5.1 Grafico a Barre con una Singola Variabile\nUn grafico a barre è uno strumento comunemente utilizzato per rappresentare visivamente una singola variabile categoriale. Questo tipo di grafico mostra le diverse categorie su uno degli assi (solitamente l’asse orizzontale) e utilizza barre di altezza proporzionale per rappresentare la frequenza o il conteggio di ciascuna categoria sull’altro asse (solitamente l’asse verticale).\nAd esempio, in un dataset che contiene informazioni su diverse specie di pinguini, un grafico a barre potrebbe mostrare il numero di pinguini per ciascuna specie. Le specie vengono visualizzate come etichette lungo l’asse delle ascisse, mentre l’altezza delle barre rappresenta il numero di pinguini osservati per ciascuna specie.\nIl grafico a barre consente di confrontare le dimensioni delle categorie in modo semplice e intuitivo.\nPer i dati in esame, creiamo un grafico a barre che rappresenta il numero totale di pinguini per isola.\n\nggplot(df, aes(x = island)) +\n  geom_bar(alpha = 0.5) +\n  ggtitle(\"Numero totale di pinguini per isola\") +\n  xlab(\"Isola\") +\n  ylab(\"Numero di pinguini\")\n\n\n\n\n\n\n\nUn secondo grafico a barre mostra il numero totale di pinguini per specie.\n\nggplot(df, aes(x = species)) +\n  geom_bar(alpha = 0.5) +\n  ggtitle(\"Numero totale di pinguini per specie\") +\n  xlab(\"Specie\") +\n  ylab(\"Numero di pinguini\")\n\n\n\n\n\n\n\n\n10.5.2 Grafico a Barre con Due Variabili\nÈ possibile visualizzare contemporaneamente le distribuzioni di due variabili categoriali utilizzando un grafico a barre. Questo tipo di grafico è particolarmente utile per esaminare la relazione tra due variabili categoriali.\nIn un grafico a barre con due variabili, una delle variabili viene rappresentata sull’asse orizzontale come categoria principale, mentre la seconda variabile è distinta tramite colori diversi o barre impilate. In questo modo, possiamo confrontare facilmente le frequenze o le proporzioni delle categorie della prima variabile, osservando allo stesso tempo come sono distribuite le categorie della seconda variabile all’interno di ciascuna categoria principale.\nAd esempio, visualizziamo il numero di pinguini per specie e isola. A qusto fine possiamo creare un grafico a barre dove le isole sono rappresentate sull’asse delle ascisse e i diversi colori delle barre mostrano la distribuzione delle specie su ciascuna isola. Questo approccio consente di esplorare come le due variabili categoriali (specie e isola) interagiscono visivamente.\n\nggplot(df, aes(x = island, fill = species)) +\n  geom_bar(position = \"stack\") +\n  ggtitle(\"Numero di pinguini per specie e isola\") +\n  xlab(\"Isola\") +\n  ylab(\"Numero di pinguini\") +\n  labs(fill = \"Specie\")\n\n\n\n\n\n\n\nIn alternativa, è possibile creare un grafico a barre dove le specie sono rappresentate sull’asse delle ascisse e i diversi colori delle barre mostrano la distribuzione delle isole per ciascuna specie.\n\nggplot(df, aes(x = species, fill = island)) +\n  geom_bar(position = \"stack\") +\n  ggtitle(\"Numero di pinguini per isola e specie\") +\n  xlab(\"Specie\") +\n  ylab(\"Numero di pinguini\") +\n  labs(fill = \"Isola\")\n\n\n\n\n\n\n\nIn alternativa all’uso delle frequenze assolute, possiamo rappresentare i dati utilizzando le frequenze relative. Questo approccio permette di confrontare meglio le categorie indipendentemente dal numero totale di osservazioni. Nella figura seguente, ad esempio, viene mostrata la proporzione di pinguini di ciascuna specie per ogni isola, evidenziando la distribuzione relativa delle specie su ogni isola, anziché il conteggio assoluto. Questa rappresentazione aiuta a visualizzare le differenze nella composizione delle specie, anche se il numero complessivo di pinguini varia tra le isole.\n\nggplot(df, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Proporzione di pinguini per specie e isola\") +\n  xlab(\"Isola\") +\n  ylab(\"Proporzione\") +\n  labs(fill = \"Specie\")",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#mosaic-plots",
    "href": "chapters/eda/04_exploring_qualitative_data.html#mosaic-plots",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.6 Mosaic plots",
    "text": "10.6 Mosaic plots\nIl Mosaic plot è una tecnica di visualizzazione particolarmente adatta per rappresentare tabelle di contingenza. Questo tipo di grafico somiglia a un grafico a barre impilate standard, ma con un vantaggio importante: oltre a visualizzare la suddivisione interna delle categorie, permette di vedere anche le dimensioni relative dei gruppi della variabile principale.\nIn altre parole, il Mosaic plot non solo mostra come si distribuiscono le categorie di una variabile secondaria all’interno di ogni gruppo della variabile principale, ma fornisce anche un’idea visiva della grandezza complessiva dei gruppi. Questo lo rende uno strumento utile per analizzare e interpretare le relazioni tra due variabili categoriali, evidenziando sia la proporzione all’interno di ciascun gruppo, sia la grandezza relativa tra i gruppi stessi.\n\nmosaic(~ species + island, data = df, main = \"Mosaic Plot of Species and Island\")",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#proporzioni-di-riga-e-colonna",
    "href": "chapters/eda/04_exploring_qualitative_data.html#proporzioni-di-riga-e-colonna",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.7 Proporzioni di Riga e Colonna",
    "text": "10.7 Proporzioni di Riga e Colonna\nNelle sezioni precedenti abbiamo esaminato la visualizzazione di due variabili categoriali utilizzando grafici a barre e Mosaic plot. Tuttavia, non abbiamo ancora discusso come vengono calcolate le proporzioni mostrate in questi grafici. In questa sezione ci concentreremo sulla suddivisione frazionaria di una variabile rispetto a un’altra, esplorando come possiamo modificare la nostra tabella di contingenza per ottenere una visione più dettagliata delle proporzioni.\nQuesto ci permetterà di comprendere meglio le relazioni tra le due variabili, visualizzando non solo i conteggi assoluti, ma anche le proporzioni relative per riga o per colonna. Le proporzioni di riga mostrano la distribuzione di una variabile all’interno delle categorie di un’altra, mentre le proporzioni di colonna evidenziano la distribuzione inversa.\nCalcoliamo le proporzioni di specie per isola.\n\n# Calcola le proporzioni di riga\nrow_proportions &lt;- df |&gt; \n  count(island, species) |&gt; \n  group_by(island) |&gt; \n  mutate(proportion = n / sum(n)) |&gt; \n  pivot_wider(names_from = species, values_from = proportion, values_fill = 0)\n\n# Aggiungi una colonna \"Totale\" che rappresenta il totale di ciascuna riga\nrow_proportions_with_total &lt;- row_proportions |&gt; \n  mutate(Totale = rowSums(across(where(is.numeric))))\n\n# Mostra la tabella con proporzioni di riga e il totale\nprint(row_proportions_with_total)\n#&gt; # A tibble: 5 × 6\n#&gt; # Groups:   island [3]\n#&gt;   island        n Adelie Gentoo Chinstrap Totale\n#&gt;   &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Biscoe       44  0.270  0         0       44.3\n#&gt; 2 Biscoe      119  0      0.730     0      120. \n#&gt; 3 Dream        55  0.447  0         0       55.4\n#&gt; 4 Dream        68  0      0         0.553   68.6\n#&gt; 5 Torgersen    47  1      0         0       48\n\nCalcoliamo nuovamente le proporzioni, ma questa volta in funzione delle colonne (per isola).\n\n# Calcola la tabella di contingenza\ncontingency_table &lt;- xtabs(~ island + species, data = df)\n\n# Calcola le proporzioni di colonna\ncolumn_proportions &lt;- prop.table(contingency_table, margin = 2)\n\n# Aggiungi una riga \"Totale\" con la somma di ciascuna colonna\ncolumn_proportions_with_total &lt;- rbind(\n  column_proportions, Totale = colSums(column_proportions)\n)\n\n# Mostra la tabella con proporzioni di colonna e il totale\nprint(column_proportions_with_total)\n#&gt;              Adelie Chinstrap Gentoo\n#&gt; Biscoe    0.3013699         0      1\n#&gt; Dream     0.3767123         1      0\n#&gt; Torgersen 0.3219178         0      0\n#&gt; Totale    1.0000000         1      1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#confronto-tra-gruppi",
    "href": "chapters/eda/04_exploring_qualitative_data.html#confronto-tra-gruppi",
    "title": "10  Esplorare i dati qualitativi",
    "section": "\n10.8 Confronto tra Gruppi",
    "text": "10.8 Confronto tra Gruppi\nAlcune delle analisi più interessanti emergono confrontando i dati numerici tra diversi gruppi. In questa sezione approfondiremo alcune delle tecniche che abbiamo già esplorato per visualizzare i dati numerici di più gruppi su uno stesso grafico e introdurremo nuovi metodi per confrontare i dati numerici tra gruppi. Queste tecniche ci permetteranno di osservare meglio le differenze e le somiglianze tra gruppi, mettendo in evidenza tendenze, variazioni e altre caratteristiche rilevanti.\nQui consideriamo due variabili qualitative. Creiamo un grafico a barre per confrontare la distribuzione del genere per specie.\n\nggplot(df, aes(x = species, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  ggtitle(\"Distribuzione del genere per specie\") +\n  xlab(\"Specie\") +\n  ylab(\"Conteggio\")\n\n\n\n\n\n\n\nSpesso, i confronti più interessanti riguardano come una variabile numerica varia in base a una o più categorie. Questo tipo di analisi ci aiuta a capire differenze tra gruppi e a individuare modelli o tendenze.\nNel grafico seguente, confrontiamo la distribuzione del peso corporeo (body_mass_g) in base alla specie e al genere. Le aree colorate rappresentano come si distribuisce il peso per maschi e femmine all’interno di ciascuna specie. Le linee più strette al centro delle aree colorate aggiungono ulteriori dettagli, mostrando i valori più comuni e come si concentrano i dati per ciascun gruppo.\n\nggplot(df, aes(x = species, y = body_mass_g, fill = sex)) +\n  geom_violin(position = position_dodge(width = 0.9), alpha = 0.5) +\n  geom_boxplot(position = position_dodge(width = 0.9), width = 0.2, alpha = 0.8) +\n  ggtitle(\"Distribuzione della massa corporea\\nin base alla specie e al genere\") +\n  xlab(\"Specie\") +\n  ylab(\"Massa corporea (g)\") +\n  labs(fill = \"Genere\")\n\n\n\n\n\n\n\n\n\nAree colorate (grafico a violino):\n\nRappresentano l’intera distribuzione dei pesi per ogni gruppo (specie e genere). Più l’area è larga in un punto, maggiore è il numero di pinguini con quel peso.\n\n\n\nLinee strette al centro (boxplot):\n\nForniscono un riassunto visivo dei dati, mostrando dove i pesi si concentrano maggiormente e quanto variano all’interno di ciascun gruppo.\n\n\n\nCosa possiamo osservare:\n\nPossiamo vedere facilmente se i maschi e le femmine di una stessa specie tendono ad avere pesi simili o differenti, e se c’è una certa sovrapposizione tra i due gruppi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/eda/04_exploring_qualitative_data.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/04_exploring_qualitative_data.html#informazioni-sullambiente-di-sviluppo",
    "title": "10  Esplorare i dati qualitativi",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] vcd_1.4-13        MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt;  [9] bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n#&gt; [13] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n#&gt; [17] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n#&gt; [21] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3        \n#&gt; [25] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gtable_0.3.6      xfun_0.49         htmlwidgets_1.6.4 rstatix_0.7.2    \n#&gt;  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#&gt;  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pacman_0.5.1     \n#&gt; [13] R.oo_1.27.0       pkgconfig_2.0.3   data.table_1.16.2 lifecycle_1.0.4  \n#&gt; [17] compiler_4.4.2    farver_2.1.2      munsell_0.5.1     mnormt_2.1.1     \n#&gt; [21] carData_3.0-5     httpuv_1.6.15     htmltools_0.5.8.1 yaml_2.3.10      \n#&gt; [25] Formula_1.2-5     car_3.1-3         pillar_1.9.0      later_1.4.0      \n#&gt; [29] R.utils_2.12.3    abind_1.4-8       nlme_3.1-166      mime_0.12        \n#&gt; [33] tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4     labeling_0.4.3   \n#&gt; [37] rprojroot_2.0.4   fastmap_1.2.0     colorspace_2.1-1  cli_3.6.3        \n#&gt; [41] magrittr_2.0.3    utf8_1.2.4        broom_1.0.7       withr_3.0.2      \n#&gt; [45] backports_1.5.0   promises_1.3.1    timechange_0.3.0  rmarkdown_2.29   \n#&gt; [49] ggsignif_0.6.4    R.methodsS3_1.8.2 zoo_1.8-12        hms_1.1.3        \n#&gt; [53] shiny_1.9.1       evaluate_1.0.1    lmtest_0.9-40     miniUI_0.1.1.1   \n#&gt; [57] rlang_1.1.4       Rcpp_1.0.13-1     xtable_1.8-4      glue_1.8.0       \n#&gt; [61] jsonlite_1.8.9    R6_2.5.1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Esplorare i dati qualitativi</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html",
    "href": "chapters/mcmc/04_stan_summary_posterior.html",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "",
    "text": "13.1 Introduzione\nPrerequisiti\nConcetti e Competenze Chiave\nPreparazione del Notebook\nL’obiettivo di questo capitolo è illustrare come sintetizzare una distribuzione a posteriori ottenuta attraverso il campionamento MCMC (Markov Chain Monte Carlo). Questa tecnica è fondamentale nell’inferenza bayesiana moderna, permettendo di comprendere e interpretare i parametri di interesse in modo probabilistico.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#introduzione",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#introduzione",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "",
    "text": "13.1.1 Cos’è la Distribuzione a Posteriori?\nLa distribuzione a posteriori rappresenta la nostra conoscenza aggiornata sui parametri di interesse. Essa combina l’informazione iniziale (distribuzione a priori) con le evidenze empiriche (dati osservati) attraverso il modello statistico. È un concetto chiave che distingue l’approccio bayesiano dall’inferenza classica.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#sintesi-della-distribuzione-a-posteriori",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#sintesi-della-distribuzione-a-posteriori",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.2 Sintesi della Distribuzione a Posteriori",
    "text": "13.2 Sintesi della Distribuzione a Posteriori\nIl risultato di un’analisi bayesiana è una distribuzione a posteriori, contenente tutte le informazioni sui parametri dati un modello e un insieme di dati. Pertanto, riassumere la distribuzione a posteriori significa sintetizzare le conseguenze logiche del modello e dei dati analizzati. È prassi comune riportare, per ciascun parametro, una misura di posizione centrale (come la media, la moda o la mediana) per fornire un’idea della localizzazione della distribuzione, accompagnata da una misura di dispersione, quale la deviazione standard, per quantificare l’incertezza delle stime. La deviazione standard è adeguata per distribuzioni simili alla normale, ma può risultare fuorviante per distribuzioni di altra natura, come quelle asimmetriche.\nPer riassumere la dispersione di una distribuzione a posteriori, si utilizza spesso l’Intervallo di Densità Più Alta (HDI, Highest-Density Interval). L’HDI è l’intervallo più breve che contiene una data porzione della densità di probabilità. Ad esempio, se diciamo che l’HDI al 95% per un’analisi è [2, 5], intendiamo che, secondo i nostri dati e modello, il parametro in questione si trova tra 2 e 5 con una probabilità di 0.95. Non vi è nulla di particolare nella scelta del 95%, del 50% o di qualsiasi altro valore; siamo liberi di scegliere, ad esempio, l’intervallo HDI all’89% o al 94% secondo le nostre preferenze. Idealmente, le giustificazioni per queste scelte dovrebbero dipendere dal contesto e non essere automatiche, ma è accettabile stabilire un valore comune come il 95%.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#campionamento-con-stan",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#campionamento-con-stan",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.3 Campionamento con Stan",
    "text": "13.3 Campionamento con Stan\nA scopo illustrativo, immaginiamo di aver condotto un’analisi campionando casualmente 100 opere dal Museum of Modern Art (MoMA) e di aver riscontrato che 14 sono di artisti della Generazione X. Utilizzeremo un modello Beta-Binomiale per affrontare questo problema. Il parametro θ rappresenterà la proporzione di artisti della Generazione X. Adotteremo una distribuzione a priori Beta(4, 6), che riflette un’aspettativa iniziale basata su conoscenze pregresse. I dati osservati (14 opere su 100) verranno utilizzati per aggiornare questa distribuzione iniziale.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#implementazione-in-stan",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#implementazione-in-stan",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.4 Implementazione in Stan",
    "text": "13.4 Implementazione in Stan\nIl seguente codice Stan definisce il nostro modello probabilistico:\n\n# Path to the Stan file\nstan_file &lt;- here::here(\"stan\", \"moma.stan\")\n\n# Create a CmdStanModel object\nmod &lt;- cmdstan_model(stan_file)\n\n\nmod$print()\n#&gt; data {\n#&gt;   int&lt;lower=0&gt; N;           // Numero totale di prove\n#&gt;   int&lt;lower=0&gt; y;           // Numero di successi osservati\n#&gt;   real&lt;lower=0&gt; alpha_prior; // Parametro alpha della distribuzione Beta a priori\n#&gt;   real&lt;lower=0&gt; beta_prior;  // Parametro beta della distribuzione Beta a priori\n#&gt; }\n#&gt; \n#&gt; parameters {\n#&gt;   real&lt;lower=0, upper=1&gt; theta; // Probabilità di successo\n#&gt; }\n#&gt; \n#&gt; model {\n#&gt;   // Distribuzione a priori Beta\n#&gt;   theta ~ beta(alpha_prior, beta_prior);\n#&gt;   \n#&gt;   // Likelihood binomiale\n#&gt;   y ~ binomial(N, theta);\n#&gt; }\n#&gt; \n#&gt; generated quantities {\n#&gt;   real log_lik; // Log-verosimiglianza\n#&gt;   log_lik = binomial_lpmf(y | N, theta);\n#&gt; }\n\nProcediamo con l’implementazione pratica del modello per stimare la proporzione di artisti della Generazione X (θ) al MoMA.\nDefiniamo i dati osservati e i parametri della distribuzione a priori:\n\nN &lt;- 100\ny &lt;- 14\n\nstan_data &lt;- list(\n  N = N,\n  y = y,\n  alpha_prior = 4,\n  beta_prior = 6\n)\n\nEseguiamo il campionamento utilizzando il modello compilato in precedenza:\n\nfit &lt;- mod$sample(\n  data = stan_data,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  iter_sampling = 2000, \n  iter_warmup = 2000,\n  show_messages = FALSE\n)\n\nPer evitare di dover ripetere il campionamento (che può essere computazionalmente costoso), possiamo salvare l’oggetto fit su disco utilizzando la funzione qsave() del pacchetto qs. Questo garantisce un caricamento rapido e senza perdita di dati.\n\n# Save the object to a file.\nqs::qsave(x = fit, file = \"fit_moma.qs\")\n\nSe in seguito vogliamo analizzare i risultati senza dover ripetere il campionamento, possiamo leggere l’oggetto salvato direttamente nel nostro ambiente R:\n\n# Read the object.\nfit2 &lt;- qs::qread(\"fit_moma.qs\")\n\nL’oggetto fit2 è identico a fit e contiene tutte le informazioni relative al modello, ai parametri, ai campioni e ai metadati.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#analisi-della-distribuzione-a-posteriori",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#analisi-della-distribuzione-a-posteriori",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.5 Analisi della distribuzione a posteriori",
    "text": "13.5 Analisi della distribuzione a posteriori\nLa distribuzione a posteriori rappresenta la nostra conoscenza aggiornata riguardo al valore del parametro \\(\\theta\\) dopo aver osservato i dati. Combina le nostre credenze a priori riguardo al parametro (la distribuzione a priori) con le nuove evidenze fornite dai dati osservati (la funzione di verosimiglianza) per ottenere una nuova distribuzione che riflette la nostra comprensione aggiornata del parametro, ovvero la distribuzione a posteriori.\nLa distribuzione a posteriori ci dice quanto sia probabile ogni possibile valore del parametro alla luce dei dati osservati. Un picco stretto indica che i dati sono molto informativi rispetto al parametro, portando a una maggiore certezza nella sua stima. Un picco largo, invece, indica maggiore incertezza.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#esaminare-i-valori-della-distribuzione-a-posteriori",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#esaminare-i-valori-della-distribuzione-a-posteriori",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.6 Esaminare i Valori della Distribuzione a Posteriori",
    "text": "13.6 Esaminare i Valori della Distribuzione a Posteriori\nDopo aver eseguito il campionamento con Stan, possiamo esaminare i valori della distribuzione a posteriori accedendo ai campioni generati.\n\n13.6.1 Estrarre i Campioni in un Oggetto draws_array\n\nIl metodo fit$draws() restituisce i campioni in un oggetto tridimensionale del tipo draws_array, che fa parte del pacchetto posterior.\n\n# Estrazione dei campioni posteriori in formato array (default)\ndraws_arr &lt;- fit2$draws()  \nstr(draws_arr)  # Struttura dell'oggetto\n#&gt;  'draws_array' num [1:2000, 1:4, 1:3] -50 -49.7 -49 -49.1 -51.4 ...\n#&gt;  - attr(*, \"dimnames\")=List of 3\n#&gt;   ..$ iteration: chr [1:2000] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;   ..$ chain    : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n#&gt;   ..$ variable : chr [1:3] \"lp__\" \"theta\" \"log_lik\"\n\nL’output sarà un array 3D con le dimensioni iterazioni × catene × variabili, ovvero il formato standard per i campioni MCMC.\nPer verificare le dimensioni dell’array:\n\ndim(draws_arr)\n#&gt; [1] 2000    4    3\n\nAd esempio, se l’output di dim(draws_arr) restituisce (2000, 4, 3), le dimensioni si riferiscono a:\n\n\n2000: Numero di iterazioni di campionamento per ciascuna catena, specificato dall’argomento iter_sampling = 2000 durante il campionamento.\n\n4: Numero di catene eseguite in parallelo, specificato da chains = 4.\n\n3: Numero di parametri o quantità campionate, inclusi:\n\nParametri definiti nel blocco parameters del modello Stan.\nQuantità trasformate (transformed parameters).\nQuantità generate (generated quantities).\n\n\n\n13.6.2 Interpretazione della Struttura\nL’array 3D permette di accedere ai campioni in modo organizzato:\n\n\nPrima dimensione: Iterazioni per ciascuna catena (es. draws_arr[1,,] restituisce i valori della prima iterazione di tutte le catene).\n\nSeconda dimensione: Catene (es. draws_arr[,1,] restituisce tutti i campioni della prima catena).\n\nTerza dimensione: Variabili campionate (es. draws_arr[,,1] restituisce i valori della prima variabile in tutte le iterazioni e catene).\n\n13.6.3 Accesso ai Parametri\nPer accedere ai campioni di un parametro specifico, possiamo utilizzare il nome del parametro con il metodo fit$draws(format = \"matrix\") o fit$draws(format = \"df\").\n\n# Estrazione dei campioni in formato data frame\ndraws_df &lt;- fit2$draws(format = \"df\")\n\n# Visualizza i primi campioni della variabile \"theta\"\nhead(draws_df$theta)\n#&gt; [1] 0.1187770 0.1252060 0.1680210 0.1477010 0.0971118 0.0866890\n\n\n13.6.4 Sintesi dei Campioni\nPer riassumere i campioni della distribuzione a posteriori:\n\n# Riassunto della distribuzione a posteriori\nfit2$summary()\n#&gt; # A tibble: 3 × 10\n#&gt;   variable    mean  median     sd    mad      q5     q95  rhat ess_bulk\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 lp__     -49.5   -49.2   0.697  0.292  -50.9   -49.0    1.00    3699.\n#&gt; 2 theta      0.164   0.162 0.0345 0.0339   0.112   0.225  1.00    2813.\n#&gt; 3 log_lik   -2.78   -2.46  0.827  0.397   -4.48   -2.17   1.00    3803.\n#&gt; # ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n13.6.5 Vantaggi del Formato draws_array\n\nIl formato draws_array è particolarmente utile per:\n\n\nAnalisi avanzate: Permette di accedere direttamente a specifiche iterazioni, catene e parametri.\n\nVisualizzazioni: È compatibile con funzioni di plotting del pacchetto bayesplot.\n\nControlli diagnostici: Facilita l’analisi della convergenza e della miscelazione delle catene.\n\nCon questa struttura, possiamo esplorare in dettaglio la distribuzione a posteriori generata dal modello Stan.\n\nfit2$metadata()$model_params\n#&gt; [1] \"lp__\"    \"theta\"   \"log_lik\"\n\nRecuperiamo i campioni posteriori per theta:\n\n# draws x variables data frame\ndraws_df &lt;- fit2$draws(format = \"df\")\nhead(draws_df)\n#&gt; # A draws_df: 6 iterations, 1 chains, and 3 variables\n#&gt;   lp__ theta log_lik\n#&gt; 1  -50 0.119    -2.4\n#&gt; 2  -50 0.125    -2.3\n#&gt; 3  -49 0.168    -2.5\n#&gt; 4  -49 0.148    -2.2\n#&gt; 5  -51 0.097    -3.1\n#&gt; 6  -52 0.087    -3.7\n#&gt; # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\ndraws_df$theta |&gt;\n  head()\n#&gt; [1] 0.1187770 0.1252060 0.1680210 0.1477010 0.0971118 0.0866890\n\n\nlength(draws_df$theta)\n#&gt; [1] 8000\n\nGeneriamo un istogramma della distribuzione a posteriori di theta:\n\ndraws_df |&gt;\n  ggplot(aes(theta)) +\n  geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nIn alternativa, possiamo passare l’oggetto fit$draws(\"theta\") alla funzione mcmc_hist() di bayesplot:\n\nmcmc_hist(fit2$draws(\"theta\"))\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nOppure possiamo usare la funzione mcmc_dens_overlay():\n\nmcmc_dens_overlay(fit2$draws(\"theta\"))\n\n\n\n\n\n\n\nLa traccia del campionamento si ottiene nel modo seguente:\n\nmcmc_trace(fit2$draws(\"theta\"))\n\n\n\n\n\n\n\nConfrontiamo la distribuzione a posteriori con la distribuzione a priori di \\(\\theta\\).\n\n# Parameters of the Beta distribution\nalpha &lt;- 4\nbeta_param &lt;- 6\n\n# Create a data frame for the prior Beta distribution\nx &lt;- seq(0, 1, length.out = 1000)\nprior_pdf &lt;- dbeta(x, alpha, beta_param)\nprior_df &lt;- data.frame(theta = x, density = prior_pdf, distribution = \"Prior\")\n\n# Extract posterior draws of theta and calculate density\nposterior_theta &lt;- as.vector(fit2$draws(\"theta\")) # Assuming `fit2$draws(\"theta\")` works\nposterior_density &lt;- density(posterior_theta)\nposterior_df &lt;- data.frame(\n  theta = posterior_density$x,\n  density = posterior_density$y,\n  distribution = \"Posterior\"\n)\n\n# Combine the prior and posterior data\ncombined_df &lt;- bind_rows(prior_df, posterior_df)\n\n# Plot using ggplot2\nggplot(combined_df, aes(x = theta, y = density, color = distribution)) +\n  geom_line(size = 1.2) +\n  labs(\n    x = expression(theta),\n    y = \"Density\",\n    title = \"Prior and Posterior Distributions\"\n  ) +\n  scale_color_manual(\n    values = c(\"Prior\" = \"black\", \"Posterior\" = \"red\"),\n    name = \"Distribution\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(hjust = 0.5)\n  )\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nNel caso presente, la distribuzione a posteriori differisce in maniera importante dalla distribuzione a priori. Ciò indica che i dati hanno avuto un forte impatto sulle nostre credenze riguardo al valore del parametro.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#test-di-ipotesi-bayesiane",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#test-di-ipotesi-bayesiane",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.8 Test di Ipotesi Bayesiane",
    "text": "13.8 Test di Ipotesi Bayesiane\nIn alcune situazioni, descrivere semplicemente la distribuzione a posteriori potrebbe non essere sufficiente. Potremmo dover prendere decisioni pratiche basate sulle inferenze, traducendo stime continue in scelte binarie. Ad esempio, possiamo voler determinare se una terapia è efficace, se un intervento ha avuto successo, o se una proporzione supera una soglia di rilevanza pratica.\nSupponiamo di voler verificare se, nel Museum of Modern Art (MoMA), gli artisti della generazione X (nati tra il 1965 e il 1980) rappresentano meno del 10% del corpus esposto. Analizzando un campione casuale di 100 opere e utilizzando un prior basato su convinzioni pregresse, stimiamo la distribuzione a posteriori della proporzione di artisti, \\(\\theta\\). L’intervallo di credibilità (Credible Interval, CI) al 94% risulta compreso tra 0.104 e 0.235.\nLa nostra ipotesi iniziale è che la proporzione di artisti della generazione X sia inferiore al 10%, cioè \\(\\theta &lt; 0.1\\). Tuttavia, il fatto che l’intero intervallo di credibilità al 94% si trovi sopra la soglia del 10% contraddice questa ipotesi. Per quantificare ulteriormente la compatibilità dei dati con questa ipotesi, calcoliamo la probabilità a posteriori che \\(\\theta &lt; 0.1\\).\n\nfit2$summary(\"theta\", pr_lt_01 = ~ mean(. &lt;= 0.1))\n#&gt; # A tibble: 1 × 2\n#&gt;   variable pr_lt_01\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 theta      0.0213\n\nLa probabilità a posteriori che \\(\\theta &lt; 0.1\\)** è molto bassa, ovvero \\(P(\\theta &lt; 0.1) = 0.0213\\), fornendo ulteriori evidenze contro l’ipotesi che gli artisti della generazione X rappresentino meno del 10% del corpus esposto.\nIn sintesi, sulla base dei dati e del modello, l’ipotesi che gli artisti della generazione X rappresentino meno del 10% non è supportata. Al contrario, i dati indicano, con un livello di certezza soggettiva del 94%, che la proporzione di artisti appartenenti a questa generazione è molto probabilmente superiore al 10%.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#riflessioni-conclusive",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#riflessioni-conclusive",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.9 Riflessioni conclusive",
    "text": "13.9 Riflessioni conclusive\nLa crescente popolarità dei metodi bayesiani in psicologia e nelle scienze sociali è stata fortemente influenzata dalla (ri)scoperta di algoritmi numerici capaci di stimare le distribuzioni a posteriori dei parametri del modello a partire dai dati osservati. Prima di questi sviluppi, ottenere misure riassuntive delle distribuzioni a posteriori, soprattutto per modelli complessi con molti parametri, era praticamente impossibile.\nQuesto capitolo fornisce un’introduzione a cmdstanr, che permette di compilare ed eseguire modelli probabilistici espressi in linguaggio Stan. Grazie a questa tecnologia, è possibile generare una stima della distribuzione a posteriori attraverso il campionamento Markov Chain Monte Carlo (MCMC), rivoluzionando la capacità di effettuare inferenze bayesiane e rendendo l’analisi di modelli complessi più accessibile e gestibile.\nInoltre, nel capitolo sono state presentate diverse strategie per la trasformazione della distribuzione a posteriori e sono state esplorate modalità per ottenere intervalli di credibilità. Successivamente, è stata discussa l’analisi delle ipotesi a posteriori, che consente di confrontare due ipotesi contrapposte riguardanti il parametro \\(\\theta\\).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#informazioni-sullambiente-di-sviluppo",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] see_0.9.0           insight_1.0.0       bayestestR_0.15.0  \n#&gt;  [4] rstanarm_2.32.1     Rcpp_1.0.13-1       qs_0.27.2          \n#&gt;  [7] posterior_1.6.0     cmdstanr_0.8.1.9000 MASS_7.3-61        \n#&gt; [10] viridis_0.6.5       viridisLite_0.4.2   ggpubr_0.6.0       \n#&gt; [13] ggExtra_0.10.1      gridExtra_2.3       patchwork_1.3.0    \n#&gt; [16] bayesplot_1.11.1    psych_2.4.6.26      scales_1.3.0       \n#&gt; [19] markdown_1.13       knitr_1.49          lubridate_1.9.3    \n#&gt; [22] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \n#&gt; [25] purrr_1.0.2         readr_2.1.5         tidyr_1.3.1        \n#&gt; [28] tibble_3.2.1        ggplot2_3.5.1       tidyverse_2.0.0    \n#&gt; [31] rio_1.2.3           here_1.0.1         \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] tensorA_0.36.2.1     jsonlite_1.8.9       datawizard_0.13.0   \n#&gt;   [4] magrittr_2.0.3       nloptr_2.1.1         farver_2.1.2        \n#&gt;   [7] rmarkdown_2.29       vctrs_0.6.5          minqa_1.2.8         \n#&gt;  [10] base64enc_0.1-3      rstatix_0.7.2        htmltools_0.5.8.1   \n#&gt;  [13] distributional_0.5.0 curl_6.0.1           broom_1.0.7         \n#&gt;  [16] Formula_1.2-5        StanHeaders_2.32.10  htmlwidgets_1.6.4   \n#&gt;  [19] plyr_1.8.9           zoo_1.8-12           igraph_2.1.1        \n#&gt;  [22] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n#&gt;  [25] colourpicker_1.3.0   Matrix_1.7-1         R6_2.5.1            \n#&gt;  [28] fastmap_1.2.0        shiny_1.9.1          digest_0.6.37       \n#&gt;  [31] colorspace_2.1-1     ps_1.8.1             rprojroot_2.0.4     \n#&gt;  [34] crosstalk_1.2.1      labeling_0.4.3       fansi_1.0.6         \n#&gt;  [37] timechange_0.3.0     abind_1.4-8          compiler_4.4.2      \n#&gt;  [40] withr_3.0.2          backports_1.5.0      inline_0.3.20       \n#&gt;  [43] shinystan_2.6.0      carData_3.0-5        QuickJSR_1.4.0      \n#&gt;  [46] pkgbuild_1.4.5       ggsignif_0.6.4       gtools_3.9.5        \n#&gt;  [49] loo_2.8.0            tools_4.4.2          httpuv_1.6.15       \n#&gt;  [52] threejs_0.3.3        glue_1.8.0           nlme_3.1-166        \n#&gt;  [55] promises_1.3.1       grid_4.4.2           checkmate_2.3.2     \n#&gt;  [58] reshape2_1.4.4       generics_0.1.3       gtable_0.3.6        \n#&gt;  [61] tzdb_0.4.0           data.table_1.16.2    RApiSerialize_0.1.4 \n#&gt;  [64] hms_1.1.3            stringfish_0.16.0    car_3.1-3           \n#&gt;  [67] utf8_1.2.4           pillar_1.9.0         later_1.4.0         \n#&gt;  [70] splines_4.4.2        lattice_0.22-6       survival_3.7-0      \n#&gt;  [73] tidyselect_1.2.1     miniUI_0.1.1.1       V8_6.0.0            \n#&gt;  [76] stats4_4.4.2         xfun_0.49            matrixStats_1.4.1   \n#&gt;  [79] DT_0.33              rstan_2.32.6         stringi_1.8.4       \n#&gt;  [82] yaml_2.3.10          pacman_0.5.1         boot_1.3-31         \n#&gt;  [85] evaluate_1.0.1       codetools_0.2-20     cli_3.6.3           \n#&gt;  [88] RcppParallel_5.1.9   shinythemes_1.2.0    xtable_1.8-4        \n#&gt;  [91] processx_3.8.4       munsell_0.5.1        parallel_4.4.2      \n#&gt;  [94] rstantools_2.4.0     dygraphs_1.1.1.6     lme4_1.1-35.5       \n#&gt;  [97] ggridges_0.5.6       xts_0.14.1           rlang_1.1.4         \n#&gt; [100] mnormt_2.1.1         shinyjs_2.1.0",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/mcmc/04_stan_summary_posterior.html#intervallo-di-credibilità",
    "href": "chapters/mcmc/04_stan_summary_posterior.html#intervallo-di-credibilità",
    "title": "13  Metodi di sintesi della distribuzione a posteriori",
    "section": "\n13.7 Intervallo di Credibilità",
    "text": "13.7 Intervallo di Credibilità\nGli intervalli di credibilità sono uno strumento fondamentale nell’inferenza bayesiana per riassumere l’incertezza sui parametri stimati. Esistono due metodi principali per calcolare gli intervalli di credibilità:\n\n\nHighest Density Interval (HDI): definisce l’intervallo più stretto che contiene la probabilità specificata, includendo le aree di maggiore densità della distribuzione a posteriori.\n\nEqual-tailed Interval (ETI): lascia una probabilità uguale (ad esempio, il 2,5% per un intervallo al 95%) in entrambe le code della distribuzione.\n\nQuesti metodi producono risultati identici in caso di distribuzioni simmetriche ma differiscono per distribuzioni asimmetriche. Di seguito vediamo come interpretare e calcolare questi intervalli.\nDistribuzione Simmetrica\nCon una distribuzione a posteriori simmetrica, come quella normale, gli intervalli HDI ed ETI coincidono.\nEsempio di calcolo:\n\n# Genera una distribuzione normale\nposterior &lt;- distribution_normal(1000)\n\n# Calcola HDI ed ETI\nci_hdi &lt;- ci(posterior, method = \"HDI\")\nci_eti &lt;- ci(posterior, method = \"ETI\")\n\n# Visualizza la distribuzione con i limiti degli intervalli\nout &lt;- estimate_density(posterior, extend = TRUE)\nggplot(out, aes(x = x, y = y)) +\n  geom_area(fill = \"orange\") +\n  # HDI in blu\n  geom_vline(xintercept = ci_hdi$CI_low, color = \"royalblue\", linewidth = 3) +\n  geom_vline(xintercept = ci_hdi$CI_high, color = \"royalblue\", linewidth = 3) +\n  # ETI in rosso\n  geom_vline(xintercept = ci_eti$CI_low, color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = ci_eti$CI_high, color = \"red\", linewidth = 1)\n\n\n\n\n\n\n\nDistribuzione Asimmetrica\nQuando la distribuzione a posteriori è asimmetrica, come una distribuzione beta, l’HDI è generalmente più stretto rispetto all’ETI poiché privilegia le regioni di maggiore densità.\nEsempio di calcolo:\n\n# Genera una distribuzione beta\nposterior &lt;- distribution_beta(1000, 6, 2)\n\n# Calcola HDI ed ETI\nci_hdi &lt;- ci(posterior, method = \"HDI\")\nci_eti &lt;- ci(posterior, method = \"ETI\")\n\n# Visualizza la distribuzione con i limiti degli intervalli\nout &lt;- estimate_density(posterior, extend = TRUE)\nggplot(out, aes(x = x, y = y)) +\n  geom_area(fill = \"orange\") +\n  # HDI in blu\n  geom_vline(xintercept = ci_hdi$CI_low, color = \"royalblue\", linewidth = 3) +\n  geom_vline(xintercept = ci_hdi$CI_high, color = \"royalblue\", linewidth = 3) +\n  # ETI in rosso\n  geom_vline(xintercept = ci_eti$CI_low, color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = ci_eti$CI_high, color = \"red\", linewidth = 1)\n\n\n\n\n\n\n\n\n13.7.1 Interpretazione dell’Intervallo di Credibilità\nL’intervallo di credibilità bayesiano offre una chiara interpretazione probabilistica: dato il modello e i dati, c’è una probabilità specificata (ad esempio, il 94%) che il parametro si trovi all’interno dell’intervallo calcolato.\n\n13.7.2 Calcolo degli intervalli in R\n\n\nHDI:\n\n\nbayestestR::ci(fit2$draws(\"theta\"), method = \"HDI\")\n#&gt; Highest Density Interval\n#&gt; \n#&gt; Parameter |      95% HDI\n#&gt; ------------------------\n#&gt; theta     | [0.10, 0.23]\n\n\n\nETI:\n\n\nbayestestR::ci(fit2$draws(\"theta\"), method = \"ETI\")\n#&gt; Equal-Tailed Interval\n#&gt; \n#&gt; Parameter |      95% ETI\n#&gt; ------------------------\n#&gt; theta     | [0.10, 0.24]\n\n\n13.7.3 Visualizzazione grafica\nUtilizzando il pacchetto bayesplot possiamo rappresentare la distribuzione a posteriori con l’HDI:\n\nmcmc_areas(fit2$draws(\"theta\"), \n           prob = 0.94) +  # Specifica il livello dell'HDI\n  ggtitle(\"Distribuzione a Posteriori di Theta con HDI al 94%\") +\n  xlab(expression(theta)) +\n  ylab(\"Densità\")\n\n\n\n\n\n\n\n\n13.7.4 Confronto con gli Intervalli di Confidenza Frequentisti\nGli intervalli di credibilità bayesiani si distinguono nettamente dagli intervalli di confidenza frequentisti. Mentre gli intervalli frequentisti si basano su una prospettiva a lungo termine (ovvero, il 95% degli intervalli costruiti su infiniti campioni includerebbe il vero valore), gli intervalli bayesiani:\n\n\nHanno una chiara interpretazione probabilistica: dato il modello e i dati, indicano direttamente la probabilità che il parametro sia nell’intervallo.\n\nIncorporano credenze a priori, combinate con l’evidenza fornita dai dati.\n\n13.7.5 Scelta del Livello dell’Intervallo (89% vs 95%)\nUna discussione comune nell’inferenza bayesiana riguarda il livello predefinito degli intervalli. Sebbene il 95% sia un valore convenzionale mutuato dal frequentismo, alcune evidenze suggeriscono che livelli più bassi (ad esempio, 89%) possano essere più stabili per le distribuzioni a posteriori, specialmente con un numero limitato di campioni posteriori (Kruschke, 2014).\n\n\nVantaggi del 95%:\n\nRelazione intuitiva con la deviazione standard.\nMaggiore probabilità di includere 0, rendendo le analisi più conservative.\n\n\n\nVantaggi dell’89%:\n\nMaggiore stabilità con campioni posteriori limitati.\nEvita l’arbitrarietà del valore 95% (McElreath, 2018).\n\n\n\nIn conclusione, la scelta tra HDI e ETI, così come il livello dell’intervallo, dipende dagli obiettivi e dal contesto dell’analisi. Gli intervalli di credibilità offrono un approccio flessibile e intuitivo per sintetizzare l’incertezza, adattandosi alle esigenze di analisi sia esplorative che confermative.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Metodi di sintesi della distribuzione a posteriori</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html",
    "href": "chapters/eda/00_introduction_r.html",
    "title": "6  Introduzione a R",
    "section": "",
    "text": "6.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#introduzione",
    "href": "chapters/eda/00_introduction_r.html#introduzione",
    "title": "6  Introduzione a R",
    "section": "",
    "text": "6.1.1 Perché usare R nella Data Analysis?\nR è uno dei linguaggi di programmazione più utilizzati nella data analysis, grazie alla sua flessibilità, potenza e vasta comunità di supporto. È stato progettato specificamente per l’analisi statistica e il data visualization, ed è particolarmente apprezzato in ambito accademico e scientifico.\nNegli ultimi anni, la crisi della replicabilità ha evidenziato problemi significativi nella scienza: molte analisi non possono essere riprodotte, compromettendo la fiducia nei risultati pubblicati (Nosek et al., 2022). Per affrontare questa sfida, è essenziale adottare strumenti che garantiscano trasparenza e riproducibilità. R soddisfa queste esigenze grazie a tre caratteristiche fondamentali:\n\n\nScripting e documentazione: Ogni analisi è basata su script che possono essere facilmente salvati, condivisi e rivisti. Questo permette di replicare ogni passaggio dell’analisi.\n\nFlessibilità: Con R è possibile creare flussi di lavoro personalizzati e adattarli a nuovi dati o problemi.\n\nIntegrazione con strumenti di reporting: Grazie a piattaforme come R Markdown o Quarto, è possibile integrare analisi, codice e risultati in documenti unici e dinamici.\n\nA differenza di software basati su interfacce grafiche (GUI), che rischiano di introdurre errori nascosti nei flussi di lavoro, R favorisce una programmazione esplicita, riducendo l’ambiguità e aumentando la verificabilità delle analisi.\nUtilizzare R significa non solo imparare uno strumento tecnico, ma adottare un approccio rigoroso e replicabile alla scienza, rispondendo alle esigenze di una ricerca moderna e affidabile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/00_introduction_r.html#informazioni-sullambiente-di-sviluppo",
    "title": "6  Introduzione a R",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [5] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt;  [9] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [13] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [17] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [21] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gtable_0.3.6      xfun_0.49         htmlwidgets_1.6.4 rstatix_0.7.2    \n#&gt;  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#&gt;  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pacman_0.5.1     \n#&gt; [13] pkgconfig_2.0.3   lifecycle_1.0.4   compiler_4.4.2    farver_2.1.2     \n#&gt; [17] munsell_0.5.1     mnormt_2.1.1      carData_3.0-5     httpuv_1.6.15    \n#&gt; [21] htmltools_0.5.8.1 yaml_2.3.10       Formula_1.2-5     car_3.1-3        \n#&gt; [25] pillar_1.9.0      later_1.4.0       abind_1.4-8       nlme_3.1-166     \n#&gt; [29] mime_0.12         tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4    \n#&gt; [33] rprojroot_2.0.4   fastmap_1.2.0     grid_4.4.2        colorspace_2.1-1 \n#&gt; [37] cli_3.6.3         magrittr_2.0.3    utf8_1.2.4        broom_1.0.7      \n#&gt; [41] withr_3.0.2       backports_1.5.0   promises_1.3.1    timechange_0.3.0 \n#&gt; [45] rmarkdown_2.29    ggsignif_0.6.4    hms_1.1.3         shiny_1.9.1      \n#&gt; [49] evaluate_1.0.1    miniUI_0.1.1.1    rlang_1.1.4       Rcpp_1.0.13-1    \n#&gt; [53] xtable_1.8-4      glue_1.8.0        jsonlite_1.8.9    R6_2.5.1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#usare-le-funzioni-in-r",
    "href": "chapters/eda/00_introduction_r.html#usare-le-funzioni-in-r",
    "title": "6  Introduzione a R",
    "section": "\n6.3 Usare le funzioni in R",
    "text": "6.3 Usare le funzioni in R\nFino ad ora abbiamo creato oggetti semplici assegnando loro direttamente un valore. Con l’aumento dell’esperienza in R, potresti voler creare oggetti più complessi. Per aiutarti, R offre numerose funzioni già disponibili nella sua installazione di base, e altre possono essere aggiunte installando pacchetti. Una funzione è un insieme di istruzioni che eseguono un compito specifico. Inoltre, è possibile creare funzioni personalizzate.\n\n6.3.1 La funzione c() per creare vettori\nLa prima funzione utile da imparare è c(), che serve a concatenare valori in un vettore. Ad esempio:\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\nQuesto codice crea un oggetto chiamato my_vec che contiene una sequenza di numeri. Alcuni concetti fondamentali sulle funzioni in R:\n\n\nNome e parentesi: Le funzioni in R sono sempre seguite da parentesi tonde ().\n\nArgomenti: Gli elementi passati alla funzione (tra le parentesi) ne personalizzano il comportamento.\n\nPer vedere il contenuto del vettore:\nmy_vec\n## [1] 2 3 1 6 4 3 3 7\n\n6.3.2 Funzioni per analizzare vettori\nPuoi utilizzare altre funzioni per calcolare statistiche sul vettore:\nmean(my_vec)    # Media\n## [1] 3.625\nvar(my_vec)     # Varianza\n## [1] 3.982143\nsd(my_vec)      # Deviazione standard\n## [1] 1.995531\nlength(my_vec)  # Numero di elementi\n## [1] 8\nPuoi anche salvare i risultati in nuovi oggetti per riutilizzarli:\nvec_mean &lt;- mean(my_vec)\nvec_mean\n## [1] 3.625\n\n6.3.3 Creare sequenze regolari\nPer creare sequenze di numeri in passi regolari, puoi usare i seguenti comandi:\n\n\nSimbolo : per sequenze semplici:\nmy_seq &lt;- 1:10\n## [1]  1  2  3  4  5  6  7  8  9 10\n\n\nFunzione seq() per maggiore controllo:\nmy_seq2 &lt;- seq(from = 1, to = 5, by = 0.5)\n## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\n6.3.4 Ripetere valori\nPuoi ripetere valori o sequenze con la funzione rep():\n\n\nRipetere un valore:\nmy_seq3 &lt;- rep(2, times = 10)\n## [1] 2 2 2 2 2 2 2 2 2 2\n\n\nRipetere una sequenza:\nmy_seq5 &lt;- rep(1:5, times = 3)\n## [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n\nRipetere ogni elemento di una sequenza:\nmy_seq6 &lt;- rep(1:5, each = 3)\n## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\n\n6.3.5 Annidare funzioni\nÈ possibile combinare funzioni per creare comandi più complessi, come nell’esempio:\nmy_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3)\n## [1]  3  3  3  1  1  1 10 10 10  7  7  7\nPer maggiore leggibilità, puoi separare i passaggi:\nin_vec &lt;- c(3, 1, 10, 7)\nmy_seq7 &lt;- rep(in_vec, each = 3)\n## [1]  3  3  3  1  1  1 10 10 10  7  7  7\nQuesta pratica facilita la comprensione del codice e lo rende più chiaro.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#lavorare-con-i-vettori-in-r",
    "href": "chapters/eda/00_introduction_r.html#lavorare-con-i-vettori-in-r",
    "title": "6  Introduzione a R",
    "section": "\n6.4 Lavorare con i vettori in R",
    "text": "6.4 Lavorare con i vettori in R\nIn R, i vettori sono uno degli elementi fondamentali per manipolare, riassumere e ordinare i dati. Qui trovi una panoramica su come estrarre, sostituire, ordinare, lavorare con dati mancanti e sfruttare la vettorizzazione dei vettori.\n\n6.4.1 Estrarre elementi da un vettore\nPuoi estrarre uno o più elementi da un vettore usando le parentesi quadre [ ].\n\n\nPer posizione: Specifica la posizione degli elementi.\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\nmy_vec[3]  # Terzo elemento\n## [1] 1\nmy_vec[c(1, 5, 6)]  # Elementi 1°, 5° e 6°\n## [1] 2 4 3\nmy_vec[3:8]  # Da 3° a 8°\n## [1] 1 6 4 3 3 7\n\n\nCon condizioni logiche: Usa espressioni logiche per selezionare elementi.\nmy_vec[my_vec &gt; 4]  # Elementi &gt; 4\n## [1] 6 7\nmy_vec[my_vec &lt;= 4]  # Elementi ≤ 4\n## [1] 2 3 1 4 3 3\nmy_vec[my_vec != 4]  # Elementi diversi da 4\n## [1] 2 3 1 6 3 3 7\n\n\nOperatori logici: Combina condizioni con & (AND) e | (OR).\nmy_vec[my_vec &gt; 2 & my_vec &lt; 6]  # Tra 2 e 6\n## [1] 3 4 3 3\n\n\n6.4.2 Sostituire elementi in un vettore\nPuoi modificare i valori di un vettore usando [ ] e l’operatore &lt;-.\n\n\nUn singolo elemento:\nmy_vec[4] &lt;- 500  # Cambia il 4° elemento\n## [1]   2   3   1 500   4   3   3   7\n\n\nPiù elementi:\nmy_vec[c(6, 7)] &lt;- 100  # Cambia il 6° e 7° elemento\n## [1]   2   3   1 500   4 100 100   7\n\n\nCon condizioni logiche:\nmy_vec[my_vec &lt;= 4] &lt;- 1000  # Cambia valori ≤ 4\n## [1] 1000 1000 1000  500 1000  100  100    7\n\n\n6.4.3 Ordinare un vettore\n\n\nDal più piccolo al più grande:\nvec_sort &lt;- sort(my_vec)\n## [1]    7  100  100  500 1000 1000 1000 1000\n\n\nDal più grande al più piccolo:\nvec_sort2 &lt;- sort(my_vec, decreasing = TRUE)\n## [1] 1000 1000 1000 1000  500  100  100    7\n\n\nOrdinare un vettore in base a un altro:\nheight &lt;- c(180, 155, 160, 167, 181)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\nheight_ord &lt;- order(height)\nnames_ord &lt;- p.names[height_ord]\n## [1] \"Charlotte\" \"Helen\"     \"Karen\"     \"Joanna\"    \"Amy\"\n\n\n6.4.4 Operazioni vettoriali (vettorizzazione)\nLe funzioni in R sono vettorizzate, cioè operano automaticamente su tutti gli elementi di un vettore senza bisogno di un ciclo.\n\n\nOperazioni aritmetiche:\nmy_vec2 &lt;- c(3, 5, 7, 1, 9, 20)\nmy_vec2 * 5  # Moltiplica ogni elemento per 5\n## [1]  15  25  35   5  45 100\n\n\nOperazioni tra vettori:\nmy_vec3 &lt;- c(17, 15, 13, 19, 11, 0)\nmy_vec2 + my_vec3  # Somma due vettori\n## [1] 20 20 20 20 20 20\n\n\nRiciclo (recycling): Attenzione quando i vettori hanno lunghezze diverse.\nmy_vec4 &lt;- c(1, 2)\nmy_vec2 + my_vec4  # R ricicla gli elementi del vettore più corto\n## [1]  4  7  8  3 10 22\n\n\n6.4.5 Gestire dati mancanti (NA)\nR rappresenta i dati mancanti con NA. La gestione dei dati mancanti dipende dalla funzione utilizzata.\n\n\nCalcolo con dati mancanti:\ntemp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5)\nmean(temp)  # Restituisce NA\n## [1] NA\n\nmean(temp, na.rm = TRUE)  # Ignora i valori mancanti\n## [1] 6.2875\n\n\nNota: na.rm = TRUE è un argomento comune per ignorare i NA, ma non tutte le funzioni lo supportano. Consulta la documentazione della funzione per verificare come gestisce i dati mancanti.\nIn conclusione, manipolare vettori è un’abilità essenziale in R. Dalla selezione e modifica degli elementi all’ordinamento e gestione di dati mancanti, queste tecniche sono alla base dell’analisi dei dati in R.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#i-dati-in-r",
    "href": "chapters/eda/00_introduction_r.html#i-dati-in-r",
    "title": "6  Introduzione a R",
    "section": "\n6.5 I dati in R",
    "text": "6.5 I dati in R\nIn R, i dati possono essere rappresentati in diversi tipi e strutture. Comprendere come gestirli è fondamentale per manipolare, analizzare e riassumere i dataset più complessi.\n\n6.5.1 Tipi di dati in R\nR supporta diversi tipi di dati:\n\n\nNumeric: Numeri decimali (es. 2.5).\n\nInteger: Numeri interi (es. 3).\n\nLogical: Valori booleani (TRUE o FALSE) e NA per dati mancanti.\n\nCharacter: Stringhe di testo (es. \"hello\").\n\nFactor: Variabili categoriche (es. livelli come \"low\", \"medium\", \"high\").\n\nPuoi verificare il tipo di un oggetto con class() e controllare se appartiene a un tipo specifico con funzioni come is.numeric(). È anche possibile convertire un tipo in un altro con funzioni come as.character().\n\n6.5.2 Strutture di dati in R\n\n\nVettori: Contengono dati dello stesso tipo (es. numeri, stringhe o logici).\nmy_vec &lt;- c(1, 2, 3)\n\n\nMatrici e array: Strutture bidimensionali (matrici) o multidimensionali (array) con dati dello stesso tipo.\nCreare una matrice:\nmy_mat &lt;- matrix(1:12, nrow = 3, byrow = TRUE)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\nOperazioni utili:\n\n\nTrasposizione: t(my_mat)\n\n\nDiagonale: diag(my_mat)\n\n\nMoltiplicazione matriciale: mat1 %*% mat2\n\n\n\n\nListe: Possono contenere elementi di tipi diversi, inclusi vettori, matrici o altre liste.\nmy_list &lt;- list(numbers = c(1, 2), text = \"hello\", mat = matrix(1:4, nrow = 2))\nmy_list$numbers  # Accedi agli elementi con il nome\n\n\nData frame: Strutture bidimensionali che possono contenere colonne di tipi diversi. Ideale per dataset strutturati.\nCreare un data frame:\nheight &lt;- c(180, 155, 160)\nweight &lt;- c(65, 50, 52)\nnames &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\")\n\ndataf &lt;- data.frame(height = height, weight = weight, names = names)\nstr(dataf)  # Mostra la struttura del data frame\n## 'data.frame':    3 obs. of  3 variables:\n##  $ height: num  180 155 160\n##  $ weight: num  65 50 52\n##  $ names : chr  \"Joanna\" \"Charlotte\" \"Helen\"\nPer convertire le stringhe in fattori durante la creazione:\ndataf &lt;- data.frame(height = height, weight = weight, names = names, stringsAsFactors = TRUE)\n\n\n6.5.3 Operazioni utili sui data frame\n\n\nVerificare dimensioni: dim(dataf)\n\n\nVisualizzare struttura: str(dataf)\n\n\nAccedere a colonne: dataf$height\n\n\n6.5.4 Considerazioni\n\nLe matrici e gli array sono utili per dati numerici omogenei.\n\nI data frame sono più flessibili, permettendo dati di tipi diversi.\n\nLe liste sono ideali per strutture dati irregolari o gerarchiche.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#importare-dati-in-r-con-rioimport",
    "href": "chapters/eda/00_introduction_r.html#importare-dati-in-r-con-rioimport",
    "title": "6  Introduzione a R",
    "section": "\n6.6 Importare dati in R con rio::import()\n",
    "text": "6.6 Importare dati in R con rio::import()\n\nIn R, è possibile creare un data frame importando dati da file esterni, che rappresenta il metodo più comune per iniziare un’analisi. Per fare questo, è importante che i dati siano ben formattati e salvati in un formato riconosciuto da R.\nLa funzione rio::import() del pacchetto rio è uno strumento generale e molto pratico per importare dati da una vasta gamma di formati di file senza dover ricordare funzioni specifiche per ciascun tipo di file.\n\n\nSupporto per diversi formati: Riconosce automaticamente il tipo di file (es. .csv, .xlsx, .sav, .json) in base all’estensione.\n\nFacilità d’uso: Richiede solo il percorso del file come argomento principale.\n\nIn sintesi, rio::import() è una funzione versatile e potente che semplifica l’importazione dei dati in R, rendendola una scelta eccellente per lavorare con dataset di vario tipo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#bibliografia",
    "href": "chapters/eda/00_introduction_r.html#bibliografia",
    "title": "6  Introduzione a R",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., et al. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73(1), 719–748.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/00_introduction_r.html#oggetti-in-r",
    "href": "chapters/eda/00_introduction_r.html#oggetti-in-r",
    "title": "6  Introduzione a R",
    "section": "\n6.2 Oggetti in R",
    "text": "6.2 Oggetti in R\nIn R, tutto è un oggetto: dai numeri o stringhe di testo semplici, fino a strutture più complesse come grafici, riassunti di analisi statistiche o script che eseguono compiti specifici. Creare e assegnare valori agli oggetti è fondamentale per lavorare in R.\n\n6.2.1 Creare oggetti\nPer creare un oggetto, basta assegnargli un nome e un valore usando l’operatore di assegnazione &lt;-:\n\nmy_obj &lt;- 48\n\nIn questo esempio, abbiamo creato un oggetto chiamato my_obj e gli abbiamo assegnato il valore 48. Anche l’operatore = può essere usato, ma è considerato una cattiva pratica.\nPer visualizzare il valore di un oggetto, basta scriverne il nome:\n\nmy_obj\n#&gt; [1] 48\n\nGli oggetti creati vengono memorizzati nell’ambiente di lavoro. In RStudio, puoi visualizzarli nella scheda Environment e ottenere dettagli come tipo, lunghezza e valore.\nÈ possibile assegnare a un oggetto anche una stringa di testo, racchiudendola tra virgolette:\n\nmy_obj2 &lt;- \"R è fantastico\"\nmy_obj2\n#&gt; [1] \"R è fantastico\"\n\nSe dimentichi le virgolette, R mostrerà un errore.\nPer modificare il valore di un oggetto esistente, basta riassegnarlo:\n\nmy_obj2 &lt;- 1024\n\nOra il tipo di my_obj2 è cambiato da carattere a numerico. È anche possibile usare oggetti per crearne di nuovi:\n\nmy_obj3 &lt;- my_obj + my_obj2\nmy_obj3\n#&gt; [1] 1072\n\nSe provi a sommare oggetti di tipo diverso, R restituirà un errore:\nchar_obj &lt;- \"ciao\"\nchar_obj2 &lt;- \"mondo\"\nchar_obj3 &lt;- char_obj + char_obj2\n# Error in char_obj + char_obj2 : non-numeric argument to binary operator\nQuando incontri errori come questo, cerca su Google la spiegazione del messaggio, per esempio: “non-numeric argument to binary operator error + r”. Un errore comune è anche:\nmy_obj &lt;- 48\nmy_obj4 &lt;- my_obj + no_obj\n# Error: object 'no_obj' not found\nR segnala che no_obj non è stato definito e, di conseguenza, l’oggetto my_obj4 non è stato creato.\n\n6.2.2 Nomi degli oggetti\nDare un nome agli oggetti può sembrare banale, ma è importante scegliere nomi brevi e informativi. Usa un formato coerente, come:\n\n\nSnake case: output_summary\n\n\nDot case: output.summary\n\n\nCamel case: outputSummary\n\n\nEvita di iniziare i nomi con numeri (es. 2my_variable) o caratteri speciali (&, ^, /, ecc.). Inoltre, non usare parole riservate (es. TRUE, NA) o nomi di funzioni già esistenti (es. data).\nEsempio da evitare:\ndata &lt;- read.table(\"mydatafile\", header = TRUE) # `data` è già una funzione!",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduzione a R</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html",
    "href": "chapters/eda/05_exploring_numeric_data.html",
    "title": "11  Esplorare i dati numerici",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nIn questo capitolo ci concentreremo sull’analisi dei dati numerici. In particolare, esamineremo le distribuzioni di frequenza e i quantili, insieme alle tecniche di visualizzazione più comuni, come l’istogramma, l’istogramma smussato e il box-plot. Tratteremo sia gli aspetti computazionali che quelli interpretativi di queste misure, fornendo strumenti utili non solo per una comprensione personale, ma anche per la comunicazione efficace dei risultati, in particolare con chi utilizza questi dati per prendere decisioni pratiche nel mondo reale.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#i-dati-sulle-aspettative-negative-nella-depressione",
    "href": "chapters/eda/05_exploring_numeric_data.html#i-dati-sulle-aspettative-negative-nella-depressione",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.1 I dati sulle aspettative negative nella depressione",
    "text": "11.1 I dati sulle aspettative negative nella depressione\nSupponimo di essere interessati alla distribuzione di una singola variabili quantitativa. Per fare un esempio, consideriamo i dati sulle aspettative negative come meccanismo chiave nel mantenimento della depressione (Zetsche et al., 2019). Importiamo i dati:\n\ndf = rio::import(here::here(\"data\", \"data.mood.csv\"))\n\nPer questo esercizio, ci concentreremo sulle colonne esm_id (il codice del soggetto), group (il gruppo) e bdi (il valore BDI-II).\n\ndf &lt;- df |&gt; \n  dplyr::select(\"esm_id\", \"group\", \"bdi\")\ndf |&gt; \n  head()\n#&gt;   esm_id group bdi\n#&gt; 1     10   mdd  25\n#&gt; 2     10   mdd  25\n#&gt; 3     10   mdd  25\n#&gt; 4     10   mdd  25\n#&gt; 5     10   mdd  25\n#&gt; 6     10   mdd  25\n\nSe elenchiamo le modalità presenti in group utilizzando il metodo unique(), scopriamo che corrispondono a mdd (pazienti) e ctl (controlli sani).\n\ndf$group |&gt; \n  unique()\n#&gt; [1] \"mdd\" \"ctl\"\n\nRimuoviamo i duplicati per ottenere un unico valore BDI-II per ogni soggetto:\n\ndf &lt;- df[!duplicated(df), ]\n\nVerifichiamo di avere ottenuto il risultato desiderato.\n\ndim(df)\n#&gt; [1] 67  3\n\n\nhead(df)\n#&gt;    esm_id group bdi\n#&gt; 1      10   mdd  25\n#&gt; 15      9   mdd  30\n#&gt; 30      6   mdd  26\n#&gt; 46      7   mdd  35\n#&gt; 65     12   mdd  44\n#&gt; 83     16   mdd  30\n\nSi noti che il nuovo DataFrame (con 67 righe) conserva il “nome” delle righe (ovvero, l’indice di riga) del DataFrame originario (con 1188 righe). Per esempio, il secondo soggetto (con codice identificativo 9) si trova sulla seconda riga del DataFrame, ma il suo indice di riga è 15. Questo non ha nessuna conseguenza perché non useremo l’indice di riga nelle analisi seguenti.\nEliminiamo eventuali valori mancanti:\n\ndf &lt;- df[!is.na(df$bdi), ]\n\nOtteniamo così il DataFrame finale per gli scopi presenti (66 righe e 3 colonne):\n\ndim(df)\n#&gt; [1] 66  3\n\nStampiamo i valori BDI-II presentandoli ordinati dal più piccolo al più grande:\n\ndf$bdi |&gt; \n  sort()\n#&gt;  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1\n#&gt; [25]  1  2  2  2  2  3  3  3  5  7  9 12 19 22 22 24 25 25 26 26 26 27 27 28\n#&gt; [49] 28 30 30 30 31 31 33 33 34 35 35 35 36 39 41 43 43 44\n\nNel linguaggio statistico, un’osservazione rappresenta l’informazione raccolta da un singolo individuo o entità che partecipa allo studio. Nel caso del dataset utilizzato da Zetsche et al. (2019), l’unità di osservazione è costituita dai partecipanti allo studio. Ogni riga del DataFrame, denominato df, corrisponde quindi a un individuo distinto incluso nell’analisi.\nLe variabili, invece, riflettono le diverse caratteristiche degli individui o delle entità considerate. Per i dati in esame, questo concetto si esprime così:\n\nOgni colonna di df rappresenta una variabile che descrive una specifica proprietà comune ai partecipanti.\nLe variabili sono identificate da etichette nelle colonne, come esa_id (l’identificativo del soggetto), mdd (il gruppo di appartenenza), e bdi (il punteggio del test BDI-II).\n\nIn termini simbolisi, per indicare una singola osservazione della variabile generica \\(X\\), si utilizza la notazione \\(X_i\\), dove \\(i\\) rappresenta l’indice dell’osservazione. Questo implica che abbiamo un valore diverso di \\(X\\) per ogni differente \\(i\\). Nel caso presente, con 67 osservazioni, \\(i\\) varia da 1 a 67. Così, per rappresentare la seconda osservazione (quella con \\(i=2\\)), useremo la notazione \\(X_2\\).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#distribuzioni-di-frequenza",
    "href": "chapters/eda/05_exploring_numeric_data.html#distribuzioni-di-frequenza",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.2 Distribuzioni di frequenza",
    "text": "11.2 Distribuzioni di frequenza\nCome osservato nell’output della sezione precedente, i dati grezzi non forniscono un’interpretazione immediata. Per rendere i dati più comprensibili e sintetici, è utile costruire una distribuzione di frequenza.\nUna distribuzione di frequenza mostra quante volte i valori di una variabile si verificano all’interno di intervalli specifici. Nel caso dei punteggi BDI-II, possiamo raggruppare i punteggi in quattro classi:\n\n0–13: depressione minima\n14–19: depressione lieve-moderata\n20–28: depressione moderata-severa\n29–63: depressione severa\n\nOgni classe, denotata come \\(\\Delta_i\\), rappresenta un intervallo di valori, definito come \\([a_i, b_i)\\) (aperto a destra) o \\((a_i, b_i]\\) (aperto a sinistra), dove \\(a_i\\) e \\(b_i\\) sono rispettivamente il limite inferiore e superiore della classe. A ciascuna classe si associa un’ampiezza, data da \\(b_i - a_i\\), e un valore centrale, indicato con \\(\\bar{x}_i\\). Poiché ogni osservazione \\(x_i\\) appartiene a una sola classe \\(\\Delta_i\\), possiamo calcolare le seguenti quantità:\n\n\nFrequenza assoluta \\(n_i\\): il numero di osservazioni che rientrano nella classe \\(\\Delta_i\\).\n\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\), dove \\(n\\) è il numero totale di osservazioni.\n\n\n\nFrequenza relativa \\(f_i\\): la proporzione di osservazioni in ciascuna classe, calcolata come \\(f_i = n_i/n\\).\n\nProprietà: \\(f_1 + f_2 + \\dots + f_m = 1\\).\n\n\nFrequenza cumulata \\(N_i\\): il numero totale di osservazioni che rientrano nelle classi fino alla \\(i\\)-esima inclusa, calcolata come \\(N_i = \\sum_{j=1}^i n_j\\).\nFrequenza cumulata relativa \\(F_i\\): la somma delle frequenze relative fino alla \\(i\\)-esima classe, data da \\(F_i = \\frac{N_i}{n} = \\sum_{j=1}^i f_j\\).\n\nQueste misure permettono di riassumere in modo efficace la distribuzione dei punteggi e facilitano l’interpretazione delle caratteristiche del campione.\n\n11.2.1 Frequenze Assolute e Relative\nPer ottenere la distribuzione di frequenza assoluta e relativa dei valori BDI-II nel dataset di zetsche_2019future, è necessario aggiungere al DataFrame df una colonna contenente una variabile categoriale che classifichi ciascuna osservazione in una delle quattro classi che descrivono la gravità della depressione. Questo risultato si ottiene utilizzando la funzione cut().\nNella funzione cut():\n\nIl primo argomento, x, è un vettore unidimensionale (ad esempio, un vettore di tipo numeric o una colonna di un DataFrame) che contiene i dati da classificare.\nIl secondo argomento, breaks, definisce gli intervalli delle classi, specificandone i limiti inferiori e superiori.\nL’argomento include.lowest = TRUE garantisce che il limite inferiore dell’intervallo più basso sia incluso nella classificazione. Nel nostro caso, questo è particolarmente utile per assicurare che i valori uguali al limite inferiore siano assegnati correttamente.\n\nDi seguito, il codice per aggiungere la variabile categoriale al DataFrame:\n\n# Creare una variabile categoriale per classi di depressione\ndf &lt;- df %&gt;% \n  mutate(\n    bdi_class = cut(\n      bdi, \n      breaks = c(0, 13.5, 19.5, 28.5, 63),\n      include.lowest = TRUE\n    )\n  )\n\nQuesto codice suddivide i valori della variabile bdi in quattro intervalli corrispondenti ai livelli di gravità della depressione:\n\n0–13: depressione minima\n14–19: depressione lieve-moderata\n20–28: depressione moderata-severa\n29–63: depressione severa\n\nOgni osservazione verrà assegnata al corrispondente intervallo, creando così una nuova colonna bdi_class nel DataFrame df.\n\n11.2.1.1 Frequenze assolute\n\ntable(df$bdi_class)\n#&gt; \n#&gt;    [0,13.5] (13.5,19.5] (19.5,28.5]   (28.5,63] \n#&gt;          36           1          12          17\n\n\n11.2.1.2 Frequenze relative\n\nprop.table(table(df$bdi_class))\n#&gt; \n#&gt;    [0,13.5] (13.5,19.5] (19.5,28.5]   (28.5,63] \n#&gt;  0.54545455  0.01515152  0.18181818  0.25757576\n\n\n11.2.2 Distribuzioni congiunte\nLe variabili possono anche essere analizzate insieme tramite le distribuzioni congiunte di frequenze. Queste distribuzioni rappresentano l’insieme delle frequenze assolute o relative ad ogni possibile combinazione di valori delle variabili. Ad esempio, se l’insieme di variabili \\(V\\) è composto da due variabili, \\(X\\) e \\(Y\\), ciascuna delle quali può assumere due valori, 1 e 2, allora una possibile distribuzione congiunta di frequenze relative per \\(V\\) potrebbe essere espressa come \\(f(X = 1, Y = 1) = 0.2\\), \\(f(X = 1, Y = 2) = 0.1\\), \\(f(X = 2, Y = 1) = 0.5\\), e \\(f(X = 2, Y = 2) = 0.2\\). Come nel caso delle distribuzioni di frequenze relative di una singola variabile, le frequenze relative di una distribuzione congiunta devono sommare a 1.\nPer i dati dell’esempio precedente, la funzione prop.table() può essere utilizzata anche per produrre questo tipo di tabella: basta indicare le serie corrispondenti alle variabili considerate come valori degli argomenti bdi_class e group.\n\nprop.table(table(df$bdi_class, df$group))\n#&gt;              \n#&gt;                      ctl        mdd\n#&gt;   [0,13.5]    0.54545455 0.00000000\n#&gt;   (13.5,19.5] 0.00000000 0.01515152\n#&gt;   (19.5,28.5] 0.00000000 0.18181818\n#&gt;   (28.5,63]   0.00000000 0.25757576",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#istogramma",
    "href": "chapters/eda/05_exploring_numeric_data.html#istogramma",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.3 Istogramma",
    "text": "11.3 Istogramma\nUn istogramma rappresenta graficamente una distribuzione di frequenze. Un istogramma mostra sulle ascisse i limiti delle classi \\(\\Delta_i\\) e sulle ordinate la densità della frequenza relativa della variabile \\(X\\) nella classe \\(\\Delta_i\\). La densità della frequenza relativa è misurata dalla funzione costante a tratti \\(\\varphi_n(x)= \\frac{f_i}{b_i-a_i}\\), dove \\(f_i\\) è la frequenza relativa della classe \\(\\Delta_i\\) e \\(b_i - a_i\\) rappresenta l’ampiezza della classe. In questo modo, l’area del rettangolo associato alla classe \\(\\Delta_i\\) sull’istogramma sarà proporzionale alla frequenza relativa \\(f_i\\). È importante notare che l’area totale dell’istogramma delle frequenze relative è uguale a 1.0, poiché rappresenta la somma delle aree dei singoli rettangoli.\nPer fare un esempio, costruiamo un istogramma per i valori BDI-II di Zetsche et al. (2019). Con i quattro intervalli individuati dai cut-off del BDI-II creo una prima versione dell’istogramma – si notino le frequenze assolute sull’asse delle ordinate.\n\nggplot(df, aes(x = bdi)) +\n  geom_histogram(\n    breaks = c(0, 13.5, 19.5, 28.5, 63),\n    aes(y = ..density..),\n    fill = \"blue\", \n    alpha = 0.5\n  ) +\n  labs(title = \"Istogramma delle frequenze relative\", x = \"BDI-II\", y = \"Densità\")\n#&gt; Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\nAnche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale.\n\nggplot(df, aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    fill = \"blue\", \n    alpha = 0.5\n  ) +\n  labs(title = \"Istogramma delle frequenze relative\", x = \"BDI-II\", y = \"Densità\")\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#kernel-density-plot",
    "href": "chapters/eda/05_exploring_numeric_data.html#kernel-density-plot",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.4 Kernel density plot",
    "text": "11.4 Kernel density plot\nConfrontando le due figure precedenti, emerge chiaramente una limitazione dell’istogramma: la sua forma dipende dall’arbitrarietà con cui vengono scelti il numero e l’ampiezza delle classi, rendendo difficile interpretare correttamente la distribuzione dei dati.\nPer superare questa difficoltà, possiamo utilizzare una tecnica alternativa chiamata stima della densità kernel (KDE) – si veda l’?sec-kde. Mentre l’istogramma utilizza barre per rappresentare i dati, la KDE crea un profilo smussato che fornisce una visione più continua e meno dipendente dall’arbitrarietà delle classi.\nImmaginiamo un istogramma con classi di ampiezza molto piccola, tanto da avere una curva continua invece di barre discrete. Questo è ciò che fa la KDE: smussa il profilo dell’istogramma per ottenere una rappresentazione continua dei dati. Invece di utilizzare barre, la KDE posiziona una piccola curva (detta kernel) su ogni osservazione nel dataset. Queste curve possono essere gaussiane (a forma di campana) o di altro tipo. Ogni kernel ha un’altezza e una larghezza determinate da parametri di smussamento (o bandwidth), che controllano quanto deve essere larga e alta la curva. Tutte le curve kernel vengono sommate per creare una singola curva complessiva. Questa curva rappresenta la densità dei dati, mostrando come i dati sono distribuiti lungo il range dei valori.\nLa curva risultante dal KDE mostra la proporzione di casi per ciascun intervallo di valori. L’area sotto la curva in un determinato intervallo rappresenta la proporzione di casi della distribuzione che ricadono in quell’intervallo. Per esempio, se un intervallo ha un’area maggiore sotto la curva rispetto ad altri, significa che in quell’intervallo c’è una maggiore concentrazione di dati.\nLa curva di densità ottenuta tramite KDE fornisce dunque un’idea chiara di come i dati sono distribuiti senza dipendere dall’arbitrarietà della scelta delle classi dell’istogramma.\nCrediamo un kernel density plot per ciascuno dei due gruppi di valori BDI-II riportati da {cite:t}zetsche_2019future.\n\nggplot(df, aes(x = bdi, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Curva di densità KDE\", x = \"BDI-II\", y = \"Densità\")",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#consigli-per-creare-visualizzazioni-di-dati-efficaci",
    "href": "chapters/eda/05_exploring_numeric_data.html#consigli-per-creare-visualizzazioni-di-dati-efficaci",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.5 Consigli per Creare Visualizzazioni di Dati Efficaci",
    "text": "11.5 Consigli per Creare Visualizzazioni di Dati Efficaci\nEcco alcuni suggerimenti per creare visualizzazioni di dati esplicative, efficaci e di qualità adatta alle presentazioni:\n\n\nMessaggio chiaro: Assicurati che il grafico trasmetta un messaggio chiaro e immediato (ad esempio, “Il livello di benessere psicologico dei partecipanti aumenta nel tempo”).\n\nUso del colore:\n\nUtilizza i colori in modo ponderato e con moderazione.\nNon eccedere nell’uso dei colori solo perché è possibile farlo.\nLimita l’uso a non più di cinque o sei colori in una singola figura.\nVerifica che le scelte cromatiche non distorcano le conclusioni della figura.\nEvita l’uso contemporaneo di rosso e verde nello stesso grafico, poiché queste tonalità sono difficili da distinguere per le persone daltoniche.\n\n\n\nGuidare l’attenzione:\n\nUtilizza dimensioni, colori e testo per guidare l’attenzione del pubblico.\nEvidenzia elementi particolari del grafico per enfatizzare punti chiave.\n\n\n\nGestione del sovraccarico visivo:\n\nUtilizza la trasparenza per ridurre il “sovrapplotting” (che si verifica quando ci sono molti elementi sovrapposti nel grafico, come punti o linee, rendendo difficile individuare i pattern).\nQuesta tecnica è particolarmente utile quando si visualizza una grande quantità di dati.\nSe il dataset è molto ampio e l’aggiunta di trasparenza non è sufficiente, considera la visualizzazione di un sottocampione dei dati (un campione casuale di punti dati, scelto senza sostituzione). Questa tecnica è nota come sottocampionamento.\n\n\n\nElementi testuali:\n\nI titoli, le etichette degli assi e il testo delle legende devono essere chiari e facilmente comprensibili.\nGli elementi della legenda dovrebbero essere ordinati in modo logico e coerente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#forma-di-una-distribuzione",
    "href": "chapters/eda/05_exploring_numeric_data.html#forma-di-una-distribuzione",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.6 Forma di una Distribuzione",
    "text": "11.6 Forma di una Distribuzione\nIn statistica, la forma di una distribuzione descrive come i dati sono distribuiti intorno ai valori centrali. Si distingue tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali e multimodali. Un’illustrazione grafica è fornita nella figura seguente. Nel pannello 1, la distribuzione è unimodale con asimmetria negativa; nel pannello 2, la distribuzione è unimodale con asimmetria positiva; nel pannello 3, la distribuzione è simmetrica e unimodale; nel pannello 4, la distribuzione è bimodale.\n\n\nDistribuzioni\n\nIl grafico della densità di kernel (Kernel Density Plot) dei valori BDI-II nel campione di Zetsche et al. (2019) è bimodale. Questo indica che le osservazioni della distribuzione si raggruppano in due cluster distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere valori BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da Zetsche et al. (2019).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#indici-di-posizione",
    "href": "chapters/eda/05_exploring_numeric_data.html#indici-di-posizione",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.7 Indici di posizione",
    "text": "11.7 Indici di posizione\n\n11.7.1 Quantili\nLa distribuzione dei valori BDI-II di Zetsche et al. (2019) può essere sintetizzata attraverso l’uso dei quantili, che sono valori caratteristici che suddividono i dati in parti ugualmente numerose. I quartili sono tre quantili specifici: il primo quartile, \\(q_1\\), divide i dati in due parti, lasciando a sinistra il 25% del campione; il secondo quartile, \\(q_2\\), corrisponde alla mediana e divide i dati in due parti uguali; il terzo quartile lascia a sinistra il 75% del campione.\nInoltre, ci sono altri indici di posizione chiamati decili e percentili che suddividono i dati in parti di dimensioni uguali a 10% e 1%, rispettivamente.\nPer calcolare i quantili, i dati vengono prima ordinati in modo crescente e poi viene determinato il valore di \\(np\\), dove \\(n\\) è la dimensione del campione e \\(p\\) è l’ordine del quantile. Se \\(np\\) non è un intero, il valore del quantile corrisponde al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\). Se \\(np\\) è un intero, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(k\\) e \\(k+1\\), dove \\(k\\) è la parte intera di \\(np\\).\nGli indici di posizione possono essere utilizzati per creare un box-plot, una rappresentazione grafica della distribuzione dei dati che è molto popolare e può essere utilizzata in alternativa ad un istogramma.\nAd esempio, per calcolare la mediana della distribuzione dei nove soggetti con un unico episodio di depressione maggiore del campione clinico di Zetsche et al. (2019), si determina il valore di \\(np = 9 \\cdot 0.5 = 4.5\\), che non è un intero. Pertanto, il valore del secondo quartile è pari al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\), ovvero \\(q_2 = x_{4 + 1} = 27\\). Per calcolare il quantile di ordine \\(2/3\\), si determina il valore di \\(np = 9 \\cdot 2/3 = 6\\), che è un intero. Quindi, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(6\\) e \\(7\\), ovvero \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).\nUsiamo numpy per trovare la soluzione dell’esercizio precedente.\n\nx = c(19, 26, 27, 28, 28, 33, 33, 41, 43)\nquantile(x, 2 / 3)\n#&gt; 66.66667% \n#&gt;        33",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#mostrare-i-dati",
    "href": "chapters/eda/05_exploring_numeric_data.html#mostrare-i-dati",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.8 Mostrare i dati",
    "text": "11.8 Mostrare i dati\n\n11.8.1 Diagramma a scatola\nIl box plot è uno strumento grafico che visualizza la dispersione di una distribuzione. Per creare un box plot, si disegna un rettangolo (la “scatola”) di altezza arbitraria, basato sulla distanza interquartile (IQR), che corrisponde alla differenza tra il terzo quartile (\\(q_{0.75}\\)) e il primo quartile (\\(q_{0.25}\\)). La mediana (\\(q_{0.5}\\)) è rappresentata da una linea all’interno del rettangolo.\nAi lati della scatola, vengono tracciati due segmenti di retta, detti “baffi”, che rappresentano i valori adiacenti inferiore e superiore. Il valore adiacente inferiore è il valore più basso tra le osservazioni che è maggiore o uguale al primo quartile meno 1.5 volte la distanza interquartile. Il valore adiacente superiore è il valore più alto tra le osservazioni che è minore o uguale al terzo quartile più 1.5 volte la distanza interquartile.\nSe ci sono dei valori che cadono al di fuori dei valori adiacenti, vengono chiamati “valori anomali” e sono rappresentati individualmente nel box plot per evidenziare la loro presenza e posizione. In questo modo, il box plot fornisce una rappresentazione visiva della distribuzione dei dati, permettendo di individuare facilmente eventuali valori anomali e di comprendere la dispersione dei dati.\n\nUtilizziamo un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.\n\nggplot(df, aes(x = group, y = bdi)) +\n  geom_boxplot() +\n  labs(title = \"Box plot per gruppo\", x = \"Gruppo\", y = \"BDI-II\")\n\n\n\n\n\n\n\nUn risultato migliore si ottiene utilizzando un grafico a violino (violin plot) e includendo anche i dati grezzi.\n\n11.8.2 Grafico a Violino\nI grafici a violino combinano le caratteristiche dei box plot e dei grafici di densità di kernel (KDE plot) per offrire una rappresentazione più dettagliata dei dati. A questi grafici vengono sovrapposti i dati grezzi, fornendo una visione completa della distribuzione e delle caratteristiche dei dati.\n\nggplot(df, aes(x = group, y = bdi)) +\n  geom_violin(fill = \"lightgray\", color = \"black\") +\n  geom_jitter(width = 0.2, color = \"red\", alpha = 0.6) +\n  labs(\n    title = \"Violin plot con overlay dei punti grezzi\",\n    x = \"Gruppo\", \n    y = \"BDI-II\"\n  )",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#riflessioni-conclusive",
    "href": "chapters/eda/05_exploring_numeric_data.html#riflessioni-conclusive",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.10 Riflessioni Conclusive",
    "text": "11.10 Riflessioni Conclusive\nAbbiamo esplorato diverse tecniche per sintetizzare e visualizzare i dati numerici, includendo distribuzioni di frequenza, istogrammi e grafici di densità. Questi strumenti sono fondamentali per comprendere meglio i dati e comunicare i risultati in modo efficace.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/05_exploring_numeric_data.html#informazioni-sullambiente-di-sviluppo",
    "title": "11  Esplorare i dati numerici",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [5] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt;  [9] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [13] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [17] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [21] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gtable_0.3.6      xfun_0.49         htmlwidgets_1.6.4 rstatix_0.7.2    \n#&gt;  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#&gt;  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pacman_0.5.1     \n#&gt; [13] R.oo_1.27.0       pkgconfig_2.0.3   data.table_1.16.2 lifecycle_1.0.4  \n#&gt; [17] compiler_4.4.2    farver_2.1.2      munsell_0.5.1     mnormt_2.1.1     \n#&gt; [21] carData_3.0-5     httpuv_1.6.15     htmltools_0.5.8.1 yaml_2.3.10      \n#&gt; [25] Formula_1.2-5     car_3.1-3         pillar_1.9.0      later_1.4.0      \n#&gt; [29] R.utils_2.12.3    abind_1.4-8       nlme_3.1-166      mime_0.12        \n#&gt; [33] tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4     labeling_0.4.3   \n#&gt; [37] rprojroot_2.0.4   fastmap_1.2.0     grid_4.4.2        colorspace_2.1-1 \n#&gt; [41] cli_3.6.3         magrittr_2.0.3    utf8_1.2.4        broom_1.0.7      \n#&gt; [45] withr_3.0.2       backports_1.5.0   promises_1.3.1    timechange_0.3.0 \n#&gt; [49] rmarkdown_2.29    ggsignif_0.6.4    R.methodsS3_1.8.2 hms_1.1.3        \n#&gt; [53] shiny_1.9.1       evaluate_1.0.1    miniUI_0.1.1.1    rlang_1.1.4      \n#&gt; [57] Rcpp_1.0.13-1     xtable_1.8-4      glue_1.8.0        jsonlite_1.8.9   \n#&gt; [61] R6_2.5.1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#bibliografia",
    "href": "chapters/eda/05_exploring_numeric_data.html#bibliografia",
    "title": "11  Esplorare i dati numerici",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html",
    "href": "chapters/eda/11_pixi.html",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "",
    "text": "12.1 Introduzione\nPrerequisiti\nPrima di iniziare, assicurati di avere:\nConcetti e competenze chiave\nPreparazione del Notebook\nIn questo capitolo esploreremo gli strumenti essenziali per garantire che il flusso di lavoro di un progetto di analisi dei dati in R sia pienamente riproducibile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#analisi-e-flussi-di-lavoro-riproducibili",
    "href": "chapters/eda/11_pixi.html#analisi-e-flussi-di-lavoro-riproducibili",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.2 Analisi e flussi di lavoro riproducibili",
    "text": "12.2 Analisi e flussi di lavoro riproducibili\nLa possibilità di confermare ripetutamente i risultati scientifici attraverso la replica è un principio fondamentale della scienza. Questo concetto si basa sull’idea che una verità scientifica debba resistere a ulteriori indagini da parte di altri osservatori. In ambito scientifico, è utile distinguere due aspetti legati alla replica: replicabilità e riproducibilità.\n\n\nReplicabilità: si riferisce alla capacità di ottenere risultati simili con dati diversi, ma seguendo lo stesso protocollo sperimentale.\n\nRiproducibilità: indica la capacità di ottenere gli stessi risultati utilizzando gli stessi dati e lo stesso metodo di analisi, sia dalla stessa persona che da altri.\n\n\n12.2.1 Riproducibilità dei dati: una sfida spesso sottovalutata\nLa replicazione di esperimenti fisici può presentare difficoltà pratiche significative. Tuttavia, anche riprodurre semplicemente un’analisi dei dati, che sembra un compito più semplice, è spesso problematico per molteplici ragioni. Tradizionalmente, i ricercatori annotavano scrupolosamente i dettagli sperimentali nei loro taccuini di laboratorio, consentendo di replicare l’esperimento. Oggi, strumenti software moderni permettono di applicare lo stesso principio alla riproduzione dei dati: tutto il necessario per rifare l’analisi deve essere documentato in modo chiaro e centralizzato.\nQuesti strumenti non solo facilitano la ripetizione dell’analisi, ma permettono anche di migliorarla e applicarla facilmente a nuovi dati. Tuttavia, per essere riproducibile, l’analisi deve essere scritta in modo appropriato, utilizzando ambienti di programmazione statistica come R o Python, che permettono l’automazione del processo. Al contrario, software come i fogli di calcolo non sono adatti a garantire riproducibilità, perché legano i comandi a celle specifiche, rendendo l’adattamento a nuovi dati complesso e soggetto a errori.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#la-crisi-della-replicazione-e-la-riproducibilità",
    "href": "chapters/eda/11_pixi.html#la-crisi-della-replicazione-e-la-riproducibilità",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.3 La crisi della replicazione e la riproducibilità",
    "text": "12.3 La crisi della replicazione e la riproducibilità\nLa crisi della replicazione evidenzia un problema crescente nella scienza moderna: molti studi pubblicati, anche sottoposti a peer review, non sono replicabili. Studi come quello di Ioannidis (2005) e Baker (2016) denunciano che molte ricerche sperimentali e statistiche non resistono alla verifica di altri ricercatori. Questo significa che, nonostante l’apparente validità dei risultati, spesso non è possibile raggiungere le stesse conclusioni ripetendo lo studio.\nTra le cause di questa crisi figurano problematiche complesse come la molteplicità e i percorsi analitici alternativi (il cosiddetto garden of forking paths). Tuttavia, indipendentemente da queste difficoltà, un’analisi riproducibile è un requisito minimo per garantire la validità dei risultati.\n\n\n\n\n\n\nUn problema che compromette la solidità delle conclusioni di molti studi è stato definito da Andrew Gelman, della Columbia University, come il garden of forking paths (giardino dei sentieri che si biforcano). La maggior parte delle analisi richiede una serie di decisioni su come codificare i dati, identificare i fattori rilevanti e formulare (e successivamente rivedere) i modelli prima di arrivare alle analisi finali. Questo processo implica spesso l’esame dei dati per costruire una rappresentazione parsimoniosa. Ad esempio, un predittore continuo potrebbe essere suddiviso arbitrariamente in gruppi per valutare la relazione con l’esito, oppure alcune variabili potrebbero essere incluse o escluse da un modello di regressione durante una fase esplorativa.\nQuesto approccio tende a favorire risultati dei test di ipotesi che sono distorti verso il rigetto dell’ipotesi nulla, poiché le decisioni prese durante il processo possono privilegiare segnali più forti (o p-value più piccoli) rispetto ad altre alternative. Nella maggior parte dei problemi di data science, questo rappresenta una sfida importante che solleva dubbi sulla riproducibilità dei risultati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#elementi-chiave-per-un-workflow-riproducibile",
    "href": "chapters/eda/11_pixi.html#elementi-chiave-per-un-workflow-riproducibile",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.4 Elementi chiave per un workflow riproducibile",
    "text": "12.4 Elementi chiave per un workflow riproducibile\nUn flusso di lavoro riproducibile si compone di tre elementi fondamentali:\n\n\nAmbienti di programmazione statistica scriptabili: software come R o Python consentono di automatizzare le analisi e ridurre gli errori manuali.\n\nAnalisi riproducibili: basate sull’approccio della literate programming, dove codice e documentazione sono integrati per garantire trasparenza e comprensione.\n\nControllo di versione: sistemi come Git e piattaforme come GitHub permettono di monitorare e documentare i cambiamenti, favorendo la collaborazione e la tracciabilità.\n\nIntegrare questi componenti nella pratica quotidiana non solo migliora la qualità delle analisi, ma contribuisce a rafforzare la fiducia nella scienza.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#gestione-del-workflow",
    "href": "chapters/eda/11_pixi.html#gestione-del-workflow",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.5 Gestione del Workflow",
    "text": "12.5 Gestione del Workflow\nAnche se l’uso di R per scrivere script che documentano le analisi dei dati rappresenta un importante passo verso la riproducibilità, questo non garantisce che il progetto possa essere riprodotto con successo da altri ricercatori. Ad esempio, Obels et al. (2020) hanno esaminato la condivisione di dati e codice per gli articoli pubblicati come Registered Reports nella letteratura psicologica tra il 2014 e il 2018, tentando di riprodurre indipendentemente i risultati principali. Riportano:\n\n“Abbiamo esaminato dati e script condivisi per i Registered Reports pubblicati nella letteratura psicologica dal 2014 al 2018 e tentato di riprodurre computazionalmente i risultati principali di ciascun articolo. Dei 62 articoli che soddisfacevano i nostri criteri di inclusione, 41 mettevano a disposizione i dati e 37 gli script di analisi. Dati e codice erano condivisi per 36 articoli. Siamo riusciti a eseguire gli script per 31 analisi e a riprodurre i risultati principali di 21 articoli. Sebbene la percentuale di articoli con dati e codice condivisi (36 su 62, ovvero il 58%) e quella degli articoli per cui i risultati principali potevano essere riprodotti computazionalmente (21 su 36, ovvero il 58%) siano relativamente alte rispetto a quelle riscontrate in altri studi, c’è un evidente margine di miglioramento.”\n\nQuesti risultati evidenziano che, anche quando dati e codice sono disponibili, la riproducibilità non è garantita. Gli script possono essere incompleti, o l’ambiente di sviluppo originariamente utilizzato dagli autori può non essere replicabile da altri ricercatori.\n\n12.5.1 Strumenti per la Gestione del Workflow\nPer affrontare queste problematiche, sono stati sviluppati numerosi strumenti di gestione del workflow. Tra i più noti ricordiamo:\n\n\nMake: uno strumento storico progettato per sistemi Unix, altamente portabile e preinstallato su tutti i sistemi Unix e derivati.\n\nSnakemake: molto popolare in biologia, offre grande flessibilità per gestire flussi di lavoro complessi.\n\ntargets: uno strumento specificamente progettato per R, che semplifica la gestione dei workflow riproducibili nei progetti di analisi dati.\n\n12.5.2 Limiti degli Strumenti di Workflow Management\nNonostante i vantaggi, l’adozione di questi strumenti comporta un certo aggravio per il ricercatore. Oltre a dover scrivere gli script per l’analisi dei dati, è necessario creare ulteriori script che definiscano il workflow e garantiscano la riproducibilità. Questo può richiedere una riorganizzazione significativa del modo in cui gli script sono strutturati, rendendo talvolta più complesso il debug e le successive modifiche. Per questi motivi, l’uso di strumenti di workflow management può risultare impegnativo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#un-workflow-manager-semplice-e-innovativo",
    "href": "chapters/eda/11_pixi.html#un-workflow-manager-semplice-e-innovativo",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.6 Un Workflow Manager Semplice e Innovativo",
    "text": "12.6 Un Workflow Manager Semplice e Innovativo\nIn questo capitolo verrà introdotto Pixi, un workflow manager di ultima generazione progettato per semplificare la gestione dei flussi di lavoro nei progetti di analisi dati. Pixi si distingue per la sua intuitività e per la capacità di affrontare molte delle difficoltà tipiche legate alla riproducibilità e all’organizzazione di progetti complessi. Questo strumento è particolarmente adatto a ricercatori e data scientist che desiderano automatizzare e strutturare efficacemente il proprio lavoro senza aggiungere complessità inutili.\nPixi è un gestore di pacchetti veloce e versatile, basato sull’ecosistema Conda, che semplifica la creazione di ambienti di sviluppo su diverse piattaforme, tra cui Windows, macOS e Linux. Supporta una vasta gamma di linguaggi di programmazione, tra cui Python, R, C/C++, Rust e Ruby, rendendolo uno strumento estremamente flessibile. Inoltre, Pixi consente di creare ambienti riproducibili senza dover ricorrere a strumenti più complessi come Docker, riducendo così il tempo e lo sforzo necessari per configurare il progetto.\nCon una configurazione iniziale ben strutturata, Pixi permette di concentrarsi sull’analisi dei dati e sulla generazione di risultati, garantendo al tempo stesso la piena riproducibilità del workflow.\n\n12.6.1 Installazione di Pixi\nPer utilizzare Pixi, è necessario avere installati R, RStudio e Pixi stesso. L’installazione di Pixi è semplice seguendo le indicazioni ufficiali. Per installare Pixi, è sufficiente eseguire il seguente comando nel terminale:\ncurl -fsSL https://pixi.sh/install.sh | bash\n\n12.6.2 Struttura del Progetto\nImmaginiamo che il progetto segua questa struttura organizzativa:\ndata-analysis-project/\n│\n├── pixi.toml               # File di configurazione Pixi\n├── pixi.lock               # Lockfile per le dipendenze\n│\n├── data/                   # Directory per i dati\n│   ├── raw/                # Dati grezzi\n│   └── processed/          # Dati processati\n│\n├── src/                    # Codice sorgente in R\n│   ├── data_cleaning.R     # Script per pulizia dei dati\n│   ├── analysis.R          # Script per analisi\n│   └── visualization.R     # Script per visualizzazioni\n│\n├── reports/                # Report e output\n│   ├── figures/            # Figure generate\n│   └── report.Rmd          # Report in R Markdown\n│\n└── README.md               # Documentazione del progetto\n\n12.6.3 Configurazione di Pixi\nPer configurare Pixi, si crea un file pixi.toml nella directory principale del progetto:\n[project]\nname = \"r-data-analysis\"\ndescription = \"Progetto di analisi dati con R\"\nauthors = [\"Tuo Nome &lt;tua.email@example.com&gt;\"]\nchannels = [\"conda-forge\"]\nplatforms = [\"osx-64\", \"linux-64\",\"win-64\"]\n\n[dependencies]\nr-base = \"&gt;=4.3,&lt;5\"\nr-tidyverse = \"*\"\nr-rio= \"*\"\nr-knitr = \"*\"\nr-rmarkdown = \"*\"\nr-here = \"*\"\n\n[tasks]\nclean = \"rm -rf reports/figures/* data/processed/*\"\ndata-prep = \"Rscript src/data_cleaning.R\"\nanalysis = \"Rscript src/analysis.R\"\nreport = \"Rscript -e 'rmarkdown::render(\\\"reports/report.Rmd\\\")'\"\nall = { depends-on = [\"clean\", \"data-prep\", \"analysis\", \"report\"] }\n\n12.6.4 Script di Pulizia Dati (src/data_cleaning.R)\nUno script per pulire e organizzare i dati grezzi:\nlibrary(readr)\nlibrary(dplyr)\n\n# Caricamento dati grezzi\nraw_data &lt;- read_csv(\"data/raw/dati_esempio.csv\")\n\n# Pulizia e trasformazione dei dati\ncleaned_data &lt;- raw_data %&gt;%\n  filter(!is.na(valore)) %&gt;%\n  mutate(categoria = factor(categoria)) %&gt;%\n  group_by(categoria) %&gt;%\n  summarise(media = mean(valore, na.rm = TRUE))\n\n# Salvataggio dei dati processati\nwrite_csv(cleaned_data, \"data/processed/dati_puliti.csv\")\n\n12.6.5 Script di Analisi (src/analysis.R)\nUno script per eseguire analisi statistiche e creare visualizzazioni:\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Caricamento dei dati processati\ndati &lt;- read_csv(\"data/processed/dati_puliti.csv\")\n\n# Analisi statistica\nrisultati_analisi &lt;- dati %&gt;%\n  group_by(categoria) %&gt;%\n  summarise(\n    media = mean(media),\n    deviazione_standard = sd(media)\n  )\n\n# Salvataggio dei risultati\nwrite_csv(risultati_analisi, \"reports/risultati_analisi.csv\")\n\n# Creazione di un grafico\ngrafico &lt;- ggplot(dati, aes(x = categoria, y = media)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Media per Categoria\")\n\n# Salvataggio del grafico\nggsave(\"reports/figures/media_categoria.png\", plot = grafico)\n\n12.6.6 Creazione di un Report (reports/report.Rmd)\nUn report generato con R Markdown per presentare i risultati dell’analisi:\n---\ntitle: \"Report di Analisi dei Dati\"\noutput: html_document\n---\n\n## Risultati dell'Analisi\n\n```\nlibrary(readr)\nlibrary(knitr)\nrisultati &lt;- read_csv(\"reports/risultati_analisi.csv\")\nkable(risultati)\n```\n\n## Grafico delle Medie per Categoria\n\n![Media per Categoria](figures/media_categoria.png)\n\n12.6.7 Esecuzione del Workflow con Pixi\nUna volta installato Pixi, puoi inizializzare un nuovo progetto con:\n# Inizializzazione del progetto\npixi init\nPer installare le dipendenze specificate in in pixi.toml, esegui:\n# Installazione delle dipendenze specificate in pixi.toml\npixi install\nQuesto comando crea un file pixi.lock che elenca tutte le dipendenze del progetto, garantendo che l’ambiente possa essere ricreato in modo identico su diverse macchine.\nPer eseguire le attività definite, utilizza:\npixi run data-prep\npixi run analysis\npixi run report\nOppure, per eseguire tutte le attività in sequenza:\npixi run all\nIl codice utilizzato in questo tutorial è disponibile nel repository GitHub dedicato.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#riflessioni-conclusive",
    "href": "chapters/eda/11_pixi.html#riflessioni-conclusive",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "\n12.7 Riflessioni Conclusive",
    "text": "12.7 Riflessioni Conclusive\nPixi semplifica la gestione dei progetti di analisi dati in R, automatizzando il workflow e garantendo riproducibilità e coerenza. Non è necessario modificare gli script esistenti; possono essere utilizzati direttamente da Pixi. Con una configurazione iniziale ben strutturata, consente di concentrarsi sull’analisi dei dati e sulla generazione di risultati, minimizzando gli errori e massimizzando l’efficienza del processo. Pixi elenca automaticamente tutte le dipendenze utilizzate nel progetto nel file pixi.lock, assicurando che l’ambiente sia pienamente riproducibile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/11_pixi.html#informazioni-sullambiente-di-sviluppo",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [5] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt;  [9] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [13] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [17] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [21] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gtable_0.3.6      xfun_0.49         htmlwidgets_1.6.4 rstatix_0.7.2    \n#&gt;  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#&gt;  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pacman_0.5.1     \n#&gt; [13] pkgconfig_2.0.3   lifecycle_1.0.4   compiler_4.4.2    farver_2.1.2     \n#&gt; [17] munsell_0.5.1     mnormt_2.1.1      carData_3.0-5     httpuv_1.6.15    \n#&gt; [21] htmltools_0.5.8.1 yaml_2.3.10       Formula_1.2-5     car_3.1-3        \n#&gt; [25] pillar_1.9.0      later_1.4.0       abind_1.4-8       nlme_3.1-166     \n#&gt; [29] mime_0.12         tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4    \n#&gt; [33] rprojroot_2.0.4   fastmap_1.2.0     grid_4.4.2        colorspace_2.1-1 \n#&gt; [37] cli_3.6.3         magrittr_2.0.3    utf8_1.2.4        broom_1.0.7      \n#&gt; [41] withr_3.0.2       backports_1.5.0   promises_1.3.1    timechange_0.3.0 \n#&gt; [45] rmarkdown_2.29    ggsignif_0.6.4    hms_1.1.3         shiny_1.9.1      \n#&gt; [49] evaluate_1.0.1    miniUI_0.1.1.1    rlang_1.1.4       Rcpp_1.0.13-1    \n#&gt; [53] xtable_1.8-4      glue_1.8.0        jsonlite_1.8.9    R6_2.5.1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/11_pixi.html#bibliografia",
    "href": "chapters/eda/11_pixi.html#bibliografia",
    "title": "12  Flusso di lavoro riproducibile",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBaker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604).\n\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124.\n\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Flusso di lavoro riproducibile</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html",
    "href": "chapters/eda/07_loc_scale.html",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "",
    "text": "12.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nLa visualizzazione grafica dei dati rappresenta il pilastro fondamentale di ogni analisi quantitativa. Grazie alle rappresentazioni grafiche adeguate, è possibile individuare importanti caratteristiche di una distribuzione, quali la simmetria o l’asimmetria, nonché la presenza di una o più mode. Successivamente, al fine di descrivere sinteticamente le principali caratteristiche dei dati, si rende necessario l’utilizzo di specifici indici numerici. In questo capitolo, verranno presentati i principali indicatori della statistica descrittiva.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#indici-di-tendenza-centrale",
    "href": "chapters/eda/07_loc_scale.html#indici-di-tendenza-centrale",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "\n12.2 Indici di tendenza centrale",
    "text": "12.2 Indici di tendenza centrale\nGli indici di tendenza centrale sono misure statistiche che cercano di rappresentare un valore tipico o centrale all’interno di un insieme di dati. Sono utilizzati per ottenere una comprensione immediata della distribuzione dei dati senza dover analizzare l’intero insieme. Gli indici di tendenza centrale sono fondamentali nell’analisi statistica, in quanto forniscono una sintesi semplice e comprensibile delle caratteristiche principali di un insieme di dati. I principali indici di tendenza centrale sono:\n\n\nMedia: La media è la somma di tutti i valori divisa per il numero totale di valori. È spesso utilizzata come misura generale di tendenza centrale, ma è sensibile agli estremi (valori molto alti o molto bassi).\n\nMediana: La mediana è il valore che divide l’insieme di dati in due parti uguali. A differenza della media, non è influenzata da valori estremi ed è quindi più robusta in presenza di outlier.\n\nModa: La moda è il valore che appare più frequentemente in un insieme di dati. In alcuni casi, può non essere presente o esserci più di una moda.\n\nLa scelta dell’indice di tendenza centrale appropriato dipende dalla natura dei dati e dall’obiettivo dell’analisi. Ad esempio, la mediana potrebbe essere preferita alla media se l’insieme di dati contiene valori anomali che potrebbero distorcere la rappresentazione centrale. La conoscenza e l’applicazione corretta di questi indici possono fornire una preziosa intuizione sulle caratteristiche centrali di una distribuzione di dati.\n\n12.2.1 Media\nLa media aritmetica di un insieme di valori rappresenta il punto centrale o il baricentro della distribuzione dei dati. È calcolata come la somma di tutti i valori divisa per il numero totale di valori, ed è espressa dalla formula:\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{12.1}\\]\ndove \\(x_i\\) rappresenta i valori nell’insieme, \\(n\\) è il numero totale di valori, e \\(\\sum\\) indica la sommatoria.\n\n12.2.1.1 Proprietà della media\nUna proprietà fondamentale della media è che la somma degli scarti di ciascun valore dalla media è zero:\n\\[\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\tag{12.2}\\]\nInfatti,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\n\\end{aligned}\n\\]\nQuesta proprietà implica che i dati sono equamente distribuiti intorno alla media.\n\n12.2.1.2 La media come centro di gravità dell’istogramma\nLa media aritmetica può essere interpretata come il centro di gravità o il punto di equilibrio della distribuzione dei dati. In termini fisici, il centro di gravità è il punto in cui la massa di un sistema è equilibrata o concentrata.\nIn termini statistici, possiamo considerare la media come il punto in cui la distribuzione dei dati è in equilibrio. Ogni valore dell’insieme di dati può essere visto come un punto materiale con una massa proporzionale al suo valore. Se immaginiamo questi punti disposti su una linea, con valori più grandi a destra e più piccoli a sinistra, la media corrisponderà esattamente al punto in cui la distribuzione sarebbe in equilibrio.\n\n12.2.1.3 Principio dei minimi quadrati\nLa posizione della media minimizza la somma delle distanze quadrate dai dati, un principio noto come “metodo dei minimi quadrati”. Matematicamente, questo si traduce nel fatto che la somma dei quadrati degli scarti tra ciascun valore e la media è minima. Questo principio è alla base dell’analisi statistica dei modelli di regressione e conferma l’interpretazione della media come centro di gravità dell’istogramma.\n\n12.2.1.4 Calcolo della media con R\n\nPer calcolare la media di un piccolo numero di valori in Python, possiamo utilizzare la somma di questi valori e dividerla per il numero totale di elementi. Consideriamo ad esempio i valori 12, 44, 21, 62, 24:\n\n(12 + 44 + 21 + 62 + 24) / 5\n#&gt; [1] 32.6\n\novvero\n\nx &lt;- c(12, 44, 21, 62, 24)\nmean(x)\n#&gt; [1] 32.6\n\n\n12.2.1.5 Le proporzioni sono medie\nSe una collezione consiste solo di uni e zeri, allora la somma della collezione è il numero di uni in essa, e la media della collezione è la proporzione di uni.\n\nzero_one &lt;- c(1, 1, 1, 0)\nresult &lt;- mean(zero_one)\nresult\n#&gt; [1] 0.75\n\nÈ possibile sostituire 1 con il valore booleano True e 0 con False:\n\nmean(c(TRUE, TRUE, TRUE, FALSE))\n#&gt; [1] 0.75\n\n\n12.2.1.6 Limiti della media aritmetica\nLa media aritmetica, tuttavia, ha alcune limitazioni: non sempre è l’indice più adeguato per descrivere accuratamente la tendenza centrale della distribuzione, specialmente quando si verificano asimmetrie o valori anomali (outlier). In queste situazioni, è più indicato utilizzare la mediana o la media spuntata (come spiegheremo successivamente).\n\n12.2.1.7 Medie per gruppi\nMolto spesso però i nostri dati sono contenuti in file e inserire i dati manualmente non è fattibile. Per fare un esempio, considereremo i dati del Progetto STAR, contenuti nel file STAR.csv, che rappresentano un’importante indagine sulle prestazioni degli studenti in relazione alla dimensione delle classi. Negli anni ’80, i legislatori del Tennessee considerarono la possibilità di ridurre le dimensioni delle classi per migliorare il rendimento degli studenti. Al fine di prendere decisioni informate, commissionarono lo studio multimilionario “Progetto Student-Teacher Achievement Ratio” (Project STAR). Lo studio coinvolgeva bambini della scuola materna assegnati casualmente a classi piccole, con 13-17 studenti, o classi di dimensioni regolari, con 22-25 studenti, fino alla fine della terza elementare. I ricercatori hanno seguito il progresso degli studenti nel tempo, concentrandosi su variabili di risultato, come i punteggi dei test standardizzati di lettura (reading) e matematica (math) alla terza elementare, oltre ai tassi di diploma di scuola superiore (graduated, con valore 1 per sì e 0 per no).\nPoniamoci il problema di calcolare la media dei punteggi math calcolata separatamente per i due gruppi di studenti: coloro che hanno completato la scuola superiore e coloro che non l’hanno completata.\nProcediamo all’importazione dei dati per iniziare l’analisi.\n\ndf &lt;- rio::import(here::here(\"data\", \"STAR.csv\"))\ndf |&gt; \n  head()\n#&gt;   classtype reading math graduated\n#&gt; 1     small     578  610         1\n#&gt; 2   regular     612  612         1\n#&gt; 3   regular     583  606         1\n#&gt; 4     small     661  648         1\n#&gt; 5     small     614  636         1\n#&gt; 6   regular     610  603         0\n\nEsaminiamo la numerosità di ciascun gruppo.\n\ndf |&gt; \n  group_by(graduated) |&gt; \n  summarize(count = n())\n#&gt; # A tibble: 2 × 2\n#&gt;   graduated count\n#&gt;       &lt;int&gt; &lt;int&gt;\n#&gt; 1         0   166\n#&gt; 2         1  1108\n\nOra procediamo al calcolo delle medie dei punteggi math all’interno dei due gruppi. Per rendere la risposta più concisa, useremo la funzione round() per stampare solo 2 valori decimali.\n\ndf |&gt; \n  group_by(graduated) |&gt; \n  summarise(\n    mean_math = round(mean(math, na.rm = TRUE), 2)\n  )\n#&gt; # A tibble: 2 × 2\n#&gt;   graduated mean_math\n#&gt;       &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1         0      607.\n#&gt; 2         1      635.\n\n\n12.2.2 Media spuntata\nLa media spuntata, indicata come \\(\\bar{x}_t\\) o trimmed mean, è un metodo di calcolo della media che prevede l’eliminazione di una determinata percentuale di dati estremi prima di effettuare la media aritmetica. Solitamente, viene eliminato il 10% dei dati, ovvero il 5% all’inizio e alla fine della distribuzione. Per ottenere la media spuntata, i dati vengono ordinati in modo crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), e quindi viene eliminato il primo 5% e l’ultimo 5% dei dati nella sequenza ordinata. Infine, la media spuntata è calcolata come la media aritmetica dei dati rimanenti. Questo approccio è utile quando ci sono valori anomali o quando la distribuzione è asimmetrica e la media aritmetica non rappresenta adeguatamente la tendenza centrale dei dati.\nA titolo di esempio, procediamo al calcolo della media spuntata dei valori math per i due gruppi definiti dalla variabile graduated, escludendo il 10% dei valori più estremi.\n\nglimpse(df)\n#&gt; Rows: 1,274\n#&gt; Columns: 4\n#&gt; $ classtype &lt;chr&gt; \"small\", \"regular\", \"regular\", \"small\", \"small\", \"regular…\n#&gt; $ reading   &lt;int&gt; 578, 612, 583, 661, 614, 610, 595, 665, 616, 624, 593, 59…\n#&gt; $ math      &lt;int&gt; 610, 612, 606, 648, 636, 603, 610, 631, 636, 626, 601, 56…\n#&gt; $ graduated &lt;int&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, …\n\n\nnot_graduated &lt;- df[df$graduated == 0, \"math\"]\nmean(not_graduated, trim = 0.10, na.rm = TRUE)\n#&gt; [1] 605.6493\n\n\nnot_graduated &lt;- df[df$graduated == 1, \"math\"]\nmean(not_graduated, trim = 0.10, na.rm = TRUE)\n#&gt; [1] 634.4403\n\n\n12.2.3 Quantili\nIl quantile non interpolato di ordine \\(p\\) \\((0 &lt; p &lt; 1)\\) rappresenta il valore che divide la distribuzione dei dati in modo tale che una frazione \\(p\\) dei dati si trovi al di sotto di esso.\nLa formula per calcolare il quantile non interpolato è la seguente:\n\\[\n    q_p = x_{(k)},\n\\]\ndove \\(x_{(k)}\\) è l’elemento \\(k\\)-esimo nell’insieme di dati ordinato in modo crescente, e \\(k\\) è calcolato come:\n\\[\nk = \\lceil p \\cdot n \\rceil,\n\\]\ndove \\(n\\) è il numero totale di dati nel campione, e \\(\\lceil \\cdot \\rceil\\) rappresenta la funzione di arrotondamento all’intero successivo. In questa definizione, il quantile non interpolato corrisponde al valore effettivo nell’insieme di dati, senza effettuare alcuna interpolazione tra i valori circostanti.\nAd esempio, consideriamo il seguente insieme di dati: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}\\). Supponiamo di voler calcolare il quantile non interpolato di ordine \\(p = 0.3\\) (cioè il 30° percentile).\nOrdiniamo i dati in modo crescente: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}.\\) Calcoliamo \\(k\\) utilizzando la formula \\(k = \\lceil p \\cdot n \\rceil\\), dove \\(n\\) è il numero totale di dati nel campione. Nel nostro caso, \\(n = 10\\) e \\(p = 0.3\\):\n\\[\nk = \\lceil 0.3 \\cdot 10 \\rceil = \\lceil 3 \\rceil = 3.\n\\]\nIl quantile non interpolato corrisponde al valore \\(x_{(k)}\\), ovvero l’elemento \\(k\\)-esimo nell’insieme ordinato: \\(q_{0.3} = x_{(3)} = 23.\\)\nOltre al quantile non interpolato, esiste anche il concetto di quantile interpolato. A differenza del quantile non interpolato, il quantile interpolato può essere calcolato anche per percentili che non corrispondono esattamente a valori presenti nell’insieme di dati. Per ottenere il valore del quantile interpolato, viene utilizzato un procedimento di interpolazione lineare tra i valori adiacenti. In genere, il calcolo del quantile interpolato viene eseguito mediante l’uso di software dedicati.\nOra, procediamo al calcolo dei quantili di ordine 0.10 e 0.90 per i valori math all’interno dei due gruppi. I quantili sono dei valori che dividono la distribuzione dei dati in parti specifiche. Ad esempio, il quantile di ordine 0.10 corrisponde al valore al di sotto del quale si trova il 10% dei dati, mentre il quantile di ordine 0.90 rappresenta il valore al di sotto del quale si trova il 90% dei dati.\nCalcoliamo i quantili di ordine 0.1 e 0.9 della distribuzione dei punteggi math nei due gruppi definiti dalla variabile graduated.\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che hanno completato \n# la scuola superiore\nquantile(df[df$graduated == 1, \"math\"], probs = c(0.1, 0.9))\n#&gt; 10% 90% \n#&gt; 588 684\n\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che non hanno \n# completato la scuola superiore\nquantile(df[df$graduated == 0, \"math\"], probs = c(0.1, 0.9))\n#&gt;   10%   90% \n#&gt; 564.5 651.0\n\n\n12.2.4 Moda e mediana\nIn precedenza abbiamo già incontrato altri due popolari indici di tendenza centrale: la moda (Mo), che rappresenta il valore centrale della classe con la frequenza massima (in alcune distribuzioni può esserci più di una moda, rendendola multimodale e facendo perdere a questo indice il suo significato di indicatore di tendenza centrale); e la mediana (\\(\\tilde{x}\\)), che rappresenta il valore corrispondente al quantile di ordine 0.5 della distribuzione.\n\n12.2.5 Quando usare media, moda, mediana\nLa moda può essere utilizzata per dati a livello nominale o ordinale ed è l’unica tra le tre statistiche che può essere calcolata in questi casi.\nLa media, d’altra parte, è una buona misura di tendenza centrale solo se la distribuzione dei dati è simmetrica, ossia se i valori sono distribuiti uniformemente a sinistra e a destra della media. Tuttavia, se ci sono valori anomali o se la distribuzione è asimmetrica, la media può essere influenzata in modo significativo e, pertanto, potrebbe non essere la scelta migliore come misura di tendenza centrale.\nIn queste situazioni, la mediana può fornire una misura migliore di tendenza centrale rispetto alla media poiché è meno influenzata dai valori anomali e si basa esclusivamente sul valore centrale dell’insieme di dati. Di conseguenza, la scelta tra media e mediana dipende dal tipo di distribuzione dei dati e dagli obiettivi dell’analisi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#indici-di-dispersione",
    "href": "chapters/eda/07_loc_scale.html#indici-di-dispersione",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "\n12.3 Indici di dispersione",
    "text": "12.3 Indici di dispersione\nLe misure di posizione descritte in precedenza, come le medie e gli indici di posizione, offrono una sintesi dei dati mettendo in evidenza la tendenza centrale delle osservazioni. Tuttavia, trascurano un aspetto importante della distribuzione dei dati: la variabilità dei valori numerici della variabile statistica. Pertanto, è essenziale completare la descrizione della distribuzione di una variabile statistica utilizzando anche indicatori che valutino la dispersione delle unità statistiche. In questo modo, otterremo una visione più completa e approfondita delle caratteristiche del campione analizzato.\n\n12.3.1 Indici basati sull’ordinamento dei dati\nPer valutare la variabilità dei dati, è possibile utilizzare indici basati sull’ordinamento dei dati. L’indice più semplice è l’intervallo di variazione, che corrisponde alla differenza tra il valore massimo e il valore minimo di una distribuzione di dati. Tuttavia, questo indice ha il limite di essere calcolato basandosi solo su due valori della distribuzione, e non tiene conto di tutte le informazioni disponibili. Inoltre, l’intervallo di variazione può essere fortemente influenzato dalla presenza di valori anomali.\nUn altro indice basato sull’ordinamento dei dati è la differenza interquartile, già incontrata in precedenza. Anche se questo indice utilizza più informazioni rispetto all’intervallo di variazione, presenta comunque il limite di essere calcolato basandosi solo su due valori della distribuzione, ossia il primo quartile \\(Q_1\\) e il terzo quartile \\(Q_3\\).\nPer valutare la variabilità in modo più completo, è necessario utilizzare altri indici di variabilità che tengano conto di tutti i dati disponibili. In questo modo, si otterrà una valutazione più accurata della dispersione dei valori nella distribuzione e si potranno individuare eventuali pattern o tendenze nascoste.\n\n12.3.2 Varianza\nDate le limitazioni delle statistiche descritte in precedenza, è più comune utilizzare una misura di variabilità che tenga conto della dispersione dei dati rispetto a un indice di tendenza centrale. La varianza è la misura di variabilità più utilizzata per valutare la variabilità di una variabile statistica. Essa è definita come la media dei quadrati degli scarti \\(x_i - \\bar{x}\\) tra ogni valore e la media della distribuzione, come segue:\n\\[\n\\begin{equation}\nS^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\end{equation}\n\\tag{12.3}\\]\nLa varianza è una misura di dispersione più completa rispetto a quelle descritte in precedenza. Tuttavia, è appropriata solo nel caso di distribuzioni simmetriche ed è fortemente influenzata dai valori anomali, come altre misure di dispersione. Inoltre, la varianza è espressa in un’unità di misura che è il quadrato dell’unità di misura dei dati originali, pertanto, potrebbe non essere facilmente interpretata in modo intuitivo.\nCalcoliamo la varianza dei valori math per i dati del progetto STAR. Applicando l’equazione della varianza, otteniamo:\n\nsum((df$math - mean(df$math))^2) / length(df$math)\n#&gt; [1] 1507.233\n\nPiù semplicemente, possiamo usare la funzione var():\n\nvar(df$math) * (length(df$math) -1) / length(df$math)\n#&gt; [1] 1507.233\n\n\n12.3.2.1 Stima della varianza della popolazione\nSi noti il denominatore della formula della varianza. Nell’Equazione 12.3, ho utilizzato \\(n\\) come denominatore (l’ampiezza campionaria, ovvero il numero di osservazioni nel campione). In questo modo, otteniamo la varianza come statistica descrittiva del campione. Tuttavia, è possibile utilizzare \\(n-1\\) come denominatore alternativo:\n\\[\n\\begin{equation}\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{equation}\n\\tag{12.4}\\]\nIn questo secondo caso, otteniamo la varianza come stimatore della varianza della popolazione. Si può dimostrare che l’Equazione 12.4 fornisce una stima corretta (ovvero, non distorta) della varianza della popolazione da cui abbiamo ottenuto il campione, mentre l’Equazione 12.3 fornisce (in media) una stima troppo piccola della varianza della popolazione. Si presti attenzione alla notazione: \\(S^2\\) rappresenta la varianza come statistica descrittiva, mentre \\(s^2\\) rappresenta la varianza come stimatore.\nPer illustrare questo punto, svolgiamo una simulazione. Consideriamo la distribuzione dei punteggi del quoziente di intelligenza (QI). I valori del QI seguono una particolare distribuzione chiamata distribuzione normale, con media 100 e deviazione standard 15. La forma di questa distribuzione è illustrata nella figura seguente.\n\n# Define parameters\nx &lt;- seq(100 - 4 * 15, 100 + 4 * 15, by = 0.001)\nmu &lt;- 100\nsigma &lt;- 15\n\n# Compute the PDF\npdf &lt;- dnorm(x, mean = mu, sd = sigma)\n\n# Plot using ggplot2\ndata &lt;- data.frame(x = x, pdf = pdf)\nggplot(data, aes(x = x, y = pdf)) +\n  geom_line() +\n  labs(x = \"x\", y = \"f(x)\") \n\n\n\n\n\n\n\nSupponiamo di estrarre un campione casuale di 4 osservazioni dalla popolazione del quoziente di intelligenza – in altre parole, supponiamo di misurare il quoziente di intelligenza di 4 persone prese a caso dalla popolazione.\n\nset.seed(123) \nx &lt;- rnorm(4, mean = 100, sd = 15)\nprint(x)\n#&gt; [1]  91.59287  96.54734 123.38062 101.05763\n\nCalcoliamo la varianza usando \\(n\\) al denominatore. Si noti che la vera varianza del quoziente di intelligenza è \\(15^2\\) = 225.\n\nvar(x)\n#&gt; [1] 196.9395\n\nConsideriamo ora 10 campioni casuali del QI, ciascuno di ampiezza 4.\n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10\nrandom_samples &lt;- list()\n\nset.seed(123) \n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nIl primo campione è\n\nrandom_samples[1]\n#&gt; [[1]]\n#&gt; [1]  91.59287  96.54734 123.38062 101.05763\n\nIl decimo campione è\n\nrandom_samples[10]\n#&gt; [[1]]\n#&gt; [1] 108.30876  99.07132  95.41056  94.29293\n\nStampiamo i valori di tutti i 10 campioni.\n\nrs &lt;- do.call(rbind, random_samples)\nrs\n#&gt;            [,1]      [,2]      [,3]      [,4]\n#&gt;  [1,]  91.59287  96.54734 123.38062 101.05763\n#&gt;  [2,] 101.93932 125.72597 106.91374  81.02408\n#&gt;  [3,]  89.69721  93.31507 118.36123 105.39721\n#&gt;  [4,] 106.01157 101.66024  91.66238 126.80370\n#&gt;  [5,] 107.46776  70.50074 110.52034  92.90813\n#&gt;  [6,]  83.98264  96.73038  84.60993  89.06663\n#&gt;  [7,]  90.62441  74.69960 112.56681 102.30060\n#&gt;  [8,]  82.92795 118.80722 106.39696  95.57393\n#&gt;  [9,] 113.42688 113.17200 112.32372 110.32960\n#&gt; [10,] 108.30876  99.07132  95.41056  94.29293\n\nPer ciascun campione (ovvero, per ciascuna riga della matrice precedente), calcoliamo la varianza usando la formula con \\(n\\) al denominatore. Otteniamo così 10 stime della varianza della popolazione del QI.\n\nx_var &lt;- apply(rs, 1, var)  # Applica la funzione var su ciascuna riga\nprint(x_var)\n#&gt;  [1] 196.939534 337.535917 168.546563 218.684024 333.475844  34.520447\n#&gt;  [7] 264.378073 234.081410   1.970867  40.468397\n\nNotiamo due cose:\n\nle stime sono molto diverse tra loro; questo fenomeno è noto con il nome di variabilità campionaria;\nin media le stime sono troppo piccole.\n\nPer aumentare la sicurezza riguardo al secondo punto menzionato in precedenza, ripeteremo la simulazione utilizzando un numero di iterazioni maggiore.\n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10000\nrandom_samples &lt;- list()\n\nset.seed(123) # Replace 123 with your desired seed for reproducibility\n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nrs &lt;- do.call(rbind, random_samples)\nx_var &lt;- apply(rs, 1, var) * (size - 1) / size  # Adjust for population variance (ddof = 0)\n\nEsaminiamo la distribuzione dei valori ottenuti.\n\n# Create a data frame for plotting\ndata &lt;- data.frame(x_var = x_var)\n\n# Plot the histogram using ggplot2\nggplot(data, aes(x = x_var)) +\n  geom_histogram(bins = 10, alpha = 0.5, fill = \"blue\") +\n  labs(x = \"Varianza\", y = \"Frequenza\", title = \"Varianza del QI in campioni di n = 4\")\n\n\n\n\n\n\n\nLa stima più verosimile della varianza del QI è dato dalla media di questa distribuzione.\n\nmean(x_var)\n#&gt; [1] 168.9337\n\nSi noti che il nostro spospetto è stato confermato: il valore medio della stima della varianza ottenuta con l’Equazione 12.3 è troppo piccolo rispetto al valore corretto di \\(15^2 = 225\\).\nRipetiamo ora la simulazione usando la formula della varianza con \\(n-1\\) al denominatore.\n\nmu &lt;- 100\nsigma &lt;- 15\nsize &lt;- 4\nniter &lt;- 10000\nrandom_samples &lt;- list()\n\nset.seed(123) # Replace 123 with your desired seed for reproducibility\n\nfor (i in 1:niter) {\n  one_sample &lt;- rnorm(size, mean = mu, sd = sigma)\n  random_samples[[i]] &lt;- one_sample\n}\n\nrs &lt;- do.call(rbind, random_samples)\nx_var &lt;- apply(rs, 1, var)  # ddof = 1 is default for var in R\n\nmean(x_var)\n#&gt; [1] 225.2449\n\nNel secondo caso, se utilizziamo \\(n-1\\) come denominatore per calcolare la stima della varianza, il valore atteso di questa stima è molto vicino al valore corretto di 225. Se il numero di campioni fosse infinito, i due valori sarebbero identici.\nIn conclusione, le due formule della varianza hanno scopi diversi. La formula della varianza con \\(n\\) al denominatore viene utilizzata come statistica descrittiva per descrivere la variabilità di un particolare campione di osservazioni. D’altro canto, la formula della varianza con \\(n-1\\) al denominatore viene utilizzata come stimatore per ottenere la migliore stima della varianza della popolazione da cui quel campione è stato estratto.\n\n12.3.3 Deviazione standard\nPer interpretare la varianza in modo più intuitivo, si può calcolare la deviazione standard (o scarto quadratico medio o scarto tipo) prendendo la radice quadrata della varianza. La deviazione standard è espressa nell’unità di misura originaria dei dati, a differenza della varianza che è espressa nel quadrato dell’unità di misura dei dati. La deviazione standard fornisce una misura della dispersione dei dati attorno alla media, rendendo più facile la comprensione della variabilità dei dati.\nLa deviazione standard (o scarto quadratico medio, o scarto tipo) è definita come:\n\\[\ns^2 = \\sqrt{(n-1)^{-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\tag{12.5}\\]\nQuando tutte le osservazioni sono uguali, \\(s = 0\\), altrimenti \\(s &gt; 0\\).\n\n\n\n\n\n\nIl termine standard deviation è stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca \\(\\sigma\\) che lo rappresenta. Il termine italiano “deviazione standard” ne è la traduzione più utilizzata nel linguaggio comune; il termine dell’Ente Nazionale Italiano di Unificazione è tuttavia “scarto tipo”, definito come la radice quadrata positiva della varianza.\n\n\n\nLa deviazione standard \\(s\\) dovrebbe essere utilizzata solo quando la media è una misura appropriata per descrivere il centro della distribuzione, ad esempio nel caso di distribuzioni simmetriche. Tuttavia, è importante tener conto che, come la media \\(\\bar{x}\\), anche la deviazione standard è fortemente influenzata dalla presenza di dati anomali, ovvero pochi valori che si discostano notevolmente dalla media rispetto agli altri dati della distribuzione. In presenza di dati anomali, la deviazione standard può risultare ingannevole e non rappresentare accuratamente la variabilità complessiva della distribuzione. Pertanto, è fondamentale considerare attentamente il contesto e le caratteristiche dei dati prima di utilizzare la deviazione standard come misura di dispersione. In alcune situazioni, potrebbe essere più appropriato ricorrere a misure di dispersione robuste o ad altre statistiche descrittive per caratterizzare la variabilità dei dati in modo più accurato e affidabile.\nPer fare un esempio, calcoliamo la deviazione standard per i valori math del campione di dati del progetto STAR. Applicando l’Equazione 12.5, per tutto il campione abbiamo\n\nsd(df$math)\n#&gt; [1] 38.83834\n\nPer ciascun gruppo, abbiamo:\n\ndf |&gt; \n  group_by(graduated) |&gt; \n  summarise(std_math = sd(math, na.rm = TRUE)) \n#&gt; # A tibble: 2 × 2\n#&gt;   graduated std_math\n#&gt;       &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1         0     34.1\n#&gt; 2         1     38.1\n\n\n12.3.3.1 Interpretazione\nLa deviazione standard può essere interpretata in modo semplice: essa rappresenta la dispersione dei dati rispetto alla media aritmetica. È simile allo scarto semplice medio campionario, cioè alla media aritmetica dei valori assoluti degli scarti tra ciascuna osservazione e la media, anche se non è identica. La deviazione standard ci fornisce un’indicazione di quanto, in media, le singole osservazioni si discostino dal centro della distribuzione.\nPer verificare l’interpretazione della deviazione standard, utilizziamo i valori math del campione di dati del progetto STAR.\n\nsd(df$math)\n#&gt; [1] 38.83834\n\nLa deviazione standard calcolata per questi dati è \\(\\approx 38.8\\). Questo valore ci indica che, in media, ogni osservazione si discosta di circa 38.8 punti dalla media aritmetica dei punteggi math. Maggiore è il valore della deviazione standard, maggiore è la dispersione dei dati attorno alla media, mentre un valore più piccolo indica che i dati sono più concentrati vicino alla media. La deviazione standard ci offre quindi una misura quantitativa della variabilità dei dati nella distribuzione.\nPer questi dati, lo scarto semplice medio campionario è\n\nmean(abs(df$math - mean(df$math, na.rm = TRUE)), na.rm = TRUE)\n#&gt; [1] 30.96827\n\nSi noti che i due valori sono simili, ma non identici.\n\n12.3.4 Deviazione mediana assoluta\nUna misura robusta della dispersione statistica di un campione è la deviazione mediana assoluta (Median Absolute Deviation, MAD) definita come la mediana del valore assoluto delle deviazioni dei dati dalla mediana. Matematicamente, la formula per calcolare la MAD è:\n\\[\n\\text{MAD} = \\text{median} \\left( |X_i - \\text{median}(X)| \\right)\n\\tag{12.6}\\]\nLa deviazione mediana assoluta è particolarmente utile quando si affrontano distribuzioni con presenza di dati anomali o asimmetrie, poiché è meno influenzata da questi valori estremi rispetto alla deviazione standard.\nQuando i dati seguono una distribuzione gaussiana (normale), esiste una relazione specifica tra MAD e la deviazione standard (si veda il Capitolo {ref}cont-rv-distr-notebook). In una distribuzione normale, la MAD è proporzionale alla deviazione standard. La costante di proporzionalità dipende dalla forma esatta della distribuzione normale, ma in generale, la relazione è data da:\n\\[\n\\sigma \\approx k \\times \\text{MAD},\n\\]\ndove:\n\n\n\\(\\sigma\\) è la deviazione standard.\nMAD è la Mediana della Deviazione Assoluta.\n\n\\(k\\) è una costante che, per una distribuzione normale, è tipicamente presa come circa 1.4826.\n\nQuesta costante di 1.4826 è derivata dal fatto che, in una distribuzione normale, circa il 50% dei valori si trova entro 0.6745 deviazioni standard dalla media. Quindi, per convertire la MAD (basata sulla mediana) nella deviazione standard (basata sulla media), si usa il reciproco di 0.6745, che è approssimativamente 1.4826.\nLa formula completa per convertire la MAD in una stima della deviazione standard in una distribuzione normale è:\n\\[\n\\sigma \\approx 1.4826 \\times \\text{MAD}\n\\]\nQuesta relazione è utile per stimare la deviazione standard in modo più robusto, specialmente quando si sospetta la presenza di outlier o si ha a che fare con campioni piccoli. Di conseguenza, molti software restituiscono il valore MAD moltiplicato per questa costante per fornire un’indicazione più intuitiva della variabilità dei dati. Tuttavia, è importante notare che questa relazione si mantiene accurata solo per le distribuzioni che sono effettivamente normali. In presenza di distribuzioni fortemente asimmetriche o con elevati outlier, la deviazione standard e la MAD possono fornire indicazioni molto diverse sulla variabilità dei dati.\nPer verificare questo principio, calcoliamo la deviazione mediana assoluta dei valori math del campione di dati del progetto STAR.\n\n1.4826 * median(abs(df$math - median(df$math, na.rm = TRUE)), na.rm = TRUE)\n#&gt; [1] 41.5128\n\nIn questo caso, la MAD per i punteggi di matematica è simile alla deviazione standard.\n\nsd(df$math)\n#&gt; [1] 38.83834\n\nInfatti, la distribuzione dei punteggi math è approssimativamente gaussiana.\n\nggplot(df, aes(x = math)) +\n  geom_histogram(bins = 10, alpha = 0.5, fill = \"blue\") +\n  labs(x = \"math\", y = \"Frequenza\", title = \"Distribuzione dei Punteggi di Matematica\")\n\n\n\n\n\n\n\nVerifichiamo nuovamente il principio usando un campione di dati estratto da una popolazione normale. Usiamo, ad esempio, la distribuzione \\(\\mathcal{N}(100, 15)\\):\n\nset.seed(123) \nx &lt;- rnorm(10000, mean = 100, sd = 15)\n1.4826 * median(abs(x - median(x)))\n#&gt; [1] 14.92317\n\n\n12.3.5 Quando usare la deviazione standard e MAD\nLa deviazione standard e la MAD sono entrambe misure di dispersione che forniscono informazioni su quanto i dati in un insieme si discostano dalla tendenza centrale. Tuttavia, ci sono alcune differenze tra le due misure e situazioni in cui può essere più appropriato utilizzare una rispetto all’altra.\n\nDeviazione standard: Questa misura è particolarmente utile per descrivere la dispersione dei dati in una distribuzione normale. La deviazione standard è una scelta appropriata se si vuole sapere quanto i dati sono distribuiti intorno alla media, o se si vuole confrontare la dispersione di due o più set di dati. Tuttavia, la deviazione standard è fortemente influenzata dalla presenza di dati anomali, e questo può rappresentare una limitazione in casi in cui sono presenti valori estremi nell’insieme di dati.\nDeviazione mediana assoluta (MAD): La MAD è meno sensibile ai valori anomali rispetto alla deviazione standard, il che la rende una scelta migliore quando ci sono valori anomali nell’insieme di dati. Inoltre, la MAD può essere una buona scelta quando si lavora con dati non normalmente distribuiti, poiché non assume una distribuzione specifica dei dati. La MAD è calcolata utilizzando la mediana e i valori assoluti delle deviazioni dei dati dalla mediana, il che la rende una misura robusta di dispersione.\n\nIn sintesi, se si sta lavorando con dati normalmente distribuiti, la deviazione standard è la misura di dispersione più appropriata. Se si lavora con dati non normalmente distribuiti o si hanno valori anomali nell’insieme di dati, la MAD può essere una scelta migliore. In ogni caso, la scelta tra le due misure dipende dal tipo di dati che si sta analizzando e dall’obiettivo dell’analisi.\n\n12.3.6 Indici di variabilità relativi\nA volte può essere necessario confrontare la variabilità di grandezze incommensurabili, ovvero di caratteri misurati con differenti unità di misura. In queste situazioni, le misure di variabilità descritte in precedenza diventano inadeguate poiché dipendono dall’unità di misura utilizzata. Per superare questo problema, si ricorre a specifici numeri adimensionali chiamati indici relativi di variabilità.\nIl più importante di questi indici è il coefficiente di variazione (\\(C_v\\)), definito come il rapporto tra la deviazione standard (\\(\\sigma\\)) e la media dei dati (\\(\\bar{x}\\)):\n\\[\nC_v = \\frac{\\sigma}{\\bar{x}}.\n\\tag{12.7}\\]\nIl coefficiente di variazione è un numero puro e permette di confrontare la variabilità di distribuzioni con unità di misura diverse.\nUn altro indice relativo di variabilità è la differenza interquartile rapportata a uno dei tre quartili (primo quartile, terzo quartile o mediana). Questo indice è definito come:\n\\[\n\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\n\\]\nQuesti indici relativi di variabilità forniscono una misura adimensionale della dispersione dei dati, rendendo possibile il confronto tra grandezze con diverse unità di misura e facilitando l’analisi delle differenze di variabilità tra i dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#la-fallacia-ergodica",
    "href": "chapters/eda/07_loc_scale.html#la-fallacia-ergodica",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "\n12.4 La fallacia ergodica",
    "text": "12.4 La fallacia ergodica\nSebbene il concetto di “media” possa sembrare chiaro, ciò non implica che il suo utilizzo non presenti delle problematiche nell’ambito della pratica psicologica. Un aspetto su cui vale la pena soffermarsi è ciò che viene definito “fallacia ergodica”.\nIl concetto di “fallacia ergodica” (Speelman et al., 2024) si riferisce all’errore compiuto dai ricercatori quando assumono che le caratteristiche medie di un gruppo di individui possano essere applicate a ciascun individuo all’interno di quel gruppo, senza considerare le differenze individuali o le variazioni nel tempo. Questa fallacia emerge dalla pratica comune nella ricerca psicologica di raccogliere dati aggregati da gruppi di persone per stimare parametri della popolazione, al fine di confrontare comportamenti in condizioni diverse o esplorare associazioni tra diverse misurazioni della stessa persona.\nIl problema di questo approccio è che l’uso dei risultati basati sul gruppo per caratterizzare le caratteristiche degli individui o per estrapolare a persone simili a quelle del gruppo è ingiustificato, poiché le medie di gruppo possono fornire informazioni solo sui risultati collettivi, come la performance media del gruppo, e non consentono di fare affermazioni accurate sugli individui che compongono quel gruppo. La fallacia ergodica si basa sull’assunzione che per utilizzare legittimamente una statistica aggregata (ad esempio, la media) derivata da un gruppo per descrivere un individuo di quel gruppo, due condizioni devono essere soddisfatte: gli individui devono essere così simili da essere praticamente interscambiabili, e le caratteristiche degli individui devono essere temporalmente stabili.\nTuttavia, i fenomeni e i processi psicologici di interesse per i ricercatori sono per natura non uniformi tra gli individui e variabili nel tempo, sia all’interno degli individui che tra di loro. Di conseguenza, i risultati ottenuti dalla media di misure di comportamenti, cognizioni o stati emotivi di più individui non descrivono accuratamente nessuno di quegli individui in un dato momento, né possono tenere conto dei cambiamenti in quelle variabili per un individuo nel tempo.\nSpeelman et al. (2024) osservano che la stragrande maggioranza degli articoli che hanno analizzato include conclusioni nelle sezioni degli Abstract e/o delle Discussioni che implicano che i risultati trovati con dati aggregati di gruppo si applichino anche agli individui in quei gruppi e/o si applichino agli individui nella popolazione. Questa pratica riflette la fallacia ergodica, che consiste nell’assumere che i campioni siano sistemi ergodici quando non lo sono.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#riflessioni-conclusive",
    "href": "chapters/eda/07_loc_scale.html#riflessioni-conclusive",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "\n12.5 Riflessioni Conclusive",
    "text": "12.5 Riflessioni Conclusive\nLe statistiche descrittive ci permettono di ottenere indicatori sintetici che riassumono i dati di una popolazione o di un campione estratto da essa. Questi indicatori includono misure di tendenza centrale, come la media, la mediana e la moda, che ci forniscono informazioni sulla posizione centrale dei dati rispetto alla distribuzione. Inoltre, ci sono gli indici di dispersione, come la deviazione standard e la varianza, che ci indicano quanto i dati si disperdono attorno alla tendenza centrale. Questi indici ci aiutano a comprendere quanto i valori si discostano dalla media, e quindi ci forniscono un’idea della variabilità dei dati. In conclusione, le statistiche descrittive ci offrono un quadro sintetico delle caratteristiche principali dei dati, consentendoci di comprendere meglio la loro distribuzione e variabilità.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/07_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] grid      stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] vcd_1.4-13        MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n#&gt;  [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n#&gt;  [9] bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n#&gt; [13] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n#&gt; [17] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n#&gt; [21] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3        \n#&gt; [25] here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] gtable_0.3.6      xfun_0.49         htmlwidgets_1.6.4 rstatix_0.7.2    \n#&gt;  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#&gt;  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pacman_0.5.1     \n#&gt; [13] R.oo_1.27.0       pkgconfig_2.0.3   data.table_1.16.2 lifecycle_1.0.4  \n#&gt; [17] compiler_4.4.2    farver_2.1.2      munsell_0.5.1     mnormt_2.1.1     \n#&gt; [21] carData_3.0-5     httpuv_1.6.15     htmltools_0.5.8.1 yaml_2.3.10      \n#&gt; [25] Formula_1.2-5     car_3.1-3         pillar_1.9.0      later_1.4.0      \n#&gt; [29] R.utils_2.12.3    abind_1.4-8       nlme_3.1-166      mime_0.12        \n#&gt; [33] tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4     labeling_0.4.3   \n#&gt; [37] rprojroot_2.0.4   fastmap_1.2.0     colorspace_2.1-1  cli_3.6.3        \n#&gt; [41] magrittr_2.0.3    utf8_1.2.4        broom_1.0.7       withr_3.0.2      \n#&gt; [45] backports_1.5.0   promises_1.3.1    timechange_0.3.0  rmarkdown_2.29   \n#&gt; [49] ggsignif_0.6.4    R.methodsS3_1.8.2 zoo_1.8-12        hms_1.1.3        \n#&gt; [53] shiny_1.9.1       evaluate_1.0.1    lmtest_0.9-40     miniUI_0.1.1.1   \n#&gt; [57] rlang_1.1.4       Rcpp_1.0.13-1     xtable_1.8-4      glue_1.8.0       \n#&gt; [61] jsonlite_1.8.9    R6_2.5.1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/07_loc_scale.html#bibliografia",
    "href": "chapters/eda/07_loc_scale.html#bibliografia",
    "title": "12  Indicatori di tendenza centrale e variabilità",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nSpeelman, C. P., Parker, L., Rapley, B. J., & McGann, M. (2024). Most Psychological Researchers Assume Their Samples Are Ergodic: Evidence From a Year of Articles in Three Major Journals. Collabra: Psychology, 10(1).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Indicatori di tendenza centrale e variabilità</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html",
    "href": "chapters/eda/10_estimand.html",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nIn questo capitolo abbiamo esplorato diverse tecniche di analisi esplorativa dei dati, utili per sintetizzare grandi quantità di informazioni e visualizzare le distribuzioni delle variabili e le relazioni tra esse. Abbiamo presunto che le variabili siano state misurate correttamente per rispondere a una specifica domanda teorica. Tuttavia, invece di considerare questo legame tra estimandi empirici e teorici come scontato, è importante riflettere criticamente su tale relazione. A tal fine, esamineremo l’articolo di Lundberg et al. (2021), intitolato “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory”. Il punto centrale dell’articolo è l’importanza di definire con chiarezza l’estimando chiave in qualsiasi studio quantitativo, affinché questo possa rispondere in modo preciso alla domanda di ricerca.\nL’estimando è la quantità che uno studio intende stimare, fungendo da collegamento tra teoria ed evidenza statistica. Gli autori propongono un approccio metodologico articolato in tre fasi principali:\nQuesto approccio chiarisce come le evidenze empiriche possano rispondere alle domande teoriche alla base della ricerca. In sintesi, Lundberg et al. (2021) esortano i ricercatori a definire con precisione l’estimando teorico, indipendentemente dal modello statistico utilizzato, per rendere esplicito il collegamento tra teoria ed evidenza empirica.\nIn altre parole, gli autori sottolineano che non è sufficiente affermare che lo scopo di uno studio è verificare se un coefficiente di regressione sia “significativamente diverso da zero”, poiché in questo caso l’estimando è definito solo in relazione al modello statistico. Al contrario, è necessario distinguere tra estimando teorico (l’obiettivo concettuale della ricerca, ad esempio l’apprendimento associativo) ed estimando empirico (la misura osservabile). Ad esempio, l’estimando empirico potrebbe essere rappresentato dal tasso di apprendimento \\(\\alpha\\) o dalla temperatura inversa \\(\\beta\\), parametri del modello Rescorla-Wagner applicato ai dati di un compito di Probabilistic Reversal Learning (PRL). Questi parametri spiegano l’apprendimento associativo in termini di predisposizione del soggetto a modificare il valore attribuito agli stimoli o la sua strategia di scelta (esplorazione vs sfruttamento) — si veda la sezione ?sec-rescorla-wagner.\nÈ cruciale riconoscere che gli estimandi empirici possono essere calcolati in modi diversi. Il modello Rescorla-Wagner è solo uno dei tanti modelli per rappresentare l’apprendimento associativo, e i parametri possono essere stimati attraverso vari metodi. Inoltre, i dati su cui si basano queste analisi possono essere raccolti in esperimenti progettati in modi differenti o applicati a popolazioni diverse. Pertanto, l’estimando empirico, ossia il valore numerico che rappresenta la capacità di apprendimento associativo, può variare a seconda del modello, delle tecniche di stima e del design sperimentale utilizzati.\nIn definitiva, Lundberg et al. (2021) sottolineano che i ricercatori devono definire la loro domanda di ricerca in termini di un estimando teorico che sia indipendente dal metodo di analisi dei dati. Successivamente, devono giustificare la scelta di uno specifico estimando empirico, spiegando perché, dati i dati disponibili, hanno optato per una particolare strategia di stima piuttosto che per altre, visto che molteplici opzioni sono generalmente disponibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#introduzione",
    "href": "chapters/eda/10_estimand.html#introduzione",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "",
    "text": "Definire un estimando teorico, collegandolo chiaramente alla teoria che guida la ricerca.\nCollegare l’estimando teorico a un estimando empirico, ossia una misura ottenuta dai dati osservabili che fornisca informazioni sull’estimando teorico, considerando una serie di assunzioni di identificazione.\nApprendere dai dati, scegliendo strategie di stima appropriate per ottenere l’estimando empirico in modo rigoroso.\n\n\n\n\n\n\n\n\n\n\n\nIn italiano, la traduzione comunemente usata di “estimand” nella letteratura scientifica è estimando. Questo termine viene utilizzato per riferirsi alla quantità o al parametro che si desidera stimare in un’analisi statistica.\nStimatore, invece, è la traduzione di “estimator” e si riferisce alla regola o alla funzione utilizzata per calcolare una stima basata sui dati osservati. Quindi, “estimando” e “stimatore” sono termini distinti: l’“estimando” è l’oggetto dell’inferenza statistica, mentre lo “stimatore” è il metodo o la formula usata per ottenere l’inferenza.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#limiti-dellapproccio-attuale",
    "href": "chapters/eda/10_estimand.html#limiti-dellapproccio-attuale",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.1 Limiti dell’Approccio Attuale",
    "text": "14.1 Limiti dell’Approccio Attuale\nLundberg et al. (2021) osservano che spesso i ricercatori sociali omettono il passaggio cruciale della definizione dell’estimando, concentrandosi direttamente sui dati e sulle procedure statistiche. Questo approccio può causare una mancanza di chiarezza riguardo a ciò che si intende effettivamente stimare, limitando anche l’uso di modelli statistici alternativi che potrebbero essere più adatti a rispondere alla domanda di ricerca. Sebbene Lundberg et al. (2021) facciano riferimento alla letteratura sociologica, questi stessi argomenti sono applicabili anche alla psicologia.\nIl problema del collegamento tra estimandi teorici ed empirici (si veda la figura seguente) può essere illustrato con un esempio in psicologia riguardante l’intelligenza. La distinzione tra estimandi teorici ed empirici è cruciale: gli estimandi teorici possono includere quantità non osservabili, come i costrutti latenti, ad esempio l’intelligenza come concetto astratto. Gli estimandi empirici, invece, riguardano esclusivamente dati osservabili, come i punteggi ottenuti in un test di intelligenza.\nNel caso dell’intelligenza, la scelta dell’estimando teorico richiede un’argomentazione sostanziale riguardo alla teoria dell’intelligenza adottata e agli obiettivi della ricerca. Ad esempio, se si vuole studiare l’intelligenza generale (fattore g), bisogna chiarire come questo costrutto viene teoricamente definito e perché è rilevante per lo studio.\nD’altra parte, la scelta dell’estimando empirico richiede un’argomentazione concettuale su come i dati osservabili, come i risultati dei test di intelligenza, possano rappresentare il costrutto latente di interesse. È necessario spiegare quali dati vengono utilizzati per inferire il costrutto teorico e quali assunzioni si fanno riguardo al rapporto tra le misure osservate e il costrutto latente.\nInfine, la scelta delle strategie di stima, come l’uso di modelli di equazioni strutturali per stimare l’intelligenza generale da diversi test, è una decisione separata, che può essere in parte guidata dai dati disponibili e dalle caratteristiche della misurazione. Separare chiaramente questi passaggi aiuta i ricercatori a fare scelte informate e fondate, consente ai lettori di valutare in modo critico le affermazioni fatte e permette alla comunità scientifica di costruire su basi solide per futuri sviluppi della ricerca.\n\n\n\nTre Scelte Critiche nelle Argomentazioni delle Scienze Sociali Quantitative. La prima scelta riguarda gli estimandi teorici, che definiscono gli obiettivi dell’inferenza. È necessario un argomento che colleghi gli estimandi teorici alla teoria più ampia. La seconda scelta riguarda gli estimandi empirici, che collegano questi obiettivi ai dati osservabili. Questo collegamento richiede delle assunzioni sostanziali, che possono essere formalizzate attraverso grafici aciclici diretti. La terza scelta riguarda le strategie di stima, che determinano come verranno effettivamente utilizzati i dati. La selezione delle strategie di stima si basa sui dati disponibili (figura tratta da Lundberg et al. (2021)).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#definizione-dellestimando-teorico",
    "href": "chapters/eda/10_estimand.html#definizione-dellestimando-teorico",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.2 Definizione dell’Estimando Teorico",
    "text": "14.2 Definizione dell’Estimando Teorico\nLa definizione dell’estimando teorico è cruciale per determinare la natura dello studio perché specifica chiaramente quale tipo di relazione tra le variabili stiamo cercando di indagare. In altre parole, l’estimando teorico indica se lo studio mira a descrivere, prevedere o stabilire una relazione causale. Vediamo come questo funziona in pratica con esempi legati all’intelligenza e all’allenamento cognitivo.\n\n14.2.1 1. Estimando Teorico in uno Studio Descrittivo\nUno studio descrittivo ha come obiettivo semplicemente quello di caratterizzare o descrivere una certa realtà o fenomeno senza inferire relazioni di causa-effetto. In questo caso, l’estimando teorico potrebbe essere una misura che riassume una caratteristica della popolazione.\nEsempio: Qual è il punteggio medio di intelligenza tra le persone che hanno partecipato a un programma di allenamento cognitivo rispetto a quelle che non l’hanno fatto?\nEstimando Teorico: La differenza media nei punteggi di intelligenza tra i due gruppi. Questo tipo di estimando descrive la distribuzione dei punteggi di intelligenza nei gruppi, ma non implica che l’allenamento abbia causato le differenze osservate.\n\n\n14.2.2 2. Estimando Teorico in uno Studio Predittivo\nUno studio predittivo si concentra sulla capacità di prevedere un risultato basato su dati osservabili. Qui, l’estimando teorico riguarda la capacità del modello di predire correttamente i risultati futuri, ma senza implicazioni causali.\nEsempio: In che misura la partecipazione a un programma di allenamento cognitivo può prevedere il punteggio di intelligenza futuro di una persona?\nEstimando Teorico: La previsione del punteggio di intelligenza basata sulla partecipazione all’allenamento cognitivo. Questo estimando si basa su modelli statistici che utilizzano variabili osservabili per fare previsioni, ma non determinano la causalità tra allenamento e punteggi di intelligenza.\n\n\n14.2.3 3. Estimando Teorico in uno Studio Causale\nUno studio causale cerca di stabilire un nesso diretto di causa-effetto tra variabili. L’estimando teorico in questo caso riguarda l’effetto diretto di una variabile indipendente su una variabile dipendente, tenendo conto di altre variabili confondenti.\nEsempio: L’allenamento cognitivo causa un aumento nei punteggi di intelligenza?\nEstimando Teorico: La differenza media nei punteggi di intelligenza che si attribuisce direttamente all’effetto dell’allenamento cognitivo, controllando per tutte le altre variabili confondenti. Questo estimando implica l’uso di un disegno di ricerca che isola l’effetto dell’allenamento, come un esperimento con assegnazione casuale.\n\n\n14.2.4 Come l’Estimando Teorico Chiarisce la Natura dello Studio\nDefinire l’estimando teorico in modo preciso aiuta a chiarire la natura dello studio perché specifica esattamente quale relazione tra le variabili viene studiata:\n\nStudi Descrittivi: L’estimando teorico è una semplice descrizione di dati, come una media o una differenza, senza inferire causalità.\nStudi Predittivi: L’estimando teorico si concentra sulla capacità di un modello di fare previsioni basate sui dati, senza implicazioni causali.\nStudi Causali: L’estimando teorico cerca di determinare l’effetto diretto di una variabile su un’altra, richiedendo un disegno di studio che possa controllare variabili confondenti per isolare la causalità.\n\nIn sintesi, l’estimando teorico orienta il ricercatore nel definire chiaramente se lo scopo dello studio è descrittivo, predittivo o causale, e guida il disegno dello studio e l’analisi dei dati di conseguenza.\n\n\n14.2.5 Importanza dei DAG nel Contesto degli Estimandi Teorici\nNella figura 2 dell’articolo di Lundberg et al. (2021), i Grafici Aciclici Diretti (DAG) vengono utilizzati per illustrare le relazioni causali tra variabili all’interno di uno studio. I DAG sono strumenti visivi che aiutano i ricercatori a rappresentare e comprendere le assunzioni causali sottostanti ai loro studi, fornendo una chiara rappresentazione grafica di come le variabili si influenzano a vicenda. Questo è particolarmente importante quando si definiscono estimandi teorici, perché i DAG consentono di identificare chiaramente le variabili confondenti e di stabilire le relazioni di causalità.\nI DAG possono contribuire alla definizione degli estimandi teorici in molti modi.\n\nChiarificazione delle Relazioni Causali: I DAG aiutano a chiarire quali variabili sono considerate come cause potenziali e quali come effetti. Questo è fondamentale per definire l’estimando teorico, soprattutto in uno studio causale, dove è importante distinguere tra correlazione e causalità. Ad esempio, se si studia l’effetto dell’allenamento cognitivo sull’intelligenza, un DAG può mostrare come l’allenamento influisce direttamente sull’intelligenza, identificando al contempo variabili confondenti come il background educativo o la motivazione.\nIdentificazione delle Variabili Confondenti: Uno dei principali vantaggi dell’utilizzo dei DAG è la loro capacità di identificare le variabili confondenti che possono influenzare entrambe le variabili di interesse. Nel contesto degli estimandi teorici, riconoscere e controllare queste variabili confondenti è cruciale per stabilire una relazione causale valida. Ad esempio, un DAG potrebbe rivelare che la motivazione personale influisce sia sulla partecipazione all’allenamento cognitivo che sui punteggi di intelligenza, indicando che questa variabile deve essere controllata per ottenere un estimando causale corretto.\nGuida nella Costruzione del Disegno di Ricerca: I DAG sono strumenti utili nella pianificazione del disegno di ricerca perché aiutano a determinare quali variabili devono essere misurate e controllate. Definendo chiaramente le relazioni tra le variabili, i ricercatori possono progettare esperimenti o studi osservazionali che minimizzano i bias e migliorano la validità interna dello studio. Ad esempio, un DAG può suggerire la necessità di randomizzare l’assegnazione all’allenamento cognitivo per garantire che l’effetto osservato sui punteggi di intelligenza sia realmente causato dall’allenamento e non da un’altra variabile.\nSupporto nella Selezione delle Strategie di Stima: Una volta definite le relazioni tra le variabili attraverso un DAG, i ricercatori possono scegliere strategie di stima appropriate per gli estimandi teorici ed empirici. Per esempio, se un DAG indica che non ci sono percorsi diretti tra alcune variabili, si possono utilizzare metodi statistici che presuppongono l’indipendenza condizionale, come la regressione lineare o i modelli di equazioni strutturali.\n\nIn sintesi, nel contesto della definizione degli estimandi teorici, i DAG sono strumenti essenziali che consentono ai ricercatori di visualizzare e comprendere le relazioni causali e le variabili confondenti all’interno di uno studio. Essi facilitano la costruzione di disegni di ricerca solidi, la selezione di strategie di stima appropriate e la comunicazione chiara delle assunzioni causali sottostanti. Utilizzando i DAG, i ricercatori possono garantire che gli estimandi teorici siano ben definiti e che le inferenze tratte dai dati siano valide e affidabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#collegamento-tra-estimando-teorico-ed-empirico",
    "href": "chapters/eda/10_estimand.html#collegamento-tra-estimando-teorico-ed-empirico",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.3 Collegamento tra Estimando Teorico ed Empirico",
    "text": "14.3 Collegamento tra Estimando Teorico ed Empirico\nLundberg et al. (2021) sottolineano l’importanza di collegare chiaramente l’estimando teorico all’estimando empirico, utilizzando assunzioni sostanziali e metodi appropriati per garantire che le conclusioni tratte dai dati siano valide.\nEstimando Empirico: L’estimando empirico è la quantità che viene effettivamente calcolata dai dati osservati. Mentre l’estimando teorico rappresenta l’obiettivo concettuale dello studio (come l’effetto dell’allenamento cognitivo sull’intelligenza), l’estimando empirico è ciò che viene effettivamente misurato nel contesto dei dati disponibili.\nPer tradurre un estimando teorico in uno empirico, è essenziale formulare assunzioni che rendano possibile l’inferenza causale. Queste assunzioni possono essere formalizzate attraverso l’uso dei Grafici Aciclici Diretti (DAG) per garantire che le variabili confondenti siano adeguatamente controllate. L’identificazione corretta assicura che le conclusioni derivate dai dati osservati siano valide rispetto all’effetto causale che si sta cercando di stimare.\nConsideriamo uno studio psicologico sull’effetto dell’allenamento cognitivo sui punteggi di intelligenza:\n\nEstimando Teorico: Il nostro obiettivo teorico potrebbe essere stimare l’effetto causale dell’allenamento cognitivo sull’aumento del punteggio di intelligenza in una popolazione adulta. L’estimando teorico qui sarebbe la differenza media nei punteggi di intelligenza tra gli individui che hanno partecipato all’allenamento e quelli che non lo hanno fatto, supponendo che l’unica differenza tra i gruppi sia l’allenamento stesso.\nEstimando Empirico: Per passare all’estimando empirico, dobbiamo considerare cosa possiamo effettivamente misurare. Supponiamo di avere dati da un campione di adulti, alcuni dei quali hanno partecipato all’allenamento cognitivo e altri no. L’estimando empirico potrebbe essere la differenza osservata nei punteggi di intelligenza tra questi due gruppi nel campione disponibile.\nAssunzioni per l’Identificazione:\n\nAssunzione di Nessuna Confusione (No Confounding): Dobbiamo assumere che non vi siano variabili non misurate che influenzano sia la partecipazione all’allenamento che i punteggi di intelligenza. Per esempio, la motivazione personale potrebbe influenzare sia la decisione di partecipare all’allenamento che il punteggio di intelligenza. Se questa variabile non è controllata, l’estimando empirico potrebbe sovrastimare o sottostimare l’effetto dell’allenamento.\nAssunzione di Non-Interferenza (Stable Unit Treatment Value Assumption, SUTVA): Dobbiamo assumere che la partecipazione di un individuo all’allenamento non influisca sui punteggi di intelligenza di altri individui. Questa assunzione potrebbe essere violata, ad esempio, se i partecipanti condividono tecniche apprese con amici che non hanno partecipato.\n\nUtilizzo dei DAG per la Chiarificazione:\n\nUn DAG può aiutare a visualizzare queste assunzioni mostrando le relazioni tra le variabili. In un DAG ben costruito, l’allenamento cognitivo influenzerebbe direttamente il punteggio di intelligenza, mentre altre variabili come l’educazione o la motivazione sarebbero rappresentate come confondenti da controllare. Se il DAG indica che ci sono variabili confondenti che non possiamo osservare o misurare, dovremo usare metodi statistici specifici, come i modelli di equazioni strutturali o l’uso di variabili strumentali, per isolare l’effetto dell’allenamento cognitivo.\n\n\nIn sintesi, collegare correttamente l’estimando teorico a uno empirico è un passo cruciale per garantire la validità delle inferenze causali in uno studio. Utilizzando l’esempio relativo all’effetto dell’allenamento cognitivo sull’intelligenza, possiamo vedere come le assunzioni sostanziali e gli strumenti come i DAG siano essenziali per identificare correttamente le relazioni causali e assicurare che i risultati siano interpretabili in modo affidabile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#dedurre-lestimando-empirico-dai-dati-osservati",
    "href": "chapters/eda/10_estimand.html#dedurre-lestimando-empirico-dai-dati-osservati",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.4 Dedurre l’Estimando Empirico dai Dati Osservati",
    "text": "14.4 Dedurre l’Estimando Empirico dai Dati Osservati\nDopo aver chiaramente definito l’estimando teorico e stabilito il collegamento con l’estimando empirico attraverso l’identificazione, il passo successivo è utilizzare tecniche statistiche per ottenere stime valide dai dati raccolti.\nRiprendiamo l’esempio psicologico sull’effetto dell’allenamento cognitivo sui punteggi di intelligenza per illustrare come l’approccio bayesiano può essere utilizzato per stimare l’estimando empirico:\n\nDefinizione dell’Estimando Empirico:\n\nL’estimando empirico in questo contesto è la differenza media nei punteggi di intelligenza tra il gruppo di individui che ha partecipato all’allenamento cognitivo e il gruppo che non ha partecipato.\n\nStrategie di Stima Appropriate:\n\nRegressione Lineare: Se ipotizziamo che i punteggi di intelligenza dipendano linearmente dalla partecipazione all’allenamento cognitivo e da altre variabili confondenti controllate, potremmo utilizzare una regressione lineare per stimare l’effetto dell’allenamento. In questa regressione, la partecipazione all’allenamento sarebbe una variabile indipendente, e i punteggi di intelligenza la variabile dipendente.\nMatching: Se i dati disponibili includono molte variabili confondenti misurate, potremmo utilizzare una tecnica di matching per creare coppie di individui simili (matchati) tra i gruppi di trattamento e controllo, basati su queste variabili. Questo metodo aiuta a bilanciare le differenze tra i gruppi che potrebbero influenzare i risultati, cercando di rendere le stime dell’effetto più affidabili.\nPropensity Score Matching: Invece di confrontare direttamente individui basandosi su caratteristiche osservabili, possiamo calcolare un punteggio di propensione per ciascun individuo, che rappresenta la probabilità di partecipare all’allenamento in base alle covariate osservate. Gli individui con punteggi di propensione simili vengono quindi confrontati, aiutando a controllare per le variabili confondenti.\nModelli di Equazioni Strutturali (SEM): Se ci sono molteplici relazioni tra variabili latenti e osservate, un modello di equazioni strutturali può essere utilizzato per stimare simultaneamente questi effetti complessi e isolare l’effetto diretto dell’allenamento cognitivo sui punteggi di intelligenza.\nRandomizzazione: In un disegno sperimentale ideale, l’assegnazione casuale dell’allenamento cognitivo elimina l’influenza delle variabili confondenti, permettendo una stima non distorta dell’effetto causale. Se i dati derivano da un esperimento randomizzato, potremmo semplicemente confrontare le medie dei due gruppi.\n\nInterpreting the Results:\n\nStime Non Distorte: Utilizzando la strategia di stima appropriata, possiamo ottenere una stima non distorta dell’effetto dell’allenamento cognitivo sui punteggi di intelligenza. Ad esempio, se utilizziamo una regressione lineare e controlliamo correttamente per tutte le variabili confondenti, l’effetto stimato rappresenterà l’effetto causale dell’allenamento.\n\n\nLa fase di stima è cruciale per trasformare i dati osservati in stime valide dell’estimando empirico. Nel contesto psicologico dell’allenamento cognitivo e dell’intelligenza, la scelta della strategia di stima appropriata dipende dalle assunzioni fatte sulla causalità e dalla natura dei dati disponibili. Utilizzando tecniche come la regressione, il matching, o i modelli di equazioni strutturali, i ricercatori possono ottenere stime precise e affidabili, garantendo che le conclusioni tratte siano valide e scientificamente robuste.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#implicazioni-per-la-ricerca",
    "href": "chapters/eda/10_estimand.html#implicazioni-per-la-ricerca",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.5 Implicazioni per la Ricerca",
    "text": "14.5 Implicazioni per la Ricerca\nLundberg et al. (2021) sottolineano l’importanza di una chiara comunicazione e dell’uso del framework presentato per migliorare la pratica della ricerca quantitativa.\n\n14.5.1 Importanza della Trasparenza e della Chiarezza nella Ricerca\nGli autori sottolineano che per garantire la validità e la replicabilità dei risultati di ricerca, è fondamentale che i ricercatori siano trasparenti e chiari su tutte le fasi del loro lavoro. Questo significa esplicitare le assunzioni fatte, il modo in cui l’estimando teorico è stato tradotto in un estimando empirico, e come i dati sono stati analizzati.\nPer esempio, in uno studio sull’effetto di una terapia cognitivo-comportamentale (CBT) sui livelli di ansia, è essenziale che i ricercatori definiscano chiaramente l’estimando teorico, ad esempio, “l’effetto medio della CBT sulla riduzione dell’ansia nella popolazione target di adulti con disturbo d’ansia generalizzato”. Devono poi descrivere come questo estimando è stato misurato empiricamente, ad esempio, utilizzando questionari standardizzati per l’ansia prima e dopo l’intervento. Infine, devono spiegare le assunzioni fatte e le tecniche utilizzate per l’analisi dei dati, come un modello bayesiano per gestire la variabilità individuale nella risposta alla terapia.\n\n\n14.5.2 Benefici dell’Utilizzo di Estimandi Chiaramente Definiti\nL’articolo discute come l’uso di estimandi chiaramente definiti può migliorare la comprensione dei risultati e facilitare il confronto tra studi diversi. Quando i ricercatori definiscono in modo preciso ciò che stanno stimando, diventa più facile per altri replicare lo studio, confrontare risultati e costruire un corpus di conoscenza cumulativo.\nPer esempio, consideriamo due studi sull’efficacia di diversi tipi di training di memoria per migliorare le funzioni cognitive negli anziani. Se entrambi gli studi definiscono chiaramente il loro estimando teorico (ad esempio, “l’effetto del training di memoria verbale sul punteggio del test di memoria a lungo termine”) e empirico (ad esempio, “la differenza media nei punteggi del test di memoria tra il gruppo che ha ricevuto il training e un gruppo di controllo”), sarà più semplice confrontare i risultati e capire quale tipo di training è più efficace.\n\n\n14.5.3 Adattabilità e Flessibilità del Framework\nIl framework proposto dagli autori è adattabile a diversi contesti di ricerca, permettendo ai ricercatori di applicare questi principi in una varietà di studi quantitativi, indipendentemente dal dominio specifico.\nPer esempio, in uno studio che esplora l’effetto della privazione del sonno sulla capacità di attenzione nei bambini, il framework potrebbe essere utilizzato per definire l’estimando teorico come “l’effetto della privazione di 8 ore di sonno sulla capacità di mantenere l’attenzione in attività ripetitive”, e l’estimando empirico potrebbe essere “la differenza media nei punteggi di attenzione tra bambini che hanno dormito 8 ore e quelli che non hanno dormito”. Questo approccio garantisce che le conclusioni siano fondate su basi metodologiche solide e che altri ricercatori possano replicare lo studio per verificare i risultati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#conclusioni-e-implicazioni-per-la-ricerca-futura",
    "href": "chapters/eda/10_estimand.html#conclusioni-e-implicazioni-per-la-ricerca-futura",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.6 Conclusioni e Implicazioni per la Ricerca Futura",
    "text": "14.6 Conclusioni e Implicazioni per la Ricerca Futura\nL’adozione del framework proposto da Lundberg et al. (2021) per la definizione degli estimandi teorici ed empirici, la chiara identificazione delle assunzioni e l’utilizzo di metodi di stima appropriati può migliorare la qualità e l’affidabilità della ricerca quantitativa nelle scienze sociali. Questo approccio promuove una pratica di ricerca più rigorosa e trasparente.\nSe la comunità psicologica integrasse questo framework, studi sugli interventi psicologici, come quelli sulla terapia cognitivo-comportamentale (CBT) discusso nell’esempio sopra, potrebbero diventare più comparabili e replicabili. Ciò migliorerebbe la nostra comprensione dell’efficacia e dei limiti di tali interventi. Ad esempio, definendo chiaramente cosa si intende per “efficacia” della CBT (come la riduzione del punteggio su una scala di ansia standardizzata) e utilizzando metodi bayesiani per incorporare dati preesistenti e nuove osservazioni, è possibile ottenere stime più robuste e interpretabili. Queste stime rifletterebbero meglio l’efficacia reale della terapia nella pratica clinica.\nLe proposte di Lundberg et al. (2021) sono in linea con le raccomandazioni di altri studiosi. Andrew Gelman, ad esempio, sottolinea spesso l’importanza di definire con precisione cosa si sta cercando di stimare in un’analisi statistica. Gelman sostiene che una definizione vaga o mal definita dell’estimando teorico può portare a interpretazioni errate e conclusioni fuorvianti. La chiara definizione dell’estimando teorico, come evidenziato nell’articolo di Lundberg et al., è cruciale per determinare se uno studio è descrittivo, predittivo o causale, e per comprendere la natura dell’inferenza da trarre dai dati (Gelman & Imbens, 2013).\nSia McElreath (2020), nel suo testo “Statistical Rethinking,” sia Andrew Gelman, enfatizzano l’importanza dell’utilizzo dei Grafici Aciclici Diretti (DAG) per rappresentare visivamente le assunzioni causali e le relazioni tra variabili in un modello statistico. Questo tipo di approccio aiuta i ricercatori a identificare variabili confondenti e a chiarire le relazioni causali, migliorando così la validità delle inferenze.\nGelman discute anche frequentemente l’importanza della trasparenza nella comunicazione dei risultati di ricerca, un principio centrale anche nell’articolo di Lundberg et al. Egli insiste sul fatto che i ricercatori dovrebbero essere espliciti riguardo alle assunzioni fatte, ai metodi utilizzati e alle limitazioni dei loro studi (Gelman et al., 1995).\nIn sintesi, sia Lundberg et al. che altri ricercatori evidenziano l’importanza di una chiara definizione degli estimandi, dell’uso dei DAG per rappresentare le assunzioni causali e della scelta di strategie di stima appropriate. L’approccio bayesiano, in particolare, offre un metodo potente e flessibile per gestire l’incertezza e aggiornare le inferenze alla luce di nuove evidenze. Adottando queste pratiche, i ricercatori nelle scienze sociali e nella psicologia possono migliorare la validità, la replicabilità e la trasparenza delle loro ricerche, contribuendo a una conoscenza scientifica più solida e affidabile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#riflessioni-conclusive",
    "href": "chapters/eda/10_estimand.html#riflessioni-conclusive",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "14.7 Riflessioni Conclusive",
    "text": "14.7 Riflessioni Conclusive\nIn questo capitolo abbiamo esaminato l’importanza della definizione dell’estimando in uno studio quantitativo, come evidenziato nell’articolo di Lundberg et al. (2021). Il concetto centrale è la distinzione tra estimando teorico ed estimando empirico e il loro collegamento, che facilita l’interpretazione dei risultati e rende l’inferenza statistica più rigorosa.\nL’articolo propone un framework strutturato in tre fasi principali:\n\nDefinire un estimando teorico collegato alla teoria sottostante.\nTradurre questo estimando in un estimando empirico, basato su dati osservabili e assunzioni di identificazione.\nScegliere le strategie di stima adeguate per ottenere stime affidabili.\n\nL’adozione di questo approccio consente di migliorare la chiarezza e la trasparenza nella ricerca, rendendo più facili il confronto tra studi diversi e la replicabilità dei risultati. La corretta definizione dell’estimando guida l’intero processo di ricerca, dalla progettazione dello studio alla scelta delle tecniche di stima e all’interpretazione dei risultati, garantendo che la teoria e le evidenze empiriche siano strettamente collegate.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/10_estimand.html#bibliografia",
    "href": "chapters/eda/10_estimand.html#bibliografia",
    "title": "14  Estimandi teorici e estimandi empirici",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman; Hall/CRC.\n\n\nGelman, A., & Imbens, G. (2013). Why ask why? Forward causal inference and reverse causal questions. National Bureau of Economic Research.\n\n\nLundberg, I., Johnson, R., & Stewart, B. M. (2021). What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), 532–565.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estimandi teorici e estimandi empirici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html",
    "href": "chapters/eda/08_correlation.html",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "",
    "text": "13.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nNonostante sia un’operazione di base, l’analisi delle associazioni tra variabili rappresenta uno degli aspetti più controversi nell’ambito dell’analisi dei dati psicologici. Sebbene possa sembrare un passaggio naturale dopo l’analisi univariata, questo processo solleva numerose questioni metodologiche e concettuali.\nTradizionalmente, in psicologia, l’analisi delle associazioni tra variabili è stata considerata come l’obiettivo finale del processo di ricerca. Questa visione si basa sull’idea che la descrizione delle relazioni tra variabili fornisca una spiegazione esaustiva dei fenomeni psicologici. Tale approccio trova le sue radici storiche nel pensiero di Karl Pearson (1911), il quale sosteneva che la spiegazione scientifica si esaurisse una volta delineate le associazioni tra le variabili osservate:\nSebbene sia indubbio che rispondere alla seconda domanda posta da Pearson sia relativamente semplice, è altresì evidente che la nostra comprensione di un fenomeno non può dipendere unicamente dalle informazioni fornite dalle correlazioni.\nIn contrasto con questa visione tradizionale, la “Causal Revolution” propone un paradigma radicalmente diverso secondo il quale le associazioni tra variabili sono considerate come epifenomeni, mentre l’obiettivo principale della ricerca è l’identificazione e la comprensione delle relazioni causali: per comprendere veramente i fenomeni psicologici è essenziale indagare le cause sottostanti, andando oltre la mera descrizione delle associazioni.\nLa discussione dei metodi utilizzati per individuare le relazioni causali sarà trattata successivamente. In questo capitolo, ci concentreremo sui concetti statistici fondamentali necessari per descrivere le associazioni lineari tra variabili. È importante sottolineare che, sebbene esistano indici statistici per quantificare associazioni non lineari, la maggior parte degli psicologi si limita all’utilizzo di indici lineari.\nNel linguaggio comune, termini come “dipendenza”, “associazione” e “correlazione” vengono spesso usati in modo intercambiabile. Tuttavia, da un punto di vista tecnico, è importante distinguere questi concetti:\nÈ cruciale comprendere che non tutte le associazioni sono correlazioni e, soprattutto, che la correlazione non implica necessariamente causalità. Questa distinzione è fondamentale per interpretare correttamente i dati e evitare conclusioni errate sulle relazioni tra variabili.\nIn questo capitolo, esamineremo due misure statistiche fondamentali per valutare la relazione lineare tra due variabili: la covarianza e la correlazione. Questi indici ci permettono di descrivere il grado e la direzione dell’associazione lineare tra variabili, quantificando come queste variano congiuntamente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#introduzione",
    "href": "chapters/eda/08_correlation.html#introduzione",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "",
    "text": "Quanto spesso, quando è stato osservato un nuovo fenomeno, sentiamo che viene posta la domanda: ‘qual è la sua causa?’. Questa è una domanda a cui potrebbe essere assolutamente impossibile rispondere. Invece, può essere più facile rispondere alla domanda: ‘in che misura altri fenomeni sono associati con esso?’. Dalla risposta a questa seconda domanda possono risultare molte preziose conoscenze.\n\n\n\n\n\n\n\nAssociazione: questo termine indica una relazione generale tra variabili, dove la conoscenza del valore di una variabile fornisce informazioni su un’altra.\n\nCorrelazione: descrive una relazione specifica e quantificabile, indicando se due variabili tendono a variare insieme in modo sistematico. Ad esempio, in una correlazione positiva, se \\(X &gt; \\mu_X\\), è probabile che anche \\(Y &gt; \\mu_Y\\). La correlazione specifica il segno e l’intensità di una relazione lineare.\n\nDipendenza: indica una relazione causale tra le variabili, dove la variazione della variabile causale porta probabilisticamente alla variazione della variabile dipendente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#i-dati-grezzi",
    "href": "chapters/eda/08_correlation.html#i-dati-grezzi",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.2 I dati grezzi",
    "text": "13.2 I dati grezzi\nPer illustrare la correlazione e la covarianza, analizzeremo i dati raccolti da Zetsche et al. (2019) in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello specifico, i ricercatori si sono proposti di determinare se gli individui depressi sviluppano aspettative accurate riguardo al loro umore futuro o se tali aspettative sono distortamente negative.\nUno dei loro studi ha coinvolto un campione di 30 soggetti con almeno un episodio depressivo maggiore, confrontati con un gruppo di controllo composto da 37 individui sani. La misurazione del livello di depressione è stata effettuata tramite il Beck Depression Inventory (BDI-II).\nIl BDI-II è uno strumento di autovalutazione utilizzato per valutare la gravità della depressione in adulti e adolescenti. Il test è stato sviluppato per identificare e misurare l’intensità dei sintomi depressivi sperimentati nelle ultime due settimane. I 21 item del test sono valutati su una scala a 4 punti, dove 0 rappresenta il grado più basso e 3 il grado più elevato di sintomatologia depressiva.\nNell’esercizio successivo, ci proponiamo di analizzare i punteggi di depressione BDI-II nel campione di dati fornito da Zetsche et al. (2019).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#definizione-delle-relazioni-tra-variabili",
    "href": "chapters/eda/08_correlation.html#definizione-delle-relazioni-tra-variabili",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.3 Definizione delle relazioni tra variabili",
    "text": "13.3 Definizione delle relazioni tra variabili\nNel contesto delle indagini statistiche, spesso non ci limitiamo a esaminare la distribuzione di una singola variabile. Invece, il nostro interesse si concentra sulla relazione che emerge nei dati tra due o più variabili. Ma cosa significa esattamente quando diciamo che due variabili hanno una relazione?\nPer comprendere ciò, prendiamo ad esempio l’altezza e l’età tra un gruppo di bambini. In generale, è possibile notare che all’aumentare dell’età di un bambino, aumenta anche la sua altezza. Pertanto, conoscere l’età di un bambino, ad esempio tredici anni, e l’età di un altro, sei anni, ci fornisce un’indicazione su quale dei due bambini sia più alto.\nNel linguaggio statistico, definiamo questa relazione tra altezza e età come positiva, il che significa che all’aumentare dei valori di una delle variabili (in questo caso, l’età), ci aspettiamo di vedere valori più elevati anche nell’altra variabile (l’altezza). Tuttavia, esistono anche relazioni negative, in cui l’aumento di una variabile è associato a un diminuzione dell’altra (ad esempio, più età è correlata a meno pianto).\nNon si tratta solo di relazioni positive o negative; ci sono anche situazioni in cui le variabili non hanno alcuna relazione tra loro, definendo così una relazione nulla. Inoltre, le relazioni possono variare nel tempo, passando da positive a negative o da fortemente positive a appena positiva. In alcuni casi, una delle variabili può essere categorica, rendendo difficile parlare di “maggioranza” o “minoranza” ma piuttosto di “differente” (ad esempio, i bambini più grandi potrebbero semplicemente avere diverse preferenze rispetto ai bambini più piccoli, senza necessariamente essere “migliori” o “peggiori”).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#sec-scatter-plot",
    "href": "chapters/eda/08_correlation.html#sec-scatter-plot",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.4 Grafico a dispersione",
    "text": "13.4 Grafico a dispersione\nIl metodo più diretto per visualizzare la relazione tra due variabili continue è tramite un grafico a dispersione, comunemente noto come “scatterplot”. Questo tipo di diagramma rappresenta le coppie di dati ottenute da due variabili, posizionandole sull’asse delle ascisse (orizzontale) e delle ordinate (verticale).\nPer rendere l’idea più chiara, consideriamo i dati dello studio condotto da Zetsche et al. (2019), in cui i ricercatori hanno utilizzato due scale psicometriche, il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D), per misurare il livello di depressione nei partecipanti. Il BDI-II è uno strumento di autovalutazione che valuta la presenza e l’intensità dei sintomi depressivi in pazienti adulti e adolescenti con diagnosi psichiatrica, mentre la CES-D è una scala di autovalutazione progettata per misurare i sintomi depressivi sperimentati nella settimana precedente nella popolazione generale, in particolare negli adolescenti e nei giovani adulti. Poiché entrambe le scale misurano lo stesso costrutto, ovvero la depressione, ci aspettiamo una relazione tra i punteggi ottenuti dal BDI-II e dalla CES-D. Un diagramma a dispersione ci consente di esaminare questa relazione in modo visuale e intuitivo.\n\n# Leggi i dati dal file CSV\ndf = rio::import(here::here(\"data\", \"data.mood.csv\"))\n\n# Seleziona le colonne di interesse\ndf &lt;- df |&gt; \n  dplyr::select(\"esm_id\", \"group\", \"bdi\", \"cesd_sum\")\n\n# Rimuovi le righe duplicate\ndf &lt;- df[!duplicated(df), ]\n\n# Rimuovi le righe con valori mancanti nella colonna \"bdi\"\ndf &lt;- df[!is.na(df$bdi), ]\n\nPosizionando i valori del BDI-II sull’asse delle ascisse e quelli del CES-D sull’asse delle ordinate, ogni punto sul grafico rappresenta un individuo, di cui conosciamo il livello di depressione misurato dalle due scale. È evidente che i valori delle scale BDI-II e CES-D non possono coincidere per due motivi principali: (1) la presenza di errori di misurazione e (2) l’utilizzo di unità di misura arbitrarie per le due variabili. L’errore di misurazione è una componente inevitabile che influisce in parte su qualsiasi misurazione, ed è particolarmente rilevante in psicologia, dove la precisione degli strumenti di misurazione è generalmente inferiore rispetto ad altre discipline, come la fisica. Il secondo motivo per cui i valori delle scale BDI-II e CES-D non possono essere identici è che l’unità di misura della depressione è una questione arbitraria e non standardizzata. Tuttavia, nonostante le differenze dovute agli errori di misurazione e all’uso di unità di misura diverse, ci aspettiamo che, se le due scale misurano lo stesso costrutto (la depressione), i valori prodotti dalle due scale dovrebbero essere associati linearmente tra di loro. Per comprendere meglio il concetto di “associazione lineare”, è possibile esaminare i dati attraverso l’utilizzo di un diagramma a dispersione.\n\n# Crea uno scatterplot con colori diversi per i due gruppi\n# Separate data by group\nmdd_data &lt;- df[df$group == \"mdd\", ]\nctl_data &lt;- df[df$group == \"ctl\", ]\n\n# Calculate linear regression coefficients\ncoeff_combined &lt;- lm(cesd_sum ~ bdi, data = df)$coefficients\n\n# Define the linear regression line\nline_combined &lt;- function(x) coeff_combined[1] + coeff_combined[2] * x\n\n# Generate x values for the regression line\nx_values &lt;- seq(min(df$bdi), max(df$bdi), length.out = 100)\n\n# Plot scatter plot and regression line\nggplot() +\n  geom_point(data = mdd_data, aes(x = bdi, y = cesd_sum, color = \"Pazienti\"), alpha = 0.7) +\n  geom_point(data = ctl_data, aes(x = bdi, y = cesd_sum, color = \"Controlli\"), alpha = 0.7) +\n  geom_line(aes(x = x_values, y = line_combined(x_values)), linetype = \"dashed\", color = \"orange\") +\n  geom_vline(aes(xintercept = mean(mdd_data$bdi, na.rm = TRUE)), color = \"blue\", alpha = 0.2) +\n  geom_vline(aes(xintercept = mean(ctl_data$bdi, na.rm = TRUE)), color = \"red\", alpha = 0.2) +\n  geom_hline(aes(yintercept = mean(mdd_data$cesd_sum, na.rm = TRUE)), color = \"blue\", alpha = 0.2) +\n  geom_hline(aes(yintercept = mean(ctl_data$cesd_sum, na.rm = TRUE)), color = \"red\", alpha = 0.2) +\n  labs(x = \"BDI-II\", y = \"CESD\", color = \"Gruppo\") +\n  scale_color_manual(values = c(\"Pazienti\" = \"blue\", \"Controlli\" = \"red\"))\n\n\n\n\n\n\n\nOsservando il grafico a dispersione, è evidente che i dati mostrano una tendenza a distribuirsi in modo approssimativamente lineare. In termini statistici, ciò suggerisce una relazione di associazione lineare tra i punteggi CES-D e BDI-II.\nTuttavia, è importante notare che la relazione lineare tra le due variabili è lontana dall’essere perfetta. In una relazione lineare perfetta, tutti i punti nel grafico sarebbero allineati in modo preciso lungo una retta. Nella realtà, la dispersione dei punti dal comportamento lineare ideale è evidente.\nDi conseguenza, sorge la necessità di quantificare numericamente la forza e la direzione della relazione lineare tra le due variabili e di misurare quanto i punti si discostino da una relazione lineare ideale. Esistono vari indici statistici a disposizione per raggiungere questo obiettivo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#covarianza",
    "href": "chapters/eda/08_correlation.html#covarianza",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.5 Covarianza",
    "text": "13.5 Covarianza\nIniziamo a considerare il più importante di tali indici, chiamato covarianza. In realtà la definizione di questo indice non ci sorprenderà più di tanto in quanto, in una forma solo apparentemente diversa, l’abbiamo già incontrata in precedenza. Ci ricordiamo infatti che la varianza di una generica variabile \\(X\\) è definita come la media degli scarti quadratici di ciascuna osservazione dalla media:\n\\[\nS_{XX} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\]\nLa varianza viene talvolta descritta come la “covarianza di una variabile con sé stessa”. Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di una sola variabile, ci chiediamo come due variabili \\(X\\) e \\(Y\\) “variano insieme” (co-variano). È facile capire come una risposta a tale domanda possa essere fornita da una semplice trasformazione della formula precedente che diventa:\n\\[\nS_{XY} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{13.1}\\]\nL’Equazione 13.1 ci fornisce la definizione della covarianza.\n\n13.5.1 Interpretazione\nPer capire il significato dell’Equazione 13.1, supponiamo di dividere il grafico riportato nella Sezione 13.4 in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.\nSe prevalgono punti nel I e III quadrante, allora la nuvola di punti avrà un andamento crescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\)) e la covarianza avrà segno positivo. Mentre se prevalgono punti nel II e IV quadrante la nuvola di punti avrà un andamento decrescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\)) e la covarianza avrà segno negativo. Dunque, il segno della covarianza ci informa sulla direzione della relazione lineare tra due variabili: l’associazione lineare si dice positiva se la covarianza è positiva, negativa se la covarianza è negativa.\nEsercizio. Implemento l’Equazione 13.1 in R.\n\ncov_value &lt;- function(x, y) {\n  mean_x &lt;- sum(x) / length(x)\n  mean_y &lt;- sum(y) / length(y)\n  \n  sub_x &lt;- x - mean_x\n  sub_y &lt;- y - mean_y\n  \n  sum_value &lt;- sum(sub_y * sub_x)\n  denom &lt;- length(x)\n  \n  cov &lt;- sum_value / denom\n  return(cov)\n}\n\nPer i dati mostrati nel diagramma, la covarianza tra BDI-II e CESD è 207.4\n\nx = df$bdi\ny = df$cesd_sum\n\ncov_value(x, y)\n#&gt; [1] 207.4265\n\nOppure, in maniera più semplice:\n\nmean((x - mean(x)) * (y - mean(y)))\n#&gt; [1] 207.4265\n\nLo stesso risultato si ottiene con la funzione cov:\n\ncov(x, y) * (length(x) - 1) / length(x)\n#&gt; [1] 207.4265\n\nLa funzione cov(x, y) calcola la covarianza tra due array, x e y utilizzando \\(n-1\\) al denominatore.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#correlazione",
    "href": "chapters/eda/08_correlation.html#correlazione",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.6 Correlazione",
    "text": "13.6 Correlazione\nLa direzione della relazione tra le variabili è indicata dal segno della covarianza, ma il valore assoluto di questo indice non fornisce informazioni utili poiché dipende dall’unità di misura delle variabili. Ad esempio, considerando l’altezza e il peso delle persone, la covarianza sarà più grande se l’altezza è misurata in millimetri e il peso in grammi, rispetto al caso in cui l’altezza è in metri e il peso in chilogrammi. Pertanto, per descrivere la forza e la direzione della relazione lineare tra due variabili in modo adimensionale, si utilizza l’indice di correlazione.\nLa correlazione è ottenuta standardizzando la covarianza tramite la divisione delle deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due variabili:\n\\[\nr = \\frac{S_{XY}}{S_X S_Y}.\n\\tag{13.2}\\]\nLa quantità che si ottiene dall’Equazione 13.2 viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, l’hanno introdotta).\nIn maniera equivalente, per una lista di coppie di valori \\((x_1, y_1), \\dots, (x_n, y_n)\\), il coefficiente di correlazione è definito come la media del prodotto dei valori standardizzati:\n\\[\nr = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\bar{x}}{\\sigma_x} \\right) \\left( \\frac{y_i - \\bar{y}}{\\sigma_y} \\right),\n\\tag{13.3}\\]\ndove \\(\\bar{x}\\) e \\(\\bar{y}\\) rappresentano, rispettivamente, le medie dei valori \\(x\\) e \\(y\\), e \\(\\sigma_x\\) e \\(\\sigma_y\\) sono le rispettive deviazioni standard.\nNell’Equazione 13.3, i valori \\(x_i\\) e \\(y_i\\) vengono prima standardizzati sottraendo la media e dividendo per la deviazione standard, e poi si calcola la media del prodotto di questi valori standardizzati.\n\n13.6.1 Proprietà\nIl coefficiente di correlazione ha le seguenti proprietà:\n\nha lo stesso segno della covarianza, dato che si ottiene dividendo la covarianza per due numeri positivi;\nè un numero puro, cioè non dipende dall’unità di misura delle variabili;\nassume valori compresi tra -1 e +1.\n\n13.6.2 Interpretazione\nAll’indice di correlazione possiamo assegnare la seguente interpretazione:\n\n\n\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti i punti si trovano esattamente su una retta con pendenza negativa (dal quadrante in alto a sinistra al quadrante in basso a destra);\n\n\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti i punti si trovano esattamente su una retta con pendenza positiva (dal quadrante in basso a sinistra al quadrante in alto a destra);\n\n\\(-1 &lt; r_{XY} &lt; +1\\) \\(\\rightarrow\\) presenza di una relazione lineare di intensità diversa;\n\n\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e \\(Y\\).\n\nEsercizio. Per i dati riportati nel diagramma della sezione {ref}sec-zetsche-scatter, la covarianza è 207.4. Il segno positivo della covarianza ci dice che tra le due variabili c’è un’associazione lineare positiva. Per capire quale sia l’intensità della relazione lineare calcoliamo la correlazione. Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a \\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore è prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.\nTroviamo la correlazione con la funzione corrcoef():\n\ncor(x, y)\n#&gt; [1] 0.904062\n\nReplichiamo il risultato implementando l’eq. {eq}eq-cor-def:\n\ns_xy &lt;- mean((x - mean(x)) * (y - mean(y)))\ns_x &lt;- sqrt(mean((x - mean(x))^2))  # Deviazione standard popolazione\ns_y &lt;- sqrt(mean((y - mean(y))^2))  # Deviazione standard popolazione\nr_xy &lt;- s_xy / (s_x * s_y)\nprint(r_xy)\n#&gt; [1] 0.904062\n\nUn altro modo ancora per trovare la correlazione tra i punteggi BDI-II e CESD è quello di applicare l’Equazione 13.3:\n\nz_x &lt;- (x - mean(x)) / sqrt(mean((x - mean(x))^2))  # Standardizzazione con deviazione standard popolazione\nz_y &lt;- (y - mean(y)) / sqrt(mean((y - mean(y))^2))  # Standardizzazione con deviazione standard popolazione\nmean(z_x * z_y)\n#&gt; [1] 0.904062\n\nEsempio. Un uso interessante delle correlazioni viene fatto in un recente articolo di Guilbeault et al. (2024). Il concetto di “gender bias” si riferisce alla tendenza sistematica di favorire un sesso rispetto all’altro, spesso a scapito delle donne. Lo studio di Guilbeault et al. (2024) analizza come le immagini online influenzino la diffusione su vasta scala di questo preconcetto di genere.\nAttraverso un vasto insieme di immagini e testi raccolti online, gli autori dimostrano che sia le misurazioni basate sulle immagini che quelle basate sui testi catturano la frequenza con cui varie categorie sociali sono associate a rappresentazioni di genere, valutate su una scala da -1 (femminile) a 1 (maschile), con 0 che indica una neutralità di genere. Questo consente di quantificare il preconcetto di genere come una forma di bias statistico lungo tre dimensioni: la tendenza delle categorie sociali ad associarsi a un genere specifico nelle immagini e nei testi, la rappresentazione relativa delle donne rispetto agli uomini in tutte le categorie sociali nelle immagini e nei testi, e il confronto tra le associazioni di genere nei dati delle immagini e dei testi con la distribuzione empirica delle donne e degli uomini nella società. Il lavoro di Guilbeault et al. (2024) evidenzia che il preconcetto di genere è molto più evidente nelle immagini rispetto ai testi, come mostrato nella {numref}gender-bias-1-fig C.\nSi noti che, nel grafico della {numref}gender-bias-1-fig C, ogni punto può essere interpretato come una misura di correlazione. La misura utilizzata da Guilbeault et al. (2024) riflette il grado di associazione tra le categorie sociali e le rappresentazioni di genere presenti nelle immagini e nei testi analizzati. Quando la misura è vicina a +1, indica una forte associazione positiva tra una categoria sociale specifica e una rappresentazione di genere maschile, mentre un valore vicino a -1 indica una forte associazione negativa con una rappresentazione di genere femminile. Un valore di 0, invece, suggerisce che non vi è alcuna associazione tra la categoria sociale considerata e un genere specifico, indicando una sorta di neutralità di genere. In sostanza, questa misura di frequenza può essere interpretata come una correlazione che riflette la tendenza delle categorie sociali a essere rappresentate in un modo o nell’altro nelle immagini e nei testi analizzati, rispetto ai concetti di genere femminile e maschile.\n\n\nIl preconcetto di genere è più prevalente nelle immagini online (da Google Immagini) e nei testi online (da Google News). A. La correlazione tra le associazioni di genere nelle immagini da Google Immagini e nei testi da Google News per tutte le categorie sociali (n = 2.986), organizzate per decili. B. La forza dell’associazione di genere in queste immagini e testi online per tutte le categorie (n = 2.986), suddivisa in base al fatto che queste categorie siano inclinate verso il femminile o il maschile. C. Le associazioni di genere per un campione di occupazioni secondo queste immagini e testi online; questo campione è stato selezionato manualmente per evidenziare i tipi di categorie sociali e preconcetti di genere esaminati. (Figura tratta da Guilbeault et al. (2024)).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#correlazione-di-spearman",
    "href": "chapters/eda/08_correlation.html#correlazione-di-spearman",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.7 Correlazione di Spearman",
    "text": "13.7 Correlazione di Spearman\nUn’alternativa per valutare la relazione lineare tra due variabili è il coefficiente di correlazione di Spearman, che si basa esclusivamente sull’ordine dei dati e non sugli specifici valori. Questo indice di associazione è particolarmente adatto quando gli psicologi sono in grado di misurare solo le relazioni di ordine tra diverse modalità di risposta dei soggetti, ma non l’intensità della risposta stessa. Tali variabili psicologiche che presentano questa caratteristica sono definite come “ordinali”.\n\n\n\n\n\n\nÈ importante ricordare che, nel caso di una variabile ordinale, non è possibile utilizzare le statistiche descrittive convenzionali come la media e la varianza per sintetizzare le osservazioni. Tuttavia, è possibile riassumere le osservazioni attraverso una distribuzione di frequenze delle diverse modalità di risposta. Come abbiamo appena visto, la direzione e l’intensità dell’associazione tra due variabili ordinali possono essere descritte utilizzando il coefficiente di correlazione di Spearman.\n\n\n\nPer fornire un esempio, consideriamo due variabili di scala ordinale e calcoliamo la correlazione di Spearman tra di esse.\n\ncor.test(c(1, 2, 3, 4, 5), c(5, 6, 7, 8, 7), method = \"spearman\")\n#&gt; Warning in cor.test.default(c(1, 2, 3, 4, 5), c(5, 6, 7, 8, 7), method =\n#&gt; \"spearman\"): Impossibile calcolare p-value esatti in presenza di ties\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  c(1, 2, 3, 4, 5) and c(5, 6, 7, 8, 7)\n#&gt; S = 3.5843, p-value = 0.08859\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; 0.8207827",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#correlazione-nulla",
    "href": "chapters/eda/08_correlation.html#correlazione-nulla",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.8 Correlazione nulla",
    "text": "13.8 Correlazione nulla\nUn aspetto finale da sottolineare riguardo alla correlazione è che essa descrive la direzione e l’intensità della relazione lineare tra due variabili. Tuttavia, la correlazione non cattura relazioni non lineari tra le variabili, anche se possono essere molto forti. È fondamentale comprendere che una correlazione pari a zero non implica l’assenza di una relazione tra le due variabili, ma indica solamente l’assenza di una relazione lineare tra di esse.\nLa figura seguente fornisce tredici esempi di correlazione nulla in presenza di una chiara relazione (non lineare) tra due variabili. In questi tredici insiemi di dati i coefficienti di correlazione di Pearson sono sempre uguali a 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.\n\ndatasaurus_data &lt;- read.csv(\"../../data/datasaurus.csv\")\n\ndatasaurus_summary &lt;- datasaurus_data %&gt;%\n  group_by(dataset) %&gt;%\n  summarise(\n    x_count = n(),\n    x_mean = mean(x, na.rm = TRUE),\n    x_std = sd(x, na.rm = TRUE),\n    y_count = n(),\n    y_mean = mean(y, na.rm = TRUE),\n    y_std = sd(y, na.rm = TRUE)\n  )\n\nprint(datasaurus_summary)\n#&gt; # A tibble: 13 × 7\n#&gt;   dataset  x_count x_mean x_std y_count y_mean y_std\n#&gt;   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 away         142   54.3  16.8     142   47.8  26.9\n#&gt; 2 bullseye     142   54.3  16.8     142   47.8  26.9\n#&gt; 3 circle       142   54.3  16.8     142   47.8  26.9\n#&gt; 4 dino         142   54.3  16.8     142   47.8  26.9\n#&gt; 5 dots         142   54.3  16.8     142   47.8  26.9\n#&gt; 6 h_lines      142   54.3  16.8     142   47.8  26.9\n#&gt; # ℹ 7 more rows\n\n\nggplot(datasaurus_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.7) +\n  facet_wrap(~ dataset, nrow = 4, ncol = 4) +\n  labs(x = \"X\", y = \"Y\") +\n  theme(strip.text = element_text(size = 10), axis.text = element_text(size = 8))",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#due-paradossi-comuni",
    "href": "chapters/eda/08_correlation.html#due-paradossi-comuni",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.9 Due Paradossi Comuni",
    "text": "13.9 Due Paradossi Comuni\nEsistono due situazioni comuni in cui le associazioni tra variabili possono ingannarci, e che vale la pena esaminare esplicitamente: il paradosso di Simpson e il paradosso di Berkson.\n\n13.9.1 Paradosso di Simpson\nIl paradosso di Simpson si verifica quando stimiamo una relazione per sottoinsiemi dei nostri dati, ma otteniamo una relazione diversa considerando l’intero dataset (Simpson 1951). È un caso particolare della fallacia ecologica, che si verifica quando cerchiamo di fare affermazioni sugli individui basandoci sui loro gruppi. Ad esempio, potrebbe esserci una relazione positiva tra i voti universitari e la performance alla scuola di specializzazione in due dipartimenti considerati individualmente. Tuttavia, se i voti universitari tendono a essere più alti in un dipartimento rispetto all’altro, mentre la performance alla scuola di specializzazione tende a essere opposta, potremmo trovare una relazione negativa tra i voti universitari e la performance alla scuola di specializzazione.\n\n13.9.2 Paradosso di Berkson\nIl paradosso di Berkson si verifica quando stimiamo una relazione basandoci sul dataset che abbiamo, ma a causa della selezione del dataset, la relazione risulta diversa in un dataset più generale (Berkson 1946). Ad esempio, se abbiamo un dataset di ciclisti professionisti, potremmo non trovare una relazione tra il loro VO2 max e la possibilità di vincere una gara di ciclismo (Coyle et al. 1988; Podlogar, Leo, and Spragg 2022). Tuttavia, se avessimo un dataset della popolazione generale, potremmo trovare una relazione tra queste due variabili. Il dataset professionale è così selezionato che la relazione scompare; non si può diventare ciclisti professionisti senza avere un VO2 max adeguato, ma tra i ciclisti professionisti, tutti hanno un VO2 max sufficiente.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#considerazioni-conclusive",
    "href": "chapters/eda/08_correlation.html#considerazioni-conclusive",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.10 Considerazioni conclusive",
    "text": "13.10 Considerazioni conclusive\nIn questo capitolo, abbiamo approfondito i concetti di correlazione e covarianza, strumenti chiave per quantificare le relazioni tra variabili nei fenomeni psicologici. L’aspetto cruciale non risiede tanto nel saper calcolare queste misure, quanto nel comprendere le informazioni che esse offrono. È fondamentale ricordare che le associazioni osservate non indicano necessariamente i meccanismi causali sottostanti.\nLe relazioni tra variabili possono presentarsi in diversi scenari:\n\nCausalità diretta: Quando una variabile \\(X\\) influisce direttamente su una variabile \\(Y\\), l’associazione tra le due sarà evidente. In un contesto ideale, con un effetto causale lineare e isolato, la correlazione rifletterebbe esattamente la forza e la direzione dell’effetto causale. Tuttavia, questo scenario è teorico e raramente applicabile ai fenomeni psicologici complessi.\nInfluenza di altre variabili: Nella realtà, anche quando esiste una relazione causale diretta tra \\(X\\) e \\(Y\\), l’intervento di altre variabili può modificare l’associazione osservata. Come vedremo nel prossimo capitolo, la struttura delle relazioni causali può portare a correlazioni positive, nulle o persino negative, pur in presenza di un effetto causale positivo.\nAssociazioni spurie: È possibile riscontrare associazioni tra variabili che non sono causate da una relazione diretta tra di esse. Questo fenomeno evidenzia l’importanza di non confondere correlazione e causalità, e di essere cauti nelle interpretazioni.\n\nQuesti scenari mettono in luce un principio fondamentale: l’osservazione di un’associazione tra due variabili non è sufficiente per inferire una relazione causale. Le associazioni, considerate isolatamente, forniscono informazioni limitate sul fenomeno in esame.\nTuttavia, in alcuni contesti, le associazioni possono rivelarsi utili:\n\nquando vengono misurate molteplici variabili e si utilizzano tecniche psicometriche come l’analisi fattoriale o lo scaling psicologico;\nquando si ha una chiara comprensione dei meccanismi causali che regolano il dominio di studio, permettendo di controllare le variabili confondenti tramite l’uso di metodi statistici avanzati.\n\nNel capitolo successivo, ci concentreremo su queste tematiche, esplorando strumenti e metodologie che ci consentiranno di andare oltre la semplice osservazione delle associazioni, per avvicinarci a una comprensione più profonda e causale dei fenomeni psicologici.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/08_correlation.html#informazioni-sullambiente-di-sviluppo",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "\n13.11 Informazioni sull’Ambiente di Sviluppo",
    "text": "13.11 Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Zagreb\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pointblank_0.12.2 haven_2.5.4       labelled_2.13.0   mice_3.16.0      \n#&gt;  [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [25] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1      rlang_1.1.4       magrittr_2.0.3    compiler_4.4.2   \n#&gt;  [5] vctrs_0.6.5       pkgconfig_2.0.3   shape_1.4.6.1     fastmap_1.2.0    \n#&gt;  [9] backports_1.5.0   labeling_0.4.3    utf8_1.2.4        promises_1.3.1   \n#&gt; [13] blastula_0.3.5    rmarkdown_2.29    tzdb_0.4.0        nloptr_2.1.1     \n#&gt; [17] xfun_0.49         glmnet_4.1-8      jomo_2.7-6        jsonlite_1.8.9   \n#&gt; [21] later_1.4.0       pan_1.9           broom_1.0.7       parallel_4.4.2   \n#&gt; [25] R6_2.5.1          stringi_1.8.4     car_3.1-3         boot_1.3-31      \n#&gt; [29] rpart_4.1.23      Rcpp_1.0.13-1     iterators_1.0.14  pacman_0.5.1     \n#&gt; [33] R.utils_2.12.3    httpuv_1.6.15     Matrix_1.7-1      splines_4.4.2    \n#&gt; [37] nnet_7.3-19       timechange_0.3.0  tidyselect_1.2.1  abind_1.4-8      \n#&gt; [41] yaml_2.3.10       codetools_0.2-20  miniUI_0.1.1.1    lattice_0.22-6   \n#&gt; [45] shiny_1.9.1       withr_3.0.2       evaluate_1.0.1    survival_3.7-0   \n#&gt; [49] pillar_1.9.0      carData_3.0-5     foreach_1.5.2     generics_0.1.3   \n#&gt; [53] rprojroot_2.0.4   hms_1.1.3         munsell_0.5.1     minqa_1.2.8      \n#&gt; [57] xtable_1.8-4      glue_1.8.0        tools_4.4.2       data.table_1.16.2\n#&gt; [61] lme4_1.1-35.5     ggsignif_0.6.4    grid_4.4.2        colorspace_2.1-1 \n#&gt; [65] nlme_3.1-166      Formula_1.2-5     cli_3.6.3         fansi_1.0.6      \n#&gt; [69] gtable_0.3.6      R.methodsS3_1.8.2 rstatix_0.7.2     digest_0.6.37    \n#&gt; [73] htmlwidgets_1.6.4 farver_2.1.2      R.oo_1.27.0       htmltools_0.5.8.1\n#&gt; [77] lifecycle_1.0.4   mitml_0.4-5       mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/08_correlation.html#bibliografia",
    "href": "chapters/eda/08_correlation.html#bibliografia",
    "title": "13  Relazioni tra variabili: correlazione e covarianza",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Relazioni tra variabili: correlazione e covarianza</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html",
    "href": "chapters/eda/09_causality.html",
    "title": "14  Causalità dai dati osservazionali",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nLa pura osservazione dei dati può rivelare correlazioni e pattern nei dati, ma senza un’indagine sulle cause che stanno alla base di tali correlazioni, le conclusioni tratte possono essere fuorvianti o incomplete.\nRichard McElreath, nel suo libro “Statistical Rethinking” (McElreath, 2020), utilizza l’analogia dei Golem - creature potenti ma prive di saggezza - per descrivere un approccio metodologico che è stato a lungo predominante in psicologia. Questo approccio si basa esclusivamente sull’analisi delle associazioni statistiche tra variabili, trascurando considerazioni più profonde sulla causalità.\nIl metodo in questione si concentra principalmente sul test delle ipotesi nulle, senza stabilire una chiara connessione tra le domande di ricerca riguardanti relazioni causali e i test statistici impiegati. Questa disconnessione è evidente nella figura successiva, tratta da un manuale di analisi dati di impostazione frequentista, che illustra la procedura raccomandata dai sostenitori di questo approccio per descrivere le associazioni tra variabili.\nÈ importante notare come tale procedura non fornisca strumenti utili per identificare le effettive cause sottostanti ai fenomeni osservati. Questa limitazione metodologica è stata identificata come uno dei fattori principali che hanno contribuito alla crisi di replicabilità nella ricerca psicologica, come approfondito nel ?sec-crisis. L’approccio descritto, pur essendo potente nell’individuare correlazioni, manca della “saggezza” necessaria per distinguere tra semplici associazioni e vere relazioni causali, analogamente ai Golem della metafora di McElreath.\nUn problema evidenziato da McElreath (2020) è che processi causali completamente distinti possono generare la stessa distribuzione di risultati osservati. Pertanto, un approccio focalizzato esclusivamente sull’analisi delle associazioni mediante il test dell’ipotesi nulla non è in grado di distinguere tra questi diversi scenari, come spiegato nel ?sec-causal-inference-regr.\nL’approccio frequentista, che si limita a descrivere le associazioni tra le variabili, ha una scarsa capacità di rilevare le caratteristiche cruciali dei fenomeni studiati e tende a produrre un alto tasso di falsi positivi (Zwet et al., 2023). È invece necessario utilizzare una metodologia che non si limiti a confutare ipotesi nulle, ma sia in grado di sviluppare modelli causali che rispondano direttamente alle domande di ricerca. In questo capitolo, ci concentreremo sull’introduzione dei concetti fondamentali dell’analisi causale.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#introduzione",
    "href": "chapters/eda/09_causality.html#introduzione",
    "title": "14  Causalità dai dati osservazionali",
    "section": "",
    "text": "Esempio di albero decisionale per la selezione di una procedura statistica appropriata. Iniziando dall’alto, l’utente risponde a una serie di domande riguardanti la misurazione e l’intento, arrivando infine al nome di una procedura. Sono possibili molti alberi decisionali simili. (Figura tratta da McElreath (2020)).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#cosè-la-causalità",
    "href": "chapters/eda/09_causality.html#cosè-la-causalità",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.1 Cos’è la causalità?",
    "text": "14.1 Cos’è la causalità?\nHardt & Recht (2022) introducono il concetto di causalità distinguendo tra osservazione e azione. Ciò che vediamo nell’osservazione passiva è il modo in cui le persone seguono i loro comportamenti abituali, le loro inclinazioni naturali, proiettando lo stato del mondo su un insieme di caratteristiche che abbiamo scelto di evidenziare. Tuttavia, le domande più importanti spesso non riguardano semplici osservazioni.\n\nNon ci basta sapere che le persone che praticano regolarmente attività fisica soffrono meno d’ansia; vogliamo capire se l’attività fisica riduce effettivamente i livelli d’ansia.\nNon ci accontentiamo di osservare che chi segue una terapia cognitivo-comportamentale (CBT) presenta meno sintomi depressivi; desideriamo verificare se la CBT riduce realmente questi sintomi.\nNon ci limitiamo a constatare che l’uso frequente dei social media è associato a un calo del benessere mentale; vogliamo determinare se l’uso intensivo dei social media causa effettivamente una diminuzione del benessere mentale.\n\nAlla base, il ragionamento causale è un quadro concettuale per affrontare domande sugli effetti di azioni o interventi ipotetici. Una volta compreso quale sia l’effetto di un’azione, possiamo invertire la domanda e chiederci quale azione plausibile abbia causato un determinato evento.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#effetto-causale",
    "href": "chapters/eda/09_causality.html#effetto-causale",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.2 Effetto Causale",
    "text": "14.2 Effetto Causale\nSebbene non esista una definizione univoca di causalità, possiamo concettualizzarla in modo pratico: diciamo che X causa Y se, intervenendo e modificando il valore di X (il trattamento), la distribuzione di Y cambia di conseguenza. Questa definizione sottolinea l’importanza cruciale dell’azione o dell’intervento nel determinare una relazione causale.\nQuando X è una variabile binaria, rappresentante la presenza o l’assenza del trattamento, la conseguenza dell’intervento su X è denominata effetto medio del trattamento. Questo ci indica quanto il trattamento (azione X = 1) aumenta l’aspettativa di Y rispetto all’assenza di trattamento (azione X = 0).\nÈ importante notare che gli effetti causali sono quantità relative alla popolazione. Si riferiscono a effetti mediati sull’intera popolazione in esame. Tuttavia, spesso l’effetto del trattamento può variare significativamente da un individuo all’altro o tra gruppi di individui. In questi casi, parliamo di effetti di trattamento eterogenei.\nPer chiarire questo concetto, consideriamo un esempio concreto: supponiamo che la terapia cognitivo-comportamentale (CBT) riduca l’ansia. Se un gruppo di persone ansiose non riceve alcun trattamento, i loro livelli d’ansia rimarranno presumibilmente invariati. Se invece interveniamo introducendo la CBT (modificando così il valore di X), i livelli d’ansia nel gruppo tenderanno a diminuire (cambiando quindi il valore di Y). Questo esempio illustra la distinzione tra semplice correlazione, basata sull’osservazione passiva, e causalità, che implica un’azione o un intervento.\nLa definizione di causalità può essere applicata anche per collegare variabili apparentemente distanti. Ad esempio, l’autoefficacia potrebbe non avere un effetto causale diretto sulle prestazioni accademiche. Tuttavia, se aumentiamo l’autoefficacia attraverso interventi mirati, è probabile che osserviamo un miglioramento nell’impegno allo studio. Questo aumento dell’impegno, a sua volta, tende a migliorare le prestazioni accademiche. Di conseguenza, possiamo affermare che l’autoefficacia influisce indirettamente sulle prestazioni accademiche attraverso una catena causale.\nÈ importante precisare che affermiamo l’esistenza di una relazione causale tra X e Y anche quando modificare X non porta necessariamente a un cambiamento immediato o deterministico in Y, ma altera la probabilità che Y si verifichi in un certo modo, modificando quindi la distribuzione di Y. Questa prospettiva probabilistica della causalità è particolarmente rilevante in campi come la psicologia, dove le relazioni tra variabili sono spesso complesse e influenzate da molteplici fattori.\n\n14.2.1 I Limiti dell’Osservazione\nPer comprendere i limiti dell’osservazione passiva, e quindi la necessità di comprendere le relazioni causali sottostanti, Hardt & Recht (2022) si riferiscono all’esempio storico delle ammissioni ai corsi di laurea dell’Università della California, Berkeley, nel 1973. In quell’anno, 12,763 candidati furono considerati per l’ammissione in uno dei 101 dipartimenti o major interdipartimentali. Di questi, 4,321 erano donne e 8,442 erano uomini. I dati mostrano che circa il 35% delle donne fu ammesso, rispetto al 44% degli uomini. Test di significatività statistica indicano che questa differenza non è attribuibile al caso, suggerendo una disparità nei tassi di ammissione tra i generi.\nUna tendenza simile si osserva quando si analizzano le decisioni aggregate di ammissione nei sei maggiori dipartimenti. Il tasso di ammissione complessivo per gli uomini era di circa il 44%, mentre per le donne era solo il 30%, un’altra differenza significativa. Tuttavia, poiché i dipartimenti hanno autonomia nelle loro decisioni di ammissione, è utile esaminare il possibile bias di genere a livello di singolo dipartimento.\nUomini\n\n\n\nDipartimento\nCandidati\nAmmessi (%)\n\n\n\n\nA\n825\n62\n\n\nB\n520\n60\n\n\nC\n325\n37\n\n\nD\n417\n33\n\n\nE\n191\n28\n\n\nF\n373\n6\n\n\n\nDonne\n\n\n\nDipartimento\nCandidati\nAmmessi (%)\n\n\n\n\nA\n108\n82\n\n\nB\n25\n68\n\n\nC\n593\n34\n\n\nD\n375\n35\n\n\nE\n393\n24\n\n\nF\n341\n7\n\n\n\nDall’osservazione di questi dati, emerge che quattro dei sei maggiori dipartimenti mostrano un tasso di ammissione più elevato per le donne, mentre due mostrano un tasso più elevato per gli uomini. Tuttavia, questi due dipartimenti non possono giustificare la sostanziale differenza nei tassi di ammissione osservata nei dati aggregati. Questo suggerisce che la tendenza generale di un tasso di ammissione più alto per gli uomini sembra invertita quando i dati sono disaggregati per dipartimento.\nQuesto fenomeno è noto come paradosso di Simpson, un paradosso statistico in cui una tendenza che appare in sottopopolazioni si inverte o scompare quando i dati vengono aggregati. Nel contesto attuale, il paradosso di Simpson si manifesta nel fatto che, mentre i dati aggregati sembrano indicare una discriminazione di genere contro le donne, l’analisi dei dati disaggregati per dipartimento rivela che in alcuni casi le donne sono favorite in termini di ammissioni.\nLa domanda fondamentale è se questi dati indicano effettivamente un problema di discriminazione di genere o se, come suggerito dallo studio originale, il bias di genere nelle ammissioni fosse principalmente dovuto al fatto che “le donne sono indirizzate dalla loro socializzazione e istruzione verso campi di studio generalmente più affollati, meno produttivi in termini di completamento dei diplomi, meno finanziati e che spesso offrono prospettive professionali peggiori.” In altre parole, il problema risiederebbe in differenze sistemiche e strutturali tra i campi di studio scelti dalle donne e quelli scelti dagli uomini.\nIl paradosso di Simpson crea disagio proprio perché l’intuizione suggerisce che una tendenza valida per tutte le sottopopolazioni dovrebbe esserlo anche a livello aggregato. Tuttavia, questo paradosso evidenzia un errore comune nell’interpretazione delle probabilità condizionate: confondere l’osservazione passiva con l’analisi causale. I dati che abbiamo rappresentano solo un’istantanea del comportamento normale di uomini e donne che si candidavano per l’ammissione a UC Berkeley nel 1973.\nNon possiamo trarre conclusioni definitive da questi dati. Possiamo solo riconoscere che l’analisi iniziale solleva ulteriori domande, come ad esempio la necessità di progettare nuovi studi per raccogliere dati più completi, che potrebbero portare a conclusioni più definitive. In alternativa, potremmo discutere su quale scenario sia più verosimile in base alle nostre convinzioni e alle notre ipotesi sul mondo.\nL’inferenza causale può essere utile in entrambi i casi. Da un lato, può guidare la progettazione di nuovi studi, aiutandoci a scegliere quali variabili includere, quali escludere e quali mantenere costanti. Dall’altro, i modelli causali possono fungere da meccanismo per incorporare le conoscenze scientifiche del dominio e passare da ipotesi plausibili a conclusioni plausibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#variabili-confondenti",
    "href": "chapters/eda/09_causality.html#variabili-confondenti",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.3 Variabili confondenti",
    "text": "14.3 Variabili confondenti\nSebbene gli esperimenti controllati offrano un elevato grado di certezza nell’identificazione di queste relazioni, molte domande di ricerca non possono essere affrontate sperimentalmente a causa di limitazioni etiche o pratiche. In questi casi, i ricercatori ricorrono a disegni osservazionali, che offrono maggiore flessibilità e applicabilità. Tuttavia, l’uso di dati osservazionali comporta una sfida significativa: la difficoltà di trarre conclusioni causali affidabili.\nAl centro di questa complessità si trovano le variabili confondenti. Possiamo dire che una variabile confondente è presente quando l’associazione osservata tra due variabili X e Y non riflette accuratamente la vera relazione causale tra di esse. In altre parole, la variabile confondente influenza sia X che Y, creando l’apparenza di una relazione diretta tra le due che potrebbe essere fuorviante o inesatta.\nNegli studi osservazionali, se le variabili confondenti non vengono misurate e controllate adeguatamente, possono distorcere le stime degli effetti causali, introducendo bias nei risultati e impedendo di riflettere il vero valore dell’effetto. In pratica, la presenza di variabili confondenti può portare a conclusioni errate quando si confrontano semplicemente i risultati osservati in diversi gruppi. Ciò che si osserva nei dati potrebbe non corrispondere a ciò che accadrebbe se si potesse manipolare direttamente la variabile di interesse in un esperimento controllato.\nUn approccio apparentemente semplice per affrontare questo problema potrebbe essere quello di controllare statisticamente tutte le variabili confondenti. In questo metodo, si stima l’effetto di X su Y separatamente in ogni segmento della popolazione definito da una condizione Z = z per ogni possibile valore di z. Successivamente, si calcola la media di questi effetti stimati nelle sottopopolazioni, ponderandoli per la probabilità di Z = z nella popolazione. Tuttavia, questo metodo presenta due difficoltà fondamentali: richiede la conoscenza di tutte le possibili variabili confondenti e la capacità di misurare ciascuna di esse, cosa che spesso non è praticabile.\nIl controllo delle variabili confondenti è cruciale per stabilire relazioni causali, poiché permette di isolare gli effetti delle variabili indipendenti da quelli delle variabili confondenti che potrebbero influenzare le variabili dipendenti. Esistono due principali metodologie di controllo:\n\nIl controllo sperimentale, implementato attraverso il disegno sperimentale e basato principalmente sulla randomizzazione.\nIl controllo statistico, applicato durante l’analisi dei dati, con l’obiettivo di neutralizzare o quantificare l’influenza delle variabili estranee.\n\nA causa di queste difficoltà, l’inferenza causale basata su dati osservazionali è spesso considerata problematica, dando origine al famoso detto “la correlazione non implica causalità”. Tuttavia, è importante notare che in alcune circostanze, è possibile fare inferenze causali anche a partire da dati osservazionali.\nL’obiettivo dell’analisi causale moderna è proprio quello di fornire gli strumenti concettuali e metodologici per affrontare queste sfide. Attraverso l’uso di tecniche avanzate come i modelli causali strutturali, i grafi aciclici diretti (DAG) e i metodi di identificazione degli effetti causali, i ricercatori possono spesso superare le limitazioni dei dati osservazionali e trarre conclusioni causali più robuste.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#modelli-causali-strutturali",
    "href": "chapters/eda/09_causality.html#modelli-causali-strutturali",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.4 Modelli Causali Strutturali",
    "text": "14.4 Modelli Causali Strutturali\nI modelli causali sono strumenti essenziali per l’analisi dei dati osservazionali, poiché consentono di rappresentare il processo sottostante a un fenomeno e di prevedere gli effetti di un intervento. Questi modelli non solo permettono di anticipare le conseguenze di una causa, ma offrono anche la possibilità di esplorare scenari controfattuali, immaginando esiti alternativi che si sarebbero potuti verificare in presenza di decisioni diverse.\nUn modello causale strutturale (Structural Causal Model, SCM) è un approccio che rappresenta le relazioni causali tra variabili. Esso si basa su una serie di assegnazioni che, partendo da variabili di rumore indipendenti (note anche come variabili esogene), generano una distribuzione di probabilità congiunta.\nLe variabili di rumore indipendenti svolgono un ruolo cruciale negli SCM. Esse rappresentano fonti di incertezza o variabilità all’interno del sistema e non sono influenzate da altre variabili del modello. Queste variabili sono mutuamente indipendenti, il che significa che il loro valore non fornisce informazioni sul valore delle altre.\nLa costruzione di un SCM segue una sequenza specifica: si parte dalle variabili di rumore indipendenti, si applicano una serie di assegnazioni che descrivono gli effetti causali delle variabili esogene su altre variabili, e si genera progressivamente un insieme di variabili casuali che dà origine a una distribuzione congiunta.\nIl principale vantaggio di un SCM risiede nella sua duplice natura: da un lato, fornisce una distribuzione di probabilità congiunta delle variabili, e dall’altro, descrive il processo generativo che porta alla formazione di tale distribuzione, partendo dalle variabili di rumore elementari.\nQuesta struttura consente non solo di modellare le relazioni probabilistiche tra le variabili, ma anche di rappresentare in modo esplicito i meccanismi causali che le governano.\nI SCM possono essere rappresentati graficamente attraverso Grafi Aciclici Direzionati (Directed Acyclic Graphs, DAG). Questi DAG visualizzano le relazioni causali tra le variabili all’interno di un SCM, facilitando l’identificazione delle variabili confondenti e il loro impatto sull’analisi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#bias-da-variabile-omessa",
    "href": "chapters/eda/09_causality.html#bias-da-variabile-omessa",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.5 Bias da Variabile Omessa",
    "text": "14.5 Bias da Variabile Omessa\nPossiamo introdurre i DAG facendo riferiento al bias da variabile omessa (Omitted Variable Bias, o OVB; Wilms et al. (2021)). Come discusso da Byrnes & Dee (2024), l’omissione dall’analisi statistica di variabili confondenti note ma non misurate, o sconosciute e non misurate, può portare a stime errate della magnitudine degli effetti, errori nel segno delle stime (stimatori distorti), correlazioni spurie, e al mascheramento delle vere relazioni causali.\nUn illustrazione di questa situazione è fornita nella Figura 14.1. La figura mostra tre DAG che illustrano diversi scenari in cui le variabili non osservate non influenzano i risultati del modello o potrebbero creare problemi a causa della confusione. Una variabile di risposta di interesse (Y) è causata sia da una variabile misurata (X) che da una variabile non misurata (U). Nel pannello di sinistra, la variabile non osservata (U) non è una variabile confondente. Nel pannello centrale, la variabile non osservata (U) è una variabile confondente e causa il bias da variabile omessa. Nel pannello di destra la variabile non osservata (U) causa il bias da variabile omessa in maniera indiretta.\n\n\n\n\n\n\nFigura 14.1: Nel pannello di sinistra, X e U sono non correlate, quindi la mancata inclusione di U in un modello statistico aumenterebbe l’errore standard della stima (riducendo la precisione del modello) ma non porterebbe a bias nella stima dell’effetto di X su Y. Tuttavia, se U influenza anche X come nel pannello centrale, o se U e X sono influenzati da un fattore comune Z come nel pannello di destra, allora omettere U da un modello statistico causa il bias da variabile omessa nella stima dell’effetto di X su Y. I casi illustrati dal pannello centrale e dal pannello di destra sono esempi di sistemi in cui le cause comuni di confusione (U e Z rispettivamente) devono essere controllate per effettuare inferenze causali non distorte (la figura è ispirata da Byrnes & Dee (2024)).\n\n\n\nAffrontare i problemi creati dalle variabili confondenti non misurate rappresenta una sfida primaria nell’inferenza causale dai dati osservazionali. A differenza dell’errore di misurazione nelle variabili predittive, che produce un bias costante verso lo zero e può essere corretto o modellato (McElreath, 2020; Schennach, 2016), con l’OVB non possiamo conoscere la grandezza o la direzione del bias senza conoscere tutte le possibili variabili confondenti e le loro relazioni nel sistema.\nNonostante queste sfide, non è necessario abbandonare l’uso dei dati osservazionali per l’inferenza causale in psicologia. È invece necessario ricorrere all’adozione delle tecniche dei SCM per potere comunque svolgere l’inferenza causale.\nÈ evidente che questo approccio porterà a conclusioni inevitabilmente parziali, destinate ad essere perfezionate da studi successivi. Tuttavia, tale metodologia offre il vantaggio di esplicitare il “modello generativo dei dati”, ovvero la struttura causale sottostante ai fenomeni psicologici oggetto di studio.\nI progressi nella ricerca empirica conducono a una maggiore comprensione e, di conseguenza, a modifiche nelle ipotesi sui meccanismi causali. Questo processo rappresenta un’evoluzione della conoscenza scientifica. Tale sviluppo è reso possibile proprio perché le ipotesi causali sono formulate in termini di modelli formali, che descrivono in modo preciso i meccanismi ipotizzati.\nAl contrario, limitarsi alla mera descrizione delle associazioni tra variabili non consente questo tipo di avanzamento conoscitivo. La formulazione di modelli causali espliciti permette infatti di testare, raffinare e, se necessario, rivedere le ipotesi sui meccanismi sottostanti ai fenomeni osservati, portando a una comprensione più profonda e dinamica dei processi psicologici.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#grafi-aciclici-diretti",
    "href": "chapters/eda/09_causality.html#grafi-aciclici-diretti",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.6 Grafi Aciclici Diretti",
    "text": "14.6 Grafi Aciclici Diretti\nI DAG sono uno strumento fondamentale per l’inferenza causale, offrendo una rappresentazione visiva delle relazioni causali ipotizzate tra variabili. Questi grafi sono definiti “diretti” perché le variabili, rappresentate da nodi, sono collegate da frecce orientate anziché da semplici linee. Sono inoltre chiamati “aciclici” poiché non è possibile tornare a un nodo di partenza seguendo il percorso delle frecce.\nIn un DAG, una freccia che va da X a Y indica un’influenza probabilistica di X su Y. La terminologia delle relazioni all’interno del grafo è importante: il nodo di origine di una freccia è chiamato “genitore”, mentre il nodo di destinazione è detto “figlio”. Quando è possibile raggiungere un nodo B partendo da un nodo A seguendo una successione di frecce, A è definito “antenato” di B, e B è considerato “discendente” di A.\nI DAG consentono di distinguere chiaramente tra cause dirette e indirette. Una causa diretta è rappresentata da un nodo genitore, mentre una causa indiretta può essere qualsiasi antenato di un nodo nel grafo causale. Questa struttura permette di differenziare efficacemente causa ed effetto basandosi sulla posizione relativa dei nodi all’interno del grafo, ovvero se un nodo è antenato o discendente di un altro.\nQuesti grafi sono particolarmente utili per identificare variabili confondenti, basandosi sulla teoria sviluppata da Judea Pearl (Pearl, 2009). È cruciale rappresentare in un DAG tutte le possibili relazioni causali, poiché l’assenza di una freccia tra due nodi implica la certezza dell’assenza di una relazione causale diretta tra le variabili corrispondenti.\nNella teoria dei DAG, due concetti fondamentali sono la d-separazione e il criterio del back-door.\n\nLa d-separazione\nLa d-separazione ci aiuta a determinare quando due variabili in un grafo causale sono indipendenti condizionatamente a un insieme di altre variabili. Questo concetto è cruciale per comprendere come l’informazione o l’influenza si propaga tra le variabili in un modello causale.\nIn termini più semplici, la d-separazione ci permette di identificare se esiste un “blocco” nel flusso di informazioni tra due variabili, dato un certo insieme di altre variabili (che chiameremo Λ). Quando due variabili sono d-separate da Λ, significa che non c’è flusso di informazioni tra di loro, condizionatamente a Λ.\nPer comprendere meglio la d-separazione, consideriamo tre situazioni principali che possono verificarsi in un DAG:\n\nCatena (X → Z → Y): In questo caso, Z è un mediatore tra X e Y. Se Z appartiene all’insieme Λ (cioè, se controlliamo o condizioniamo su Z), blocchiamo il flusso di informazioni da X a Y attraverso questo percorso. Per esempio, se X è “esercizio fisico”, Z è “pressione sanguigna” e Y è “rischio di malattie cardiache”, controllando per la pressione sanguigna (Z) blocchiamo il percorso attraverso il quale l’esercizio fisico influenza il rischio di malattie cardiache.\nFork (X ← Z → Y): Qui, Z è una causa comune sia di X che di Y. Se Z appartiene a Λ, blocchiamo la correlazione spuria tra X e Y che deriva dalla loro causa comune. Per esempio, se Z è “status socioeconomico”, X è “livello di istruzione” e Y è “stato di salute”, controllando per lo status socioeconomico (Z) eliminiamo la correlazione apparente tra istruzione e salute che potrebbe derivare dal fatto che entrambe sono influenzate dallo status socioeconomico.\nCollider (X → Z ← Y): In questa situazione, Z è un effetto comune di X e Y. Sorprendentemente, se né Z né i suoi discendenti appartengono a Λ, il percorso è già bloccato. Controllare per Z (o i suoi discendenti) in realtà aprirebbe un percorso tra X e Y, creando una correlazione spuria. Per esempio, se X è “intelligenza”, Y è “bellezza” e Z è “successo in una carriera di attore”, controllare per il successo nella carriera di attore (Z) creerebbe una correlazione apparente tra intelligenza e bellezza, anche se queste potrebbero essere indipendenti nella popolazione generale.\n\nIn sintesi, la d-separazione ci permette di determinare, dato un certo insieme di variabili Λ, se due variabili X e Y sono indipendenti condizionatamente a Λ. Questo ci aiuta a identificare quali variabili dobbiamo controllare (e quali non dobbiamo controllare) per ottenere stime causali non distorte, facilitando così l’inferenza causale corretta. La d-separazione è quindi uno strumento potente che ci permette di leggere le indipendenze condizionali direttamente dal grafo, senza dover fare calcoli probabilistici complessi.\n\n\nIl criterio del back-door\nIl criterio del back-door consente di identificare un insieme di variabili che, se controllate adeguatamente, permettono di stimare gli effetti causali in modo non distorto. L’obiettivo principale di questo criterio è eliminare l’influenza di percorsi non causali tra la variabile di esposizione (causa potenziale) e l’outcome (effetto), mantenendo aperto solo il percorso causale diretto di interesse.\nIn questo contesto, due variabili sono considerate “confuse” se esiste tra di esse un percorso di tipo back-door. Un back-door path da X a Y è definito come qualsiasi percorso che inizia da X con una freccia entrante in X. Per esempio, consideriamo il seguente percorso:\nX ← A → B ← C → Y\nIn questo caso, il percorso rappresenta un flusso di informazioni da X a Y che non è causale, ma potrebbe creare l’apparenza di una relazione causale.\nPer “deconfondere” una coppia di variabili, è necessario selezionare un insieme di variabili (chiamato back-door set) che “blocchi” tutti i back-door paths tra i due nodi di interesse. Il blocco di questi percorsi avviene in modi diversi a seconda della struttura del percorso:\n\nUn back-door path che coinvolge una catena di variabili (ad esempio, A → B → C) può essere bloccato controllando per la variabile intermedia (in questo caso, B).\nUn percorso che coinvolge un “collider” (una variabile che riceve frecce da entrambe le direzioni, come in A → B ← C) è naturalmente bloccato e non permette il flusso di informazioni.\n\nÈ importante notare che bisogna prestare attenzione a non aprire involontariamente un flusso di informazioni attraverso un collider. Questo può accadere se si condiziona l’analisi sul collider stesso o su un suo discendente, il che potrebbe erroneamente aprire il percorso e introdurre bias nell’analisi.\n\nPunti chiave:\n\nIl criterio del back-door aiuta a identificare il set minimale di variabili da controllare.\nNon tutte le variabili associate sia all’esposizione che all’outcome devono essere controllate; solo quelle che creano percorsi back-door.\nIn alcuni casi, potrebbe non essere necessario controllare alcuna variabile (se non ci sono percorsi back-door aperti).\nIn altri casi, potrebbe essere impossibile bloccare tutti i percorsi back-door con le variabili disponibili, indicando che l’effetto causale non può essere identificato con i dati a disposizione.\n\nUtilizzando il criterio del back-door in combinazione con i DAG, i ricercatori possono fare scelte più informate su quali variabili includere nelle loro analisi, migliorando così la validità delle loro inferenze causali.\n\n\n\n14.6.1 Applicazioni\nConsideriamo nuovamente la struttura causale illustrata nella Figura 14.1, pannello centrale. Dopo aver costruito un DAG come descritto nella sezione precedente, è possibile identificare le potenziali fonti di bias da variabili omesse, inclusi i confondenti non misurati (ad esempio, U). Non controllare per le variabili confondenti apre una “back-door” permettendo alla variazione confondente di influenzare la relazione tra la variabile causale e la variabile di risposta di interesse attraverso un percorso non valutato (Pearl, 2009). In altre parole, omettere una variabile confondente come U nella Figura 14.1 (pannello centrale) in un’analisi statistica significa che questa viene incorporata nel termine di errore del modello statistico, insieme alle fonti di errore casuali. La Figura 14.2 illustra le conseguenze di un confondente U che ha un effetto positivo su X ma un effetto negativo su Y. Se adattiamo un modello come mostrato nella Figura 14.2 bi, l’effetto stimato di X su Y è positivo quando si controlla per U. Tuttavia, se non si controlla per U, come mostrato nella Figura 14.2 bii, U viene incorporato nel termine di errore, inducendo una correlazione tra l’errore e X, come illustrato nella Figura 14.2 biii, portando a una stima errata. Pertanto, il termine di errore del modello e X risultano correlati, il che viola un’assunzione fondamentale dei modelli lineari (ovvero, il teorema di Gauss-Markov; Abdallah et al., 2015; Antonakis et al., 2010). Questo produce una stima errata, evidenziata in blu.\n\n\n\n\n\n\nFigura 14.2: Una visualizzazione del bias da variabile omessa e delle conseguenze per l’inferenza causale. (A) mostra un DAG di un sistema in cui X ha un effetto positivo su Y, e una variabile confondente U ha un effetto positivo su Y ma un effetto negativo su X. Le variabili non osservate (cioè non misurate) sono rappresentate in ellissi, come la variabile U e il termine di errore e nel pannello B. (B) illustra diverse stime del DAG in (A) utilizzando un’analisi del percorso. Vedi Box 1 per una breve spiegazione delle principali differenze tra DAG e diagrammi dei percorsi. Presumiamo che U non sia misurata. In (Bi), presumiamo di poter misurare e controllare U, rappresentata dalla freccia a doppia testa tra U e X, che rappresenta la correlazione tra le due variabili considerata dal modello. La variabile non misurata e è la fonte residua di variazione che si presume non sia correlata con nessun predittore. La freccia rossa rappresenta il percorso stimato. Al contrario, (Bii) e (Biii) rappresentano la realtà, dove non abbiamo una misurazione di U e non la controlliamo nel modello dei percorsi. Il ricercatore pensa di adattare il modello in (Bii) ma in realtà sta adattando il modello in (Biii), dove il termine di errore non è solo e, ma la somma di e e la variazione dovuta alla variabile omessa U. A causa di ciò, c’è un percorso diretto dal termine di errore del modello a X (e quindi X è endogeno). (C) mostra le relazioni stimate risultanti dai modelli in (Bi) rispetto a (Bii). Le linee rappresentano la relazione stimata tra X e Y dai rispettivi modelli. La linea rossa è la vera relazione causale, stimata da (Bi), mentre la linea blu contiene il bias da variabile omessa, poiché non si tiene conto della variabile confondente U, come stimato dal modello in Bii/Biii (Figura tratta da Byrnes & Dee (2024)).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#commenti-e-considerazioni-finali",
    "href": "chapters/eda/09_causality.html#commenti-e-considerazioni-finali",
    "title": "14  Causalità dai dati osservazionali",
    "section": "14.7 Commenti e Considerazioni Finali",
    "text": "14.7 Commenti e Considerazioni Finali\nI diagrammi causali sono uno dei primi strumenti per identificare il bias da variabili omesse (Pearl, 1995; Pearl et al., 2016). I diagrammi causali, sotto forma di DAG, visualizzano la nostra comprensione delle relazioni causali e delle variabili confondenti all’interno di un sistema. In questo modo, i DAG chiariscono in modo trasparente le assunzioni dietro le affermazioni causali derivate dai dati e mostrano le potenziali fonti di bias derivanti da variabili confondenti.\nÈ fondamentale che i DAG includano tutte le cause comuni di un predittore e della risposta di interesse, comprendendo tutte le variabili confondenti misurate e non misurate. Questo significa che l’inferenza causale è possibile solo quando il ricercatore dispone di adeguate conoscenze del dominio.\nDopo aver costruito un DAG, è possibile determinare le potenziali fonti di bias da variabili omesse, incluse quelle derivanti da variabili confondenti non misurate (es., U nella figura fig-byrnes-dee-1, pannello centrale). Non controllare le variabili confondenti apre una “back-door” per la variazione confondente, permettendo a quest’ultima di fluire tra la variabile causale e la variabile di risposta di interesse attraverso un percorso non valutato (Pearl, 2009).\nPertanto, un diagramma causale è un primo passo fondamentale per identificare potenziali bias da variabili omesse. I DAG giustificano anche la scelta delle variabili di controllo, rendendo trasparenti le assunzioni che un ricercatore fa su come funziona il sistema oggetto di studio.\nÈ importante notare che i DAG possono essere incorretti o non includere variabili confondenti sconosciute. Infatti, un DAG rappresenta solo la comprensione attuale e le assunzioni del ricercatore riguardo alle relazioni causali all’interno di un sistema.\nUn sommario ironico di questi concetti è fornito nella vignetta di xkcd.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/09_causality.html#bibliografia",
    "href": "chapters/eda/09_causality.html#bibliografia",
    "title": "14  Causalità dai dati osservazionali",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAlexander, R. (2023). Telling Stories with Data: With Applications in R. Chapman; Hall/CRC.\n\n\nByrnes, J. E., & Dee, L. E. (2024). Causal inference with observational data and unobserved confounding variables. bioRxiv, 2024–2002.\n\n\nHardt, M., & Recht, B. (2022). Patterns, Predictions, and Actions: Foundations of Machine Learning. Princeton University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nPearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4), 669–688.\n\n\nPearl, J. (2009). Causality. Cambridge University Press.\n\n\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal inference in statistics: A primer. John Wiley & Sons.\n\n\nRiederer, E. (2021). Causal design patterns for data analysts. https://emilyriederer.netlify.app/post/causal-design-patterns/\n\n\nSchennach, S. M. (2016). Recent advances in the measurement error literature. Annual Review of Economics, 8(1), 341–377.\n\n\nWilms, R., Mäthner, E., Winnen, L., & Lanwehr, R. (2021). Omitted variable bias: A threat to estimating causal relationships. Methods in Psychology, 5, 100075.\n\n\nZwet, E. van, Gelman, A., Greenland, S., Imbens, G., Schwab, S., & Goodman, S. N. (2023). A New Look at P Values for Randomized Clinical Trials. NEJM Evidence, 3(1), EVIDoa2300003.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causalità dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/eda/05_exploring_numeric_data.html#commenti-e-considerazioni-finali",
    "href": "chapters/eda/05_exploring_numeric_data.html#commenti-e-considerazioni-finali",
    "title": "11  Esplorare i dati numerici",
    "section": "\n11.9 Commenti e considerazioni finali",
    "text": "11.9 Commenti e considerazioni finali\nAbbiamo esplorato diverse tecniche per sintetizzare e visualizzare i dati, includendo distribuzioni di frequenze, istogrammi e grafici di densità. Questi strumenti sono essenziali per comprendere meglio i dati e presentare risultati in modo chiaro e informativo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Esplorare i dati numerici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html",
    "href": "chapters/eda/06_data_visualization.html",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "",
    "text": "Introduzione\nPrerequisiti\nConcetti e competenze chiave\nIn questo capitolo verranno introdotti i principi fondamentali della visualizzazione dei dati, accompagnati da una descrizione concisa. Per un approfondimento su ciascun principio, si rimanda al capitolo Data Visualization del libro Introduction to Data Science.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#codificare-i-dati-attraverso-segnali-visivi",
    "href": "chapters/eda/06_data_visualization.html#codificare-i-dati-attraverso-segnali-visivi",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.1 Codificare i dati attraverso segnali visivi",
    "text": "12.1 Codificare i dati attraverso segnali visivi\nIniziamo con una panoramica dei principali segnali visivi utilizzati per codificare i dati: posizione, lunghezza, angoli, area, luminosità e tonalità del colore. Tra questi, posizione e lunghezza sono i segnali visivi più efficaci e intuitivi, poiché il cervello umano è particolarmente abile nel riconoscere variazioni spaziali. Questo rende la posizione e la lunghezza strumenti potenti per la rappresentazione quantitativa. In altre parole, le persone riescono a confrontare con maggiore precisione altezze e lunghezze (come le barre in un barplot) rispetto ad angoli o aree (come in un grafico a torta).\nAngoli e aree, sebbene comunemente usati, sono segnali visivi meno efficaci. Grafici come i pie chart, che si basano su angoli e aree per rappresentare quantità, risultano spesso meno precisi e più difficili da interpretare, specialmente quando le differenze sono piccole. Anche l’uso dell’area, ad esempio nei bubble plot, può distorcere la percezione delle differenze tra i dati, a meno che non venga gestita correttamente. Anche se l’area di una bolla può essere proporzionale al valore rappresentato, la percezione umana tende a sovrastimare le differenze tra aree più grandi.\nLuminosità e tonalità del colore sono utili per rappresentare variabili qualitative o categoriali, ma possono risultare difficili da interpretare quando si tratta di confrontare quantità precise. Tuttavia, il colore gioca un ruolo cruciale nelle visualizzazioni multidimensionali, come le heatmap, dove è necessario rappresentare più di due variabili contemporaneamente. È importante, però, usare il colore con attenzione, soprattutto per garantire l’accessibilità a persone con problemi di daltonismo.\nLe tabelle sono utili quando si ha una quantità limitata di dati e si richiede una precisione numerica rigorosa. Tuttavia, per set di dati più grandi o per evidenziare tendenze e differenze, i grafici (come i barplot) sono generalmente più efficaci. Le tabelle non offrono lo stesso impatto visivo immediato e rendono più difficile l’individuazione di pattern complessi.\n\n12.1.1 Ulteriori considerazioni sulla scelta della visualizzazione\nLa scelta della visualizzazione più appropriata dipende sia dalla natura dei dati che dallo scopo della comunicazione. Per esempio:\n\nBarplot o dot plot sono ideali per confrontare valori quantitativi tra categorie.\nIstogrammi, boxplot e raincloud plots sono più adatti per descrivere la distribuzione di dati continui e fare confronti tra categorie.\nGrafici di dispersione (scatter plot) sono eccellenti per esplorare relazioni tra due variabili continue.\n\nLa chiarezza e la leggibilità sono principi fondamentali nella creazione di visualizzazioni efficaci. L’aggiunta di elementi visivi eccessivi, come decorazioni superflue o troppi colori, può distrarre dal messaggio principale. Un buon grafico deve essere semplice, ma allo stesso tempo completo, includendo solo gli elementi visivi necessari per trasmettere il messaggio desiderato.\nIn conclusione, scegliere i segnali visivi adeguati e il tipo di grafico più appropriato non solo migliora l’accuratezza della comunicazione, ma rende le informazioni più accessibili e comprensibili per il pubblico.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#quando-includere-lo-zero",
    "href": "chapters/eda/06_data_visualization.html#quando-includere-lo-zero",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.2 Quando includere lo zero",
    "text": "12.2 Quando includere lo zero\nQuando si usa la lunghezza come segnale visivo, come nei barplot, è essenziale che l’asse parta da zero. Non farlo può essere fuorviante e far sembrare le differenze più grandi di quanto non siano in realtà. Questo errore viene spesso sfruttato nei media per esagerare differenze apparentemente significative.\nTuttavia, quando si usa la posizione (ad esempio in un grafico a dispersione), non è sempre necessario includere lo zero, soprattutto se l’interesse principale è il confronto tra gruppi rispetto alla variabilità interna.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#evitare-le-distorsioni",
    "href": "chapters/eda/06_data_visualization.html#evitare-le-distorsioni",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.3 Evitare le distorsioni",
    "text": "12.3 Evitare le distorsioni\nUna distorsione comune si verifica quando le differenze tra quantità sono rappresentate utilizzando aree, come nei bubble plot, dove il raggio dei cerchi è proporzionale al dato. Il problema è che, poiché l’area di un cerchio è proporzionale al quadrato del raggio, le differenze sembrano molto più ampie di quanto siano realmente. Per evitare queste distorsioni, è meglio utilizzare la posizione o la lunghezza, come in un grafico a barre, per confrontare direttamente le quantità.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#ordinare-le-categorie",
    "href": "chapters/eda/06_data_visualization.html#ordinare-le-categorie",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.4 Ordinare le categorie",
    "text": "12.4 Ordinare le categorie\nQuando si visualizzano categorie, come nei barplot o nei boxplot, è opportuno ordinarle in base al valore della variabile di interesse, anziché in ordine alfabetico. Questo aiuta a evidenziare pattern significativi e facilita il confronto tra categorie.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#evitare-i-dynamite-plots",
    "href": "chapters/eda/06_data_visualization.html#evitare-i-dynamite-plots",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.5 Evitare i Dynamite Plots",
    "text": "12.5 Evitare i Dynamite Plots\nI dynamite plots, che mostrano la media e l’errore standard (o la deviazione standard), sono spesso utilizzati in psicologia ma sono fuorvianti. Questi grafici tendono a esagerare le differenze e possono indurre false interpretazioni. È preferibile mostrare tutti i dati, ad esempio tramite un dot plot, che fornisce un’immagine più chiara della distribuzione dei dati (Butler, 2022).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#facilitare-i-confronti",
    "href": "chapters/eda/06_data_visualization.html#facilitare-i-confronti",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.6 Facilitare i confronti",
    "text": "12.6 Facilitare i confronti\nQuando si confrontano due distribuzioni, come in un istogramma, è fondamentale mantenere gli stessi assi per entrambi i grafici. Se le distribuzioni sono presentate su assi con scale diverse, il confronto diventa difficile e potrebbe portare a conclusioni errate. Allineare i grafici verticalmente o orizzontalmente consente di percepire più facilmente le differenze tra i gruppi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#trasformazioni-logaritmiche",
    "href": "chapters/eda/06_data_visualization.html#trasformazioni-logaritmiche",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.7 Trasformazioni logaritmiche",
    "text": "12.7 Trasformazioni logaritmiche\nLe trasformazioni logaritmiche sono utili quando si lavora con dati distribuiti su più ordini di grandezza o quando le variazioni tra le quantità sono moltiplicative (West, 2022). L’uso della scala logaritmica in un grafico a barre o a dispersione può ridurre le distorsioni visive e migliorare l’interpretazione dei dati. Questo approccio è particolarmente utile quando alcuni valori estremi potrebbero dominare il grafico, nascondendo dettagli rilevanti.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#codificare-una-terza-variabile",
    "href": "chapters/eda/06_data_visualization.html#codificare-una-terza-variabile",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.8 Codificare una terza variabile",
    "text": "12.8 Codificare una terza variabile\nPer rappresentare tre variabili, è possibile utilizzare un grafico di dispersione con variabili codificate attraverso dimensioni aggiuntive come il colore, la dimensione o la forma dei punti. Ad esempio, in un grafico che confronta aspettativa di vita e reddito, la dimensione dei punti potrebbe rappresentare la popolazione e il colore la regione geografica. Quando si utilizza il colore per rappresentare una variabile, è importante scegliere palette cromatiche accessibili anche per chi è affetto da daltonismo, evitando combinazioni problematiche come rosso-verde.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#evitare-pseudo-tre-dimensioni",
    "href": "chapters/eda/06_data_visualization.html#evitare-pseudo-tre-dimensioni",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.9 Evitare pseudo-tre dimensioni",
    "text": "12.9 Evitare pseudo-tre dimensioni\nGrafici tridimensionali, come barre o pie chart 3D, spesso aggiungono confusione senza fornire informazioni aggiuntive significative. Sebbene visivamente accattivanti, questi grafici distorcono la percezione e rendono difficile l’interpretazione accurata dei dati. È preferibile mantenere le visualizzazioni bidimensionali, a meno che la terza dimensione non rappresenti effettivamente una variabile aggiuntiva.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#scegliere-il-numero-giusto-di-cifre-significative",
    "href": "chapters/eda/06_data_visualization.html#scegliere-il-numero-giusto-di-cifre-significative",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.10 Scegliere il numero giusto di cifre significative",
    "text": "12.10 Scegliere il numero giusto di cifre significative\nÈ importante evitare l’uso di troppe cifre decimali nelle tabelle e nei grafici. Spesso, una o due cifre significative sono sufficienti per rappresentare accuratamente i dati, mentre l’aggiunta di cifre inutili può confondere il lettore e dare un falso senso di precisione. Limitiamoci a mostrare solo le cifre necessarie per trasmettere il messaggio in modo chiaro.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#conoscere-il-pubblico",
    "href": "chapters/eda/06_data_visualization.html#conoscere-il-pubblico",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.11 Conoscere il pubblico",
    "text": "12.11 Conoscere il pubblico\nInfine, è fondamentale adattare la visualizzazione dei dati al pubblico di riferimento. Grafici progettati per l’analisi esplorativa interna possono contenere dettagli tecnici complessi, ma quando si comunica a un pubblico più ampio o non specializzato, è necessario semplificare. Ad esempio, utilizzare una scala logaritmica può essere utile per un pubblico esperto, ma confondere un pubblico generale. In questi casi, mantenere la scala lineare e spiegare chiaramente i dati aiuta a evitare malintesi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#conclusioni",
    "href": "chapters/eda/06_data_visualization.html#conclusioni",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.12 Conclusioni",
    "text": "12.12 Conclusioni\nI principi di visualizzazione dei dati trattati in questo capitolo sono strumenti fondamentali per garantire chiarezza e accuratezza nella rappresentazione delle informazioni. Scelte appropriate di grafici, segnali visivi e trasformazioni facilitano la comprensione, riducendo la possibilità di distorsioni o interpretazioni errate.\n\n\n\n\nButler, R. C. (2022). Popularity leads to bad habits: Alternatives to «the statistics» routine of significance,«alphabet soup» and dynamite plots. In Annals of Applied Biology (Fasc. 2; Vol. 180, pp. 182–195). Wiley Online Library.\n\n\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press.\n\n\nWest, R. M. (2022). Best practice in statistics: The use of log transformation. Annals of Clinical Biochemistry, 59(3), 162–165.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: a primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#riflessioni-conclusive",
    "href": "chapters/eda/06_data_visualization.html#riflessioni-conclusive",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "12.12 Riflessioni Conclusive",
    "text": "12.12 Riflessioni Conclusive\nI principi di visualizzazione dei dati trattati in questo capitolo sono strumenti fondamentali per garantire chiarezza e accuratezza nella rappresentazione delle informazioni. Scelte appropriate di grafici, segnali visivi e trasformazioni facilitano la comprensione, riducendo la possibilità di distorsioni o interpretazioni errate.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/06_data_visualization.html#bibliografia",
    "href": "chapters/eda/06_data_visualization.html#bibliografia",
    "title": "12  Principi della visualizzazione dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nButler, R. C. (2022). Popularity leads to bad habits: Alternatives to «the statistics» routine of significance,«alphabet soup» and dynamite plots. In Annals of Applied Biology (Fasc. 2; Vol. 180, pp. 182–195). Wiley Online Library.\n\n\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press.\n\n\nWest, R. M. (2022). Best practice in statistics: The use of log transformation. Annals of Clinical Biochemistry, 59(3), 162–165.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science. \" O’Reilly Media, Inc.\".\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: a primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principi della visualizzazione dei dati</span>"
    ]
  }
]