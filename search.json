[
  {
    "objectID": "chapters/key_notions/introduction_key_notions.html",
    "href": "chapters/key_notions/introduction_key_notions.html",
    "title": "Introduzione",
    "section": "",
    "text": "La data science è un campo che si sviluppa all’intersezione tra la statistica e l’informatica. La statistica fornisce una serie di metodologie per analizzare i dati e ottenere informazioni significative, mentre l’informatica si occupa dello sviluppo di software e strumenti per implementare tali metodologie. In questa sezione della dispensa, approfondiremo alcuni concetti fondamentali della statistica e della misurazione psicologica. Considereremo anche in termini generali quali sono gli obiettivi e i limiti dell’analisi dei dati psicologici.",
    "crumbs": [
      "Fondamenti",
      "Introduzione"
    ]
  },
  {
    "objectID": "chapters/eda/introduction_eda.html",
    "href": "chapters/eda/introduction_eda.html",
    "title": "Introduzione",
    "section": "",
    "text": "Dopo aver acquisito un dataset, è fondamentale comprendere a fondo le caratteristiche dei dati in esso contenuti. Sebbene le statistiche descrittive e altre misure numeriche siano spesso efficaci per ottenere una visione d’insieme, talvolta è un’immagine a valere più di mille parole.\nIn questa sezione della dispensa, esploreremo alcuni concetti chiave della statistica descrittiva. Oltre a fornire definizioni teoriche, presenteremo istruzioni pratiche in Python per condurre analisi statistiche su dati reali. Il capitolo si concluderà con una riflessione critica sui limiti di un approccio epistemologico basato esclusivamente sull’analisi delle associazioni tra variabili, evidenziando l’importanza di indagare le cause sottostanti ai fenomeni osservati.",
    "crumbs": [
      "EDA",
      "Introduzione"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psicometria",
    "section": "",
    "text": "Benvenuti\nBenvenuti nel sito web dell’insegnamento di Psicometria, parte del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#descrizione",
    "href": "index.html#descrizione",
    "title": "Psicometria",
    "section": "Descrizione",
    "text": "Descrizione\nL’insegnamento offre una formazione teorico-pratica nell’ambito dell’inferenza statistica, con un focus particolare sulle applicazioni in campo psicologico. Attraverso esercitazioni pratiche in Python e R, gli studenti acquisiranno competenze nell’analisi di dati e nell’uso di modelli statistici avanzati.\n\nAnno Accademico: 2024-2025\nCodice Insegnamento: B000286\nOrario e Luogo: Lunedì e Martedì (8:30-10:30), Giovedì (11:30-13:30), Plesso didattico La Torretta.\n\n\n\n\n\n\n\nQuesto sito web è la fonte ufficiale per il programma dell’insegnamento B000286 - Psicometria e le modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#struttura-dellinsegnamento",
    "href": "index.html#struttura-dellinsegnamento",
    "title": "Psicometria",
    "section": "Struttura dell’Insegnamento",
    "text": "Struttura dell’Insegnamento\nL’insegnamento è organizzato in diverse sezioni, ciascuna dedicata a uno specifico argomento chiave.\n\nFondamenti: Introduzione ai concetti statistici di base.\nInferenza Bayesiana: Applicazioni avanzate dell’inferenza causale.\nProgrammazione in Python e R: Utilizzo pratico per l’analisi dati.\nVisualizzazione dei Dati: Tecniche per interpretare e comunicare risultati.\n\nConsulta il syllabus completo per ulteriori dettagli.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#materiale-didattico",
    "href": "index.html#materiale-didattico",
    "title": "Psicometria",
    "section": "Materiale Didattico",
    "text": "Materiale Didattico\nIl presente sito web ospita la dispensa ufficiale del corso, contenente tutte le note e i materiali relativi alle lezioni. Per quanto riguarda le esercitazioni pratiche e gli esempi applicativi, è possibile accedere al sito dedicato, disponibile al seguente indirizzo: Psicometria Esercizi. Entrambi i materiali sono forniti gratuitamente agli studenti, senza necessità di ulteriori acquisti.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "Prefazione",
    "section": "",
    "text": "Come possiamo migliorare l’analisi dei dati psicologici per renderla più affidabile e robusta? È possibile affrontare questa sfida semplicemente applicando una serie di algoritmi o procedure standard? L’analisi dei dati in psicologia può davvero essere ridotta a un insieme di “ricette” preconfezionate (McElreath, 2020)?\nQueste domande ci portano a riflettere sulla natura stessa dell’analisi dei dati psicologici. A differenza di ciò che suggerisce l’approccio frequentista del test dell’ipotesi nulla, l’analisi dei dati non è una disciplina che si esaurisce con l’applicazione meccanica di metodi predefiniti. Anzi, considerare l’analisi dei dati come un insieme di procedure automatiche contribuisce a uno dei problemi più gravi della psicologia contemporanea: la crisi della replicabilità dei risultati (Korbmacher et al., 2023).\nMa perché la replicabilità è così cruciale? Se i risultati delle ricerche psicologiche non sono replicabili, significa che la nostra comprensione dei fenomeni psicologici è superficiale e inaffidabile. Questo non è solo un problema teorico o accademico; ha implicazioni dirette sulle applicazioni pratiche della psicologia. Se le basi scientifiche sono incerte, anche le strategie di intervento psicologico rischiano di essere inefficaci o addirittura dannose (Funder et al., 2014; Ioannidis, 2019; Shrout & Rodgers, 2018; Tackett et al., 2019).\nPerché le pratiche di analisi dei dati derivanti dal frequentismo potrebbero contribuire a questa crisi? In che modo gli incentivi accademici influenzano la qualità della ricerca psicologica? E, soprattutto, quali alternative abbiamo per migliorare l’affidabilità e la validità delle nostre conclusioni?\nL’analisi bayesiana emerge come una delle proposte per superare i limiti dell’approccio frequentista (Gelman et al., 1995). Tuttavia, è sufficiente abbandonare l’inferenza frequentista per risolvere i problemi della psicologia? Come possiamo integrare metodi robusti e flessibili, come quelli bayesiani, con una comprensione più approfondita e trasparente dei fenomeni psicologici?\nIn questo corso, esploreremo queste domande, cercando di identificare le “buone pratiche” dell’analisi dei dati psicologici. Discuteremo i limiti delle metodologie attuali, esamineremo le cause sottostanti della crisi della replicabilità e valuteremo come l’adozione di metodi avanzati, come l’inferenza bayesiana e la modellazione causale, possa offrire soluzioni efficaci (Oberauer & Lewandowsky, 2019; Wagenmakers et al., 2018; Yarkoni, 2022). Il nostro obiettivo è fornire una visione critica e costruttiva, che non solo identifichi le sfide della ricerca psicologica, ma proponga anche percorsi concreti per migliorare la qualità e l’affidabilità della scienza psicologica.\n\n\n\n\nFunder, D. C., Levine, J. M., Mackie, D. M., Morf, C. C., Sansone, C., Vazire, S., & West, S. G. (2014). Improving the dependability of research in personality and social psychology: Recommendations for research and educational practice. Personality and Social Psychology Review, 18(1), 3–12.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman; Hall/CRC.\n\n\nIoannidis, J. P. (2019). What have we (not) learnt from millions of scientific papers with P values? The American Statistician, 73(sup1), 20–25.\n\n\nKorbmacher, M., Azevedo, F., Pennington, C. R., Hartmann, H., Pownall, M., Schmidt, K., Elsherif, M., Breznau, N., Robertson, O., Kalandadze, T., et al. (2023). The replication crisis has led to positive structural, procedural, and community changes. Communications Psychology, 1(1), 3.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26, 1596–1618.\n\n\nShrout, P. E., & Rodgers, J. L. (2018). Psychology, science, and knowledge construction: Broadening perspectives from the replication crisis. Annual Review of Psychology, 69(1), 487–510.\n\n\nTackett, J. L., Brandes, C. M., King, K. M., & Markon, K. E. (2019). Psychology’s replication crisis and clinical psychological science. Annual Review of Clinical Psychology, 15(1), 579–604.\n\n\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmı́ra, M., Epskamp, S., et al. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25, 35–57.\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "programmazione2024.html",
    "href": "programmazione2024.html",
    "title": "1  Calendario delle lezioni",
    "section": "",
    "text": "1.1 Calendario delle relazioni in itinere\nLe relazioni di avanzamento del progetto di gruppo dovranno essere consegnate entro le scadenze stabilite. Ogni gruppo dovrà presentare un unico elaborato.\nOgni relazione rappresenta una tappa del progetto di gruppo, che culminerà nella presentazione finale durante gli ultimi incontri del corso.",
    "crumbs": [
      "Programmazione",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Calendario delle lezioni</span>"
    ]
  },
  {
    "objectID": "programmazione2024.html#calendario-delle-relazioni-in-itinere",
    "href": "programmazione2024.html#calendario-delle-relazioni-in-itinere",
    "title": "1  Calendario delle lezioni",
    "section": "",
    "text": "Data di Scadenza\nContenuto della Relazione\n\n\n\n\n18 marzo\nRelazione 1: Importazione dei dati, data wrangling, data tidying, dizionario dei dati, statistiche descrittive\n\n\n25 marzo\nRelazione 2: Priori coniugati e metodo basato su griglia\n\n\n31 marzo\nRelazione 3: Metodi Monte Carlo e algoritmi MCMC\n\n\n7 aprile\nRelazione 4: Regressione lineare\n\n\n8 maggio\nRelazione 5: Confronto di modelli\n\n\n18 maggio\nRelazione 6: Analisi frequentista; limiti dell’approccio frequentista",
    "crumbs": [
      "Programmazione",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Calendario delle lezioni</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html",
    "href": "chapters/key_notions/00_uncertainty.html",
    "title": "2  Abbracciare l’incertezza",
    "section": "",
    "text": "2.1 Introduzione\nL’espressione “abbracciare l’incertezza” è tra le più emblematiche nel panorama della statistica bayesiana. In questo capitolo, approfondiremo il significato di questa affermazione, seguendo la trattazione introduttiva proposta nel primo capitolo di Understanding Uncertainty di Lindley (Lindley, 2013).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#lincertezza-nella-ricerca-psicologica",
    "href": "chapters/key_notions/00_uncertainty.html#lincertezza-nella-ricerca-psicologica",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.2 L’incertezza nella ricerca psicologica",
    "text": "2.2 L’incertezza nella ricerca psicologica\nL’incertezza rappresenta un elemento cruciale non solo nella statistica, ma in tutte le discipline scientifiche, con particolare rilievo per la psicologia, che affronta fenomeni complessi e difficili da misurare. Nell’indagare processi cognitivi, emozioni e comportamenti, i ricercatori si confrontano con dati complessi, spesso ambigui e suscettibili di interpretazioni molteplici. Sebbene alcune affermazioni possano essere sostenute con elevata confidenza o confutate con certezza, la maggior parte delle ipotesi scientifiche si colloca in una zona grigia dominata dall’incertezza.\nL’obiettivo di questo insegnamento è guidare gli studenti nella comprensione e nella gestione dell’incertezza nella ricerca psicologica, adottando l’approccio bayesiano all’analisi dei dati. Questo metodo, basato sulla quantificazione e sull’aggiornamento delle credenze alla luce di nuove evidenze, fornirà agli studenti gli strumenti per affrontare l’incertezza in modo rigoroso e sistematico, sia nella carriera accademica sia nella pratica clinica.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#la-natura-soggettiva-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#la-natura-soggettiva-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.3 La natura soggettiva dell’incertezza",
    "text": "2.3 La natura soggettiva dell’incertezza\nUn elemento cruciale dell’incertezza, spesso trascurato, è la sua dimensione soggettiva. De Finetti (Finetti, 1970) ha evidenziato come l’incertezza sia, almeno in parte, una questione personale: ciò che è incerto per uno psicologo può non esserlo per un altro, in funzione delle loro esperienze, conoscenze pregresse e interpretazioni dei dati disponibili. Anche di fronte a una stessa questione, due ricercatori possono condividere un’incertezza comune, ma con gradi di intensità diversi.\nQuesta componente soggettiva è particolarmente significativa in psicologia, dove le differenze individuali e culturali influenzano la percezione e l’interpretazione dei fenomeni. L’approccio bayesiano offre un potente strumento per affrontare questa soggettività, consentendo di quantificare le differenze tra credenze individuali e di aggiornarle in modo coerente sulla base di nuove evidenze oggettive.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#lonnipresenza-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#lonnipresenza-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.4 L’onnipresenza dell’incertezza",
    "text": "2.4 L’onnipresenza dell’incertezza\nL’incertezza pervade ogni aspetto della ricerca psicologica. Ogni esperimento, misurazione o interpretazione dei dati comporta un margine di incertezza. Questa condizione è particolarmente evidente nello studio di fenomeni complessi come il comportamento umano o i processi mentali, dove innumerevoli variabili interagiscono, molte delle quali difficili da misurare o controllare con precisione.\nTuttavia, l’incertezza non deve essere vista come un ostacolo insormontabile. Al contrario, riconoscerla e quantificarla può favorire una comprensione più profonda e realistica dei fenomeni psicologici. Attraverso l’approccio bayesiano, diventa possibile integrare l’incertezza nel processo di indagine scientifica, trattandola non come un limite, ma come una risorsa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#superare-la-soppressione-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#superare-la-soppressione-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.5 Superare la soppressione dell’incertezza",
    "text": "2.5 Superare la soppressione dell’incertezza\nNonostante la sua onnipresenza, l’incertezza è spesso ignorata o minimizzata nella comunicazione scientifica. Questo può avvenire attraverso interpretazioni eccessivamente ottimistiche dei risultati, la presentazione di conclusioni come fatti certi, o una riluttanza a riconoscere i limiti degli studi condotti. Tale atteggiamento, sebbene comprensibile, può condurre a conclusioni errate e a una visione distorta della realtà.\nL’approccio bayesiano permette di affrontare l’incertezza in modo esplicito e costruttivo. Fornendo un quadro rigoroso per quantificarla, analizzarla e comunicarla chiaramente, migliora la trasparenza della ricerca e promuove conclusioni più oneste e accurate.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#i-benefici-dellincertezza",
    "href": "chapters/key_notions/00_uncertainty.html#i-benefici-dellincertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.6 I benefici dell’incertezza",
    "text": "2.6 I benefici dell’incertezza\nContrariamente a quanto si possa pensare, l’incertezza offre numerosi vantaggi per la ricerca psicologica:\n\nStimola l’esplorazione scientifica: La consapevolezza dell’incertezza incoraggia i ricercatori a formulare nuove ipotesi e a migliorare i metodi di studio.\nPromuove l’onestà intellettuale: Accettare l’incertezza rende i ricercatori più cauti e aperti a prospettive alternative.\nMigliora la qualità delle analisi: Integrare l’incertezza porta a disegni sperimentali più robusti e interpretazioni più accurate.\nFacilita la collaborazione interdisciplinare: Riconoscere i limiti delle proprie conoscenze stimola la ricerca di input da altri esperti.\nRiflette la complessità dei fenomeni psicologici: L’incertezza è intrinseca ai processi mentali e riconoscerla consente di rappresentarli in modo più realistico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#tipi-di-incertezza",
    "href": "chapters/key_notions/00_uncertainty.html#tipi-di-incertezza",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.7 Tipi di incertezza",
    "text": "2.7 Tipi di incertezza\nL’incertezza nella ricerca può essere classificata in tre categorie principali, in base alla sua origine: aleatoria, epistemica e ontologica (Gansch & Adee, 2020).\n\n2.7.1 Incertezza Aleatoria\nL’incertezza aleatoria è intrinseca alla natura casuale di un processo e non può essere eliminata per un dato modello probabilistico. Essa è considerata irreducibile e viene quantificata tramite distribuzioni probabilistiche. Ad esempio, nella misurazione della risposta di un individuo a uno stimolo, la variabilità intrinseca nel comportamento umano, dovuta a fattori imprevedibili, rappresenta un caso di incertezza aleatoria. Questo tipo di incertezza è una caratteristica fondamentale di molti fenomeni psicologici e biologici.\n\n\n2.7.2 Incertezza Epistemica\nL’incertezza epistemica deriva dalla conoscenza limitata o incompleta di un fenomeno. Essa rappresenta il “noto-ignoto”, cioè ciò che sappiamo di non sapere, ed è legata alle semplificazioni insite in ogni modello scientifico. Ad esempio, un modello psicologico che non consideri le influenze culturali o ambientali potrebbe risultare incompleto, introducendo incertezza epistemica. Diversamente dall’incertezza aleatoria, l’incertezza epistemica può essere ridotta attraverso il miglioramento dei modelli, l’inclusione di variabili rilevanti o la raccolta di ulteriori dati.\n\n\n2.7.3 Incertezza Ontologica\nL’incertezza ontologica riguarda l’“ignoto-ignoto”, ovvero aspetti di un sistema che non sono ancora stati identificati. In psicologia, questo potrebbe riferirsi a variabili o processi non ancora scoperti che influenzano un comportamento. Ad esempio, studiando i disturbi mentali, potrebbero emergere nuovi fattori di rischio precedentemente sconosciuti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#il-calcolo-dellincertezza-nellapproccio-bayesiano",
    "href": "chapters/key_notions/00_uncertainty.html#il-calcolo-dellincertezza-nellapproccio-bayesiano",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.8 Il calcolo dell’incertezza nell’approccio bayesiano",
    "text": "2.8 Il calcolo dell’incertezza nell’approccio bayesiano\nL’insegnamento si propone di fornire agli studenti strumenti per affrontare e quantificare l’incertezza attraverso l’approccio bayesiano (Gelman et al., 1995). Fondato sul teorema di Bayes, questo metodo rappresenta un quadro teorico rigoroso e sistematico per aggiornare le credenze alla luce di nuove evidenze, configurandosi come una componente centrale della metodologia scientifica.\nIl processo si basa su quattro passaggi essenziali. In primo luogo, si parte dalla quantificazione delle credenze iniziali, note come prior, che rappresentano le conoscenze pregresse o le ipotesi relative a un determinato fenomeno psicologico. Successivamente, si analizza la forza delle evidenze empiriche fornite dai dati raccolti, formalizzata nella likelihood. Queste due informazioni vengono combinate per generare le credenze aggiornate, chiamate posterior, che sintetizzano la conoscenza disponibile integrando i dati empirici e le ipotesi iniziali. Infine, le credenze aggiornate possono essere utilizzate per prendere decisioni più informate, pianificare ricerche future e orientare interventi.\n\n2.8.1 Il ruolo delle credenze e delle decisioni nella ricerca psicologica\nLe credenze rivestono un ruolo fondamentale nella ricerca psicologica, influenzando tutte le fasi del processo scientifico, dalla progettazione degli esperimenti all’interpretazione dei risultati, fino alla scelta di interventi clinici. L’approccio bayesiano si distingue per la sua capacità di esplicitare e formalizzare queste credenze, consentendo di aggiornare il loro contenuto in modo coerente e trasparente man mano che emergono nuove evidenze.\nQuesto metodo permette non solo di ottimizzare le decisioni basandosi su informazioni aggiornate, ma anche di comunicare chiaramente l’incertezza associata alle conclusioni, evidenziandone i limiti e garantendo maggiore trasparenza scientifica. Affrontare l’incertezza come una componente intrinseca della ricerca non solo migliora la qualità dell’analisi, ma consente anche di promuovere un approccio più realistico e rigoroso nello studio dei fenomeni psicologici.\nIn sintesi, l’approccio bayesiano offre un modello operativo per integrare l’incertezza nel processo decisionale, trattandola non come un ostacolo, ma come un elemento essenziale per una comprensione più sfumata e accurata della realtà.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#riflessioni-conclusive",
    "href": "chapters/key_notions/00_uncertainty.html#riflessioni-conclusive",
    "title": "2  Abbracciare l’incertezza",
    "section": "2.9 Riflessioni Conclusive",
    "text": "2.9 Riflessioni Conclusive\nQuesto insegnamento fornisce gli strumenti per applicare l’analisi bayesiana nell’ambito dei dati psicologici, insegnando a considerare l’incertezza come una parte integrante e preziosa del processo scientifico. Attraverso questo approccio, gli studenti potranno acquisire una comprensione più raffinata e strutturata dei fenomeni psicologici, integrando l’incertezza come elemento fondamentale per interpretare i dati e formulare inferenze rigorose.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/00_uncertainty.html#bibliografia",
    "href": "chapters/key_notions/00_uncertainty.html#bibliografia",
    "title": "2  Abbracciare l’incertezza",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nFinetti, B. de. (1970). Teoria delle probabilità: sintesi introduttiva con appendice critica. Einaudi.\n\n\nGansch, R., & Adee, A. (2020). System theoretic view on uncertainties. 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE), 1345–1350.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman; Hall/CRC.\n\n\nLindley, D. V. (2013). Understanding uncertainty. John Wiley & Sons.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abbracciare l'incertezza</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html",
    "href": "chapters/key_notions/01_key_notions.html",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Introduzione\nQuesto capitolo introduce il contesto e i principi base dell’analisi dei dati, con un focus su come le tecniche statistiche, combinate con una solida teoria dei fenomeni, siano strumentali all’avanzamento delle conoscenze scientifiche.\nL’analisi dei dati consente di sintetizzare grandi quantità di informazioni e di verificare le previsioni avanzate dalle teorie. Tuttavia, senza una teoria che dia significato ai dati, le osservazioni rimangono mere descrizioni prive di un contesto esplicativo. È attraverso l’integrazione tra dati e teoria che si raggiunge una comprensione profonda dei fenomeni e si favorisce l’avanzamento scientifico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#introduzione",
    "href": "chapters/key_notions/01_key_notions.html#introduzione",
    "title": "3  Concetti chiave",
    "section": "",
    "text": "Most of the fundamental ideas of science are essentially simple, and may, as a rule, be expressed in a language comprehensible to everyone.\n(Einstein A and Infeld L, 1938)\n\n\n\n\n\n\n\n\nStatistica\n\n\n\nIl termine “statistica” può assumere diversi significati, a seconda del contesto in cui viene utilizzato.\n\nNel primo senso, la statistica è una scienza e una disciplina che si occupa dello studio e dell’applicazione di metodi e tecniche per la raccolta, l’organizzazione, l’analisi, l’interpretazione e la presentazione di dati.\nNel secondo senso, il termine “statistica” si riferisce a una singola misura o un valore numerico che è stato calcolato a partire da un campione di dati. Questo tipo di statistica rappresenta una caratteristica specifica del campione. Esempi comuni di statistiche in questo senso includono la media campionaria, la deviazione standard campionaria o il coefficiente di correlazione campionario.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#la-spiegazione-scientifica",
    "href": "chapters/key_notions/01_key_notions.html#la-spiegazione-scientifica",
    "title": "3  Concetti chiave",
    "section": "3.1 La Spiegazione Scientifica",
    "text": "3.1 La Spiegazione Scientifica\nLa scienza non si limita a descrivere o prevedere i fenomeni; essa mira a spiegare il perché degli eventi, fornendo una comprensione delle cause e dei meccanismi che governano il mondo. La spiegazione scientifica è quindi uno strumento essenziale per costruire teorie che non solo descrivono e prevedono, ma anche chiariscono le dinamiche causali e le connessioni tra fenomeni, aiutando così a sviluppare un controllo informato sugli stessi.\nSe prendiamo l’esempio del successo accademico in psicologia dell’educazione, possiamo osservare che i dati rivelano una forte associazione tra il livello di istruzione dei genitori e il successo scolastico dei figli. Tuttavia, una semplice previsione basata su questa associazione – “provenendo da una famiglia con basso livello d’istruzione, è improbabile che tu ottenga un titolo universitario” – non risponde alle domande fondamentali per migliorare il sistema educativo: perché esiste questa disparità? Quali interventi potrebbero ridurre questa disuguaglianza?\nPer andare oltre la previsione, la scienza deve individuare i fattori causali che contribuiscono al fenomeno, esplorare il modo in cui agire su questi fattori potrebbe alterare l’outcome, e stimare le incertezze e le dinamiche temporali di questi effetti. Ad esempio, per ridurre la disuguaglianza educativa, è necessario comprendere se e come aumentare il sostegno finanziario agli studenti possa realmente facilitare il percorso scolastico di chi proviene da contesti meno favoriti, e prevedere gli effetti di lungo termine di tali politiche. Questo approccio permette non solo di prevedere ma anche di controllare e migliorare i fenomeni studiati.\n\n3.1.1 Elementi Fondamentali della Spiegazione Scientifica\nLa filosofia della scienza ha individuato tre elementi chiave di una spiegazione scientifica:\n\nExplanandum: il fenomeno da spiegare. Ad esempio, “si è verificata una crisi petrolifera nel 1973.”\nExplanans: un insieme di affermazioni che spiegano il fenomeno. Per esempio, “gli stati membri dell’OAPEC hanno imposto un embargo sul petrolio in risposta al sostegno degli Stati Uniti a Israele nella guerra del Kippur.”\nLegame esplicativo: i principi o le leggi che descrivono il meccanismo sottostante, ossia il modo in cui l’explanans causa l’explanandum. Nel caso dell’embargo, il legame potrebbe essere: “gli stati dell’OAPEC usarono il petrolio come strumento politico per influenzare la politica estera degli Stati Uniti.”\n\nI modelli scientifici incorporano questi elementi, rappresentando una metodologia per ottenere spiegazioni scientifiche. Essi includono il fenomeno da spiegare, i fattori causali rilevanti e i meccanismi che collegano i fattori all’esito. A differenza dei modelli puramente descrittivi o predittivi, i modelli scientifici in psicologia sono progettati per rispondere a domande causali, facilitando la comprensione e il controllo dei fenomeni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#modelli-psicologici",
    "href": "chapters/key_notions/01_key_notions.html#modelli-psicologici",
    "title": "3  Concetti chiave",
    "section": "3.2 Modelli Psicologici",
    "text": "3.2 Modelli Psicologici\nUn modello è una rappresentazione matematica semplificata di un fenomeno reale. È composto da un insieme di equazioni e ipotesi che definiscono la struttura probabilistica e le relazioni tra le variabili, cercando di cogliere gli aspetti essenziali del fenomeno senza includerne ogni dettaglio. Esistono spesso diversi modelli applicabili a uno stesso problema, e il compito della scienza dei dati è identificare quello che meglio si adatta ai dati, soddisfacendo criteri di validità e accuratezza.\nI modelli psicologici sono strumenti concettuali per descrivere, spiegare e prevedere il comportamento umano e i processi mentali. Un buon modello psicologico dovrebbe avere alcune caratteristiche fondamentali:\n\nCoerenza descrittiva: Il modello deve rappresentare in modo logico e coerente il fenomeno studiato, catturando gli aspetti chiave del processo psicologico e organizzando le osservazioni in una struttura comprensibile.\nCapacità predittiva: Un modello efficace deve essere in grado di fare previsioni accurate sui futuri sviluppi del fenomeno. Questa capacità non solo ne aumenta l’utilità, ma permette anche di testarne la validità.\nSupporto empirico: Le ipotesi e le previsioni del modello devono essere confermate da dati raccolti attraverso ricerche sistematiche e rigorose.\nFalsificabilità: Un modello scientifico deve poter essere testato e, se necessario, confutato con l’osservazione e l’esperimento. Questo principio assicura che il modello rimanga aperto alla revisione e al miglioramento in base a nuove evidenze.\nParsimonia: Il modello dovrebbe spiegare il fenomeno nel modo più semplice possibile, evitando complessità inutili.\nGeneralizzabilità: Deve essere applicabile a una vasta gamma di situazioni e contesti, non limitandosi a casi specifici o condizioni sperimentali particolari.\nUtilità pratica: Un modello efficace dovrebbe fornire spunti utili per interventi, terapie o applicazioni nel mondo reale.\n\nLa modellazione in psicologia affronta sfide uniche dovute alla natura soggettiva e variabile dell’esperienza umana. I ricercatori devono bilanciare la precisione scientifica con la flessibilità necessaria per cogliere la complessità dei fenomeni psicologici, considerando al contempo i limiti etici della sperimentazione e le potenziali implicazioni sociali dei loro modelli.\nL’analisi dei dati, attraverso tecniche statistiche, è il mezzo per valutare un modello psicologico. Oltre a stabilire se il modello riesce a spiegare i dati osservati, l’analisi verifica la capacità del modello di fare previsioni su dati non ancora raccolti. In questo modo, la modellazione non solo consente di comprendere i fenomeni psicologici ma permette anche di prevedere e, in certi casi, influenzare il comportamento e i processi mentali.\n\n3.2.1 Rappresentare i Fenomeni per Ragionare e Comunicare\nLa spiegazione scientifica, oltre a chiarire i meccanismi causali, serve anche a fornire un linguaggio per ragionare sui fenomeni e per condividere la conoscenza. In psicologia, la costruzione di modelli scientifici permette di rappresentare i fenomeni attraverso variabili, funzioni e parametri, fornendo un vocabolario per descrivere componenti, dipendenze e proprietà dei fenomeni. Un modello semplice e chiaro consente di emulare il comportamento del fenomeno senza necessità di simulazioni complesse, facilitando la comunicazione e l’intuizione.\nUn aspetto importante della spiegazione scientifica è la possibilità di utilizzare i modelli per stimolare l’intuizione e generare nuove domande. La comprensione dei fenomeni attraverso una rappresentazione scientifica accessibile permette di formulare ipotesi, collegare concetti, e trasferire conoscenze da un campo all’altro.\nIn sintesi, la spiegazione scientifica va oltre la mera previsione: mira a fornire una comprensione completa dei fenomeni, basata su nessi causali e su un linguaggio formale per ragionare e comunicare. I modelli scientifici non solo predicono eventi, ma spiegano come e perché questi eventi si verificano, offrendo una struttura con cui intervenire e influenzare i fenomeni stessi.\nNell’analisi dei dati bayesiana, questa attenzione alle cause e agli effetti trova un’applicazione naturale. La possibilità di aggiornare le proprie credenze alla luce di nuove informazioni consente di costruire modelli che non si limitano alla descrizione o alla previsione, ma che forniscono spiegazioni coerenti e profonde dei fenomeni, aiutando a sviluppare teorie sempre più raffinate e applicabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#ruolo-dellanalisi-dei-dati",
    "href": "chapters/key_notions/01_key_notions.html#ruolo-dellanalisi-dei-dati",
    "title": "3  Concetti chiave",
    "section": "3.3 Ruolo dell’Analisi dei Dati",
    "text": "3.3 Ruolo dell’Analisi dei Dati\nL’analisi dei dati riveste un ruolo centrale nelle scienze, specialmente in psicologia, per due ragioni principali:\n\nRiassumere grandi quantità di informazioni: consente di sintetizzare dati complessi in statistiche descrittive, grafici e altre rappresentazioni che rendono i dati accessibili e comprensibili. Questo processo evidenzia tendenze generali, variazioni e anomalie, facilitando l’identificazione di schemi comportamentali e differenze tra gruppi.\nVerificare le predizioni di un modello scientifico: permette di confrontare le aspettative teoriche con i dati osservati, valutando la validità delle ipotesi sottostanti. Questa verifica contribuisce direttamente all’avanzamento della conoscenza scientifica, sostenendo, modificando o confutando una teoria.\n\nSebbene l’analisi dei dati possa portare alla scoperta di correlazioni o schemi interessanti, questi risultati, senza una teoria, offrono solo una comprensione limitata. Per esempio, rilevare che due variabili psicologiche sono correlate non fornisce informazioni sulla natura di questa relazione o sul motivo per cui esiste. Per interpretare e attribuire un significato a queste osservazioni, è necessario un quadro teorico che le contestualizzi e proponga meccanismi causali o esplicativi.\n\n3.3.1 Carattere Multidisciplinare dell’Analisi dei Dati\nL’analisi dei dati si situa all’intersezione di tre discipline principali: statistica, teoria della probabilità e informatica. Ciascuna contribuisce con strumenti e approcci specifici essenziali per comprendere i dati, estrarre conoscenza e generare nuove ipotesi scientifiche.\n\nStatistica: offre tecniche per raccogliere, analizzare e interpretare i dati, fornendo strumenti descrittivi e inferenziali utili per trarre conclusioni e prendere decisioni.\nTeoria della probabilità: fornisce la base matematica della statistica, consentendo di modellare e quantificare l’incertezza e di comprendere i fenomeni aleatori che caratterizzano molte osservazioni in psicologia.\nInformatica: supporta l’analisi attraverso strumenti per la gestione, l’elaborazione e la visualizzazione di grandi quantità di dati. La programmazione consente di sviluppare modelli avanzati e gestire dataset complessi.\n\nQuesta natura multidisciplinare riflette la complessità dell’analisi dei dati e la necessità di integrare diverse competenze per affrontare le sfide scientifiche contemporanee.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "href": "chapters/key_notions/01_key_notions.html#concetti-chiave-nellanalisi-dei-dati",
    "title": "3  Concetti chiave",
    "section": "3.4 Concetti Chiave nell’Analisi dei Dati",
    "text": "3.4 Concetti Chiave nell’Analisi dei Dati\nPer condurre un’analisi dei dati efficace, è fondamentale comprendere alcuni concetti chiave.\n\n3.4.1 Popolazioni e Campioni\nIn ogni analisi dei dati, è fondamentale identificare la popolazione di interesse, l’insieme completo di entità o individui che rappresentano il fenomeno studiato. In psicologia, ad esempio, si può voler studiare il benessere in una popolazione generale o in una sotto-popolazione specifica, come gli individui che hanno subito un evento stressante.\nPer ottenere informazioni dettagliate su una popolazione, si utilizzano campioni: sottoinsiemi rappresentativi dai quali si possono fare inferenze sull’intera popolazione. La rappresentatività del campione è cruciale, poiché un campione non rappresentativo può portare a conclusioni errate e limitare la generalizzabilità dei risultati.\n\n\n3.4.2 Bias nella Raccolta Dati\nI bias nella raccolta e interpretazione dei dati possono influenzare profondamente i risultati di uno studio. Capire chi ha raccolto i dati, come e con quali finalità, è fondamentale per garantire una corretta interpretazione. I dati non sono mai neutri e le intenzioni che ne guidano la raccolta spesso ne influenzano l’interpretazione (Murray & Carr, 2024; Nobles, 2000)\n\n\n\nTabella creata da Ellie Murray.\n\n\n\n\n3.4.3 Variabili e Costanti\nNell’analisi statistica, le variabili rappresentano le caratteristiche osservate che possono assumere diversi valori (numerici o categorici). Al contrario, le costanti sono valori che rimangono fissi in un dato contesto. Si distinguono poi le variabili indipendenti (o predittive), che influenzano le variabili dipendenti, e le variabili dipendenti, che rappresentano gli esiti di interesse.\n\n\n3.4.4 Effetti\nIn statistica, un effetto misura il cambiamento osservato nelle variabili dipendenti in relazione alle variabili indipendenti. Ad esempio, l’efficacia di una terapia può essere valutata misurando la differenza nei sintomi prima e dopo il trattamento (Huntington-Klein, 2021).\n\n\n3.4.5 Stima e Inferenza\n\n3.4.5.1 Stima\nLa stima statistica consente di ottenere informazioni su una popolazione a partire da un campione. Si utilizzano statistiche campionarie (come la media campionaria) per stimare i parametri della popolazione (come la media vera della popolazione).\nGli stimatori devono possedere proprietà come:\n\nconsistenza: la stima converge al vero valore del parametro all’aumentare della dimensione del campione;\nnon distorsione: il valore atteso dello stimatore è uguale al vero valore del parametro;\nefficienza: lo stimatore ha la minor varianza possibile.\n\nL’accuratezza della stima dipende da vari fattori, tra cui la dimensione e la rappresentatività del campione, la variabilità nella popolazione e il metodo di campionamento utilizzato.\n\n\n\n3.4.6 Inferenza Statistica\nDopo aver ottenuto le stime, l’inferenza statistica permette di trarre conclusioni più generali sulla popolazione. Essa consente di valutare ipotesi specifiche o rispondere a domande di ricerca basate sui dati raccolti.\nAd esempio, se abbiamo stimato la media del rendimento accademico in un campione di studenti, l’inferenza statistica ci consente di quantificare l’incertezza riguardo alla differenza di rendimento tra maschi e femmine all’interno della popolazione più ampia. In questo modo, l’inferenza statistica ci fornisce gli strumenti per fare previsioni e trarre conclusioni su fenomeni che riguardano l’intera popolazione.\nEsistono due approcci principali.\nL’inferenza bayesiana:\n\nSi basa sul teorema di Bayes;\nUtilizza probabilità a priori, che riflettono conoscenze o credenze iniziali su un fenomeno;\nAggiorna queste probabilità con nuovi dati per ottenere probabilità a posteriori;\nFornisce una interpretazione delle probabilità come gradi di credenza soggettivi.\n\nL’approccio frequentista:\n\nSi fonda sulla frequenza relativa di eventi osservati in esperimenti ripetuti;\nUtilizza strumenti come il test di ipotesi nulla e gli intervalli di confidenza per trarre conclusioni;\nNon fa uso di probabilità a priori, concentrandosi esclusivamente sui dati osservati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "href": "chapters/key_notions/01_key_notions.html#le-sfide-dellinferenza-statistica-in-psicologia",
    "title": "3  Concetti chiave",
    "section": "3.5 Le Sfide dell’Inferenza Statistica in Psicologia",
    "text": "3.5 Le Sfide dell’Inferenza Statistica in Psicologia\nSecondo Gelman et al. (2020), l’inferenza statistica in psicologia affronta tre sfide principali:\n\nGeneralizzare dai campioni alla popolazione: Questa sfida è strettamente legata al problema del campionamento di comodo, spesso usato in psicologia, ma presente in quasi tutte le applicazioni dell’inferenza statistica. La difficoltà risiede nel trarre conclusioni affidabili su una popolazione più ampia partendo da un campione limitato e, a volte, non rappresentativo.\nGeneralizzare dal gruppo trattato al gruppo di controllo: Questa sfida riguarda l’inferenza causale, un aspetto centrale per determinare l’efficacia dei trattamenti psicologici. L’obiettivo è stabilire se i risultati osservati nel gruppo trattato possano essere applicati al gruppo di controllo o ad altre popolazioni, permettendo una valutazione valida dell’effetto del trattamento.\nGeneralizzare dalle misurazioni osservate ai costrutti sottostanti: In psicologia, i dati raccolti non corrispondono mai perfettamente ai costrutti teorici di interesse. La sfida è inferire questi costrutti latenti dai dati osservati, che rappresentano spesso solo un’approssimazione imperfetta.\n\nQueste sfide evidenziano la complessità dell’inferenza in psicologia e la necessità di metodologie robuste per affrontarle.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#riflessioni-conclusive",
    "href": "chapters/key_notions/01_key_notions.html#riflessioni-conclusive",
    "title": "3  Concetti chiave",
    "section": "3.6 Riflessioni Conclusive",
    "text": "3.6 Riflessioni Conclusive\nIn psicologia, le teorie forniscono ipotesi testabili che spiegano il “come” e il “perché” di determinati fenomeni mentali e comportamentali. Una teoria robusta permette di formulare previsioni chiare e specifiche, che possono essere verificate empiricamente attraverso l’analisi dei dati. Ad esempio, una teoria sull’ansia potrebbe prevedere che, in un compito di esposizione graduale a stimoli ansiogeni, il livello di ansia diminuisca progressivamente. Senza una teoria che spieghi perché questo dovrebbe accadere, tale osservazione rimane solo un dato descrittivo, privo di valore esplicativo o predittivo.\nL’analisi dei dati diventa davvero potente quando è integrata a una teoria. Senza teoria, i dati possono descrivere fenomeni ma non spiegare i meccanismi sottostanti. La teoria fornisce il contesto interpretativo, orientando la raccolta e l’analisi dei dati, e permettendo una comprensione profonda dei fenomeni psicologici.\nUn esempio è l’uso della data science per analizzare l’efficacia di un trattamento psicoterapeutico. I dati possono mostrarci una diminuzione dei sintomi in seguito alla terapia, ma è solo la teoria alla base del trattamento che fornisce un quadro interpretativo per questo miglioramento, proponendo i meccanismi per cui il trattamento riduce i sintomi. La teoria orienta quindi l’analisi e permette di interpretare i dati in un contesto scientifico.\nSviluppare una teoria in psicologia è complesso a causa della notevole variabilità umana. Un buon modello psicologico deve prevedere con precisione i comportamenti osservabili e rappresentare i processi mentali latenti. Queste previsioni devono essere testabili e falsificabili (Eronen & Bringmann, 2021).\nLa relazione tra teoria e analisi dei dati è dinamica e iterativa. I modelli e le teorie si evolvono grazie alla verifica empirica. Se i dati non supportano le previsioni di una teoria, essa viene modificata o sostituita, favorendo l’avanzamento scientifico.\nIn conclusione, la teoria e l’analisi dei dati sono complementari e interdipendenti. L’analisi dei dati offre gli strumenti per testare e affinare le teorie psicologiche, mentre la teoria dà significato e contesto ai dati, rendendo possibile una comprensione profonda e utile dei fenomeni psicologici.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/01_key_notions.html#bibliografia",
    "href": "chapters/key_notions/01_key_notions.html#bibliografia",
    "title": "3  Concetti chiave",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nEronen, M. I., & Bringmann, L. F. (2021). The theory crisis in psychology: How to move forward. Perspectives on Psychological Science, 16(4), 779–788.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press.\n\n\nHuntington-Klein, N. (2021). The effect: An introduction to research design and causality. Chapman; Hall/CRC.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMurray, E. J., & Carr, K. C. (2024). Measuring Racial Sentiment Using Social Media Is Harder Than It Seems. Epidemiology, 35(1), 60–63.\n\n\nNobles, M. (2000). Shades of citizenship: Race and the census in modern politics. Stanford University Press.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html",
    "href": "chapters/key_notions/02_measurement.html",
    "title": "4  La misurazione in psicologia",
    "section": "",
    "text": "4.1 Introduzione\nLa scienza si avvale di modelli per interpretare i dati, ma opera sempre con teorie incomplete e misurazioni soggette a errori. Di conseguenza, è fondamentale riconoscere le incertezze quando si cerca di estrarre informazioni dalle misurazioni utilizzando i nostri modelli. Nessuna misurazione, spiegazione o previsione è perfettamente accurata e precisa, e non possiamo mai conoscere con esattezza l’entità dei loro errori.\nQuesta incertezza viene catturata in tre equazioni fondamentali. La prima è l’Equazione di Misurazione, che riconosce l’errore osservativo: \\(y = z + ϵ_y\\), dove \\(y\\) rappresenta il valore misurato, \\(z\\) il valore reale e \\(ϵ_y\\) l’errore di misurazione. La seconda è l’Equazione di Modellazione, che esprime la presenza di un diverso tipo di errore: \\(z = f(x,θ) + ϵ_\\text{model}\\), dove \\(f\\) è il modello, \\(x\\) sono le condizioni ambientali per cui eseguiamo il modello, θ sono i valori dei parametri del modello e \\(ϵ_\\text{model}\\) rappresenta l’errore del modello, che sorge perché \\(f\\), \\(x\\) e θ saranno tutti in qualche misura imprecisi.\nCombinando queste due equazioni, otteniamo l’Equazione della Scienza: \\(y = f(x,θ) + ϵ_\\text{model} + ϵ_y\\). La scienza è il tentativo di spiegare le osservazioni \\(y\\) utilizzando un modello \\(f\\), cercando di minimizzare l’errore di misurazione \\(ϵ_y\\) e l’errore del modello \\(ϵ_\\text{model}\\), in modo che il modello possa essere utilizzato per fare previsioni sul mondo reale (\\(z\\)). L’approccio bayesiano alla scienza riconosce e quantifica le incertezze su tutti e sei gli elementi dell’Equazione della Scienza: \\(y\\), \\(f\\), \\(x\\), θ, \\(ϵ_\\text{model}\\) e \\(ϵ_y\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#la-teoria-della-misurazione",
    "href": "chapters/key_notions/02_measurement.html#la-teoria-della-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.2 La teoria della Misurazione",
    "text": "4.2 La teoria della Misurazione\nLa teoria della misurazione, oggetto di questo capitolo, si concentra sull’errore di misurazione e sull’equazione fondamentale \\(y = z + ϵ_y\\). Questa equazione può essere esaminata da tre prospettive distinte. La prima concerne l’affidabilità della misura, rappresentata dal termine \\(ϵ_y\\). La psicometria, branca dedicata alla teoria della misurazione psicologica, si occupa di quantificare l’affidabilità delle misure psicologiche attraverso metodi come la Teoria Classica dei Test e la Teoria di Risposta all’Item.\nLa seconda prospettiva riguarda la validità delle misure psicologiche, ovvero quanto adeguatamente la misura \\(y\\) rappresenti il costrutto \\(z\\). Questo aspetto, più complesso dell’affidabilità, non può essere risolto meramente con metodi statistici, ma richiede una profonda comprensione delle teorie psicologiche e della loro capacità di descrivere e prevedere i fenomeni psicologici.\nLa terza prospettiva si concentra sulle procedure di assegnazione dei valori a \\(y\\), esplorando quali metodi (questionari, interviste, esperimenti) siano più appropriati e come valutarne l’adeguatezza.\n\n4.2.1 Costrutti Psicologici\nLa teoria della misurazione sottolinea l’importanza di distinguere tra la procedura di misurazione e il costrutto che si intende misurare. Ad esempio, mentre la temperatura è un costrutto, il termometro è lo strumento di misurazione. Analogamente, l’abilità matematica è un costrutto, mentre un test di matematica è la procedura per misurarla.\nNelle scienze psicologiche e sociali, la misurazione presenta sfide uniche rispetto alle scienze fisiche, poiché i costrutti in esame sono spesso astratti e non direttamente osservabili. Ciò richiede una particolare attenzione alla validità e all’affidabilità degli strumenti di misurazione, nonché una costante riflessione sulle limitazioni e le potenziali fonti di errore.\nIl capitolo introduce concetti fondamentali relativi alla misurazione quantitativa delle caratteristiche psicologiche, con un focus sulla teoria delle scale di misura di Stevens (1946). Questa teoria fornisce un quadro concettuale per comprendere i diversi tipi di scale di misurazione e le operazioni matematiche appropriate per ciascuna. Inoltre, vengono esplorate alcune procedure di scaling psicologico, ovvero l’assegnazione di numeri all’intensità di fenomeni psicologici.\n\n\n4.2.2 Scaling Psicologico\nLo scaling psicologico si occupa della trasformazione dei dati empirici raccolti durante uno studio psicologico in misure o punteggi che rappresentino accuratamente le caratteristiche psicologiche oggetto di indagine.\nScaling di Guttman. Uno dei metodi di scaling più noti è lo «Scaling di Guttman», che viene utilizzato per rappresentare relazioni ordinate tra gli elementi di una scala. Ad esempio, in un questionario sui sintomi dell’ansia, le domande possono essere disposte in ordine di intensità crescente dei sintomi. Secondo il modello di Guttman, se un partecipante risponde “sì” a una domanda che riflette un sintomo più intenso, ci si aspetta che abbia risposto “sì” anche a tutte le domande precedenti, che rappresentano sintomi di intensità minore. Questo approccio consente di costruire una scala che riflette in modo sistematico e coerente la gravità dei sintomi.\nScaling Thurstoniano. Lo «Scaling Thurstoniano» è un metodo utilizzato per misurare preferenze o giudizi soggettivi. Ad esempio, per valutare la preferenza tra diversi tipi di cibi, i partecipanti confrontano due cibi alla volta ed esprimono una preferenza. Le risposte vengono poi utilizzate per assegnare punteggi che riflettono la preferenza media per ciascun cibo.\nQuestionari Likert. I questionari Likert richiedono ai partecipanti di esprimere il loro grado di accordo con una serie di affermazioni su una scala a più livelli, che va da «fortemente in disaccordo» a «fortemente d’accordo». I punteggi ottenuti vengono sommati per rappresentare la posizione complessiva dell’individuo rispetto all’oggetto di studio.\n\n\n4.2.3 Metodi di Valutazione delle Scale Psicologiche\nPer valutare le proprietà delle scale psicologiche, vengono utilizzati vari metodi. Ad esempio, l’affidabilità delle misure può essere analizzata utilizzando il coefficiente alpha di Cronbach o il coefficiente Omega di McDonald, entrambi utilizzati per misurare la coerenza interna delle risposte ai diversi item di un questionario. Inoltre, la validità delle scale può essere esaminata confrontando i risultati ottenuti con misure simili o attraverso analisi statistiche che verificano se la scala cattura accuratamente il costrutto psicologico che si intende misurare. La validità di costrutto è particolarmente cruciale, poiché riguarda la capacità della scala di misurare effettivamente il concetto psicologico che si intende esplorare.\n\n\n4.2.4 Prospettive Moderne\nNegli ultimi anni, il dibattito sulla misurazione psicologica si è arricchito di nuove prospettive, grazie all’avvento di tecnologie avanzate e all’integrazione di approcci interdisciplinari. Ecco alcune delle tendenze più rilevanti.\nTeoria della Risposta agli Item. La Teoria della Risposta agli Item (IRT) ha guadagnato popolarità per la sua capacità di fornire stime più precise delle abilità latenti rispetto ai modelli classici. La IRT considera la probabilità che un individuo risponda correttamente a un item in funzione della sua abilità e delle caratteristiche dell’item stesso, offrendo una visione più dettagliata delle proprietà psicometriche degli strumenti di misurazione.\nApprocci Bayesiani. Gli approcci bayesiani stanno rivoluzionando il campo della psicometria, permettendo di incorporare informazioni a priori nelle stime e di aggiornare le credenze sulla base di nuovi dati. Questi metodi sono particolarmente utili per affrontare la complessità e l’incertezza inerenti alla misurazione psicologica.\nAnalisi di Rete. L’analisi di rete è un’altra metodologia emergente che vede i costrutti psicologici non come variabili latenti indipendenti, ma come reti di sintomi interconnessi. Questo approccio può offrire nuove intuizioni sulla struttura delle psicopatologie e sulla dinamica dei sintomi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#le-scale-di-misurazione",
    "href": "chapters/key_notions/02_measurement.html#le-scale-di-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.3 Le scale di misurazione",
    "text": "4.3 Le scale di misurazione\nLe scale di misurazione sono strumenti fondamentali per assegnare numeri ai dati osservati, rappresentando le proprietà psicologiche. La teoria delle scale di Stevens Stevens (1946) identifica quattro tipi di scale di misurazione: nominali, ordinali, a intervalli e di rapporti. Ognuna di queste scale consente di effettuare operazioni aritmetiche diverse, poiché ciascuna di esse è in grado di “catturare” solo alcune delle proprietà dei fenomeni psicologici che si intende misurare.\n\n\n\nScale di misurazione.\n\n\n\n4.3.1 Scala nominale\nILa scala nominale è il livello di misurazione più semplice e corrisponde ad una tassonomia o classificazione delle categorie che utilizziamo per descrivere i fenomeni psicologici. I simboli o numeri che costituiscono questa scala rappresentano i nomi delle categorie e non hanno alcun valore numerico intrinseco. Con la scala nominale possiamo solo distinguere se una caratteristica psicologica è uguale o diversa da un’altra.\nI dati raccolti con la scala nominale sono suddivisi in categorie qualitative e mutuamente esclusive, in cui ogni dato appartiene ad una sola categoria. In questa scala, esiste solo la relazione di equivalenza tra le misure delle unità di studio: gli elementi del campione appartenenti a classi diverse sono differenti, mentre tutti quelli della stessa classe sono tra loro equivalenti.\nL’unica operazione algebrica consentita dalla scala nominale è quella di contare le unità di studio che appartengono ad ogni categoria e il numero totale di categorie. Di conseguenza, la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.\nDalla scala nominale è possibile costruire altre scale nominali equivalenti alla prima, trasformando i valori della scala di partenza in modo tale da cambiare i nomi delle categorie, ma lasciando inalterata la suddivisione delle unità di studio nelle medesime classi di equivalenza. In altre parole, cambiando i nomi delle categorie di una variabile misurata su scala nominale, si ottiene una nuova variabile esattamente equivalente alla prima.\n\n\n4.3.2 Scala ordinale\nLa scala ordinale mantiene la caratteristica della scala nominale di classificare ogni unità di misura all’interno di una singola categoria, ma introduce la relazione di ordinamento tra le categorie. In quanto basata su una relazione di ordine, una scala ordinale descrive solo il rango di ordine tra le categorie e non fornisce informazioni sulla distanza tra di esse. Non ci dice, ad esempio, se la distanza tra le categorie \\(a\\) e \\(b\\) è uguale, maggiore o minore della distanza tra le categorie \\(b\\) e \\(c\\).\nUn esempio classico di scala ordinale è quello della scala Mohs per la determinazione della durezza dei minerali. Per stabilire la durezza dei minerali si usa il criterio empirico della scalfittura. Vengono stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo, topazio, corindone e diamante. Un minerale appartenente ad uno di questi livelli se scalfisce quello di livello inferiore ed è scalfito da quello di livello superiore.\n\n\n\nLa scala di durezza dei minerali di Mohs. Un oggetto è considerato più duro di X se graffia X. Sono incluse anche misure di durezza relativa utilizzando uno sclerometro, da cui emerge la non linearità della scala di Mohs (Burchard, 2004).\n\n\n\n\n4.3.3 Scala ad intervalli\nLa scala ad intervalli di misurazione include le proprietà della scala nominale e della scala ordinale e permette di misurare le distanze tra le coppie di unità statistiche in termini di un intervallo costante, chiamato “unità di misura”, a cui viene attribuito il valore “1”. L’origine della scala, ovvero il punto zero, è scelta arbitrariamente e non indica l’assenza della proprietà che si sta misurando. Ciò significa che la scala ad intervalli consente anche valori negativi e lo zero non viene attribuito all’unità statistica in cui la proprietà risulta assente.\nLa scala ad intervalli equivalenti consente l’esecuzione di operazioni algebriche basate sulla differenza tra i numeri associati ai diversi punti della scala, operazioni algebriche non possibili con le scale di misura nominale o ordinale. Tuttavia, il limite della scala ad intervalli è che non consente di calcolare il rapporto tra coppie di misure. È possibile affermare la differenza tra \\(a\\) e \\(b\\) come la metà della differenza tra \\(c\\) e \\(d\\) o che le due differenze sono uguali, ma non è possibile affermare che \\(a\\) abbia una proprietà misurata in quantità doppia rispetto a \\(b\\). In altre parole, non è possibile stabilire rapporti diretti tra le misure ottenute. Solo le differenze tra le modalità permettono tutte le operazioni aritmetiche, come la somma, l’elevazione a potenza o la divisione, che sono alla base della statistica inferenziale.\nNelle scale ad intervalli equivalenti, l’unità di misura è arbitraria e può essere cambiata attraverso una dilatazione, ovvero la moltiplicazione di tutti i valori della scala per una costante positiva. Inoltre, la traslazione, ovvero l’aggiunta di una costante a tutti i valori della scala, è ammessa poiché non altera le differenze tra i valori della scala. La scala rimane invariata rispetto a traslazioni e dilatazioni e dunque le uniche trasformazioni ammissibili sono le trasformazioni lineari:\n\\[\ny' = a + by, \\quad b &gt; 0.\n\\]\nInfatti, l’uguaglianza dei rapporti fra gli intervalli rimane invariata a seguito di una trasformazione lineare.\nEsempio di scala ad intervalli è la temperatura misurata in gradi Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è possibile stabilire se due modalità sono uguali o diverse: 30\\(^\\circ\\)C \\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale è possibile mettere due modalità in una relazione d’ordine: 30\\(^\\circ\\)C \\(&gt;\\) 20\\(^\\circ\\)C. In aggiunta ai casi precedenti, però, è possibile definire una unità di misura per cui è possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c’è una differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. I valori di temperatura, oltre a poter essere ordinati secondo l’intensità del fenomeno, godono della proprietà che le differenze tra loro sono direttamente confrontabili e quantificabili.\nIl limite della scala ad intervalli è quello di non consentire il calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura di 80\\(^\\circ\\)C non è il doppio di una di 40\\(^\\circ\\)C. Se infatti esprimiamo le stesse temperature nei termini della scala Fahrenheit, allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti, 20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa che la relazione “il doppio di” che avevamo individuato in precedenza si applicava ai numeri della scala centigrada, ma non alla proprietà misurata (cioè la temperatura). La decisione di che scala usare (Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non deve influenzare le inferenze che traiamo dai dati. Queste inferenze, infatti, devono dirci qualcosa a proposito della realtà empirica e non possono in nessun modo essere condizionate dalle nostre scelte arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che quella Fahrenheit.\nConsideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature: \\(20^\\circ C = 68^\\circ F\\), \\(15^\\circ C = 59^\\circ F\\), \\(10^\\circ C = 50 ^\\circ F\\).\nÈ facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:\n\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]\n\n\n4.3.4 Scala di rapporti\nNella scala a rapporti equivalenti, lo zero non è arbitrario e rappresenta l’elemento che ha intensità nulla rispetto alla proprietà misurata. Per costruire questa scala, si associa il numero 0 all’elemento con intensità nulla e si sceglie un’unità di misura \\(u\\). Ad ogni elemento si assegna un numero \\(a\\) definito come \\(a=d/u\\), dove \\(d\\) rappresenta la distanza dall’origine. In questo modo, i numeri assegnati riflettono le differenze e i rapporti tra le intensità della proprietà misurata.\nIn questa scala, è possibile effettuare operazioni aritmetiche non solo sulle differenze tra i valori della scala, ma anche sui valori stessi della scala. L’unica scelta arbitraria è l’unità di misura, ma lo zero deve sempre rappresentare l’intensità nulla della proprietà considerata.\nLe trasformazioni ammissibili in questa scala sono chiamate trasformazioni di similarità e sono del tipo \\(y' = by\\), dove \\(b&gt;0\\). In questa scala, i rapporti tra i valori rimangono invariati dopo le trasformazioni. In altre parole, se rapportiamo due valori originali e due valori trasformati, il rapporto rimane lo stesso: \\(\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "href": "chapters/key_notions/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.4 Gerarchia dei livelli delle scale di misurazione",
    "text": "4.4 Gerarchia dei livelli delle scale di misurazione\nSecondo Stevens (1946), esiste una gerarchia dei livelli delle scale di misurazione, denominati “livelli di scala”. Questi livelli sono organizzati in modo gerarchico, in cui la scala nominale rappresenta il livello più basso della misurazione, mentre la scala a rapporti equivalenti rappresenta il livello più alto.\n\nLa scala nominale è il livello più elementare, in cui le categorie o le etichette vengono assegnate agli oggetti o agli individui senza alcuna valutazione di grandezza o ordine.\nAl livello successivo si trova la scala ordinale, in cui le categorie sono ordinate in base a una qualche qualità o caratteristica. Qui, è possibile stabilire un ordine di preferenza o gerarchia tra le categorie, ma non è possibile quantificare la differenza tra di esse in modo preciso.\nLa scala intervallo rappresenta un livello successivo, in cui le categorie sono ordinate e la differenza tra di esse è quantificabile in modo preciso. In questa scala, è possibile effettuare operazioni matematiche come l’addizione e la sottrazione tra i valori, ma non è possibile stabilire un vero e proprio punto zero significativo.\nInfine, la scala a rapporti equivalenti rappresenta il livello più alto. In questa scala, le categorie sono ordinate, la differenza tra di esse è quantificabile in modo preciso e esiste un punto zero assoluto che rappresenta l’assenza totale della grandezza misurata. Questo livello di scala permette di effettuare tutte le operazioni matematiche, compresa la moltiplicazione e la divisione.\n\nPassando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.\n\n\n\nRelazioni tra i livelli di misurazione.\n\n\nPer ciò che riguarda le trasformazioni ammissibili, più il livello di scala è basso, più le funzioni sono generali (sono minori cioè i vincoli per passare da una rappresentazione numerica ad un’altra equivalente). Salendo la gerarchia, la natura delle funzioni di trasformazione si fa più restrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#variabili-discrete-o-continue",
    "href": "chapters/key_notions/02_measurement.html#variabili-discrete-o-continue",
    "title": "4  La misurazione in psicologia",
    "section": "4.5 Variabili discrete o continue",
    "text": "4.5 Variabili discrete o continue\nLe variabili possono essere classificate come variabili a livello di intervalli o di rapporti e possono essere sia discrete che continue.\n\nLe variabili discrete assumono valori specifici ma non possono assumere valori intermedi. Una volta che l’elenco dei valori accettabili è stato definito, non vi sono casi che si trovano tra questi valori. In genere, le variabili discrete assumono valori interi, come il numero di eventi, il numero di persone o il numero di oggetti.\nD’altra parte, le variabili continue possono assumere qualsiasi valore all’interno di un intervallo specificato. Teoricamente, ciò significa che è possibile utilizzare frazioni e decimali per ottenere qualsiasi grado di precisione.\n\n\n\n\nVariabili discrete e continue.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "href": "chapters/key_notions/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "title": "4  La misurazione in psicologia",
    "section": "4.6 Comprendere gli errori nella misurazione",
    "text": "4.6 Comprendere gli errori nella misurazione\nGli errori di misurazione possono essere casuali o sistematici. Gli errori casuali sono fluttuazioni aleatorie, mentre gli errori sistematici sono costanti e derivano da problemi nel metodo di misurazione o negli strumenti.\n\n4.6.1 Precisione e Accuratezza\nLa precisione indica la coerenza tra misurazioni ripetute, mentre l’accuratezza si riferisce alla vicinanza del valore misurato al valore reale. Entrambi i concetti sono cruciali per l’assessment psicometrico.\nUtilizzando l’analogia del tiro al bersaglio, si può avere una serie di colpi vicini tra loro ma lontani dal centro (precisione senza accuratezza) oppure colpi distribuiti in modo sparso ma in media vicini al centro (accuratezza senza precisione).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#assessment-psicometrico",
    "href": "chapters/key_notions/02_measurement.html#assessment-psicometrico",
    "title": "4  La misurazione in psicologia",
    "section": "4.7 Assessment psicometrico",
    "text": "4.7 Assessment psicometrico\nL’assessment psicometrico valuta la qualità delle misurazioni psicologiche, considerando la validità e l’affidabilità.\n\n4.7.1 Validità nella Misurazione Psicologica\nLa validità è una proprietà psicometrica fondamentale dei test psicologici. Secondo gli Standards for Educational and Psychological Testing (2014), la validità si riferisce al grado in cui evidenza e teoria supportano le interpretazioni dei punteggi dei test per gli usi proposti. Questo concetto evidenzia che la validità riguarda sia il significato dei punteggi sia il loro utilizzo, rendendola “la considerazione più fondamentale nello sviluppo e nella valutazione dei test”.\n\n\n4.7.2 Evoluzione del Concetto di Validità\nTradizionalmente, la validità era suddivisa in tre categorie:\n\nValidità di Contenuto: Si riferisce alla corrispondenza tra il contenuto degli item di un test e il dominio dell’attributo psicologico che il test intende misurare. È importante che gli item siano pertinenti e rappresentativi dell’attributo misurato.\nValidità di Criterio: Valuta il grado di concordanza tra i risultati ottenuti tramite lo strumento di misurazione e i risultati ottenuti da altri strumenti che misurano lo stesso costrutto o da un criterio esterno. Include validità concorrente e predittiva.\nValidità di Costrutto: Riguarda il grado in cui un test misura effettivamente il costrutto che si intende misurare. Si suddivide in validità convergente (accordo con strumenti che misurano lo stesso costrutto) e validità divergente (capacità di discriminare tra costrutti diversi).\n\nLa moderna teoria della validità non adotta più questa visione tripartita. Gli Standards del 2014 descrivono la validità come un concetto unitario, dove diverse forme di evidenza concorrono a supportare l’interpretazione dei punteggi del test per il loro utilizzo previsto.\n\n\n4.7.3 Tipologie di Prove di Validità\nGli Standards del 2014 identificano cinque categorie principali di prove di validità:\n\nProve Basate sul Contenuto del Test: Valutano quanto il contenuto del test rappresenti adeguatamente il dominio del costrutto da misurare.\nProve Basate sui Processi di Risposta: Analizzano se i processi cognitivi e comportamentali degli esaminandi riflettono il costrutto valutato.\nProve Basate sulla Struttura Interna: Esaminano la coerenza tra gli elementi del test e la struttura teorica del costrutto. L’analisi fattoriale è uno strumento chiave in questo contesto.\nProve Basate sulle Relazioni con Altre Variabili: Studiano la correlazione tra i punteggi del test e altre variabili teoricamente correlate, utilizzando metodi come la validità convergente e divergente.\nProve Basate sulle Conseguenze del Test: Considerano le implicazioni e gli effetti dell’uso del test, sia intenzionali che non intenzionali.\n\n\n\n4.7.4 Minacce alla Validità\nLa validità può essere compromessa quando un test non misura integralmente il costrutto di interesse (sotto-rappresentazione del costrutto) o quando include varianza estranea al costrutto. Inoltre, fattori esterni come l’ansia o la bassa motivazione degli esaminandi, e deviazioni nelle procedure di amministrazione e valutazione, possono influenzare negativamente la validità delle interpretazioni dei risultati.\n\n\n4.7.5 Integrazione delle Prove di Validità\nLa validità di un test si costruisce attraverso l’integrazione di diverse linee di evidenza. Ogni interpretazione o uso di un test deve essere validato specificamente, richiedendo una valutazione continua e accurata delle prove disponibili. Questo processo implica la costruzione di un argomento di validità che consideri attentamente la qualità tecnica del test e l’adeguatezza delle sue interpretazioni per gli scopi previsti.\nIn conclusione, la validità è un concetto complesso e integrato che richiede un’analisi continua e multidimensionale delle evidenze. La moderna teoria della validità enfatizza l’importanza di considerare diverse forme di evidenza per supportare le interpretazioni dei punteggi dei test, garantendo che siano utilizzati in modo appropriato e significativo. Gli sviluppatori e gli utilizzatori di test devono impegnarsi a valutare costantemente la validità per assicurare misurazioni psicologiche accurate e affidabili.\n\n\n4.7.6 Affidabilità\nL’affidabilità concerne la consistenza e stabilità delle misurazioni, verificata attraverso metodi come l’affidabilità test-retest, inter-rater, intra-rater e l’affidabilità interna.\n\nAffidabilità Test-Retest: Questa forma di affidabilità verifica la consistenza delle misurazioni nel tempo. Se un individuo viene testato in due momenti diversi, i risultati dovrebbero essere simili, assumendo che non ci siano stati cambiamenti significativi nel costrutto misurato.\nAffidabilità Inter-rater: In questo caso, l’affidabilità è determinata dalla concordanza tra le valutazioni di diversi esaminatori. Ad esempio, se più psicologi dovessero valutare un individuo utilizzando lo stesso strumento, le loro valutazioni dovrebbero essere simili.\nAffidabilità Intra-rater: Questa misura dell’affidabilità si riferisce alla consistenza delle valutazioni dello stesso esaminatore in momenti diversi.\nAffidabilità Interna: Si riferisce alla coerenza delle risposte all’interno dello stesso test. Ad esempio, se un test misura un costrutto come l’ansia, gli item che misurano l’ansia dovrebbero correlare positivamente l’uno con l’altro. Un modo comune per valutare l’affidabilità interna è utilizzare il coefficiente \\(\\omega\\) di McDonald.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#commenti-e-considerazioni-finali",
    "href": "chapters/key_notions/02_measurement.html#commenti-e-considerazioni-finali",
    "title": "4  La misurazione in psicologia",
    "section": "4.8 Commenti e considerazioni finali",
    "text": "4.8 Commenti e considerazioni finali\nLa teoria della misurazione è fondamentale nella ricerca empirica per valutare l’attendibilità e la validità delle misurazioni. È cruciale valutare l’errore nella misurazione per garantire la precisione e l’accuratezza delle misure. L’assessment psicometrico si occupa di valutare la qualità delle misurazioni psicologiche, considerando l’affidabilità e la validità per garantire misure accurate dei costrutti teorici. Le moderne tecnologie e metodologie stanno continuamente arricchendo questo campo, offrendo strumenti sempre più raffinati per la comprensione delle caratteristiche psicologiche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/02_measurement.html#bibliografia",
    "href": "chapters/key_notions/02_measurement.html#bibliografia",
    "title": "4  La misurazione in psicologia",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nLilienfeld, S. O., & Strother, A. N. (2020). Psychological measurement and the replication crisis: Four sacred cows. Canadian Psychology/Psychologie Canadienne, 61(4), 281–288.\n\n\nMaul, A., Irribarra, D. T., & Wilson, M. (2016). On the philosophical foundations of psychological measurement. Measurement, 79, 311–320.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677–680.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html",
    "href": "chapters/key_notions/03_data_analysis.html",
    "title": "5  L’analisi dei dati psicologici",
    "section": "",
    "text": "Introduzione\nNel panorama contemporaneo delle scienze sociali e della psicologia, gli ultimi due decenni hanno visto l’emergere di una profonda trasformazione metodologica ed epistemologica. Questo movimento, caratterizzato da concetti chiave quali “Credibility Revolution” (Angrist & Pischke, 2010), “Causal Revolution” (Pearl & Mackenzie, 2018) e “Replication Crisis” (Collaboration, 2015), ha determinato un cambiamento paradigmatico nelle pratiche delle scienze sociali e, in particolare, della psicologia (Korbmacher et al., 2023). Questa transizione verso quella che Munger (2023) definisce “Science versione 2” è stata motivata dalle lacune metodologiche precedenti e ha catalizzato l’adozione di approcci più rigorosi e replicabili.\nLa genesi di questa Riforma è radicata nella constatazione di problematiche metodologiche pervasive, tra cui la proliferazione di falsi positivi (Simmons et al., 2011), l’abuso dei “gradi di libertà dei ricercatori” (Gelman & Loken, 2013), e l’inadeguatezza delle pratiche statistiche tradizionali (Gelman & Loken, 2014). Fenomeni come il p-hacking, l’uso di campioni sottodimensionati (Button et al., 2013), e la mancanza di trasparenza nei metodi di ricerca hanno contribuito a minare la credibilità delle scoperte psicologiche (Ioannidis, 2005; Meehl, 1967), portando alla cosiddetta “Replication Crisis” (Baker, 2016; Bishop, 2019) – si veda il ?sec-crisis.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano",
    "href": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.1 L’Approccio Bayesiano",
    "text": "5.1 L’Approccio Bayesiano\nIn risposta a queste sfide, l’approccio bayesiano è emerso come un paradigma statistico fondamentale nella “Credibility Revolution”. Contrariamente all’inferenza frequentista basata sul Test dell’Ipotesi Nulla, la statistica bayesiana offre un framework più flessibile e intuitivo per l’analisi dei dati e l’inferenza causale. Il principio cardine dell’approccio bayesiano, l’aggiornamento delle distribuzioni di probabilità a priori (priors) alla luce di nuove evidenze, si allinea perfettamente con l’obiettivo di una scienza cumulativa e auto-correttiva.\nL’adozione di metodi bayesiani in psicologia comporta diversi vantaggi significativi:\n\nQuantificazione dell’incertezza: L’inferenza bayesiana fornisce distribuzioni di probabilità posteriori complete per i parametri di interesse, offrendo una rappresentazione più ricca e sfumata dell’incertezza rispetto agli intervalli di confidenza frequentisti.\nIncorporazione di conoscenze pregresse: Le priors bayesiane consentono l’integrazione formale di conoscenze precedenti nel processo inferenziale, promuovendo un approccio cumulativo alla ricerca.\nRobustezza alle pratiche di ricerca discutibili: I metodi bayesiani sono meno suscettibili a pratiche come il p-hacking, poiché l’inferenza si basa sull’intera distribuzione posteriore piuttosto che su soglie arbitrarie di significatività.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano-nella-ricerca",
    "href": "chapters/key_notions/03_data_analysis.html#lapproccio-bayesiano-nella-ricerca",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.2 L’approccio bayesiano nella ricerca",
    "text": "5.2 L’approccio bayesiano nella ricerca\nL’impiego delle statistiche bayesiane nella ricerca psicologica presenta notevoli vantaggi rispetto ad altri metodi statistici tradizionali, come il test di significatività dell’ipotesi nulla. Un punto di forza importante risiede nella sua indipendenza dalla teoria dei grandi campioni, rendendolo particolarmente adatto per gli studi psicologici che spesso si basano su campioni di dimensioni ridotte (Larson et al., 2023).\nLa ricerca psicologica è frequentemente caratterizzata da campioni limitati, dovuti a diversi fattori quali la bassa prevalenza di determinate condizioni, le difficoltà nel reclutamento dei partecipanti e le complessità nelle procedure di valutazione. Questi campioni di piccole dimensioni sono intrinsecamente soggetti a una maggiore eterogeneità, che si manifesta nella variabilità del fenotipo comportamentale delle condizioni psicologiche esaminate e nella discrepanza tra le stime degli effetti in diversi studi. Tale eterogeneità può condurre a stime degli effetti distorte e scarsamente riproducibili.\nL’approccio bayesiano offre una soluzione efficace a queste problematiche. In primo luogo, consente di valutare l’adeguatezza della dimensione del campione attraverso un’analisi della sensibilità dei risultati rispetto alla specificazione delle distribuzioni a priori. In secondo luogo, permette di ottenere risultati precisi anche con campioni ridotti, a condizione che le conoscenze a priori siano accurate e ben definite.\nUn ulteriore vantaggio dell’approccio bayesiano è la sua capacità di ottimizzare l’uso dei campioni di partecipanti, favorendo un’inclusione equa delle popolazioni diversificate. Questo è particolarmente rilevante per gruppi spesso sottorappresentati, come le minoranze etniche. Le statistiche bayesiane aiutano a superare questa sfida evitando di esercitare una pressione eccessiva su questi gruppi per aumentarne la partecipazione, permettendo così una ricerca più equa e rappresentativa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#modellazione-formale",
    "href": "chapters/key_notions/03_data_analysis.html#modellazione-formale",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.3 Modellazione Formale",
    "text": "5.3 Modellazione Formale\nLa “Credibility Revolution” ha catalizzato l’integrazione della Data Science nelle pratiche di ricerca psicologica. L’adozione di pipeline di analisi dei dati riproducibili, l’uso di controllo di versione, e la condivisione di dati e codice sono diventati standard de facto nella comunità scientifica. Questi strumenti non solo migliorano la trasparenza e la replicabilità della ricerca, ma facilitano anche la collaborazione e l’accumulo di conoscenze nel campo.\nParallelamente, si è osservato un rinnovato interesse per la modellazione formale in psicologia, che consente non solo la verifica ma anche lo sviluppo di modelli dei meccanismi sottostanti ai fenomeni psicologici (Oberauer & Lewandowsky, 2019; Van Dongen et al., 2024). Questo approccio supera la mera descrizione delle associazioni tra variabili, che era tipica della pratica dominante dell’ANOVA nel contesto pre-riforma.\nLa modellazione bayesiana si presta particolarmente bene a questo approccio, offrendo un framework unificato per la specificazione di modelli formali, l’incorporazione di incertezza parametrica, e la valutazione dell’evidenza empirica. Attraverso tecniche come il confronto tra modelli bayesiano e l’analisi di sensibilità, i ricercatori possono valutare rigorosamente la plausibilità relativa di diverse teorie psicologiche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#riflessioni-epistemologiche",
    "href": "chapters/key_notions/03_data_analysis.html#riflessioni-epistemologiche",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.4 Riflessioni Epistemologiche",
    "text": "5.4 Riflessioni Epistemologiche\nL’adozione di metodi bayesiani e della Data Science in psicologia deve essere accompagnata da una profonda riflessione epistemologica. Come sottolineato da George Box\n\ntutti i modelli sono sbagliati, ma alcuni sono utili.\n\nQuesta massima risuona particolarmente nel contesto della ricerca psicologica, dove i fenomeni di interesse sono spesso complessi e multifattoriali.\nL’approccio bayesiano, con la sua enfasi sull’aggiornamento iterativo delle credenze alla luce di nuove evidenze, si allinea naturalmente con una visione della scienza come processo di apprendimento continuo piuttosto che come ricerca di verità assolute. Questa prospettiva riconosce i limiti intrinseci dei nostri modelli e delle nostre teorie, pur valorizzandone l’utilità euristica e predittiva (si veda la discussione nella ?sec-poetic-validity).\nIn particolare, McElreath (2020) sottolinea l’importanza di riconoscere la dualità tra il “mondo del modello” e il mondo reale più ampio che cerchiamo di comprendere. Questa consapevolezza è cruciale per evitare la reificazione dei nostri modelli statistici e per mantenere una prospettiva critica sulle nostre inferenze.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#conclusione",
    "href": "chapters/key_notions/03_data_analysis.html#conclusione",
    "title": "5  L’analisi dei dati psicologici",
    "section": "5.5 Conclusione",
    "text": "5.5 Conclusione\nL’integrazione dell’approccio bayesiano e della data science nella ricerca psicologica rappresenta una risposta promettente alle sfide poste dalla “Replication Crisis”. Offrendo un framework coerente per la modellazione formale, l’inferenza statistica e l’incorporazione di conoscenze pregresse, questi approcci promettono di elevare il rigore e la credibilità della ricerca psicologica. Tuttavia, è fondamentale che l’adozione di questi metodi sia accompagnata da una adeguata consapevolezza metodologica ed epistemologica – si veda, ad esempio, il ?sec-causal-inference-regr.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/key_notions/03_data_analysis.html#bibliografia",
    "href": "chapters/key_notions/03_data_analysis.html#bibliografia",
    "title": "5  L’analisi dei dati psicologici",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of economic perspectives, 24(2), 3–30.\n\n\nBaker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604).\n\n\nBishop, D. (2019). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research.\n\n\nButton, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376.\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no «fishing expedition» or «p-hacking» and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 348(1-17), 3.\n\n\nGelman, A., & Loken, E. (2014). The statistical crisis in science. American scientist, 102(6), 460–465.\n\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124.\n\n\nKorbmacher, M., Azevedo, F., Pennington, C. R., Hartmann, H., Pownall, M., Schmidt, K., Elsherif, M., Breznau, N., Robertson, O., Kalandadze, T., et al. (2023). The replication crisis has led to positive structural, procedural, and community changes. Communications Psychology, 1(1), 3.\n\n\nLabatut, B. (2021). Quando abbiamo smesso di capire il mondo. Adelphi Edizioni spa.\n\n\nLarson, C., Kaplan, D., Girolamo, T., Kover, S. T., & Eigsti, I.-M. (2023). A Bayesian statistics tutorial for clinical research: Prior distributions and meaningful results for small clinical samples. Journal of Clinical Psychology, 79(11), 2602–2624.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of science, 34(2), 103–115.\n\n\nMunger, K. (2023). Temporal validity as meta-science. Research & Politics, 10(3), 20531680231187271.\n\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26, 1596–1618.\n\n\nPearl, J., & Mackenzie, D. (2018). The book of why: the new science of cause and effect. Basic books.\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science, 22(11), 1359–1366.\n\n\nVan Dongen, N., Bork, R. van, Finnemann, A., Haslbeck, J., Maas, H. L. van der, Robinaugh, D. J., Ron, J. de, Sprenger, J., & Borsboom, D. (2024). Productive explanation: A framework for evaluating explanations in psychological science. Psychological Review.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L'analisi dei dati psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html",
    "href": "chapters/eda/01_project_structure.html",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "",
    "text": "6.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nSeguendo Yu & Barter (2024), in questo capitolo introdurremo l’analisi esplorativa dei dati situandola all’interno dell’intero ciclo di vita di un progetto di data science (DSLC). Secondo Yu & Barter (2024), ogni progetto di analisi dei dati segue una combinazione delle seguenti fasi:\nMentre quasi tutti i progetti di data science attraversano le fasi 1-2 e 4-5, non tutti includono la fase 3.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#introduzione",
    "href": "chapters/eda/01_project_structure.html#introduzione",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "",
    "text": "Formulazione del problema e raccolta dei dati.\nPulizia dei dati, preprocessing e analisi esplorativa.\nAnalisi predittiva e/o inferenziale.\nValutazione dei risultati.\nComunicazione dei risultati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-1-formulazione-del-problema-e-raccolta-dei-dati",
    "href": "chapters/eda/01_project_structure.html#fase-1-formulazione-del-problema-e-raccolta-dei-dati",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.2 Fase 1: Formulazione del Problema e Raccolta dei Dati",
    "text": "6.2 Fase 1: Formulazione del Problema e Raccolta dei Dati\nLa formulazione di una domanda di ricerca precisa è il punto di partenza di ogni progetto di data science. È cruciale che la domanda sia formulata in modo tale da poter essere risolta attraverso l’analisi dei dati disponibili. Alle volte la domanda iniziale è troppo vaga o non risolvibile. L’obiettivo è riformulare la domanda in modo tale che possa trovare una risposta utilizzando i dati a disposizione.\n\n6.2.1 Raccolta dei Dati\nAlcuni progetti utilizzano dati esistenti (da repository pubblici, database interni o esperimenti passati), mentre altri richiedono la raccolta di nuovi dati. Ogni volta che è possibile, è necessario avere ben chiaro quali analisi statistiche verranno svolte prima di raccogliere i dati. Se questo non viene fatto, può succedere che i dati raccolti non siano adeguati per rispondere alle domande di interesse, in quanto mancano informazioni cruciali, o vengono violate assunzioni richieste dai modelli statistici che si vogliono impiegare.\nÈ fondamentale sviluppare una comprensione approfondita dei processi di acquisizione dei dati e del significato delle misure ottenute. Parallelamente, è cruciale essere pienamente consapevoli degli strumenti e delle metodologie impiegate nella raccolta dei dati. In altri termini, è essenziale riconoscere e valutare i potenziali bias che possono emergere dalle tecniche e dalle procedure adottate durante il processo di raccolta dati.\n\n6.2.2 Terminologia dei Dati\nIn una matrice di dati (comunemente denominata “dataset”), ogni colonna rappresenta una diversa tipologia di misurazione, definita come variabile, carattere o attributo. In alcuni contesti, specialmente nell’analisi di regressione, queste possono essere anche chiamate covariate.\nGeneralmente, le variabili in un dataset si classificano in una delle seguenti categorie:\n\n\nQuantitative:\n\nContinue: Valori che possono assumere qualsiasi numero reale all’interno di un intervallo (es. importo di spesa, durata di permanenza su un sito web).\nDiscrete: Valori numerici interi, spesso risultato di conteggi (es. numero di visitatori di un sito web in un determinato periodo, numero di esemplari di una specie in una data località).\n\n\n\nQualitative (o Categoriche):\n\nNominali: Categorie senza un ordine intrinseco (es. partito politico, reparto ospedaliero, nazione).\nOrdinali: Categorie con un ordine naturale ma senza una metrica definita tra i livelli (es. livello di istruzione, grado di soddisfazione).\n\n\nTemporali: Date e orari in vari formati (es. “01/01/2020 23:00:05” o “1 gen 2020”).\n\nTestuali:\n\nStrutturate: Testo con formato predefinito (es. nominativo, indirizzo postale, email).\nNon strutturate: Corpo di testo esteso senza struttura predefinita (es. cartelle cliniche, recensioni, post sui social media).\n\n\n\nLa dimensionalità dei dati si riferisce al numero di variabili (colonne) presenti nel dataset. Si parla di “dati ad alta dimensionalità” quando il numero di variabili è elevato (generalmente superiore a 100, sebbene non esista una soglia universalmente accettata).\nOgni riga del dataset corrisponde a una singola unità statistica, anche detta caso o osservazione. Queste rappresentano le entità su cui vengono effettuate le misurazioni.\nQuesta struttura, in cui i dati sono organizzati in colonne (variabili) e righe (unità statistiche), viene definita come matrice dei dati o, in ambito informatico, come formato tabellare.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-2-pulizia-dei-dati-e-analisi-esplorativa",
    "href": "chapters/eda/01_project_structure.html#fase-2-pulizia-dei-dati-e-analisi-esplorativa",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.3 Fase 2: Pulizia dei Dati e Analisi Esplorativa",
    "text": "6.3 Fase 2: Pulizia dei Dati e Analisi Esplorativa\n\n6.3.1 Pulizia dei Dati\nDopo aver definito la domanda della ricerca e avere raccolto i dati rilevanti, è il momento di pulire i dati. Un dataset pulito è ordinato, formattato in modo appropriato e ha voci non ambigue. La fase iniziale di pulizia dei dati consiste nell’identificare problemi con i dati (come formattazioni anomale e valori non validi) e modificarli in modo che i valori siano validi e formattati in modo comprensibile sia per il computer che per noi. La pulizia dei dati è una fase estremamente importante di un progetto di data science perché non solo aiuta a garantire che i dati siano interpretati correttamente dal computer, ma aiuta anche a sviluppare una comprensione dettagliata delle informazioni contenute nei dati e delle loro limitazioni.\nL’obiettivo della pulizia dei dati è creare una versione dei dati che rifletta nella maniera più fedele possibile la realtà e che sia interpretata correttamente dal computer. Per garantire che il computer utilizzi fedelmente le informazioni contenute nei dati, è necessario modificare i dati (scrivendo codice, non modificando il file dati grezzo stesso) in modo che siano in linea con ciò che il computer “si aspetta”. Tuttavia, il processo di pulizia dei dati è necessariamente soggettivo e comporta fare assunzioni sulle quantità reali sottostanti misurate e decisioni su quali modifiche siano le più sensate.\n\n6.3.2 Preprocessing\nIl preprocessing si riferisce al processo di modifica dei dati puliti per soddisfare i requisiti di un algoritmo specifico che si desidera applicare. Ad esempio, se si utilizza un algoritmo che richiede che le variabili siano sulla stessa scala, potrebbe essere necessario trasformarle, oppure, se si utilizza un algoritmo che non consente valori mancanti, potrebbe essere necessario imputarli o rimuoverli. Durante il preprocessing, potrebbe essere utile anche definire nuove caratteristiche/variabili utilizzando le informazioni esistenti nei dati, se si ritiene che queste possano essere utili per l’analisi.\nCome per la pulizia dei dati, non esiste un unico modo corretto per pre-elaborare un dataset, e la procedura finale comporta tipicamente una serie di decisioni che dovrebbero essere documentate nel codice e nei file di documentazione.\n\n6.3.3 Analisi Esplorativa dei Dati\nDopo l’acquisizione dei dati, si procede con un’analisi approfondita che si articola in due fasi principali:\n\n\nAnalisi Esplorativa dei Dati (EDA - Exploratory Data Analysis):\nQuesta fase iniziale mira a far familiarizzare il ricercatore con il dataset e a scoprire pattern nascosti. Si realizza attraverso:\n\nLa costruzione di tabelle di frequenza e contingenza\nIl calcolo di statistiche descrittive (come indici di posizione, dispersione e forma della distribuzione)\nLa creazione di rappresentazioni grafiche preliminari\n\nL’EDA permette di generare ipotesi sui dati e di guidare le successive analisi statistiche.\n\n\nAnalisi Esplicativa:\nIn questa fase successiva, l’obiettivo è raffinare e perfezionare le analisi per comunicare efficacemente i risultati a un pubblico più ampio. Ciò comporta:\n\nL’ottimizzazione delle tabelle per una maggiore leggibilità\nIl perfezionamento delle visualizzazioni grafiche per una comunicazione chiara ed efficace\nLa selezione delle statistiche più rilevanti per supportare le conclusioni\n\nL’analisi esplicativa si concentra sulla presentazione chiara e convincente dei risultati, adattando il livello di dettaglio e il linguaggio al pubblico di riferimento.\n\n\nEntrambe le fasi sono cruciali: l’EDA consente di comprendere a fondo la struttura e le caratteristiche dei dati, mentre l’analisi esplicativa assicura che le scoperte siano comunicate in modo efficace e comprensibile.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-3-analisi-predittiva-e-inferenziale",
    "href": "chapters/eda/01_project_structure.html#fase-3-analisi-predittiva-e-inferenziale",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.4 Fase 3: Analisi Predittiva e Inferenziale",
    "text": "6.4 Fase 3: Analisi Predittiva e Inferenziale\nMolte domande nella data science si presentano come problemi di inferenza e/o previsione, in cui l’obiettivo principale è utilizzare dati osservati, passati o presenti, per descrivere le caratteristiche di una popolazione più ampia o per fare previsioni su dati futuri non ancora disponibili. Questo tipo di analisi è spesso orientato a supportare decisioni nel mondo reale.\nNel corso, ci concentreremo principalmente sull’approccio bayesiano per affrontare questi problemi inferenziali, fornendo un’introduzione a come tale prospettiva possa essere applicata efficacemente in questo contesto.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-4-valutazione-dei-risultati",
    "href": "chapters/eda/01_project_structure.html#fase-4-valutazione-dei-risultati",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.5 Fase 4: Valutazione dei Risultati",
    "text": "6.5 Fase 4: Valutazione dei Risultati\nIn questa fase, i risultati ottenuti vengono analizzati alla luce della domanda di ricerca iniziale. Si procede a una valutazione sia quantitativa, attraverso l’applicazione di tecniche statistiche appropriate, sia qualitativa, attraverso un’attenta riflessione critica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#fase-5-comunicazione-dei-risultati",
    "href": "chapters/eda/01_project_structure.html#fase-5-comunicazione-dei-risultati",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.6 Fase 5: Comunicazione dei Risultati",
    "text": "6.6 Fase 5: Comunicazione dei Risultati\nL’ultima fase di un progetto di analisi dei dati consiste nel condividere i risultati con un pubblico più ampio, il che richiede la preparazione di materiali comunicativi chiari e concisi. L’obiettivo è trasformare i risultati dell’analisi in informazioni utili per supportare il processo decisionale. Questo può includere la stesura di un articolo scientifico, la creazione di un report per un team di lavoro, o la preparazione di una presentazione con diapositive.\nLa comunicazione deve essere adattata al pubblico di riferimento. Non si deve dare per scontato che il pubblico abbia familiarità con il progetto: è fondamentale spiegare l’analisi e le visualizzazioni in modo chiaro e dettagliato. Anche se per il ricercatore il messaggio principale di una figura o diapositiva può sembrare ovvio, è sempre una buona pratica guidare il pubblico nella sua interpretazione, evitando l’uso di gergo tecnico complesso.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "href": "chapters/eda/01_project_structure.html#organizzazione-del-progetto",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.7 Organizzazione del Progetto",
    "text": "6.7 Organizzazione del Progetto\nUn requisito fondamentale per un progetto di analisi dei dati è organizzare in modo efficiente i file sul proprio computer. Questo include i file dei dati, il codice e la documentazione del progetto. Tutti questi elementi dovrebbero essere raccolti all’interno di una singola cartella dedicata al progetto. Yu & Barter (2024) propone il seguente template per la struttura di un progetto:\n\nLe due cartelle principali sono:\n\n\ndata/: contiene il dataset grezzo (ad esempio, data.csv) e una sottocartella con documentazione relativa ai dati, come metadati e codebook.\n\ndslc_documentation/: raccoglie i file di documentazione e codice necessari per le varie fasi del progetto. Questi possono essere file .qmd (per Quarto, in R) o .ipynb (per Jupyter Notebook, in Python), utilizzati per condurre ed esplorare le analisi. I file sono prefissati da un numero per mantenerli in ordine cronologico. All’interno di questa cartella, è presente una sottocartella functions/, che contiene script .R (per R) o .py (per Python) con funzioni utili per le diverse analisi.\n\nUn file README.md descrive la struttura del progetto e riassume il contenuto di ogni file.\nUn’organizzazione come quella proposta da Yu & Barter (2024) offre un notevole vantaggio: permette di specificare i percorsi dei file in modo relativo, utilizzando come radice la cartella del progetto. Questo rende il progetto facilmente trasferibile e condivisibile tra diversi utenti o computer.\n\nEsempio 6.1 Per esplorare come gestire l’archiviazione dei dati sul computer e importarli in Python, consideriamo i dati raccolti da Zetsche et al. (2019) in uno studio che ha esaminato le aspettative negative come meccanismo chiave nel mantenimento della depressione. In questo studio, i ricercatori hanno confrontato 30 soggetti con episodi depressivi a un gruppo di controllo di 37 individui sani, utilizzando il Beck Depression Inventory (BDI-II) per valutare i livelli di depressione.\nIl file CSV contenente questi dati, così come tutti gli altri file utilizzati in questa dispensa, è memorizzato nella cartella data, situata all’interno della cartella psicometria, che rappresenta la directory principale del progetto.\nCon le istruzioni seguenti, viene specificato il percorso della directory principale del progetto in relazione alla mia directory personale:\n\nhere::here()\n#&gt; [1] \"/Users/corradocaudek/_repositories/psicometria-r\"\n\nDopo aver definito project_directory come directory principale, è possibile indicare il percorso del file CSV contenente i dati in modo relativo a project_directory.\nLa seguente istruzione permette di importare i dati dal file data.mood.csv in un DataFrame di pandas.\n\ndf &lt;- rio::import(\n  here::here(\"data\", \"data.mood.csv\")\n)\n\nPer conoscere le dimensioni del DataFrame utilizziamo l’istruzione dim():\n\ndim(df)\n#&gt; [1] 1188   44\n\nIl DataFrame ha 1188 righe e 44 colonne. Visualizziamo il nome delle colonne con il metodo .columns.\n\ndf |&gt; names()\n#&gt;  [1] \"V1\"                  \"vpn_nr\"              \"esm_id\"             \n#&gt;  [4] \"group\"               \"bildung\"             \"bdi\"                \n#&gt;  [7] \"nr_of_episodes\"      \"nobs_mood\"           \"trigger_counter\"    \n#&gt; [10] \"form\"                \"traurig_re\"          \"niedergeschlagen_re\"\n#&gt; [13] \"unsicher_re\"         \"nervos_re\"           \"glucklich_re\"       \n#&gt; [16] \"frohlich_re\"         \"mood_sad.5\"          \"mood_fearful.5\"     \n#&gt; [19] \"mood_neg.5\"          \"mood_happy.5\"        \"cesd_sum\"           \n#&gt; [22] \"rrs_sum\"             \"rrs_brood\"           \"rrs_reflect\"        \n#&gt; [25] \"forecast_sad\"        \"forecast_fear\"       \"forecast_neg\"       \n#&gt; [28] \"forecast_happy\"      \"recall_sad\"          \"recall_fear\"        \n#&gt; [31] \"recall_neg\"          \"recall_happy\"        \"diff_neg.fore.5\"    \n#&gt; [34] \"diff_sad.fore.5\"     \"diff_fear.fore.5\"    \"diff_happy.fore.5\"  \n#&gt; [37] \"diff_neg.retro.5\"    \"diff_sad.retro.5\"    \"diff_fear.retro.5\"  \n#&gt; [40] \"diff_happy.retro.5\"  \"mood_sad5_tm1\"       \"mood_neg5_tm1\"      \n#&gt; [43] \"mood_fearful5_tm1\"   \"mood_happy5_tm1\"\n\nDato che il DataFrame è troppo grande (1188 righe e 44 colonne), stampiamo sullo schermo solo le prime 5 righe.\n\ndf |&gt; head()\n#&gt;   V1 vpn_nr esm_id group bildung bdi nr_of_episodes nobs_mood\n#&gt; 1  1    101     10   mdd  abitur  25              2        14\n#&gt; 2  2    101     10   mdd  abitur  25              2        14\n#&gt; 3  3    101     10   mdd  abitur  25              2        14\n#&gt; 4  4    101     10   mdd  abitur  25              2        14\n#&gt; 5  5    101     10   mdd  abitur  25              2        14\n#&gt; 6  6    101     10   mdd  abitur  25              2        14\n#&gt;   trigger_counter        form traurig_re niedergeschlagen_re unsicher_re\n#&gt; 1               5 Forecasting       3.67                3.00        2.33\n#&gt; 2               6 Forecasting       3.67                3.67        3.67\n#&gt; 3               7 Forecasting       1.67                1.67        2.33\n#&gt; 4               8 Forecasting       3.00                3.00        3.00\n#&gt; 5              10 Forecasting       3.00                3.00        2.33\n#&gt; 6              11 Forecasting       2.33                2.33        2.33\n#&gt;   nervos_re glucklich_re frohlich_re mood_sad.5 mood_fearful.5 mood_neg.5\n#&gt; 1      3.00         3.00        3.00       3.33           2.67       3.00\n#&gt; 2      3.67         2.33        2.33       3.67           3.67       3.67\n#&gt; 3      2.33         2.33        3.00       1.67           2.33       2.00\n#&gt; 4      3.67         2.33        3.00       3.00           3.33       3.17\n#&gt; 5      3.00         3.00        3.00       3.00           2.67       2.83\n#&gt; 6      2.33         2.33        3.00       2.33           2.33       2.33\n#&gt;   mood_happy.5 cesd_sum rrs_sum rrs_brood rrs_reflect forecast_sad\n#&gt; 1         3.00       25      59        14          11            2\n#&gt; 2         2.33       25      59        14          11            2\n#&gt; 3         2.67       25      59        14          11            2\n#&gt; 4         2.67       25      59        14          11            2\n#&gt; 5         3.00       25      59        14          11            2\n#&gt; 6         2.67       25      59        14          11            2\n#&gt;   forecast_fear forecast_neg forecast_happy recall_sad recall_fear\n#&gt; 1             3          2.5              2        3.5           3\n#&gt; 2             3          2.5              2        3.5           3\n#&gt; 3             3          2.5              2        3.5           3\n#&gt; 4             3          2.5              2        3.5           3\n#&gt; 5             3          2.5              2        3.5           3\n#&gt; 6             3          2.5              2        3.5           3\n#&gt;   recall_neg recall_happy diff_neg.fore.5 diff_sad.fore.5 diff_fear.fore.5\n#&gt; 1       3.25            2          -0.500          -1.333            0.333\n#&gt; 2       3.25            2          -1.167          -1.667           -0.667\n#&gt; 3       3.25            2           0.500           0.333            0.667\n#&gt; 4       3.25            2          -0.667          -1.000           -0.333\n#&gt; 5       3.25            2          -0.333          -1.000            0.333\n#&gt; 6       3.25            2           0.167          -0.333            0.667\n#&gt;   diff_happy.fore.5 diff_neg.retro.5 diff_sad.retro.5 diff_fear.retro.5\n#&gt; 1            -1.000           0.2500            0.167             0.333\n#&gt; 2            -0.333          -0.4167           -0.167            -0.667\n#&gt; 3            -0.667           1.2500            1.833             0.667\n#&gt; 4            -0.667           0.0833            0.500            -0.333\n#&gt; 5            -1.000           0.4167            0.500             0.333\n#&gt; 6            -0.667           0.9167            1.167             0.667\n#&gt;   diff_happy.retro.5 mood_sad5_tm1 mood_neg5_tm1 mood_fearful5_tm1\n#&gt; 1             -1.000            NA            NA                NA\n#&gt; 2             -0.333          3.33          3.00              2.67\n#&gt; 3             -0.667          3.67          3.67              3.67\n#&gt; 4             -0.667          1.67          2.00              2.33\n#&gt; 5             -1.000          3.00          3.17              3.33\n#&gt; 6             -0.667          3.00          2.83              2.67\n#&gt;   mood_happy5_tm1\n#&gt; 1              NA\n#&gt; 2            3.00\n#&gt; 3            2.33\n#&gt; 4            2.67\n#&gt; 5            2.67\n#&gt; 6            3.00",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "href": "chapters/eda/01_project_structure.html#riflessioni-conclusive",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.8 Riflessioni Conclusive",
    "text": "6.8 Riflessioni Conclusive\nLa bellezza del codice risiede nella sua riusabilità: una volta scritto, può essere utilizzato tutte le volte che si desidera. Se configurato correttamente, lo stesso codice applicato agli stessi dati produrrà sempre gli stessi risultati. Questo principio, noto come riproducibilità computazionale, offre numerosi vantaggi.\n\n\nTracciare le modifiche al progetto: La riproducibilità semplifica il monitoraggio delle evoluzioni e dei cambiamenti nel progetto, permettendo di vedere come si sviluppa nel tempo.\n\nRiprodurre il proprio lavoro: L’utente più interessato alla riproducibilità sei tu stesso. La capacità di replicare i risultati è una caratteristica essenziale, poiché in futuro potresti aver bisogno di riprendere in mano il lavoro e comprenderne i dettagli. La riproducibilità rende questo processo molto più semplice.\n\nCostruire su basi solide: Anche altri ricercatori possono utilizzare il tuo lavoro come punto di partenza, espandendo e approfondendo le conoscenze che hai contribuito a sviluppare.\n\nTuttavia, rendere il codice riproducibile è più difficile di quanto sembri. In questo capitolo abbiamo esplorato alcuni metodi che possono aiutare a raggiungere questo obiettivo.\n\n\n\n\n\n\nUno dei problemi più importanti nella psicologia contemporanea è la crisi di replicabilità: molti risultati di ricerca non sono replicabili (Collaboration, 2015). La riproducibilità computazionale si concentra su un obiettivo più ristretto: ottenere gli stessi risultati utilizzando lo stesso codice sugli stessi dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/01_project_structure.html#informazioni-sullambiente-di-sviluppo",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "\n6.9 Informazioni sull’Ambiente di Sviluppo",
    "text": "6.9 Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats4    stats     graphics  grDevices utils     datasets  methods  \n#&gt; [8] base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] mokken_3.1.2         poLCA_1.6.0.1        scatterplot3d_0.3-44\n#&gt;  [4] mirt_1.43            lattice_0.22-6       MASS_7.3-61         \n#&gt;  [7] viridis_0.6.5        viridisLite_0.4.2    ggpubr_0.6.0        \n#&gt; [10] ggExtra_0.10.1       gridExtra_2.3        patchwork_1.3.0     \n#&gt; [13] bayesplot_1.11.1     psych_2.4.6.26       scales_1.3.0        \n#&gt; [16] markdown_1.13        knitr_1.49           lubridate_1.9.3     \n#&gt; [19] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4         \n#&gt; [22] purrr_1.0.2          readr_2.1.5          tidyr_1.3.1         \n#&gt; [25] tibble_3.2.1         ggplot2_3.5.1        tidyverse_2.0.0     \n#&gt; [28] rio_1.2.3            here_1.0.1          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1         pbapply_1.7-2        testthat_3.2.1.1    \n#&gt;  [4] permute_0.9-7        airports_0.1.0       rlang_1.1.4         \n#&gt;  [7] magrittr_2.0.3       compiler_4.4.2       mgcv_1.9-1          \n#&gt; [10] vctrs_0.6.5          pkgconfig_2.0.3      fastmap_1.2.0       \n#&gt; [13] backports_1.5.0      utf8_1.2.4           promises_1.3.1      \n#&gt; [16] rmarkdown_2.29       sessioninfo_1.2.2    tzdb_0.4.0          \n#&gt; [19] openintro_2.5.0      xfun_0.49            jsonlite_1.8.9      \n#&gt; [22] later_1.4.0          styler_1.10.3        Deriv_4.1.6         \n#&gt; [25] broom_1.0.7          parallel_4.4.2       cluster_2.1.6       \n#&gt; [28] R6_2.5.1             stringi_1.8.4        parallelly_1.39.0   \n#&gt; [31] car_3.1-3            brio_1.1.5           Rcpp_1.0.13-1       \n#&gt; [34] future.apply_1.11.3  snow_0.4-4           audio_0.1-11        \n#&gt; [37] cherryblossom_0.1.0  pacman_0.5.1         R.utils_2.12.3      \n#&gt; [40] R.cache_0.16.0       httpuv_1.6.15        Matrix_1.7-1        \n#&gt; [43] splines_4.4.2        timechange_0.3.0     tidyselect_1.2.1    \n#&gt; [46] abind_1.4-8          yaml_2.3.10          vegan_2.6-8         \n#&gt; [49] codetools_0.2-20     miniUI_0.1.1.1       dcurver_0.9.2       \n#&gt; [52] curl_6.0.1           listenv_0.9.1        shiny_1.9.1         \n#&gt; [55] withr_3.0.2          evaluate_1.0.1       future_1.34.0       \n#&gt; [58] pillar_1.9.0         carData_3.0-5        generics_0.1.3      \n#&gt; [61] rprojroot_2.0.4      hms_1.1.3            munsell_0.5.1       \n#&gt; [64] globals_0.16.3       xtable_1.8-4         glue_1.8.0          \n#&gt; [67] RPushbullet_0.3.4    tools_4.4.2          data.table_1.16.2   \n#&gt; [70] beepr_2.0            SimDesign_2.17.1     ggsignif_0.6.4      \n#&gt; [73] grid_4.4.2           colorspace_2.1-1     nlme_3.1-166        \n#&gt; [76] Formula_1.2-5        usdata_0.3.1         cli_3.6.3           \n#&gt; [79] fansi_1.0.6          gtable_0.3.6         R.methodsS3_1.8.2   \n#&gt; [82] rstatix_0.7.2        digest_0.6.37        progressr_0.15.1    \n#&gt; [85] GPArotation_2024.3-1 htmlwidgets_1.6.4    farver_2.1.2        \n#&gt; [88] htmltools_0.5.8.1    R.oo_1.27.0          lifecycle_1.0.4     \n#&gt; [91] mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/01_project_structure.html#bibliografia",
    "href": "chapters/eda/01_project_structure.html#bibliografia",
    "title": "6  Le fasi del progetto di analisi dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nCollaboration, O. S. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.\n\n\nZetsche, U., Buerkner, P.-C., & Renneberg, B. (2019). Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7), 678.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le fasi del progetto di analisi dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html",
    "href": "chapters/eda/02_data_cleaning.html",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "7.1 Introduzione\nPrerequisiti\nConcetti e competenze chiave\nPreparazione del Notebook\nNonostante la fase più interessante di un progetto di analisi dei dati sia quella in cui si riesce a rispondere alla domanda che ha dato avvio all’indagine, gran parte del tempo di un analista è in realtà dedicata a una fase preliminare: la pulizia e il preprocessing dei dati, operazioni che vengono svolte ancor prima dell’analisi esplorativa.\nIn questo capitolo, esamineremo un caso concreto di data cleaning e preprocessing, seguendo il tutorial di Crystal Lewis. Il problema viene presentato come segue:\nCrystal Lewis elenca i seguenti passaggi da seguire nel processo di data cleaning:\nSebbene l’ordine di questi passaggi sia flessibile e possa essere adattato alle esigenze specifiche, c’è un passaggio che non dovrebbe mai essere saltato: il primo, ovvero la revisione dei dati. Senza una revisione preliminare, l’analista rischia di sprecare ore a pulire i dati per poi scoprire che mancano dei partecipanti, che i dati non sono organizzati come previsto o, peggio ancora, che sta lavorando con i dati sbagliati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#introduzione",
    "href": "chapters/eda/02_data_cleaning.html#introduzione",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "",
    "text": "I am managing data for a longitudinal randomized controlled trial (RCT) study. For this RCT, schools are randomized to either a treatment or control group. Students who are in a treatment school receive a program to boost their math self-efficacy. Data is collected on all students in two waves (wave 1 is in the fall of a school year, and wave 2 is collected in the spring). At this point in time, we have collected wave 1 of our student survey on a paper form and we set up a data entry database for staff to enter the information into. Data has been double-entered, checked for entry errors, and has been exported in a csv format (“w1_mathproj_stu_svy_raw.csv”) to a folder (called “data”) where it is waiting to be cleaned.\n\n\n\nRevisione dei dati.\nRegolazione del numero di casi.\nDe-identificazione dei dati.\nEliminazione delle colonne irrilevanti.\nDivisione delle colonne, se necessario.\nRidenominazione delle variabili.\nTrasformazione/normalizzazione delle variabili.\nStandardizzazione delle variabili.\nAggiornamento dei tipi di variabili, se necessario.\nRicodifica delle variabili.\nCreazione di eventuali variabili necessarie.\nGestione dei valori mancanti, se necessario.\nAggiunta di metadati, se necessario.\nValidazione dei dati.\nFusione e/o unione dei dati, se necessario.\nTrasformazione dei dati, se necessario.\nSalvataggio dei dati puliti.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#importare-i-dati",
    "href": "chapters/eda/02_data_cleaning.html#importare-i-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.2 Importare i Dati",
    "text": "7.2 Importare i Dati\nI dati grezzi non dovrebbero mai essere modificati direttamente. È consigliabile organizzare i dati in una struttura di cartelle all’interno di una directory chiamata data, che contiene due sottocartelle: raw e processed. I dati originali, non ancora elaborati, devono essere conservati nella cartella raw e mantenuti inalterati. I dati ripuliti e preprocessati, invece, devono essere salvati nella cartella processed.\nPer fare un esempio, importiamo i dati dal file w1_mathproj_stu_svy_raw.csv e iniziamo il processo di pulizia. È importante notare che tutte le istruzioni sono formulate in modo relativo alla home directory del progetto. Prima di tutto, definiamo il percorso della home directory del progetto.\n\nEsaminare i Dati\n\nProcediamo con l’importazione dei dati.\n\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-0…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt;\n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nModifica i casi secondo necessità\n\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  dim()\n#&gt; [1] 5 7\n\n\nDe-identificazione dei Dati\n\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"\n\n\nRimuovere le Colonne non Necessarie\n\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\nDividere le Colonne Secondo Necessità\n\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\nRinominare le Colonne\n\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2\n\n\n\n\n\nTrasformare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n\nStandardizzare / Normalizzare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.\n\nAggiornare i Tipi delle Variabili\n\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.\n\nRicodificare le Variabili\n\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)\n\n\nAggiungere Nuove Variabili nel Data Frame\n\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3\n\n\nAffrontare il Problema dei Dati Mancanti\n\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale\noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.\n\nAggiungere i Metadati\n\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4\n\n\nValidazione dei Dati\n\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(\n    columns = c(stu_id),\n    left = 1300, right = 1400, na_pass = TRUE\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(grade_level),\n    set = c(9, 10, 11, 12, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(int),\n    set = c(0, 1, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(math1:math4),\n    set = c(1, 2, 3, 4, NA)\n  ) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-27|21:46:50]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-27 21:46:50 CET &lt; 1 s 2024-11-27 21:46:50 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.\n\nUnire e/o aggiungere dati se necessario\n\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.\n\nTrasformare i dati se necessario\n\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#esaminare-i-dati",
    "href": "chapters/eda/02_data_cleaning.html#esaminare-i-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.3 (1) Esaminare i Dati",
    "text": "7.3 (1) Esaminare i Dati\nProcediamo con l’importazione dei dati.\n\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-0…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt;\n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#modifica-i-casi-secondo-necessità",
    "href": "chapters/eda/02_data_cleaning.html#modifica-i-casi-secondo-necessità",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.4 (2) Modifica i casi secondo necessità",
    "text": "7.4 (2) Modifica i casi secondo necessità\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  dim()\n#&gt; [1] 5 7",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#de-identificazione-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#de-identificazione-dei-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.5 (3) De-identificazione dei Dati",
    "text": "7.5 (3) De-identificazione dei Dati\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#rimuovere-le-colonne-non-necessarie",
    "href": "chapters/eda/02_data_cleaning.html#rimuovere-le-colonne-non-necessarie",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.6 (4) Rimuovere le Colonne non Necessarie",
    "text": "7.6 (4) Rimuovere le Colonne non Necessarie\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#dividere-le-colonne-secondo-necessità",
    "href": "chapters/eda/02_data_cleaning.html#dividere-le-colonne-secondo-necessità",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.7 (5) Dividere le Colonne Secondo Necessità",
    "text": "7.7 (5) Dividere le Colonne Secondo Necessità\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#rinominare-le-colonne",
    "href": "chapters/eda/02_data_cleaning.html#rinominare-le-colonne",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.8 (6) Rinominare le Colonne",
    "text": "7.8 (6) Rinominare le Colonne\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#trasformare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#trasformare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.9 (7) Trasformare le Variabili",
    "text": "7.9 (7) Trasformare le Variabili\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#standardizzare-normalizzare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#standardizzare-normalizzare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.10 (8) Standardizzare / Normalizzare le Variabili",
    "text": "7.10 (8) Standardizzare / Normalizzare le Variabili\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiornare-i-tipi-delle-variabili",
    "href": "chapters/eda/02_data_cleaning.html#aggiornare-i-tipi-delle-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.11 (9) Aggiornare i Tipi delle Variabili",
    "text": "7.11 (9) Aggiornare i Tipi delle Variabili\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#ricodificare-le-variabili",
    "href": "chapters/eda/02_data_cleaning.html#ricodificare-le-variabili",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.12 (10) Ricodificare le Variabili",
    "text": "7.12 (10) Ricodificare le Variabili\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiungere-nuove-variabili-nel-data-frame",
    "href": "chapters/eda/02_data_cleaning.html#aggiungere-nuove-variabili-nel-data-frame",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.13 (11) Aggiungere Nuove Variabili nel Data Frame",
    "text": "7.13 (11) Aggiungere Nuove Variabili nel Data Frame\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#affrontare-il-problema-dei-dati-mancanti",
    "href": "chapters/eda/02_data_cleaning.html#affrontare-il-problema-dei-dati-mancanti",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.14 (12) Affrontare il Problema dei Dati Mancanti",
    "text": "7.14 (12) Affrontare il Problema dei Dati Mancanti\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale\noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#aggiungere-i-metadati",
    "href": "chapters/eda/02_data_cleaning.html#aggiungere-i-metadati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.15 (13) Aggiungere i Metadati",
    "text": "7.15 (13) Aggiungere i Metadati\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#validazione-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#validazione-dei-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.16 (14) Validazione dei Dati",
    "text": "7.16 (14) Validazione dei Dati\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(\n    columns = c(stu_id),\n    left = 1300, right = 1400, na_pass = TRUE\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(grade_level),\n    set = c(9, 10, 11, 12, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(int),\n    set = c(0, 1, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(math1:math4),\n    set = c(1, 2, 3, 4, NA)\n  ) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-27|21:41:32]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-27 21:41:32 CET &lt; 1 s 2024-11-27 21:41:32 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#unire-eo-aggiungere-dati-se-necessario",
    "href": "chapters/eda/02_data_cleaning.html#unire-eo-aggiungere-dati-se-necessario",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.17 (15) Unire e/o aggiungere dati se necessario",
    "text": "7.17 (15) Unire e/o aggiungere dati se necessario\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#trasformare-i-dati-se-necessario",
    "href": "chapters/eda/02_data_cleaning.html#trasformare-i-dati-se-necessario",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.18 (16) Trasformare i dati se necessario",
    "text": "7.18 (16) Trasformare i dati se necessario\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#salvare-il-dataset-pulito-finale",
    "href": "chapters/eda/02_data_cleaning.html#salvare-il-dataset-pulito-finale",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.3 (17) Salvare il dataset pulito finale",
    "text": "7.3 (17) Salvare il dataset pulito finale\nL’ultimo passaggio del processo di pulizia consiste nell’esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file .csv e un file .parquet).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "href": "chapters/eda/02_data_cleaning.html#organizzazione-dei-file-e-informazioni-aggiuntive",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.3 Organizzazione dei file e informazioni aggiuntive",
    "text": "7.3 Organizzazione dei file e informazioni aggiuntive\nInfine, è essenziale includere una documentazione adeguata per garantire che le informazioni siano interpretate correttamente, sia da altri utenti che da te stesso, se dovessi tornare a lavorare su questo progetto in futuro. La documentazione minima da fornire dovrebbe includere:\n\n\nDocumentazione a livello di progetto: Questa sezione fornisce informazioni contestuali sul perché e come i dati sono stati raccolti. È utile per chiunque voglia comprendere lo scopo e la metodologia del progetto.\n\nMetadati a livello di progetto: Se condividi i dati in un repository pubblico o privato, è importante includere metadati a livello di progetto. Questi metadati forniscono informazioni dettagliate che facilitano la ricerca, la comprensione e la consultabilità dei dati. I metadati a livello di progetto possono includere descrizioni generali del progetto, parole chiave, e riferimenti bibliografici.\n\nDizionario dei dati: Un documento che descrive tutte le variabili presenti nel dataset, inclusi i loro nomi, tipi, range di valori, categorie e qualsiasi altra informazione rilevante. Questo strumento è fondamentale per chiunque voglia comprendere o analizzare i dati.\n\nREADME: Un file che fornisce una panoramica rapida dei file inclusi nel progetto, spiegando cosa contengono e come sono interconnessi. Il README è spesso il primo documento consultato e serve a orientare l’utente tra i vari file e risorse del progetto. Questa documentazione non solo aiuta a mantenere il progetto organizzato, ma è anche cruciale per facilitare la collaborazione e l’archiviazione a lungo termine.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "href": "chapters/eda/02_data_cleaning.html#dizionario-dei-dati",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.4 Dizionario dei Dati",
    "text": "7.4 Dizionario dei Dati\nApprofondiamo qui il problema della creazione del Dizionario dei dati.\nUn dizionario dei dati è un documento che descrive le caratteristiche di ciascuna variabile in un dataset. Include informazioni come il nome della variabile, il tipo di dato, il range di valori, le categorie (per le variabili categoriche), e altre informazioni rilevanti. Questo strumento è essenziale per comprendere e analizzare correttamente il dataset.\nSi presti particolare attenzione alle guide di stile per la denominazione delle variabili e la codifica dei valori delle risposte.\nEcco come tradurre i passi per creare un dizionario dei dati in R, utilizzando il pacchetto tibble per creare il dizionario e writexl o readr per esportarlo in formato .xlsx o .csv.\n\n7.4.1 Passi per Creare un Dizionario dei Dati\n\n\nIdentificare le variabili: Elencare tutte le variabili presenti nel dataset.\n\nDescrivere ogni variabile: Per ciascuna variabile, definire il tipo (ad esempio, integer, numeric, character), il range di valori accettabili o le categorie, e fornire una descrizione chiara.\n\nSalvare il dizionario dei dati: Il dizionario può essere salvato in un file .csv o .xlsx per una facile consultazione.\n\n7.4.2 Esempio in R\n\nCreeremo un dizionario dei dati per un dataset di esempio e lo salveremo sia in formato CSV che Excel.\n\nlibrary(tibble)\nlibrary(readr)\nlibrary(writexl)\n\n# Creazione del Dizionario dei Dati\ndata_dict &lt;- tibble(\n  `Variable Name` = c(\n    \"stu_id\",\n    \"svy_date\",\n    \"grade_level\",\n    \"math1\",\n    \"math2\",\n    \"math3\",\n    \"math4\"\n  ),\n  `Type` = c(\n    \"integer\",\n    \"datetime\",\n    \"integer\",\n    \"integer\",\n    \"integer\",\n    \"numeric\",\n    \"numeric\"\n  ),\n  `Description` = c(\n    \"Student ID\",\n    \"Survey Date\",\n    \"Grade Level\",\n    \"Math Response 1 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 2 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 3 (1: Strongly Disagree, 4: Strongly Agree)\",\n    \"Math Response 4 (1: Strongly Disagree, 4: Strongly Agree)\"\n  ),\n  `Range/Values` = c(\n    \"1347-1399\",\n    \"2023-02-13 to 2023-02-14\",\n    \"9-12\",\n    \"1-4\",\n    \"1-4\",\n    \"1.0-4.0 (NA allowed)\",\n    \"1.0-4.0 (NA allowed)\"\n  )\n)\n\n# Visualizza il Dizionario dei Dati\nprint(data_dict)\n\n# Salva il Dizionario dei Dati in un file CSV\nwrite_csv(data_dict, \"data_dictionary.csv\")\n\n# Salva il Dizionario dei Dati in un file Excel\nwrite_xlsx(data_dict, \"data_dictionary.xlsx\")\n\nOutput Atteso: file CSV (data_dictionary.csv).\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\nRange/Values\n\n\n\nstu_id\ninteger\nStudent ID\n1347-1399\n\n\nsvy_date\ndatetime\nSurvey Date\n2023-02-13 to 2023-02-14\n\n\ngrade_level\ninteger\nGrade Level\n9-12\n\n\nmath1\ninteger\nMath Response 1 (1: Strongly Disagree, 4: Strongly Agree)\n1-4\n\n\nmath2\ninteger\nMath Response 2 (1: Strongly Disagree, 4: Strongly Agree)\n1-4\n\n\nmath3\nnumeric\nMath Response 3 (1: Strongly Disagree, 4: Strongly Agree)\n1.0-4.0 (NA allowed)\n\n\nmath4\nnumeric\nMath Response 4 (1: Strongly Disagree, 4: Strongly Agree)\n1.0-4.0 (NA allowed)\n\n\n\n\n\nDocumentazione: Il dizionario dei dati offre una descrizione chiara e standardizzata, utile per analisi successive e per la condivisione del dataset.\n\nSalvataggio multiplo: I formati .csv e .xlsx garantiscono la massima compatibilità con altri software e sistemi.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#guide-di-stile",
    "href": "chapters/eda/02_data_cleaning.html#guide-di-stile",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.5 Guide di Stile",
    "text": "7.5 Guide di Stile\nLe guide di stile possono essere applicate a diversi aspetti di un progetto di analisi dei dati, non soltanto al dizionario dei dati. Un’ottima introduzione alle regole di stile per un progetto di analisi dei dati è fornita in questo capitolo.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "href": "chapters/eda/02_data_cleaning.html#riflessioni-conclusive",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.6 Riflessioni Conclusive",
    "text": "7.6 Riflessioni Conclusive\nNel processo di analisi dei dati, la fase di pulizia e pre-elaborazione è cruciale per garantire la qualità e l’integrità dei risultati finali. Sebbene questa fase possa sembrare meno interessante rispetto all’analisi vera e propria, essa costituisce la base su cui si costruiscono tutte le successive elaborazioni e interpretazioni. Attraverso una serie di passaggi strutturati, come quelli illustrati in questo capitolo, è possibile trasformare dati grezzi e disordinati in un dataset pulito, coerente e pronto per l’analisi. La cura nella gestione dei dati, dalla rimozione di duplicati alla creazione di un dizionario dei dati, è fondamentale per ottenere risultati affidabili e riproducibili.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/eda/02_data_cleaning.html#informazioni-sullambiente-di-sviluppo",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.1.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] C/UTF-8/C/C/C/C\n#&gt; \n#&gt; time zone: Europe/Rome\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] pointblank_0.12.2 haven_2.5.4       labelled_2.13.0   mice_3.16.0      \n#&gt;  [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n#&gt;  [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n#&gt; [13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n#&gt; [17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n#&gt; [21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n#&gt; [25] ggplot2_3.5.1     tidyverse_2.0.0   rio_1.2.3         here_1.0.1       \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] mnormt_2.1.1        airports_0.1.0      rlang_1.1.4        \n#&gt;  [4] magrittr_2.0.3      compiler_4.4.2      vctrs_0.6.5        \n#&gt;  [7] pkgconfig_2.0.3     shape_1.4.6.1       fastmap_1.2.0      \n#&gt; [10] backports_1.5.0     utf8_1.2.4          promises_1.3.1     \n#&gt; [13] blastula_0.3.5      rmarkdown_2.29      tzdb_0.4.0         \n#&gt; [16] openintro_2.5.0     nloptr_2.1.1        xfun_0.49          \n#&gt; [19] glmnet_4.1-8        jomo_2.7-6          jsonlite_1.8.9     \n#&gt; [22] later_1.4.0         styler_1.10.3       pan_1.9            \n#&gt; [25] broom_1.0.7         parallel_4.4.2      R6_2.5.1           \n#&gt; [28] stringi_1.8.4       rpart_4.1.23        car_3.1-3          \n#&gt; [31] boot_1.3-31         Rcpp_1.0.13-1       iterators_1.0.14   \n#&gt; [34] base64enc_0.1-3     R.utils_2.12.3      cherryblossom_0.1.0\n#&gt; [37] pacman_0.5.1        R.cache_0.16.0      httpuv_1.6.15      \n#&gt; [40] Matrix_1.7-1        splines_4.4.2       nnet_7.3-19        \n#&gt; [43] timechange_0.3.0    tidyselect_1.2.1    abind_1.4-8        \n#&gt; [46] yaml_2.3.10         codetools_0.2-20    miniUI_0.1.1.1     \n#&gt; [49] lattice_0.22-6      shiny_1.9.1         withr_3.0.2        \n#&gt; [52] evaluate_1.0.1      survival_3.7-0      xml2_1.3.6         \n#&gt; [55] pillar_1.9.0        carData_3.0-5       foreach_1.5.2      \n#&gt; [58] generics_0.1.3      rprojroot_2.0.4     hms_1.1.3          \n#&gt; [61] commonmark_1.9.2    munsell_0.5.1       minqa_1.2.8        \n#&gt; [64] xtable_1.8-4        glue_1.8.0          tools_4.4.2        \n#&gt; [67] data.table_1.16.2   lme4_1.1-35.5       ggsignif_0.6.4     \n#&gt; [70] grid_4.4.2          colorspace_2.1-1    nlme_3.1-166       \n#&gt; [73] Formula_1.2-5       usdata_0.3.1        cli_3.6.3          \n#&gt; [76] fansi_1.0.6         gt_0.11.1           gtable_0.3.6       \n#&gt; [79] R.methodsS3_1.8.2   rstatix_0.7.2       sass_0.4.9.9000    \n#&gt; [82] digest_0.6.37       htmlwidgets_1.6.4   farver_2.1.2       \n#&gt; [85] R.oo_1.27.0         htmltools_0.5.8.1   lifecycle_1.0.4    \n#&gt; [88] mitml_0.4-5         mime_0.12",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#bibliografia",
    "href": "chapters/eda/02_data_cleaning.html#bibliografia",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "Bibliografia",
    "text": "Bibliografia\n\n\n\n\nBuchanan, E. M., Crain, S. E., Cunningham, A. L., Johnson, H. R., Stash, H., Papadatou-Pastou, M., Isager, P. M., Carlsson, R., & Aczel, B. (2021). Getting started creating data dictionaries: How to create a shareable data set. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920928007.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  },
  {
    "objectID": "chapters/eda/02_data_cleaning.html#tutorial",
    "href": "chapters/eda/02_data_cleaning.html#tutorial",
    "title": "7  Flusso di lavoro per la pulizia dei dati",
    "section": "\n7.2 Tutorial",
    "text": "7.2 Tutorial\nEsamineremo questi passaggi seguendo il tutorial di Crystal Lewis.\nImportare i Dati\nI dati grezzi non dovrebbero mai essere modificati direttamente. È consigliabile organizzare i dati in una struttura di cartelle all’interno di una directory chiamata data, che contiene due sottocartelle: raw e processed. I dati originali, non ancora elaborati, devono essere conservati nella cartella raw e mantenuti inalterati. I dati ripuliti e preprocessati, invece, devono essere salvati nella cartella processed.\nPer fare un esempio, importiamo i dati dal file w1_mathproj_stu_svy_raw.csv e iniziamo il processo di pulizia. È importante notare che tutte le istruzioni sono formulate in modo relativo alla home directory del progetto. Prima di tutto, definiamo il percorso della home directory del progetto.\n\nEsaminare i Dati\n\nProcediamo con l’importazione dei dati.\n\nsvy &lt;- rio::import(here::here(\"data\", \"w1_mathproj_stu_svy_raw.csv\"))\nglimpse(svy)\n#&gt; Rows: 6\n#&gt; Columns: 7\n#&gt; $ stu_id      &lt;int&gt; 1347, 1368, 1377, 1387, 1347, 1399\n#&gt; $ svy_date    &lt;IDate&gt; 2023-02-13, 2023-02-13, 2023-02-13, 2023-02-13, 2023-0…\n#&gt; $ grade_level &lt;int&gt; 9, 10, 9, 11, 9, 12\n#&gt; $ math1       &lt;int&gt; 2, 3, 4, 3, 2, 4\n#&gt; $ math2       &lt;chr&gt; \"1\", \"2\", \"\\n4\", \"3\", \"2\", \"1\"\n#&gt; $ math3       &lt;int&gt; 3, 2, 4, NA, 4, 3\n#&gt; $ math4       &lt;int&gt; 3, 2, 4, NA, 2, 1\n\nÈ utile esaminare visivamente le prime o le ultime righe del data frame per verificare che i dati siano stati importati correttamente.\n\nsvy |&gt;\n  head()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  tail()\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 5   1347 2023-02-14           9     2     2     4     2\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nModificare i casi secondo necessità\n\nIl secondo passo è quello in cui vengono fatte delle semplici ma necessarie modifiche al data frame. Crystal Lewis descrive così questo passo per i dati in esame:\n\nVerificare la presenza di duplicati - Il record 1347 è duplicato.\nRimuovere i duplicati.\nOrdinare per svy_date in ordine crescente.\nEsaminare i dati dopo aver rimosso i duplicati.\n\n\n# Trova i duplicati basati su 'stu_id'\nduplicates &lt;- svy[duplicated(svy$stu_id) | duplicated(svy$stu_id, fromLast = TRUE), ]\n\n# Ordina per 'svy_date' in ordine crescente e rimuovi i duplicati mantenendo il primo\nsvy &lt;- svy[order(svy$svy_date), ]\nsvy &lt;- svy[!duplicated(svy$stu_id), ]\n\n# Mostra il DataFrame finale\nprint(svy)\n#&gt;   stu_id   svy_date grade_level math1 math2 math3 math4\n#&gt; 1   1347 2023-02-13           9     2     1     3     3\n#&gt; 2   1368 2023-02-13          10     3     2     2     2\n#&gt; 3   1377 2023-02-13           9     4   \\n4     4     4\n#&gt; 4   1387 2023-02-13          11     3     3    NA    NA\n#&gt; 6   1399 2023-02-14          12     4     1     3     1\n\n\nsvy |&gt;\n  dim()\n#&gt; [1] 5 7\n\n\nDe-identificazione dei Dati\n\n\n# Rimuovi la colonna 'svy_date'\nsvy &lt;- svy |&gt;\n  dplyr::select(-svy_date)\n\n# Mostra i nomi delle colonne rimaste\nnames(svy)\n#&gt; [1] \"stu_id\"      \"grade_level\" \"math1\"       \"math2\"       \"math3\"      \n#&gt; [6] \"math4\"\n\n\nRimuovere le Colonne non Necessarie\n\nNel caso presente, la rimozione di colonne non è necessaria. Tuttavia, in molti progetti di analisi dei dati, soprattutto quando i dati vengono raccolti utilizzando software di terze parti o strumenti specifici per esperimenti psicologici, è comune trovarsi con colonne che non sono pertinenti allo studio in corso.\nQueste colonne possono includere dati come identificatori interni, timestamp generati automaticamente, informazioni di debug, o variabili che non sono rilevanti per l’analisi che si intende condurre. Quando tali colonne sono irrilevanti per la ricerca, possono essere rimosse per semplificare il dataset e ridurre il rischio di confusione o errori durante l’analisi. Rimuovere le colonne non necessarie non solo rende il dataset più gestibile, ma aiuta anche a focalizzare l’analisi sulle variabili che realmente importano per rispondere alle domande di ricerca.\n\nDividere le Colonne Secondo Necessità\n\nNel caso presente, questa operazione non è necessaria. Tuttavia, se si lavora con un dataset che include una colonna chiamata “NomeCompleto”, contenente sia il nome che il cognome di uno studente, è buona pratica separare questa colonna in due colonne distinte, “Nome” e “Cognome”. Questa suddivisione facilita l’analisi e la manipolazione dei dati, rendendoli più organizzati e accessibili.\n\nRinominare le Colonne\n\nÈ importante assegnare nomi chiari alle colonne del dataset. Utilizzare nomi di variabili comprensibili aiuta a rendere l’analisi dei dati più intuitiva e a ridurre il rischio di errori interpretativi.\nEsempi di buone pratiche:\n\nEvita nomi di colonne come “x” o acronimi incomprensibili. Questi possono creare confusione durante l’analisi, specialmente se il dataset viene condiviso con altri ricercatori o se viene ripreso dopo un lungo periodo di tempo.\nInvece, cerca di utilizzare nomi di variabili che descrivano chiaramente il contenuto della colonna. Ad esempio, invece di “x1” o “VAR123”, un nome come “ansia_base” o “liv_autoefficacia” è molto più comprensibile e immediato.\nPer i nomi composti, utilizza un separatore come il trattino basso _. Ad esempio, se stai lavorando con dati relativi a un test psicologico, potresti avere colonne chiamate “test_ansia_pre” e “test_ansia_post” per indicare i risultati del test di ansia prima e dopo un intervento.\n\nEsempi di nomi di colonne ben scelti:\n\n\nNome generico: TS, AE\n\n\nNome migliore: tempo_studio, auto_efficacia\n\n\n\n\nNome generico: S1, S2\n\n\nNome migliore: stress_situazione1, stress_situazione2\n\n\n\n\nNome generico: Q1, Q2\n\n\nNome migliore: qualità_sonno_sett1, qualità_sonno_sett2\n\n\n\n\n\nTrasformare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di trasformazione delle variabili:\n\nLogaritmo di una variabile: Immaginiamo di avere una variabile che misura i tempi di reazione dei partecipanti a un esperimento. Se i tempi di reazione hanno una distribuzione fortemente asimmetrica (con alcuni valori molto elevati), potrebbe essere utile applicare una trasformazione logaritmica per rendere la distribuzione più simmetrica e migliorare l’interpretabilità dei risultati.\nCodifica delle variabili categoriche: Se è presente una variabile categorica come il “tipo di intervento” con valori come “cognitivo”, “comportamentale” e “farmacologico”, potrebbe essere necessario trasformare questa variabile in variabili dummy (ad esempio, intervento_cognitivo, intervento_comportamentale, intervento_farmacologico), dove ogni variabile assume il valore 0 o 1 a seconda della presenza o meno di quel tipo di intervento. Questo è utile quando si utilizzano tecniche di regressione.\n\n\nStandardizzare / Normalizzare le Variabili\n\nNel caso presente non si applica, ma è un passo importante in molte analisi dei dati.\nEsempi di standardizzazione delle variabili:\n\nStandardizzazione dei punteggi: Supponiamo di avere una variabile che misura il livello di ansia su una scala da 0 a 100. Se desideriamo confrontare i livelli di ansia tra diversi gruppi o includere questa variabile in un modello di regressione, potrebbe essere utile standardizzare i punteggi (cioè, sottrarre la media e dividere per la deviazione standard) per ottenere una variabile con media 0 e deviazione standard 1. Questo processo rende i punteggi comparabili e facilita l’interpretazione dei coefficienti in un modello di regressione.\nNormalizzazione delle variabili: Se hai dati su diverse variabili come “ore di sonno”, “livello di stress” e “auto-efficacia”, e queste variabili hanno scale molto diverse, potrebbe essere utile normalizzarle (ad esempio, ridimensionarle tutte su una scala da 0 a 1) per garantire che abbiano lo stesso peso in un’analisi multivariata.\n\nTrasformare e standardizzare le variabili sono passaggi cruciali in molte analisi psicologiche, specialmente quando si confrontano dati provenienti da diverse fonti o gruppi. Questi processi aiutano a garantire che le variabili siano trattate in modo appropriato e che i risultati dell’analisi siano validi e interpretabili.\n\nAggiornare i Tipi delle Variabili\n\nNel caso presente non è necessario. Supponiamo invece di avere una colonna in un dataset psicologico che contiene punteggi di un questionario, ma i dati sono stati importati come stringhe (testo) invece che come numeri. Per eseguire calcoli statistici, sarà necessario convertire questa colonna da stringa a numerico.\nIn R, si potrebbe usare il seguente codice:\n\n# Supponiamo di avere un data frame chiamato 'df' con una colonna 'punteggio' importata come carattere\ndf$punteggio &lt;- as.numeric(df$punteggio)\n\n# Ora la colonna 'punteggio' è stata convertita in un tipo numerico ed è possibile eseguire calcoli su di essa\n\nIn questo esempio, la funzione as.numeric() viene utilizzata per convertire la colonna punteggio in un formato numerico, permettendo di eseguire analisi quantitative sui dati.\nUn altro caso molto comune si verifica quando si importano dati da file Excel. Spesso capita che, all’interno di una cella di una colonna che dovrebbe contenere solo valori numerici, venga inserito erroneamente uno o più caratteri alfanumerici. Di conseguenza, l’intera colonna viene interpretata come di tipo alfanumerico, anche se i valori dovrebbero essere numerici. In questi casi, è fondamentale individuare la cella problematica, correggere il valore errato, e poi riconvertire l’intera colonna da alfanumerica a numerica.\n\nRicodificare le Variabili\n\nAnche se in questo caso non è necessario, la ricodifica delle variabili è una pratica molto comune nelle analisi dei dati psicologici.\nPer esempio, consideriamo una variabile categoriale con modalità descritte da stringhe poco comprensibili, che vengono ricodificate con nomi più chiari e comprensibili.\nSupponiamo di avere un DataFrame chiamato df con una colonna tipo_intervento che contiene le modalità \"CT\", \"BT\", e \"MT\" per rappresentare rispettivamente “Terapia Cognitiva”, “Terapia Comportamentale” e “Terapia Mista”. Queste abbreviazioni potrebbero non essere immediatamente chiare a chiunque analizzi i dati, quindi decidiamo di ricodificarle con nomi più espliciti. Ecco come farlo in R:\n\n# Supponiamo di avere un tibble chiamato 'df' con una colonna 'tipo_intervento'\ndf &lt;- tibble(tipo_intervento = c(\"CT\", \"BT\", \"MT\", \"CT\", \"BT\"))\n\n# Ricodifica delle modalità della variabile 'tipo_intervento' in nomi più comprensibili\ndf &lt;- df %&gt;%\n  mutate(tipo_intervento_ricodificato = dplyr::recode(\n    tipo_intervento,\n    \"CT\" = \"Terapia Cognitiva\",\n    \"BT\" = \"Terapia Comportamentale\",\n    \"MT\" = \"Terapia Mista\"\n  ))\n\n# Mostra il tibble con la nuova colonna ricodificata\nprint(df)\n\n\nAggiungere Nuove Variabili nel Data Frame\n\nNel caso presente non è richiesto, ma aggiungere nuove variabili a un DataFrame è un’operazione comune durante l’analisi dei dati. Un esempio è il calcolo dell’indice di massa corporea (BMI).\nSupponiamo di avere un DataFrame chiamato df che contiene le colonne peso_kg (peso in chilogrammi) e altezza_m (altezza in metri) per ciascun partecipante a uno studio psicologico. Per arricchire il dataset, possiamo calcolare il BMI per ogni partecipante e aggiungerlo come una nuova variabile.\nIl BMI si calcola con la formula:\n\\[ \\text{BMI} = \\frac{\\text{peso in kg}}{\\text{altezza in metri}^2} .\\]\nEcco come aggiungere la nuova colonna.\n\n# Supponiamo di avere un tibble chiamato 'df' con le colonne 'peso_kg' e 'altezza_m'\ndf &lt;- tibble(\n  peso_kg = c(70, 85, 60, 95),\n  altezza_m = c(1.75, 1.80, 1.65, 1.90)\n)\n\n# Calcola il BMI e aggiungilo come una nuova colonna 'BMI'\ndf &lt;- df %&gt;%\n  mutate(BMI = peso_kg / (altezza_m^2))\n\n# Mostra il tibble con la nuova variabile aggiunta\nprint(df)\n#&gt; # A tibble: 4 × 3\n#&gt;   peso_kg altezza_m   BMI\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1      70      1.75  22.9\n#&gt; 2      85      1.8   26.2\n#&gt; 3      60      1.65  22.0\n#&gt; 4      95      1.9   26.3\n\n\nAffrontare il Problema dei Dati Mancanti\n\nL’imputazione è una tecnica utilizzata per gestire i dati mancanti in un dataset, un problema comune in molte analisi. Lasciare i valori mancanti nel DataFrame può compromettere la qualità dell’analisi, poiché molti algoritmi statistici non sono in grado di gestire direttamente i dati incompleti, portando a risultati distorti o poco affidabili.\nI valori mancanti possono causare diversi problemi:\n\n\nBias dei risultati: I dati mancanti possono introdurre un bias nelle stime se i valori mancanti non sono distribuiti in modo casuale.\n\nRiduzione della potenza statistica: Quando si eliminano le righe con dati mancanti (rimozione listwise), si riduce la dimensione del campione, diminuendo la potenza dell’analisi.\n\nImpossibilità di utilizzare alcuni algoritmi: Molti algoritmi di statistica richiedono che tutti i valori siano presenti per eseguire correttamente i calcoli.\n\nEsistono vari approcci per affrontare i dati mancanti:\n\n\nImputazione Semplice:\n\n\nMedia/Mediana: Un metodo comune e semplice è sostituire i valori mancanti con la media o la mediana della colonna. Questo metodo è facile da implementare, ma può ridurre la variabilità dei dati e portare a una sottostima della varianza.\n\nMode (moda): Per le variabili categoriche, è possibile sostituire i valori mancanti con la moda (il valore più frequente). Tuttavia, questo può portare a una distorsione se la distribuzione dei dati è molto eterogenea.\n\n\n\nImputazione Multipla:\n\n\nRegressione Iterativa: L’imputazione multipla, come implementata con algoritmi come IterativeImputer, è una procedura più sofisticata che predice i valori mancanti in modo iterativo utilizzando un modello basato sulle altre variabili del dataset. Questa tecnica tiene conto delle relazioni tra le variabili, migliorando l’accuratezza delle imputazioni rispetto ai metodi semplici.\nL’imputazione multipla conserva la variabilità nei dati e riduce il bias, fornendo stime più accurate rispetto ai metodi di imputazione semplice.\n\n\n\nL’imputazione dei dati mancanti è essenziale per garantire che l’analisi statistica sia accurata e robusta. Sebbene i metodi semplici come la sostituzione con la media possano essere utili in alcuni casi, l’imputazione multipla offre un approccio più completo e sofisticato, particolarmente utile quando si desidera preservare le relazioni tra le variabili e mantenere l’integrità statistica del dataset. Questo argomento verrà ulteriormente discusso nel ?sec-missing-data.\nApplichiamo la procedura dell’imputazione multipla al caso presente.\n\n# Supponiamo di avere un data frame chiamato 'd'\nd &lt;- svy %&gt;% as_tibble()\n\n# Mantieni l'indice originale\noriginal_index &lt;- rownames(d)\n\n# Converti solo le colonne numeriche relative ai punteggi in numerico per l'imputazione\nnumeric_columns &lt;- c(\"math1\", \"math2\", \"math3\", \"math4\")\nd &lt;- d %&gt;%\n  mutate(across(all_of(numeric_columns), as.numeric))\n\n# Applica mice per l'imputazione multipla\nimputed &lt;- mice(d[numeric_columns], m = 1, maxit = 10, method = \"norm.predict\", seed = 0)\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  math3  math4\n#&gt;   2   1  math3  math4\n#&gt;   3   1  math3  math4\n#&gt;   4   1  math3  math4\n#&gt;   5   1  math3  math4\n#&gt;   6   1  math3  math4\n#&gt;   7   1  math3  math4\n#&gt;   8   1  math3  math4\n#&gt;   9   1  math3  math4\n#&gt;   10   1  math3  math4\n\n# Estrai il dataset imputato\ndf_imputed &lt;- complete(imputed)\n\n# Arrotonda i valori imputati ai numeri interi più vicini\ndf_imputed &lt;- df_imputed %&gt;%\n  mutate(across(everything(), round))\n\n# Inserisci i valori imputati e arrotondati nel data frame originale\nd[numeric_columns] &lt;- df_imputed\n\n# Mostra il data frame dopo l'imputazione e l'arrotondamento\ncat(\"\\nDataFrame dopo l'imputazione e l'arrotondamento:\\n\")\n#&gt; \n#&gt; DataFrame dopo l'imputazione e l'arrotondamento:\nprint(d)\n#&gt; # A tibble: 5 × 6\n#&gt;   stu_id grade_level math1 math2 math3 math4\n#&gt;    &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1347           9     2     1     3     3\n#&gt; 2   1368          10     3     2     2     2\n#&gt; 3   1377           9     4     4     4     4\n#&gt; 4   1387          11     3     3     2     3\n#&gt; 5   1399          12     4     1     3     1\n\nPer eseguire l’imputazione multipla in R, utilizziamo il pacchetto mice, uno strumento avanzato per gestire i valori mancanti nei dati. Questo approccio si basa su metodi di regressione iterativa, in cui ogni valore mancante viene stimato utilizzando un modello predittivo che considera tutte le altre variabili presenti nel dataset.\n\n\nSelezione delle colonne numeriche per l’imputazione:\n\nAbbiamo identificato le colonne numeriche che richiedono l’imputazione (math1, math2, math3, math4).\n\n\n\nImputazione Multipla con mice:\n\nIl pacchetto mice utilizza un processo iterativo per stimare i valori mancanti. Ogni variabile con valori mancanti viene modellata a turno come una funzione delle altre variabili, utilizzando metodi specifici (ad esempio, regressione lineare o modelli bayesiani).\nL’imputazione iterativa procede in cicli successivi. Durante ogni ciclo, i valori mancanti di una variabile vengono stimati utilizzando le imputazioni correnti delle altre variabili.\n\nParametro maxit=10: Il processo iterativo viene ripetuto fino a un massimo di 10 volte, o fino al raggiungimento della convergenza (stabilità dei valori imputati).\n\n\n\nApplicazione e Arrotondamento:\n\nDopo l’imputazione, i valori stimati vengono reinseriti nel dataset. Per le variabili numeriche che rappresentano conteggi o valori discreti, i valori imputati sono stati arrotondati al numero intero più vicino.\n\n\n\nRisultato:\n\nIl dataset risultante non contiene più valori mancanti nelle colonne numeriche specificate (math1, math2, math3, math4), poiché questi sono stati imputati utilizzando le relazioni con le altre variabili del dataset.\n\n\n\nIn sintesi, l’imputazione multipla con mice è una tecnica potente per gestire i valori mancanti senza eliminare intere righe o colonne. Questo approccio preserva le relazioni tra variabili, garantendo che l’inferenza statistica rimanga accurata e valida. Nel nostro caso, abbiamo utilizzato un modello predittivo iterativo per stimare i valori mancanti basandoci sulle informazioni fornite dalle altre variabili. Questo metodo aumenta la qualità dei dati e consente analisi più robuste e affidabili.\n\nAggiungere i Metadati\n\nI metadati sono informazioni che descrivono i dati stessi, come etichette di variabili, etichette di valori, informazioni sull’origine dei dati, unità di misura e altro ancora. Questi metadati sono essenziali per comprendere, documentare e condividere correttamente un dataset.\nIn R, i metadati sono gestiti in modo molto dettagliato e strutturato attraverso pacchetti come haven, labelled, e Hmisc. Questi pacchetti consentono di associare etichette ai dati, come etichette di variabili e di valori, e persino di gestire i valori mancanti con etichette specifiche.\n\n\nEtichette di variabili: Si possono aggiungere direttamente alle colonne di un DataFrame usando funzioni come labelled::set_variable_labels().\n\nEtichette di valori: Possono essere aggiunte a variabili categoriali utilizzando labelled::labelled().\n\nValori mancanti: In R, è possibile etichettare specifici valori come mancanti usando labelled::na_values&lt;-.\n\nQuesti strumenti rendono molto facile documentare un dataset all’interno del processo di analisi, assicurando che tutte le informazioni critiche sui dati siano facilmente accessibili e ben documentate.\n\n# Creazione del dataset\nsvy &lt;- tibble(\n  stu_id = c(1347, 1368, 1377, 1387, 1399),\n  grade_level = c(9, 10, 9, 11, 12),\n  math1 = c(2, 3, 4, 3, 4),\n  math2 = c(1, 2, 4, 3, 1),\n  math3 = c(3.0, 2.0, 4.0, NA, 3.0),\n  math4 = c(3.0, 2.0, 4.0, NA, 1.0),\n  int = c(1, 0, 1, 0, 1)\n)\n\n# Definizione delle etichette di valore per le variabili math1:math4\nvalue_labels_math &lt;- set_names(\n  as.numeric(names(c(\n    `1` = \"strongly disagree\",\n    `2` = \"disagree\",\n    `3` = \"agree\",\n    `4` = \"strongly agree\"\n  ))),\n  c(\"strongly disagree\", \"disagree\", \"agree\", \"strongly agree\")\n)\n\n# Aggiunta delle etichette di valore alle colonne math1:math4\nsvy &lt;- svy %&gt;%\n  mutate(across(starts_with(\"math\"), ~ labelled(., labels = value_labels_math)))\n\n# Verifica delle etichette\nval_labels(svy$math1)\n#&gt; strongly disagree          disagree             agree    strongly agree \n#&gt;                 1                 2                 3                 4\n\n\nValidazione dei Dati\n\nLa validazione dei dati è un passaggio fondamentale per garantire che il dataset soddisfi i criteri previsti e sia pronto per le analisi successive. Questo processo include il controllo della coerenza e della correttezza dei dati in base a specifiche regole definite dal dizionario dei dati. Alcune verifiche comuni includono:\n\n\nUnicità delle righe: Assicurarsi che ogni riga sia unica, verificando l’assenza di ID duplicati.\n\nValidità degli ID: Controllare che gli ID rientrino in un intervallo previsto (es. numerico).\n\nValori accettabili nelle variabili categoriali: Verificare che variabili come grade_level, int e le colonne math contengano esclusivamente valori appartenenti a un set di valori validi.\n\nIl pacchetto pointblank fornisce strumenti flessibili e intuitivi per eseguire verifiche di validazione e generare report dettagliati. Questo pacchetto consente di:\n\n\nDefinire le regole di validazione: Specificare controlli come unicità, intervalli di valori e appartenenza a insiemi predefiniti.\n\nEseguire i controlli: Applicare le regole di validazione su un dataset per identificare eventuali discrepanze.\n\nGenerare report interattivi: Creare un riepilogo chiaro e visivo dei controlli, evidenziando eventuali errori o anomalie.\n\nCon pointblank, è possibile integrare la validazione dei dati come parte di un workflow strutturato, garantendo la qualità dei dati in modo sistematico e ripetibile.\n\ncreate_agent(svy) %&gt;%\n  rows_distinct(columns = vars(stu_id)) %&gt;%\n  col_vals_between(\n    columns = c(stu_id),\n    left = 1300, right = 1400, na_pass = TRUE\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(grade_level),\n    set = c(9, 10, 11, 12, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(int),\n    set = c(0, 1, NA)\n  ) %&gt;%\n  col_vals_in_set(\n    columns = c(math1:math4),\n    set = c(1, 2, 3, 4, NA)\n  ) %&gt;%\n  interrogate()\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-27|21:49:24]\n\n\ntibble svy\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n1\n\n\n\nrows_distinct\n\n\n\n rows_distinct()\n\n\n▮stu_id\n\n—\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n2\n\n\n\ncol_vals_between\n\n\n\n col_vals_between()\n\n\n▮stu_id\n\n\n[1,300, 1,400]\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n3\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮grade_level\n\n\n9, 10, 11, 12, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n4\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮int\n\n\n0, 1, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n5\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math1\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n6\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math2\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n7\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math3\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n8\n\n\n\ncol_vals_in_set\n\n\n\n col_vals_in_set()\n\n\n▮math4\n\n\n1, 2, 3, 4, NA\n\n\n\n✓\n5\n\n51\n\n\n00\n\n—\n—\n—\n—\n\n\n\n\n2024-11-27 21:49:24 CET &lt; 1 s 2024-11-27 21:49:24 CET\n\n\n\n\n\n\nIl dataset ripulito soddisfa tutte le aspettative delineate da Crystal Lewis.\n\n\nCompleto: Tutti i dati raccolti sono stati inseriti e/o recuperati. Non dovrebbero esserci dati estranei che non appartengono al dataset (come duplicati o partecipanti non autorizzati).\n\nValido: Le variabili rispettano i vincoli definiti nel tuo dizionario dei dati. Ricorda che il dizionario dei dati specifica i nomi delle variabili, i tipi, i range, le categorie e altre informazioni attese.\n\nAccurato: Sebbene non sia sempre possibile determinare l’accuratezza dei valori durante il processo di pulizia dei dati (ovvero, se un valore è realmente corretto o meno), in alcuni casi è possibile valutarla sulla base della conoscenza pregressa riguardante quel partecipante o caso specifico.\n\nCoerente: I valori sono allineati tra le varie fonti. Ad esempio, la data di nascita raccolta attraverso un sondaggio studentesco dovrebbe avere un formato corrispondere alla data di nascita raccolta dal distretto scolastico.\n\nUniforme: I dati sono standardizzati attraverso i moduli e nel tempo. Ad esempio, lo stato di partecipazione ai programmi di pranzo gratuito o a prezzo ridotto è sempre fornito come una variabile numerica con la stessa rappresentazione, oppure il nome della scuola è sempre scritto in modo coerente in tutto il dataset.\n\nDe-identificato: Tutte le informazioni personali identificabili (PII) sono state rimosse dal dataset per proteggere la riservatezza dei partecipanti (se richiesto dal comitato etico/consenso informato).\n\nInterpretabile: I dati hanno nomi di variabili leggibili sia da umani che dal computer, e sono presenti etichette di variabili e valori laddove necessario per facilitare l’interpretazione.\n\nAnalizzabile: Il dataset è in un formato rettangolare (righe e colonne), leggibile dal computer e conforme alle regole di base della struttura dei dati.\n\nUna volta completati i 14 passaggi precedenti, è possibile esportare questo dataset ripulito nella cartella processed per le successive analisi statistiche.\n\nUnire e/o aggiungere dati se necessario\n\nIn questo passaggio, è possibile unire o aggiungere colonne o righe presenti in file diversi. È importante eseguire nuovamente i controlli di validazione dopo l’unione/aggiunta di nuovi dati.\n\nTrasformare i dati se necessario\n\nEsistono vari motivi per cui potrebbe essere utile memorizzare i dati in formato long o wide. In questo passaggio, è possibile ristrutturare i dati secondo le esigenze.\n\nSalvare il dataset pulito finale\n\nL’ultimo passaggio del processo di pulizia consiste nell’esportare o salvare il dataset pulito. Come accennato in precedenza, può essere utile esportare/salvare il dataset in più di un formato di file (ad esempio, un file .csv e un file .parquet).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Flusso di lavoro per la pulizia dei dati</span>"
    ]
  }
]