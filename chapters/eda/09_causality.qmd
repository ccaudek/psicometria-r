# Causalità dai dati osservazionali {#sec-eda-causality}

::: callout-note
## In questo capitolo imparerai a

- affrontare il problema della causalità anche in assenza di esperimenti;
- riconoscere i quattro confondenti fondamentali (catena, biforcazione, collider, discendente);
- valutare con cautela le inferenze dai dati osservazionali, tenendo conto di debolezze e assunzioni.
:::

::: callout-tip
## Prerequisiti

- Leggere [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). Focalizzati sul capitolo 1 *The Golem of Prague*. 
- Leggere *Causal inference with observational data and unobserved confounding variables* di @byrnes2024causal.
- Leggere *Causal design patterns for data analysts* [@riedererdesignpatterns]. Questo post sul blog fornisce una panoramica di diversi approcci per fare affermazioni causali dai dati osservazionali.
- Leggere [The Effect: An Introduction to Research Design and Causality](https://theeffectbook.net). Focalizzati sul capitolo 10 *Treatment Effects*.
- Leggere [Causal Inference](https://mixtape.scunning.com/03-directed_acyclical_graphs) di Scott Cunningham. Focalizzati sul capitolo 3 *Directed Acyclic Graphs*.
- Leggere *Telling Stories with Data* [@alexander2023telling]. Concentrati sul capitolo 15 *Causality from observational data*.
:::

## Introduzione 

La pura osservazione dei dati può rivelare correlazioni e pattern nei dati, ma senza un'indagine sulle cause che stanno alla base di tali correlazioni, le conclusioni tratte possono essere fuorvianti o incomplete.

Richard McElreath, nel suo libro "Statistical Rethinking" [@McElreath_rethinking], utilizza l'analogia dei Golem - creature potenti ma prive di saggezza - per descrivere un approccio metodologico che è stato a lungo predominante in psicologia. Questo approccio si basa esclusivamente sull'analisi delle associazioni statistiche tra variabili, trascurando considerazioni più profonde sulla causalità.

Il metodo in questione si concentra principalmente sul test delle ipotesi nulle, senza stabilire una chiara connessione tra le domande di ricerca riguardanti relazioni causali e i test statistici impiegati. Questa disconnessione è evidente nella figura successiva, tratta da un manuale di analisi dati di impostazione frequentista, che illustra la procedura raccomandata dai sostenitori di questo approccio per descrivere le associazioni tra variabili. 

![Esempio di albero decisionale per la selezione di una procedura statistica appropriata. Iniziando dall'alto, l'utente risponde a una serie di domande riguardanti la misurazione e l'intento, arrivando infine al nome di una procedura. Sono possibili molti alberi decisionali simili. (Figura tratta da @McElreath_rethinking).](../../figures/mcelreath_golem.png){ width=95% }

È importante notare come tale procedura non fornisca strumenti utili per identificare le effettive cause sottostanti ai fenomeni osservati. Questa limitazione metodologica è stata identificata come uno dei fattori principali che hanno contribuito alla crisi di replicabilità nella ricerca psicologica, come approfondito nel @sec-crisis. L'approccio descritto, pur essendo potente nell'individuare correlazioni, manca della "saggezza" necessaria per distinguere tra semplici associazioni e vere relazioni causali, analogamente ai Golem della metafora di McElreath.

Un problema evidenziato da @McElreath_rethinking è che processi causali completamente distinti possono generare la stessa distribuzione di risultati osservati. Pertanto, un approccio focalizzato esclusivamente sull'analisi delle associazioni mediante il test dell'ipotesi nulla non è in grado di distinguere tra questi diversi scenari, come spiegato nel @sec-causal-inference-regr.

L'approccio frequentista, che si limita a descrivere le associazioni tra le variabili, ha una scarsa capacità di rilevare le caratteristiche cruciali dei fenomeni studiati e tende a produrre un alto tasso di falsi positivi [@van2023new]. È invece necessario utilizzare una metodologia che non si limiti a confutare ipotesi nulle, ma sia in grado di sviluppare modelli causali che rispondano direttamente alle domande di ricerca. In questo capitolo, ci concentreremo sull'introduzione dei concetti fondamentali dell'analisi causale.

## Cos'è la causalità?

@hardt2022patterns introducono il concetto di causalità distinguendo tra osservazione e azione. Ciò che vediamo nell'osservazione passiva è il modo in cui le persone seguono i loro comportamenti abituali, le loro inclinazioni naturali, proiettando lo stato del mondo su un insieme di caratteristiche che abbiamo scelto di evidenziare. Tuttavia, le domande più importanti spesso non riguardano semplici osservazioni.

- Non ci basta sapere che le persone che praticano regolarmente attività fisica soffrono meno d'ansia; vogliamo capire se l'attività fisica riduce effettivamente i livelli d'ansia.
- Non ci accontentiamo di osservare che chi segue una terapia cognitivo-comportamentale (CBT) presenta meno sintomi depressivi; desideriamo verificare se la CBT riduce realmente questi sintomi.
- Non ci limitiamo a constatare che l'uso frequente dei social media è associato a un calo del benessere mentale; vogliamo determinare se l'uso intensivo dei social media causa effettivamente una diminuzione del benessere mentale.

Alla base, il ragionamento causale è un quadro concettuale per affrontare domande sugli effetti di azioni o interventi ipotetici. Una volta compreso quale sia l'effetto di un'azione, possiamo invertire la domanda e chiederci quale azione plausibile abbia causato un determinato evento.

## Effetto Causale

Sebbene non esista una definizione univoca di causalità, possiamo concettualizzarla in modo pratico: diciamo che X causa Y se, intervenendo e modificando il valore di X (il trattamento), la distribuzione di Y cambia di conseguenza. Questa definizione sottolinea l'importanza cruciale dell'azione o dell'intervento nel determinare una relazione causale.

Quando X è una variabile binaria, rappresentante la presenza o l'assenza del trattamento, la conseguenza dell'intervento su X è denominata effetto medio del trattamento. Questo ci indica quanto il trattamento (azione X = 1) aumenta l'aspettativa di Y rispetto all'assenza di trattamento (azione X = 0).

È importante notare che gli effetti causali sono quantità relative alla popolazione. Si riferiscono a effetti mediati sull'intera popolazione in esame. Tuttavia, spesso l'effetto del trattamento può variare notevolmente da un individuo all'altro o tra gruppi di individui. In questi casi, parliamo di effetti di trattamento eterogenei.

Per chiarire questo concetto, consideriamo un esempio concreto: supponiamo che la terapia cognitivo-comportamentale (CBT) riduca l'ansia. Se un gruppo di persone ansiose non riceve alcun trattamento, i loro livelli d'ansia rimarranno presumibilmente invariati. Se invece interveniamo introducendo la CBT (modificando così il valore di X), i livelli d'ansia nel gruppo tenderanno a diminuire (cambiando quindi il valore di Y). Questo esempio illustra la distinzione tra semplice correlazione, basata sull'osservazione passiva, e causalità, che implica un'azione o un intervento.

La definizione di causalità può essere applicata anche per collegare variabili apparentemente distanti. Ad esempio, l'autoefficacia potrebbe non avere un effetto causale diretto sulle prestazioni accademiche. Tuttavia, se aumentiamo l'autoefficacia attraverso interventi mirati, è probabile che osserviamo un miglioramento nell'impegno allo studio. Questo aumento dell'impegno, a sua volta, tende a migliorare le prestazioni accademiche. Di conseguenza, possiamo affermare che l'autoefficacia influisce indirettamente sulle prestazioni accademiche attraverso una catena causale.

È importante precisare che esiste una relazione causale tra X e Y anche quando modificare X non porta necessariamente a un cambiamento immediato o deterministico in Y, ma altera la probabilità che Y si verifichi in un certo modo, modificando quindi la distribuzione di Y. Questa prospettiva probabilistica della causalità è particolarmente rilevante in campi come la psicologia, dove le relazioni tra variabili sono spesso complesse e influenzate da molteplici fattori.

### I Limiti dell'Osservazione

Per comprendere i limiti dell'osservazione passiva, e quindi la necessità di comprendere le relazioni causali sottostanti, @hardt2022patterns si riferiscono all'esempio storico delle ammissioni ai corsi di laurea dell'Università della California, Berkeley, nel 1973. In quell'anno, 12,763 candidati furono considerati per l'ammissione in uno dei 101 dipartimenti o major interdipartimentali. Di questi, 4,321 erano donne e 8,442 erano uomini. I dati mostrano che circa il 35% delle donne fu ammesso, rispetto al 44% degli uomini. Test di significatività statistica indicano che questa differenza non è attribuibile al caso, suggerendo una disparità nei tassi di ammissione tra i generi.

Una tendenza simile si osserva quando si analizzano le decisioni aggregate di ammissione nei sei maggiori dipartimenti. Il tasso di ammissione complessivo per gli uomini era di circa il 44%, mentre per le donne era solo il 30%, un'altra differenza statisticamente significativa. Tuttavia, poiché i dipartimenti hanno autonomia nelle loro decisioni di ammissione, è utile esaminare il possibile bias di genere a livello di singolo dipartimento.

**Uomini**

| Dipartimento | Candidati | Ammessi (%) |
|--------------|-----------|-------------|
| A            | 825       | 62          |
| B            | 520       | 60          |
| C            | 325       | 37          |
| D            | 417       | 33          |
| E            | 191       | 28          |
| F            | 373       | 6           |

**Donne**

| Dipartimento | Candidati | Ammessi (%) |
|--------------|-----------|-------------|
| A            | 108       | 82          |
| B            | 25        | 68          |
| C            | 593       | 34          |
| D            | 375       | 35          |
| E            | 393       | 24          |
| F            | 341       | 7           |

Dall'osservazione di questi dati, emerge che quattro dei sei maggiori dipartimenti mostrano un tasso di ammissione più elevato per le donne, mentre due mostrano un tasso più elevato per gli uomini. Tuttavia, questi due dipartimenti non possono giustificare la sostanziale differenza nei tassi di ammissione osservata nei dati aggregati. Questo suggerisce che la tendenza generale di un tasso di ammissione più alto per gli uomini sembra invertita quando i dati sono disaggregati per dipartimento.

Questo fenomeno è noto come *paradosso di Simpson*, un paradosso statistico in cui una tendenza che appare in sottopopolazioni si inverte o scompare quando i dati vengono aggregati. Nel contesto attuale, il paradosso di Simpson si manifesta nel fatto che, mentre i dati aggregati sembrano indicare una discriminazione di genere contro le donne, l'analisi dei dati disaggregati per dipartimento rivela che in alcuni casi le donne sono favorite in termini di ammissioni.

La domanda fondamentale è se questi dati indicano effettivamente un problema di discriminazione di genere o se, come suggerito dallo studio originale, il bias di genere nelle ammissioni fosse principalmente dovuto al fatto che "le donne sono indirizzate dalla loro socializzazione e istruzione verso campi di studio generalmente più affollati, meno produttivi in termini di completamento dei diplomi, meno finanziati e che spesso offrono prospettive professionali peggiori." In altre parole, il problema risiederebbe in differenze sistemiche e strutturali tra i campi di studio scelti dalle donne e quelli scelti dagli uomini.

Il paradosso di Simpson crea disagio proprio perché l'intuizione suggerisce che una tendenza valida per tutte le sottopopolazioni dovrebbe esserlo anche a livello aggregato. Tuttavia, questo paradosso evidenzia un errore comune nell'interpretazione delle probabilità condizionate: confondere l'osservazione passiva con l'analisi causale. I dati che abbiamo rappresentano solo un'istantanea del comportamento normale di uomini e donne che si candidavano per l'ammissione a UC Berkeley nel 1973.

Non possiamo trarre conclusioni definitive da questi dati. Possiamo solo riconoscere che l'analisi iniziale solleva ulteriori domande, come ad esempio la necessità di progettare nuovi studi per raccogliere dati più completi, che potrebbero portare a conclusioni più definitive. In alternativa, potremmo discutere su quale scenario sia più verosimile in base alle nostre convinzioni e alle notre ipotesi sul mondo.

L'inferenza causale può essere utile in entrambi i casi. Da un lato, può guidare la progettazione di nuovi studi, aiutandoci a scegliere quali variabili includere, quali escludere e quali mantenere costanti. Dall'altro, i modelli causali possono fungere da meccanismo per incorporare le conoscenze scientifiche del dominio e passare da ipotesi plausibili a conclusioni plausibili.

## Variabili Confondenti

Sebbene gli esperimenti controllati siano considerati il metodo più affidabile per identificare relazioni causali, molte domande di ricerca non possono essere affrontate sperimentalmente a causa di vincoli etici o pratici. In tali situazioni, i ricercatori si rivolgono a disegni osservazionali, che garantiscono maggiore flessibilità e adattabilità ai contesti reali. Tuttavia, l’utilizzo di dati osservazionali introduce una sfida cruciale: la difficoltà di stabilire conclusioni causali solide e affidabili.

Al centro di questa complessità si trovano le **variabili confondenti**. Una variabile confondente è presente quando l'associazione osservata tra due variabili, X e Y, non riflette accuratamente la vera relazione causale tra di esse. In altre parole, la variabile confondente influenza sia X che Y, creando l'apparenza di una relazione diretta tra le due che potrebbe essere fuorviante o inesatta.

Negli studi osservazionali, se le variabili confondenti non vengono misurate e controllate adeguatamente, possono distorcere le stime degli effetti causali, introducendo bias nei risultati e impedendo di riflettere il vero valore dell'effetto. In pratica, la presenza di variabili confondenti può portare a conclusioni errate quando si confrontano semplicemente i risultati osservati in diversi gruppi. Ciò che si osserva nei dati potrebbe non corrispondere a ciò che accadrebbe se si potesse manipolare direttamente la variabile di interesse in un esperimento controllato.

### Approcci per il Controllo delle Variabili Confondenti

Un approccio apparentemente semplice per affrontare questo problema potrebbe essere quello di controllare statisticamente tutte le variabili confondenti. Questo metodo prevede di stimare l'effetto di X su Y separatamente in ogni segmento della popolazione definito da una condizione Z = z per ogni possibile valore di z. Successivamente, si calcola la media di questi effetti stimati nelle sottopopolazioni, ponderandoli per la probabilità di Z = z nella popolazione. Tuttavia, questo metodo presenta due difficoltà fondamentali:

1. **Conoscenza di tutte le variabili confondenti**: Richiede la conoscenza di tutte le possibili variabili confondenti.
2. **Misurazione delle variabili confondenti**: Richiede la capacità di misurare ciascuna di esse, cosa che spesso non è praticabile.

Il controllo delle variabili confondenti è cruciale per stabilire relazioni causali, poiché permette di isolare gli effetti delle variabili indipendenti da quelli delle variabili confondenti che potrebbero influenzare le variabili dipendenti. Esistono due principali metodologie di controllo:

1. **Controllo Sperimentale**: Implementato attraverso il disegno sperimentale e basato principalmente sulla randomizzazione.
2. **Controllo Statistico**: Applicato durante l'analisi dei dati, con l'obiettivo di neutralizzare o quantificare l'influenza delle variabili estranee.

### Inferenza Causale nei Dati Osservazionali

A causa di queste difficoltà, l'inferenza causale basata su dati osservazionali è spesso considerata problematica, dando origine al famoso detto "la correlazione non implica causalità". 

Anche se il metodo preferito per l'inferenza causale in molte scienze è l'esperimento controllato e randomizzato, tali esperimenti non sono, e non sono mai stati, lo stile dominante di inferenza causale nelle scienze umane. Una ragione è che molte delle domande più importanti sul comportamento umano non possono essere studiate sperimentalmente. Alla fine, il comportamento deve essere studiato negli ambienti naturali che le persone costruiscono, poiché questi ambienti sono sia cause che conseguenze del comportamento. Senza studiare le persone nelle loro comunità, non sapremmo nemmeno cosa stiamo cercando di spiegare.

L'inferenza causale è possibile anche in contesti osservazionali, e gli statistici hanno sviluppato metodi specializzati per progettare analisi che affrontano specifici obiettivi di inferenza causale [@pearl_2016]. 

### Strumenti Moderni per l'Analisi Causale

L'obiettivo dell'analisi causale moderna è proprio quello di fornire gli strumenti concettuali e metodologici per affrontare queste sfide. Attraverso l'uso di tecniche avanzate come i modelli causali strutturali, i grafi aciclici diretti (DAG) e i metodi di identificazione degli effetti causali, i ricercatori possono spesso superare le limitazioni dei dati osservazionali e trarre conclusioni causali più robuste.

In sintesi, mentre le variabili confondenti rappresentano una sfida significativa nell'analisi dei dati osservazionali, i progressi metodologici nell'analisi causale offrono strumenti potenti per mitigare questi problemi e migliorare l'affidabilità delle inferenze causali.

## Modelli Causali Strutturali

I modelli causali sono strumenti essenziali per l'analisi dei dati osservazionali, poiché consentono di rappresentare il processo sottostante a un fenomeno e di prevedere gli effetti di un intervento. Questi modelli non solo permettono di anticipare le conseguenze di una causa, ma offrono anche la possibilità di esplorare scenari controfattuali, immaginando esiti alternativi che si sarebbero potuti verificare in presenza di decisioni diverse.

Un modello causale strutturale (*Structural Causal Model*, SCM) è un approccio che rappresenta le relazioni causali tra variabili. Esso si basa su una serie di assegnazioni che, partendo da variabili di rumore indipendenti (note anche come variabili esogene), generano una distribuzione di probabilità congiunta.

Le variabili di rumore indipendenti svolgono un ruolo cruciale negli SCM. Esse rappresentano fonti di incertezza o variabilità all'interno del sistema e non sono influenzate da altre variabili del modello. Queste variabili sono mutuamente indipendenti, il che significa che il loro valore non fornisce informazioni sul valore delle altre.

La costruzione di un SCM segue una sequenza specifica: si parte dalle variabili di rumore indipendenti, si applicano una serie di assegnazioni che descrivono gli effetti causali delle variabili esogene su altre variabili, e si genera progressivamente un insieme di variabili casuali che dà origine a una distribuzione congiunta.

Il principale vantaggio di un SCM risiede nella sua duplice natura: da un lato, fornisce una distribuzione di probabilità congiunta delle variabili, e dall'altro, descrive il processo generativo che porta alla formazione di tale distribuzione, partendo dalle variabili di rumore elementari.

Questa struttura consente non solo di modellare le relazioni probabilistiche tra le variabili, ma anche di rappresentare in modo esplicito i meccanismi causali che le governano.

I SCM possono essere rappresentati graficamente attraverso Grafi Aciclici Direzionati (*Directed Acyclic Graphs*, DAG). Questi DAG visualizzano le relazioni causali tra le variabili all'interno di un SCM, facilitando l'identificazione delle variabili confondenti e il loro impatto sull'analisi.

## Bias da Variabile Omessa

Possiamo introdurre i DAG facendo riferiento al *bias da variabile omessa* (*Omitted Variable Bias*, o OVB; @wilms2021omitted). Come discusso da @byrnes2024causal, l'omissione dall'analisi statistica di variabili confondenti note ma non misurate, o sconosciute e non misurate, può portare a stime errate della magnitudine degli effetti, errori nel segno delle stime (stimatori distorti), correlazioni spurie, e al mascheramento delle vere relazioni causali.

Un illustrazione di questa situazione è fornita nella @fig-byrnes-dee-1. La figura mostra tre DAG che illustrano diversi scenari in cui le variabili non osservate non influenzano i risultati del modello o potrebbero creare problemi a causa della confusione. Una variabile di risposta di interesse (Y) è causata sia da una variabile misurata (X) che da una variabile non misurata (U). Nel pannello di sinistra, la variabile non osservata (U) non è una variabile confondente. Nel pannello centrale, la variabile non osservata (U) è una variabile confondente e causa il bias da variabile omessa. Nel pannello di destra la variabile non osservata (U) causa il bias da variabile omessa in maniera indiretta.

::: {#fig-byrnes-dee-1}
![](../../figures/byrnes_dee_fig1.png){width="80%"}

Nel pannello di sinistra, X e U sono non correlate, quindi la mancata inclusione di U in un modello statistico aumenterebbe l'errore standard della stima (riducendo la precisione del modello) ma non porterebbe a bias nella stima dell'effetto di X su Y. Tuttavia, se U influenza anche X come nel pannello centrale, o se U e X sono influenzati da un fattore comune Z come nel pannello di destra, allora omettere U da un modello statistico causa il bias da variabile omessa nella stima dell'effetto di X su Y. I casi illustrati dal pannello centrale e dal pannello di destra sono esempi di sistemi in cui le cause comuni di confusione (U e Z rispettivamente) devono essere controllate per effettuare inferenze causali non distorte (la figura è ispirata da @byrnes2024causal).
:::

Affrontare i problemi creati dalle variabili confondenti non misurate rappresenta una sfida primaria nell'inferenza causale dai dati osservazionali. A differenza dell'errore di misurazione nelle variabili predittive, che produce un bias costante verso lo zero e può essere corretto o modellato [@McElreath_rethinking; @schennach2016recent], con l'OVB non possiamo conoscere la grandezza o la direzione del bias senza conoscere tutte le possibili variabili confondenti e le loro relazioni nel sistema.

Nonostante queste sfide, non è necessario abbandonare l'uso dei dati osservazionali per l'inferenza causale in psicologia. È invece necessario ricorrere all'adozione delle tecniche dei SCM per potere comunque svolgere l'inferenza causale. 

È evidente che questo approccio porterà a conclusioni inevitabilmente parziali, destinate ad essere perfezionate da studi successivi. Tuttavia, tale metodologia offre il vantaggio di esplicitare il "modello generativo dei dati", ovvero la struttura causale sottostante ai fenomeni psicologici oggetto di studio.

I progressi nella ricerca empirica conducono a una maggiore comprensione e, di conseguenza, a modifiche nelle ipotesi sui meccanismi causali. Questo processo rappresenta un'evoluzione della conoscenza scientifica. Tale sviluppo è reso possibile proprio perché le ipotesi causali sono formulate in termini di modelli formali, che descrivono in modo preciso i meccanismi ipotizzati.

Al contrario, limitarsi alla mera descrizione delle associazioni tra variabili non consente questo tipo di avanzamento conoscitivo. La formulazione di modelli causali espliciti permette infatti di testare, raffinare e, se necessario, rivedere le ipotesi sui meccanismi sottostanti ai fenomeni osservati, portando a una comprensione più profonda e dinamica dei processi psicologici.

## Grafi Aciclici Diretti

I DAG sono uno strumento fondamentale per l'inferenza causale, offrendo una rappresentazione visiva delle relazioni causali ipotizzate tra variabili. Questi grafi sono definiti "diretti" perché le variabili, rappresentate da nodi, sono collegate da frecce orientate anziché da semplici linee. Sono inoltre chiamati "aciclici" poiché non è possibile tornare a un nodo di partenza seguendo il percorso delle frecce.

In un DAG, una freccia che va da X a Y indica un'influenza probabilistica di X su Y. La terminologia delle relazioni all'interno del grafo è importante: il nodo di origine di una freccia è chiamato "genitore", mentre il nodo di destinazione è detto "figlio". Quando è possibile raggiungere un nodo B partendo da un nodo A seguendo una successione di frecce, A è definito "antenato" di B, e B è considerato "discendente" di A.

I DAG consentono di distinguere chiaramente tra cause dirette e indirette. Una causa diretta è rappresentata da un nodo genitore, mentre una causa indiretta può essere qualsiasi antenato di un nodo nel grafo causale. Questa struttura permette di differenziare efficacemente causa ed effetto basandosi sulla posizione relativa dei nodi all'interno del grafo, ovvero se un nodo è antenato o discendente di un altro.

Questi grafi sono particolarmente utili per identificare variabili confondenti, basandosi sulla teoria sviluppata da Judea Pearl [@pearl2009causality]. È cruciale rappresentare in un DAG tutte le possibili relazioni causali, poiché l'assenza di una freccia tra due nodi implica la certezza dell'assenza di una relazione causale diretta tra le variabili corrispondenti.

Nella teoria dei DAG, due concetti fondamentali sono la d-separazione e il criterio del back-door. 

### La d-separazione  

La *d-separazione* ci aiuta a determinare quando due variabili in un grafo causale sono indipendenti condizionatamente a un insieme di altre variabili. Questo concetto è cruciale per comprendere come l'informazione o l'influenza si propaga tra le variabili in un modello causale.

In termini più semplici, la d-separazione ci permette di identificare se esiste un "blocco" nel flusso di informazioni tra due variabili, dato un certo insieme di altre variabili (che chiameremo Λ). Quando due variabili sono d-separate da Λ, significa che non c'è flusso di informazioni tra di loro, condizionatamente a Λ.

Per comprendere meglio la d-separazione, consideriamo tre situazioni principali che possono verificarsi in un DAG:

1. Catena (X → Z → Y):
   In questo caso, Z è un mediatore tra X e Y. Se Z appartiene all'insieme Λ (cioè, se controlliamo o condizioniamo su Z), blocchiamo il flusso di informazioni da X a Y attraverso questo percorso. Per esempio, se X è "esercizio fisico", Z è "pressione sanguigna" e Y è "rischio di malattie cardiache", controllando per la pressione sanguigna (Z) blocchiamo il percorso attraverso il quale l'esercizio fisico influenza il rischio di malattie cardiache.

2. Fork (X ← Z → Y):
   Qui, Z è una causa comune sia di X che di Y. Se Z appartiene a Λ, blocchiamo la correlazione spuria tra X e Y che deriva dalla loro causa comune. Per esempio, se Z è "status socioeconomico", X è "livello di istruzione" e Y è "stato di salute", controllando per lo status socioeconomico (Z) eliminiamo la correlazione apparente tra istruzione e salute che potrebbe derivare dal fatto che entrambe sono influenzate dallo status socioeconomico.

3. Collider (X → Z ← Y):
   In questa situazione, Z è un effetto comune di X e Y. Sorprendentemente, se né Z né i suoi discendenti appartengono a Λ, il percorso è già bloccato. Controllare per Z (o i suoi discendenti) in realtà aprirebbe un percorso tra X e Y, creando una correlazione spuria. Per esempio, se X è "intelligenza", Y è "bellezza" e Z è "successo in una carriera di attore", controllare per il successo nella carriera di attore (Z) creerebbe una correlazione apparente tra intelligenza e bellezza, anche se queste potrebbero essere indipendenti nella popolazione generale.

In sintesi, la d-separazione ci permette di determinare, dato un certo insieme di variabili Λ, se due variabili X e Y sono indipendenti condizionatamente a Λ. Questo ci aiuta a identificare quali variabili dobbiamo controllare (e quali non dobbiamo controllare) per ottenere stime causali non distorte, facilitando così l'inferenza causale corretta. La d-separazione è quindi uno strumento potente che ci permette di leggere le indipendenze condizionali direttamente dal grafo, senza dover fare calcoli probabilistici complessi. 

### Il criterio del back-door  

Il criterio del back-door consente di identificare un insieme di variabili che, se controllate adeguatamente, permettono di stimare gli effetti causali in modo non distorto. L'obiettivo principale di questo criterio è eliminare l'influenza di percorsi non causali tra la variabile di esposizione (causa potenziale) e l'outcome (effetto), mantenendo aperto solo il percorso causale diretto di interesse.

In questo contesto, due variabili sono considerate "confuse" se esiste tra di esse un percorso di tipo back-door. Un back-door path da X a Y è definito come qualsiasi percorso che inizia da X con una freccia entrante in X. Per esempio, consideriamo il seguente percorso:

X ← A → B ← C → Y

In questo caso, il percorso rappresenta un flusso di informazioni da X a Y che non è causale, ma potrebbe creare l'apparenza di una relazione causale.

Per "deconfondere" una coppia di variabili, è necessario selezionare un insieme di variabili (chiamato back-door set) che "blocchi" tutti i back-door paths tra i due nodi di interesse. Il blocco di questi percorsi avviene in modi diversi a seconda della struttura del percorso:

1. Un back-door path che coinvolge una catena di variabili (ad esempio, A → B → C) può essere bloccato controllando per la variabile intermedia (in questo caso, B).

2. Un percorso che coinvolge un "collider" (una variabile che riceve frecce da entrambe le direzioni, come in A → B ← C) è naturalmente bloccato e non permette il flusso di informazioni.

È importante notare che bisogna prestare attenzione a non aprire involontariamente un flusso di informazioni attraverso un collider. Questo può accadere se si condiziona l'analisi sul collider stesso o su un suo discendente, il che potrebbe erroneamente aprire il percorso e introdurre bias nell'analisi.

::: {.callout-caution}
## Punti chiave

- Il criterio del back-door aiuta a identificare il set minimale di variabili da controllare.
- Non tutte le variabili associate sia all'esposizione che all'outcome devono essere controllate; solo quelle che creano percorsi back-door.
- In alcuni casi, potrebbe non essere necessario controllare alcuna variabile (se non ci sono percorsi back-door aperti).
- In altri casi, potrebbe essere impossibile bloccare tutti i percorsi back-door con le variabili disponibili, indicando che l'effetto causale non può essere identificato con i dati a disposizione.

Utilizzando il criterio del back-door in combinazione con i DAG, i ricercatori possono fare scelte più informate su quali variabili includere nelle loro analisi, migliorando così la validità delle loro inferenze causali.
:::

### Applicazioni

Consideriamo nuovamente la struttura causale illustrata nella @fig-byrnes-dee-1, pannello centrale. 
Dopo aver costruito un DAG come descritto nella sezione precedente, è possibile identificare le potenziali fonti di bias da variabili omesse, inclusi i confondenti non misurati (ad esempio, U). Non controllare per le variabili confondenti apre una “back-door” permettendo alla variazione confondente di influenzare la relazione tra la variabile causale e la variabile di risposta di interesse attraverso un percorso non valutato [@pearl2009causality]. In altre parole, omettere una variabile confondente come U nella @fig-byrnes-dee-1 (pannello centrale) in un'analisi statistica significa che questa viene incorporata nel termine di errore del modello statistico, insieme alle fonti di errore casuali. La @fig-byrnes-dee-2 illustra le conseguenze di un confondente U che ha un effetto positivo su X ma un effetto negativo su Y. Se adattiamo un modello come mostrato nella @fig-byrnes-dee-2 bi, l'effetto stimato di X su Y è positivo quando si controlla per U. Tuttavia, se non si controlla per U, come mostrato nella @fig-byrnes-dee-2 bii, U viene incorporato nel termine di errore, inducendo una correlazione tra l'errore e X, come illustrato nella @fig-byrnes-dee-2 biii, portando a una stima errata. Pertanto, il termine di errore del modello e X risultano correlati, il che viola un'assunzione fondamentale dei modelli lineari (ovvero, il teorema di Gauss-Markov; Abdallah et al., 2015; Antonakis et al., 2010). Questo produce una stima errata, evidenziata in blu.

::: {#fig-byrnes-dee-2}

![](../../figures/byrnes_dee_fig2.png){width="100%"}

**Una visualizzazione del bias da variabile omessa e delle conseguenze per l'inferenza causale.** (A) mostra un DAG di un sistema in cui X ha un effetto positivo su Y, e una variabile confondente U ha un effetto positivo su Y ma un effetto negativo su X. Le variabili non osservate (cioè non misurate) sono rappresentate in ellissi, come la variabile U e il termine di errore e nel pannello B. (B) illustra diverse stime del DAG in (A) utilizzando un'analisi del percorso. Vedi Box 1 per una breve spiegazione delle principali differenze tra DAG e diagrammi dei percorsi. Presumiamo che U non sia misurata. In (Bi), presumiamo di poter misurare e controllare U, rappresentata dalla freccia a doppia testa tra U e X, che rappresenta la correlazione tra le due variabili considerata dal modello. La variabile non misurata e è la fonte residua di variazione che si presume non sia correlata con nessun predittore. La freccia rossa rappresenta il percorso stimato. Al contrario, (Bii) e (Biii) rappresentano la realtà, dove non abbiamo una misurazione di U e non la controlliamo nel modello dei percorsi. Il ricercatore pensa di adattare il modello in (Bii) ma in realtà sta adattando il modello in (Biii), dove il termine di errore non è solo e, ma la somma di e e la variazione dovuta alla variabile omessa U. A causa di ciò, c'è un percorso diretto dal termine di errore del modello a X (e quindi X è endogeno). (C) mostra le relazioni stimate risultanti dai modelli in (Bi) rispetto a (Bii). Le linee rappresentano la relazione stimata tra X e Y dai rispettivi modelli. La linea rossa è la vera relazione causale, stimata da (Bi), mentre la linea blu contiene il bias da variabile omessa, poiché non si tiene conto della variabile confondente U, come stimato dal modello in Bii/Biii (Figura tratta da @byrnes2024causal).
:::

## Le Pratiche Scientifiche per Inferire la Causalità

Come visto in precedenza, in qualsiasi sistema complesso, consideriamo due variabili, *X* e *Y*. Sebbene possiamo osservare la loro distribuzione di probabilità congiunta $p(x, y)$, stabilire se *X* causa *Y*, *Y* causa *X*, o se una terza variabile *Z* influenzi entrambe, rimane un problema complesso (Salmon, 1984). Questa difficoltà è al centro del dibattito filosofico sulla causalità, ma è anche il motore che spinge lo sviluppo di metodi scientifici per decifrare il funzionamento del mondo. Ad esempio, per capire se una terapia cognitivo-comportamentale (TCC) riduce effettivamente i sintomi di ansia, se la lettura di un articolo scientifico influenza le opinioni dei lettori riguardo a un tema controverso, o se l'attivazione di un neurone in una specifica area cerebrale influenza il comportamento o l'attività di un altro neurone, dobbiamo affrontare queste questioni con rigore scientifico.

### Il Framework Interventista di Judea Pearl

Come discusso sopra, il lavoro di Judea Pearl ha rivoluzionato la formalizzazione dell'inferenza causale. Mettiamo qui in evidenza il ruolo che l'intervento attivo ha nella comprensione della causalità [ricordiamo il punto di vista di @hardt2022patterns descritto in precedenza]. Nella tradizionale probabilità bayesiana, consideriamo $p(y \mid x)$ — la probabilità di *Y* dato *X*. Tuttavia, Pearl ha introdotto l'operatore *"do"*, che permette di esprimere il risultato di un intervento attivo, scritto come $p(y \mid do(x))$. Questo rappresenta la probabilità di *Y* quando *X* viene impostato artificialmente a un certo valore (Pearl, 2009). Questo approccio si concentra sugli interventi piuttosto che sulle sole osservazioni, permettendo di isolare l'effetto causale di *X* su *Y* senza necessariamente conoscere i meccanismi sottostanti.

Per esempio, supponiamo di voler studiare se la terapia cognitivo-comportamentale (TCC) riduce i sintomi di ansia. Osservare una correlazione tra la partecipazione alla TCC e il miglioramento dei sintomi non è sufficiente per stabilire una relazione causale. Utilizzando l'operatore *"do"*, possiamo simulare un intervento ipotetico: "Cosa accadrebbe se forzassimo tutti i pazienti a seguire la TCC?" Questo approccio ci permette di isolare l'effetto della TCC sui sintomi di ansia, eliminando l'influenza di fattori confondenti come lo stato socioeconomico o la predisposizione genetica.

### Tre Fonti di Conoscenza Causale Pragmatica

Alla luce delle considerazioni precedenti, possiamo individuare tre fonti di conoscenza che ci informano sui meccanismi causali.

#### Esperimenti Randomizzati Controllati (RCT)

Gli RCT sono considerati lo standard aureo per stimare gli effetti causali. Assegnando casualmente i partecipanti a gruppi di trattamento e controllo, gli RCT minimizzano l'influenza di variabili confondenti, fornendo stime non distorte dell'effetto di un intervento. Ad esempio, numerosi esperimenti dimostrano che attivare un interruttore causa l'accensione di una luce:

$$
p(\text{luce accesa}|do(\text{interruttore acceso})) \approx 1, \quad p(\text{luce spenta}|do(\text{interruttore spento})) \approx 1
$$

Questo approccio è particolarmente utile in psicologia, dove fattori confondenti possono facilmente distorcere i risultati.

Per esempio, immaginiamo uno studio in cui alcuni volontari vengono assegnati casualmente a dormire 8 ore o a restare svegli tutta la notte prima di un test di memoria. Gli RCT permettono di stabilire se il sonno causa un miglioramento delle prestazioni cognitive, eliminando l'influenza di fattori come l'età o lo stress.

#### Comprensione Specifica del Dominio

In discipline come la psicologia, le neuroscienze o l'elettronica, la conoscenza specialistica consente agli esperti di basarsi su principi consolidati per formulare ipotesi causali e progettare esperimenti significativi. Ad esempio, anche se un semplice movimento della mano non causa direttamente l'accensione di una luce, la nostra comprensione dei circuiti elettrici ci informa che chiudere un interruttore completa un circuito, producendo luce. Queste intuizioni, basate su conoscenze approfondite del dominio, raffinano i nostri modelli causali e migliorano la progettazione sperimentale. Tale comprensione deriva spesso da esperimenti randomizzati precedenti e permette l'uso di quasi-esperimenti per produrre risultati causali convincenti.

Un esempio rilevante in psicologia e neuroscienze è il ruolo dell'amigdala nella risposta alla paura. Grazie a studi clinici e sperimentali, sappiamo che l'amigdala è una struttura cerebrale chiave nel processamento delle emozioni, in particolare della paura. Pazienti con danni all'amigdala, ad esempio, mostrano una ridotta capacità di riconoscere espressioni facciali di paura e di rispondere a stimoli minacciosi. Questa conoscenza specifica del dominio ci permette di formulare ipotesi causali precise, come "L'attività nell'amigdala influenza la risposta emotiva alla paura."

Un altro esempio è il legame tra l'area di Broca e la produzione del linguaggio. Studi clinici su pazienti con lesioni in questa area hanno dimostrato che danni specifici portano a deficit nella produzione del linguaggio, un disturbo noto come *afasia di Broca*. Questa conoscenza deriva da decenni di ricerca neuroscientifica e ci permette di affermare con sicurezza che "L'attività nell'area di Broca è necessaria per la produzione fluente del linguaggio."

La comprensione specifica del dominio non solo aiuta a formulare ipotesi causali, ma guida anche la progettazione degli esperimenti. Ad esempio, se vogliamo studiare l'effetto di un farmaco sulla riduzione dell'ansia, la conoscenza dei meccanismi neurobiologici alla base dell'ansia (come il ruolo del sistema limbico e dei neurotrasmettitori come la serotonina) ci permette di identificare i parametri rilevanti da misurare e di controllare per possibili fattori confondenti. Questo approccio è particolarmente utile quando non è possibile condurre esperimenti randomizzati, come nel caso di studi su pazienti con condizioni cliniche specifiche.

In sintesi, la comprensione specifica del dominio è un pilastro fondamentale per l'inferenza causale in psicologia e neuroscienze. Essa non solo fornisce le basi per formulare ipotesi precise, ma migliora anche la qualità e l'affidabilità della ricerca, permettendo di trarre conclusioni causali anche in contesti complessi e non sperimentali.

#### Testimonianze Scientifiche e Prove Cumulative

L'avanzamento della conoscenza scientifica dipende dalla validazione collettiva dei risultati. Quando numerosi studi indipendenti e randomizzati giungono a conclusioni causali simili, la comunità scientifica acquisisce una comprensione pragmatica, se non definitiva, dei meccanismi causali sottostanti. Questo approccio si giustifica grazie ai successi passati nella descrizione della causalità.

Per esempio, numerosi studi indicano che le relazioni sociali positive aumentano il benessere psicologico. Benché nessuno studio singolo possa offrire una prova definitiva, l'accumulo di prove da diverse fonti conferma l'esistenza di un legame causale tra socializzazione e benessere.

### Conciliare Pragmatismo con Scetticismo Filosofico

Nonostante i metodi sperimentali, come gli esperimenti randomizzati controllati (RCT), e gli strumenti teorici, come l'operatore "do" introdotto da Judea Pearl, abbiano dimostrato una grande efficacia nella pratica scientifica, essi non risolvono completamente le sfide epistemologiche sollevate da filosofi come David Hume. Hume ha argomentato che la causalità non è qualcosa che possiamo osservare direttamente; piuttosto, è un concetto inferito attraverso l'esperienza. Questo scetticismo radicale ci ricorda che la nostra comprensione causale del mondo è sempre mediata da assunzioni e interpretazioni.

Tuttavia, ciò non significa che dobbiamo abbandonare il concetto di causalità. Al contrario, i metodi scientifici moderni forniscono uno strumento pragmatico estremamente potente per navigare nei sistemi complessi e fare previsioni affidabili. Secondo Woodward (2003), la causalità in scienza non deve essere vista come una finestra diretta ipotetiche verità metafisiche, ma piuttosto come uno strumento pratico per manipolare il mondo e prevedere i risultati delle nostre azioni. Questa prospettiva pragmatica ci permette di superare le limitazioni teoriche e concentrarci sui risultati concreti.

Consideriamo due esempi concreti.

1. Ogni volta che saliamo su un aereo, ci affidiamo implicitamente alle leggi della fisica e all'ingegneria aeronautica. Sappiamo, grazie a studi rigorosi e prove empiriche, che le forze aerodinamiche e i principi di propulsione consentono agli aerei di volare. Questa fiducia non deriva da una certezza assoluta, ma dalla robustezza delle evidenze accumulate nel tempo. La causalità tra il disegno dell'aereo e il suo funzionamento è stata testata e confermata milioni di volte, rendendo questo mezzo di trasporto sicuro e affidabile.

2. Analogamente, quando scegliamo di seguire un trattamento psicologico, come la terapia cognitivo-comportamentale (TCC), lo facciamo basandoci su una vasta quantità di ricerche scientifiche che dimostrano l'efficacia di tali interventi. Gli studi randomizzati hanno mostrato che la TCC modifica i processi mentali e comportamentali, riducendo in maniera rilevante i sintomi di ansia e depressione. Anche qui, la nostra fiducia non deriva da una prova definitiva della causalità, ma da un corpus consistente di evidenze empiriche.

La chiave del successo del pragmatismo scientifico sta nella sua capacità di fornire risultati **misurabili** e **replicabili**. Non dobbiamo risolvere il problema filosofico della causalità per utilizzarla efficacemente. Ciò che conta è la sua applicabilità pratica: se un'intervento produce risultati coerenti e prevedibili, allora possiamo considerarlo causa di quei risultati, almeno per tutti gli scopi pratici. Questo approccio ci permette di progredire senza rimanere bloccati dalle domande metafisiche irrisolte.

## Riflessioni Conclusive

Il dibattito filosofico sulla natura della causalità rimane un campo ricco e dinamico, ma i progressi metodologici degli ultimi decenni hanno fornito strumenti pratici e robusti per affrontare le complessità dell'inferenza causale.  Esperimenti randomizzati controllati (RCT), diagrammi causali (DAG) e l’operatore “do” introdotto da Judea Pearl rappresentano esempi concreti di come, pur senza pretendere di risolvere le questioni metafisiche sulla natura ultima della causalità, la ricerca empirica sia in grado di offrire modelli affidabili per comprendere e intervenire su fenomeni complessi.

I DAG si distinguono come uno degli strumenti più potenti per rappresentare e analizzare le relazioni causali. Essi mettono in luce le influenze dirette e indirette tra variabili, rendendo esplicite le assunzioni alla base di ogni ricerca. Tuttavia, la loro efficacia dipende in modo cruciale dalla qualità delle conoscenze del dominio: senza una comprensione approfondita del contesto, anche il DAG più raffinato rischia di omettere variabili rilevanti o di rappresentare in modo fuorviante i rapporti causali. Nonostante ciò, i DAG promuovono la **trasparenza scientifica**, poiché rendono visibile il processo decisionale e aiutano a identificare potenziali fonti di bias, contribuendo a una maggiore chiarezza metodologica.

L’approccio pragmatico, che accetta di lavorare con concetti causali anche in assenza di risposte definitive ai grandi interrogativi metafisici, è al cuore del progresso scientifico. La nostra fiducia nel viaggiare in aereo o nel ricorrere a terapie come quella cognitivo-comportamentale si basa sull’evidenza empirica accumulata e confermata innumerevoli volte, non su una certezza assoluta. Questo pragmatismo non è solo utile, ma indispensabile per la ricerca scientifica e per la vita quotidiana.

Tuttavia, strumenti come i DAG non sono infallibili. Se le loro premesse sono scorrette o incomplete, le inferenze causali che ne derivano possono risultare distorte. Per mantenere la credibilità di queste tecniche, è essenziale un *rigoroso controllo metodologico*, accompagnato da una *riflessione critica* costante sulle assunzioni sottostanti. Solo questa vigilanza epistemologica permette di evitare semplificazioni eccessive e di confrontarsi in modo costruttivo con i limiti intrinseci dell’inferenza causale.

In definitiva, l’indagine causale contemporanea richiede un bilanciamento tra *pragmatismo* — che valorizza e applica i risultati empirici in modo concreto — e *scetticismo filosofico*, che stimola un’analisi continua delle nostre ipotesi e premesse. Questa prospettiva equilibrata è essenziale per l'avanzamento delle conoscenze. Pur rimanendo consapevoli che la “vera natura” della causalità possa restare, almeno in parte, un mistero irrisolvibile, possiamo comunque costruire strumenti sufficientemente robusti per prevedere con accuratezza i fenomeni di interesse e prendere decisioni informate. È questa combinazione di umiltà epistemologica e fiducia pragmatica a permetterci di avanzare nella comprensione del mondo, pur senza pretendere di esaurirne tutta la complessità.

Un sommario ironico di questi concetti è fornito nella vignetta di [xkcd](https://www.explainxkcd.com/wiki/index.php/2560:_Confounding_Variables).

## Esercizi

::: {.callout-tip title="Esercizio" collapse="true"}
**Esercizi teorici**

**Esercizio 1: Concetti chiave della causalità**

Per ciascuna delle seguenti affermazioni, indica se è vera o falsa e spiega il motivo della tua risposta.  

1. Se X e Y sono correlate, allora X causa Y.  
2. Se condizioniamo su una variabile collider, la correlazione tra X e Y aumenta.  
3. Il paradosso di Simpson dimostra che i risultati osservati in gruppi disaggregati devono sempre essere preferiti a quelli aggregati.  
4. Gli esperimenti randomizzati controllati eliminano completamente il problema della confusione.  
5. Un DAG può rappresentare relazioni causali solo se tutte le variabili sono misurate.  

**Esercizio 2: Interpretazione di un DAG**

Considera il seguente DAG che rappresenta l’effetto dell’esercizio fisico (X) sulla salute cardiaca (Y):

```
    X → Y
    Z → X
    Z → Y
```
dove:

- **X** = Esercizio fisico  
- **Y** = Salute cardiaca  
- **Z** = Predisposizione genetica  

1. Quale ruolo svolge **Z** in questo DAG? È una variabile confondente, collider o mediatore? 
2. Per stimare correttamente l’effetto causale di **X** su **Y**, è necessario controllare per **Z**? Spiega il perché.  
3. Se aggiungiamo un'altra variabile **W** che influenza sia **Z** che **X**, ma non direttamente **Y**, come cambierebbe il DAG?  

**Esercizio 3: Causalità nei dati osservazionali**

Leggi le seguenti situazioni e identifica quale problema potrebbe invalidare l’inferenza causale:  

1. Uno studio osservazionale mostra che le persone che bevono caffè vivono più a lungo. Tuttavia, chi beve caffè tende ad avere un reddito più alto e accesso a migliori cure mediche.
2. Un’azienda scopre che i dipendenti che frequentano corsi di formazione hanno salari più alti. Ma i corsi sono aperti solo a coloro che già hanno più esperienza lavorativa.  
3. Una ricerca mostra che gli studenti che usano di più il tablet per studiare hanno punteggi più bassi nei test. Tuttavia, gli studenti con difficoltà di apprendimento tendono a usare di più il tablet.  

Per ogni caso, identifica una possibile variabile confondente e suggerisci un metodo per controllare il bias.

**Esercizi pratici in R**

**Esercizio 4: Paradosso di Simpson con dati reali**

Utilizziamo i dati delle ammissioni di UC Berkeley per verificare il paradosso di Simpson.

```r
# Dati di UC Berkeley
data(UCBAdmissions)
df <- as.data.frame(UCBAdmissions)

# Convertiamo i dati in formato long
df_long <- df |> tidyr::pivot_wider(names_from = "Admit", values_from = "Freq") 

# Calcoliamo il tasso di ammissione per uomini e donne aggregati
total_admitted_m <- sum(df$Freq[df$Admit == "Admitted" & df$Gender == "Male"])
total_applicants_m <- sum(df$Freq[df$Gender == "Male"])

total_admitted_f <- sum(df$Freq[df$Admit == "Admitted" & df$Gender == "Female"])
total_applicants_f <- sum(df$Freq[df$Gender == "Female"])

admit_rate_m <- total_admitted_m / total_applicants_m
admit_rate_f <- total_admitted_f / total_applicants_f

c(admit_rate_m, admit_rate_f)

# Calcoliamo il tasso di ammissione per ogni dipartimento
df_long$rate_m <- df_long$Admitted / (df_long$Admitted + df_long$Rejected)

df_long |> dplyr::group_by(Dept) |> dplyr::summarize(mean_rate_m = mean(rate_m))

# Visualizziamo il tasso di ammissione per genere e dipartimento
ggplot(df_long, aes(x = Dept, y = rate_m, fill = Gender)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Tasso di Ammissione per Genere nei Dipartimenti UC Berkeley",
       x = "Dipartimento", y = "Tasso di Ammissione")
```

**Domande:**  

1. Dai dati aggregati, sembra che le donne siano discriminate. Questo è confermato dall'analisi per dipartimento?  
2. Quale variabile confondente è responsabile del paradosso di Simpson in questo caso?  
3. Come potrebbe essere interpretato male un modello che considera solo i dati aggregati?  

**Esercizio 5: Analisi causale con DAG**

Usiamo il pacchetto `dagitty` per costruire e analizzare un DAG.

```r
library(dagitty)

dag <- dagitty("dag {
    E -> H
    G -> E
    G -> H
}")

plot(graphLayout(dag))
```

**Domande:**  

1. Quali sono le variabili confondenti nel DAG?  
2. Quali percorsi sono back-door paths?  
3. Quale set di variabili dovremmo controllare per ottenere una stima non distorta dell’effetto di **E** su **H**?  
:::

::: {.callout-tip title="Soluzione" collapse="true"}

**Esercizio 1: Concetti chiave della causalità**

1. **Falso** – La correlazione non implica causalità. Potrebbero esserci variabili confondenti o una relazione di causalità inversa tra X e Y.  
2. **Vero** – Condizionare su un collider introduce un’associazione spuriosa tra X e Y, aumentando la correlazione.  
3. **Falso** – Il paradosso di Simpson mostra che i dati aggregati possono essere fuorvianti, ma non significa che i dati disaggregati siano sempre più affidabili. È necessario analizzare il contesto e le possibili variabili confondenti.  
4. **Falso** – Gli RCT minimizzano i problemi di confondimento grazie alla randomizzazione, ma possono comunque avere limitazioni dovute a bias di selezione, mancate assegnazioni casuali, e problemi etici.  
5. **Falso** – Un DAG può rappresentare le relazioni causali anche se alcune variabili non sono misurate. Tuttavia, la validità dell'inferenza dipende dalla correttezza del DAG.  

**Esercizio 2: Interpretazione di un DAG**

1. **Z è una variabile confondente**, poiché influenza sia X che Y, creando un percorso di back-door.  
2. **Sì**, per stimare correttamente l’effetto di X su Y dobbiamo controllare per Z. Se non lo facciamo, la relazione osservata tra X e Y includerà l’influenza di Z.  
3. Se aggiungiamo una variabile W che influenza Z e X, il DAG diventa:  

   ```
       W → Z → X → Y
       W → X
       Z → Y
   ```
   
   Ora W è una variabile a monte di X e Z, ma non confonde direttamente la relazione tra X e Y.  

**Esercizio 3: Causalità nei dati osservazionali**

1. **Confondente: reddito** – Le persone con un reddito più alto possono avere accesso a cure migliori, che a loro volta migliorano la salute. Soluzione: **Propensity Score Matching (PSM)** o regressione con controllo per il reddito.  
2. **Confondente: esperienza lavorativa** – Chi ha più esperienza può già avere salari più alti. Soluzione: **Matching o modello di regressione con controllo per esperienza lavorativa**.  
3. **Confondente: difficoltà di apprendimento** – Studenti con difficoltà possono usare più il tablet e avere punteggi più bassi. Soluzione: **Includere il livello di abilità di partenza nei modelli statistici**.  

**Soluzioni esercizi pratici in R**

**Esercizio 4: Paradosso di Simpson con dati reali**

1. **Differenza nei tassi di ammissione aggregati:**  

   ```r
   admit_rate_m <- total_admitted_m / total_applicants_m
   admit_rate_f <- total_admitted_f / total_applicants_f
   ```
   **Risultato:**  
   
   - Tasso di ammissione uomini: ~44%  
   - Tasso di ammissione donne: ~35%  
   → **Sembra che le donne siano discriminate.**  

2. **Analisi per dipartimento:** 

   ```r
   df_long |> dplyr::group_by(Dept) |> dplyr::summarize(mean_rate_m = mean(rate_m))
   ```
   - Nei singoli dipartimenti, le donne hanno tassi di ammissione **uguali o superiori** rispetto agli uomini.  
   → **Il problema non è discriminazione diretta, ma la distribuzione delle domande nei dipartimenti.**  

3. **Conclusione:** Il paradosso di Simpson mostra che le donne tendono a candidarsi più spesso a dipartimenti molto competitivi con bassi tassi di ammissione, mentre gli uomini si candidano di più in dipartimenti con tassi di ammissione più alti.  

  **Moralità**: Non sempre una differenza aggregata indica un bias. Bisogna analizzare i sottogruppi.  

**Esercizio 5: Analisi causale con DAG**

1. **Variabili confondenti**  
   - G è una variabile confondente perché influenza sia E (esposizione) che H (esito).  

2. **Back-door paths**  
   - Il percorso `E ← G → H` è un back-door path che deve essere bloccato.  

3. **Soluzione: controllare per G** 

   ```r
   adjustmentSets(dag)
   ```
   Output: `{G}`  
   → **Controllare per G permette di ottenere una stima causale non distorta di E su H.**  

**Conclusioni**

1. **La correlazione non implica causalità**. Abbiamo visto come variabili confondenti possano generare relazioni spurie.  
2. **Il paradosso di Simpson dimostra che i dati aggregati possono essere fuorvianti**. Bisogna sempre analizzare i sottogruppi.  
3. **I DAG aiutano a identificare le variabili da controllare per ottenere stime causali corrette**.  
4. **L’analisi causale è fondamentale per evitare inferenze errate e migliorare la qualità della ricerca**.  
:::

## Bibliografia {.unnumbered}
