<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>75&nbsp; Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/formal_models/introduction.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-5ce6d56fc2a85cf1942de8a9da5c14ea.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a5df851992ffb0fa85403c99d5182cd1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori (Prior Predictive Check)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04a_stan_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Regressione lineare in Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Il modello di Rescorla–Wagner</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Decisioni</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/decision_analysis/01_study_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Analisi delle decisioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Missing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/missing/01_mnar_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Dati mancanti in psicologia: identificare e modellare i casi MNAR con un approccio Bayesiano in Stan</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#distribuzione-predittiva-posteriore" id="toc-distribuzione-predittiva-posteriore" class="nav-link active" data-scroll-target="#distribuzione-predittiva-posteriore"><span class="header-section-number">75.1</span> Distribuzione predittiva posteriore</a></li>
  <li><a href="#sec-logscore" id="toc-sec-logscore" class="nav-link" data-scroll-target="#sec-logscore"><span class="header-section-number">75.2</span> Il log-score: accuratezza predittiva punto per punto</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" id="toc-leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica"><span class="header-section-number">75.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</a></li>
  <li><a href="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" id="toc-criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="nav-link" data-scroll-target="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl"><span class="header-section-number">75.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span></a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi di apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alla fine di questo capitolo, sarai in grado di:</p>
<ul>
<li>comprendere cos’è la distribuzione predittiva posteriore e come si costruisce;</li>
<li>spiegare cosa misura il log-score e come si calcola nella pratica;</li>
<li>distinguere tra LPPD ed ELPD e comprendere il loro significato;</li>
<li>capire come LOO-CV fornisca una stima dell’ELPD;</li>
<li>collegare il confronto tra modelli alla divergenza di Kullback-Leibler.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://conflicted.r-lib.org/">conflicted</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mc-stan.org/loo/">loo</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://conflicted.r-lib.org/reference/conflicts_prefer.html">conflicts_prefer</a></span><span class="op">(</span><span class="fu">rstan</span><span class="fu">::</span><span class="va"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<div class="lead">
<p>Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’<em>entropia</em>, che misura l’incertezza insita in una distribuzione, e la <em>divergenza di Kullback–Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per <em>valutare e confrontare modelli statistici</em> nel contesto bayesiano.</p>
</div>
<p>Il punto di partenza è una domanda cruciale: <em>quanto bene il modello riesce a prevedere nuovi dati?</em> Un buon modello non deve solo adattarsi bene ai dati già osservati, ma anche saper <em>generalizzare</em> a situazioni future o a campioni mai visti. Questa distinzione — adattamento vs.&nbsp;generalizzazione — è il cuore della valutazione predittiva.</p>
<p>Per rendere concreta questa idea, immaginiamo di aver sviluppato un test psicologico per prevedere il livello di ansia degli studenti alla vigilia di un esame. Non basta sapere che il modello descrive bene i dati del campione che abbiamo usato per costruirlo: vogliamo anche essere ragionevolmente sicuri che le stesse previsioni funzionino per studenti che non hanno partecipato allo studio. In psicologia, scegliere tra due modelli non è diverso dal decidere quale test usare per prevedere un disturbo: entrambi mirano a capire quale strumento fornisce previsioni più affidabili sui dati futuri.</p>
<p>In questo capitolo esploreremo gli strumenti fondamentali per la valutazione e il confronto di modelli nell’ambito dell’inferenza bayesiana.</p>
<p><strong>1. La distribuzione predittiva posteriore</strong><br>
Introdurremo la <em>distribuzione predittiva posteriore</em>, che incorpora l’incertezza sui parametri per generare previsioni coerenti con lo stato di conoscenza del modello. Questo strumento rappresenta il ponte naturale tra stima e previsione, garantendo una quantificazione probabilistica completa dell’incertezza.</p>
<p><strong>2. Misure di accuratezza predittiva</strong><br>
Discuteremo il <em>log-score</em>, una metrica punto per punto che valuta la qualità delle previsioni, e due sue sintesi fondamentali:</p>
<ul>
<li>la <em>LPPD</em> (<em>Log Pointwise Predictive Density</em>), che misura la bontà di adattamento su dati osservati;<br>
</li>
<li>l’<em>ELPD</em> (<em>Expected Log Predictive Density</em>), che stima l’abilità predittiva attesa su nuove osservazioni.</li>
</ul>
<p><strong>3. Validazione empirica e confronto tra modelli</strong><br>
Presenteremo la tecnica <em>Leave-One-Out Cross-Validation</em> (LOO-CV), un approccio efficiente per stimare l’ELPD senza bisogno di nuovi dati, dimostrando come questa metodologia fornisca una valutazione robusta delle prestazioni predittive.</p>
<p><strong>4. Fondamenti teorici e interpretazione</strong><br>
Approfondiremo il legame tra ELPD e <em>divergenza di Kullback-Leibler</em>, che consente di interpretare il confronto tra modelli come una ricerca del modello più vicino alla vera distribuzione generatrice dei dati. Questa connessione teorica fornisce una solida giustificazione informazionale per le procedure di selezione bayesiana.</p>
<p>L’obiettivo del capitolo è offrire una panoramica completa e operativa, che unisca principi teorici a strumenti applicativi, guidando il lettore nella scelta razionale del modello più adatto al problema in esame.</p>
</section><section id="distribuzione-predittiva-posteriore" class="level2" data-number="75.1"><h2 data-number="75.1" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore">
<span class="header-section-number">75.1</span> Distribuzione predittiva posteriore</h2>
<p>Nel capitolo precedente abbiamo usato la <em>divergenza di Kullback–Leibler (KL)</em> come misura teorica della distanza tra realtà e modello. Qui ci chiediamo: <em>come stimiamo questa distanza quando la “vera” distribuzione generatrice è ignota?</em> Un tassello fondamentale è la <em>distribuzione predittiva posteriore</em>.</p>
<p>Nel capitolo sul modello <em>beta–binomiale</em> l’abbiamo già incontrata: è lo strumento che, nell’approccio bayesiano, consente di prevedere nuovi dati incorporando <strong>sia la struttura del modello sia l’incertezza sui parametri</strong>.</p>
<p>In sintesi: dopo aver osservato i dati <span class="math inline">\(y\)</span>, non otteniamo un singolo “miglior” valore dei parametri, ma una <em>distribuzione posteriore</em> <span class="math inline">\(p(\theta \mid y)\)</span> che quantifica i valori plausibili di <span class="math inline">\(\theta\)</span> e la nostra incertezza.</p>
<blockquote class="blockquote">
<p>Esempio. Uno psicologo che stima il livello medio di ansia in una popolazione, invece di affermare “la media è 4.7”, dirà: “il valore più plausibile è 4.7, <strong>ma</strong> è ragionevole che sia tra 4.2 e 5.1”, riflettendo la variabilità posteriore.</p>
</blockquote>
<p>Per prevedere un nuovo dato <span class="math inline">\(\tilde y\)</span>, non fissiamo <span class="math inline">\(\theta\)</span>. <em>Media</em>mo invece tutte le previsioni condizionate <span class="math inline">\(p(\tilde y \mid \theta)\)</span> pesandole con la posteriore <span class="math inline">\(p(\theta\mid y)\)</span>:</p>
<p><span class="math display">\[
q(\tilde{y} \mid y)
\;=\;
\int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta .
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Intuizione">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuizione
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Se conoscessimo <span class="math inline">\(\theta\)</span>, useremmo direttamente <span class="math inline">\(p(\tilde y\mid \theta)\)</span>. Poiché non lo conosciamo, <em>combiniamo</em> le previsioni per tutti i <span class="math inline">\(\theta\)</span> possibili, pesandole in base a quanto ciascun <span class="math inline">\(\theta\)</span> è plausibile a posteriori—come chiedere il parere a più esperti e fare una media ponderata della loro opinione.</p>
</div>
</div>
</div>
<p><strong>Notazione.</strong> Useremo talvolta la forma compatta <span class="math inline">\(q(\cdot \mid y)\)</span> per indicare la predittiva posteriore del modello. Quando ci servirà evidenziare la previsione <em>marginale</em> per una singola osservazione <span class="math inline">\(y_i\)</span>, scriveremo:</p>
<p><span class="math display">\[
p(y_i \mid y)
\;=\;
\int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta,
\]</span></p>
<p>cioè la verosimiglianza <span class="math inline">\(p(y_i\mid\theta)\)</span> integrata rispetto alla posteriore <span class="math inline">\(p(\theta\mid y)\)</span>.</p>
<p><strong>Idea chiave:</strong> la predittiva posteriore <em>propaga l’incertezza sui parametri alle previsioni</em>. È questo passaggio a rendere le valutazioni predittive coerenti con il principio bayesiano, e quindi utilizzabili nel <em>confronto tra modelli</em> e nella stima di quantità legate alla “distanza” dal generatore dei dati.</p>
<section id="il-problema-della-valutazione-predittiva" class="level3" data-number="75.1.1"><h3 data-number="75.1.1" class="anchored" data-anchor-id="il-problema-della-valutazione-predittiva">
<span class="header-section-number">75.1.1</span> Il problema della valutazione predittiva</h3>
<p>Il nostro obiettivo è capire <em>quanto</em> la distribuzione predittiva posteriore <span class="math inline">\(q(\tilde{y} \mid y)\)</span> si avvicini alla <em>vera distribuzione generatrice</em> dei dati futuri, <span class="math inline">\(p(\tilde{y})\)</span>. In teoria, questa distanza si misura con la <em>divergenza di Kullback–Leibler (KL)</em>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) \;=\; \mathbb{E}_p\!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right].
\]</span></p>
<p>Qui però incontriamo subito un problema concettuale: <em>non conosciamo <span class="math inline">\(p(\tilde{y})\)</span></em>. È come voler giudicare la precisione di una mappa senza poter vedere il territorio reale.</p>
<p>Per superare questo ostacolo, possiamo ricorrere a <em>misure surrogate</em> che, pur non avendo accesso diretto a <span class="math inline">\(p(\tilde{y})\)</span>, permettono di stimare la qualità predittiva del modello utilizzando in modo ingegnoso i dati osservati. Tra queste, vedremo il <em>log-score</em>, la <em>LPPD</em> e l’<em>ELPD</em>, che forniscono stime indirette ma utili della bontà predittiva.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Mappa concettuale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 39%">
<col style="width: 34%">
</colgroup>
<thead><tr class="header">
<th>Quantità</th>
<th>Significato</th>
<th>Uso principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid \theta)\)</span></td>
<td>Verosimiglianza</td>
<td>Calcolo predittivo</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(\theta \mid y)\)</span></td>
<td>Distribuzione posteriore</td>
<td>Ponderazione</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid y)\)</span></td>
<td>Predizione bayesiana media</td>
<td>Log-score, LPPD</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(y_i \mid y_{-i})\)</span></td>
<td>Predizione LOO (<em>leave-one-out</em>)</td>
<td>ELPD</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(q(\tilde{y} \mid y)\)</span></td>
<td>Distribuzione predittiva complessiva</td>
<td>Divergenza KL, confronto modelli</td>
</tr>
</tbody>
</table>
</div>
</div>
</section></section><section id="sec-logscore" class="level2" data-number="75.2"><h2 data-number="75.2" class="anchored" data-anchor-id="sec-logscore">
<span class="header-section-number">75.2</span> Il log-score: accuratezza predittiva punto per punto</h2>
<p>Definita la <em>distribuzione predittiva posteriore</em>, possiamo chiederci: <em>quanto bene il modello ha “previsto” ciascun dato osservato?</em> Il <em>log-score</em> risponde proprio a questa domanda: per ogni osservazione <span class="math inline">\(y_i\)</span> calcola il logaritmo della probabilità predittiva che il modello le assegna:</p>
<p><span id="eq-log-score-def"><span class="math display">\[
\log p(y_i \mid y) \;=\; \log \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
\tag{75.1}\]</span></span></p>
<p>Un valore alto (cioè meno negativo) indica che l’osservazione era plausibile per il modello; un valore basso che era improbabile.</p>
<p>Il <em>log-score totale</em> si ottiene sommando i contributi di tutte le osservazioni:</p>
<p><span id="eq-log-score-sum-def"><span class="math display">\[
S \;=\; \sum_{i=1}^n \log p(y_i \mid y) .
\tag{75.2}\]</span></span></p>
<p>Più il punteggio è alto, maggiore è la probabilità che il modello attribuisce ai dati realmente osservati. In altre parole, è un <em>indice di fiducia</em>: se ciò che accade era atteso dal modello, il punteggio cresce.</p>
<section id="frequentista-vs.-bayesiano" class="level3" data-number="75.2.1"><h3 data-number="75.2.1" class="anchored" data-anchor-id="frequentista-vs.-bayesiano">
<span class="header-section-number">75.2.1</span> Frequentista vs.&nbsp;Bayesiano</h3>
<ul>
<li>
<p><strong>Versione frequentista</strong> Si valuta <span class="math inline">\(p(y_i \mid \hat{\theta})\)</span> usando una <em>stima puntuale</em> dei parametri (ad esempio MLE o MAP), senza considerare l’incertezza sui parametri:</p>
<p><span class="math display">\[
\log p(y_i \mid \hat{\theta}) .
\]</span></p>
</li>
<li>
<p><strong>Versione bayesiana</strong> Si usa la <em>densità predittiva puntuale</em>, integrando la verosimiglianza sulla distribuzione a posteriori dei parametri:</p>
<p><span class="math display">\[
p(y_i \mid y) \;=\; \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta ,
\]</span></p>
<p>dove <span class="math inline">\(\theta^{(s)}\)</span> sono campioni MCMC dalla posteriore.</p>
</li>
</ul></section><section id="sec-lppd" class="level3" data-number="75.2.2"><h3 data-number="75.2.2" class="anchored" data-anchor-id="sec-lppd">
<span class="header-section-number">75.2.2</span> La LPPD: log-score “bayesiano”</h3>
<p>Calcolando il log-score in versione bayesiana e sommando su tutte le osservazioni si ottiene la <em>Log Pointwise Predictive Density (LPPD)</em>:</p>
<p><span id="eq-lppd-def"><span class="math display">\[
\text{LPPD} \;=\; \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right] .
\tag{75.3}\]</span></span></p>
<p><strong>In breve:</strong></p>
<ul>
<li>
<em>Log-score totale (frequentista)</em> → usa una sola stima dei parametri.</li>
<li>
<em>LPPD (bayesiana)</em> → stessa idea, ma integra l’incertezza sui parametri usando la posteriore.</li>
</ul>
<section id="calcolo-pratico-con-mcmc" class="level4" data-number="75.2.2.1"><h4 data-number="75.2.2.1" class="anchored" data-anchor-id="calcolo-pratico-con-mcmc">
<span class="header-section-number">75.2.2.1</span> Calcolo pratico con MCMC</h4>
<p>Nella formula teorica dell’<a href="#eq-lppd-def" class="quarto-xref">Equazione&nbsp;<span>75.3</span></a>, la quantità <span class="math inline">\(p(y_i \mid y)\)</span> è una <em>media predittiva</em> ottenuta integrando:</p>
<ul>
<li>
<span class="math inline">\(p(y_i \mid \theta)\)</span>: la verosimiglianza condizionata, cioè la distribuzione dei dati futuri se i parametri fossero <span class="math inline">\(\theta\)</span>;</li>
<li>
<span class="math inline">\(p(\theta \mid y)\)</span>: la distribuzione posteriore dei parametri, che rappresenta la nostra incertezza residua dopo aver osservato i dati.</li>
</ul>
<p>Poiché questo integrale raramente è calcolabile in forma chiusa, lo <em>stimiamo usando i campioni MCMC</em> dalla posteriore:</p>
<p><span id="eq-mcmc-posterior-parameter-distr"><span class="math display">\[
p(y_i \mid y) \;\approx\; \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) ,
\tag{75.4}\]</span></span></p>
<p>e quindi:</p>
<p><span id="eq-mcmc-log-score"><span class="math display">\[
\text{Log-score} \;\approx\; \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right] .
\tag{75.5}\]</span></span></p>
<p>Il log-score così calcolato fornisce <em>una misura complessiva di accuratezza predittiva sui dati osservati</em> (<em>in-sample</em>).</p>
<p><strong>Limite importante:</strong> essendo calcolato sugli stessi dati usati per stimare i parametri, tende a favorire modelli più complessi, rischiando di sopravvalutarne la capacità di generalizzazione (<em>overfitting</em>). Per valutazioni più affidabili, serviranno tecniche di validazione incrociata che misurino la performance <em>out-of-sample</em>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere tre valori posteriori di <span class="math inline">\(\theta\)</span>: 0.3 (peso 0.2), 0.5 (peso 0.5) e 0.7 (peso 0.3). Se la nuova osservazione è <span class="math inline">\(y = 3\)</span> su <span class="math inline">\(n = 5\)</span> tentativi:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">3</span>, size <span class="op">=</span> <span class="fl">5</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span><span class="va">p_y_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">posterior_weights</span><span class="op">)</span></span>
<span><span class="va">log_score</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_y_given_y</span><span class="op">)</span></span>
<span><span class="va">log_score</span></span>
<span><span class="co">#&gt; [1] -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il log-score è circa -1.29. Un valore meno negativo indica una previsione migliore.</p>
</div>
</div>
</div>
</section></section><section id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" class="level3" data-number="75.2.3"><h3 data-number="75.2.3" class="anchored" data-anchor-id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati">
<span class="header-section-number">75.2.3</span> Expected Log Predictive Density (ELPD): guardare oltre i dati osservati</h3>
<p>Se vogliamo valutare la <em>capacità di generalizzazione</em> di un modello, la domanda chiave è: <em>quanto bene predirebbe dati che non ha mai visto?</em> L’<em>ELPD</em> (<em>Expected Log Predictive Density</em>) risponde a questa domanda con la stessa logica della LPPD, ma introduce una differenza fondamentale: la previsione di <span class="math inline">\(y_i\)</span> viene calcolata <em>escludendo <span class="math inline">\(y_i\)</span> dall’adattamento del modello</em> (<em>Leave-One-Out</em>, LOO):</p>
<p><span id="eq-elpd-def"><span class="math display">\[
\text{ELPD} \;=\; \sum_{i=1}^n \log p(y_i \mid y_{-i}),
\tag{75.6}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset a cui è stata rimossa l’osservazione <span class="math inline">\(i\)</span>.</p>
<p><strong>Esempio</strong> Nel caso di un test sull’ansia:</p>
<ul>
<li>
<em>LPPD</em> → misura quanto bene il modello predice i punteggi di ansia degli studenti <em>già presenti</em> nel campione osservato.</li>
<li>
<em>ELPD</em> → misura quanto bene predirebbe il punteggio di un <em>nuovo</em> studente, usando solo i dati degli altri.</li>
</ul>
<p>In sostanza, l’ELPD è una <em>stima empirica</em> (con segno cambiato) della <em>divergenza di Kullback–Leibler</em> tra la vera distribuzione dei dati futuri e la distribuzione predittiva del modello. Ci fornisce quindi un indicatore diretto di <em>quanto</em> le previsioni del modello si avvicinano a ciò che accadrà davvero, senza richiedere di conoscere la distribuzione reale.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Interpretazione:</strong> l’ELPD è un <em>log-score out-of-sample</em>: per ogni <span class="math inline">\(y_i\)</span>, lo escludiamo, adattiamo il modello agli altri dati, e valutiamo la probabilità predittiva di <span class="math inline">\(y_i\)</span>. Più alto è l’ELPD, migliore è la capacità del modello di generalizzare a dati nuovi.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere tre osservazioni <span class="math inline">\(y_1, y_2, y_3\)</span> e che il modello stimi:</p>
<p><span class="math display">\[
p(y_1 \mid y_2,y_3)=0.6,\quad p(y_2 \mid y_1,y_3)=0.7,\quad p(y_3 \mid y_1,y_2)=0.5.
\]</span></p>
<p>L’ELPD è:</p>
<p><span class="math display">\[
\log 0.6 + \log 0.7 + \log 0.5 \; \approx\; -0.5108 -0.3567 -0.6931 = -1.5606.
\]</span></p>
<p>Un valore meno negativo indica maggiore capacità predittiva fuori campione.</p>
</div>
</div>
</div>
</section><section id="lppd-vs.-elpd-in-sintesi" class="level3" data-number="75.2.4"><h3 data-number="75.2.4" class="anchored" data-anchor-id="lppd-vs.-elpd-in-sintesi">
<span class="header-section-number">75.2.4</span> LPPD vs.&nbsp;ELPD in sintesi</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 39%">
<col style="width: 25%">
<col style="width: 26%">
</colgroup>
<thead><tr class="header">
<th>Misura</th>
<th>Dati usati per predire <span class="math inline">\(y_i\)</span>
</th>
<th>Valuta</th>
<th>Limite principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>LPPD</strong></td>
<td>Tutti i dati, incluso <span class="math inline">\(y_i\)</span>
</td>
<td>Adattamento in-sample</td>
<td>Rischio di overfitting</td>
</tr>
<tr class="even">
<td><strong>ELPD</strong></td>
<td>Tutti i dati tranne <span class="math inline">\(y_i\)</span> (<em>LOO</em>)</td>
<td>Generalizzazione</td>
<td>—</td>
</tr>
</tbody>
</table>
<p><strong>Metafora</strong> In un esperimento di riconoscimento di volti, mostriamo a un partecipante 100 fotografie e lo alleniamo a riconoscerle:</p>
<ul>
<li>
<em>LPPD</em> → misura quanto bene riconosce <em>quelle stesse foto</em>, già viste in fase di addestramento (<em>in-sample</em>).</li>
<li>
<em>ELPD</em> → misura quanto bene riconosce <em>nuove foto</em>, mai viste prima, cioè immagini fuori dall’insieme di addestramento (<em>out-of-sample</em>).</li>
</ul>
<p>Se il punteggio LPPD è alto ma l’ELPD è basso, significa che il partecipante — o il modello — ha <em>memorizzato</em> i casi specifici, senza aver appreso regole generali utili per nuovi dati.</p>
</section><section id="il-collegamento-con-la-divergenza-kl" class="level3" data-number="75.2.5"><h3 data-number="75.2.5" class="anchored" data-anchor-id="il-collegamento-con-la-divergenza-kl">
<span class="header-section-number">75.2.5</span> Il collegamento con la divergenza KL</h3>
<p>La <em>divergenza di Kullback–Leibler</em> <span class="math inline">\(D_{\text{KL}}\)</span> misura teoricamente la distanza tra la distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, e la distribuzione predittiva del modello, <span class="math inline">\(q(\tilde{y} \mid y)\)</span>.</p>
<p>Nel confronto tra due modelli <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span>, la differenza nelle loro <span class="math inline">\(D_{\text{KL}}\)</span> equivale alla differenza nelle rispettive <em>accuratezze predittive medie</em> rispetto a <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, non possiamo calcolare direttamente la KL. L’<em>ELPD</em> fornisce una stima empirica di questa accuratezza predittiva: un valore più alto implica un modello più “vicino” alla distribuzione vera.</p>
<p><span class="math display">\[
\text{Massimizzare ELPD} \;\; \approx \;\; \text{Minimizzare la divergenza KL}.
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Perché ELPD ≈ - KL?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Perché ELPD ≈ - KL?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Per definizione:</p>
<p><span class="math display">\[
D_{\text{KL}}\big(p \parallel q\big)
= \mathbb{E}_{p} \!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<ul>
<li>Il primo termine <span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span> <em>non dipende dal modello</em> (è fisso per tutti).</li>
<li>Confrontare due modelli equivale quindi a confrontare <em>solo il secondo termine</em>, che è <span class="math inline">\(-\)</span>ELPD.</li>
</ul>
<p>Ecco perché <em>massimizzare l’ELPD equivale a minimizzare la divergenza KL</em>: si sta massimizzando la media log-predittiva che il modello assegna ai dati futuri.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Vogliamo confrontare due modelli predittivi per il numero di “teste” in <span class="math inline">\(n=10\)</span> lanci.</p>
<ul>
<li>La <strong>distribuzione vera</strong> è <span class="math inline">\(p(y)=\text{Binom}(n=10,\;p=0.6)\)</span>.</li>
<li>Il <strong>modello candidato</strong> prevede <span class="math inline">\(q(y)=\text{Binom}(n=10,\;q=0.5)\)</span>.</li>
</ul>
<p>L’<em>ELPD</em> di un modello è l’aspettativa, rispetto alla distribuzione vera <span class="math inline">\(p\)</span>, del <em>log-score</em> del modello: <span class="math inline">\(\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]\)</span>. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori <span class="math inline">\(y=0,\dots,n\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri del problema</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># probabilità vera di "testa"</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità ipotizzata dal modello candidato</span></span>
<span></span>
<span><span class="co"># 1) Supporto dei possibili esiti</span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span></span>
<span><span class="co"># 2) Distribuzione vera p(y) su tutto il supporto</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Log-predittiva del modello candidato q su tutto il supporto</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)</span></span>
<span><span class="va">elpd_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 5) "Modello vero": usa q = p. Log-predittiva del modello vero</span></span>
<span><span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)</span></span>
<span><span class="va">elpd_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]</span></span>
<span><span class="va">kl_pq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="op">(</span><span class="va">log_p_y</span> <span class="op">-</span> <span class="va">log_q_y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello candidato (q=0.5): %.4f\n"</span>, <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello candidato (q=0.5): -2.0549</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello vero      (q=0.6): %.4f\n"</span>, <span class="va">elpd_p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello vero      (q=0.6): -1.8536</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza ELPD (vero - candidato): %.4f\n"</span>, <span class="va">elpd_p</span> <span class="op">-</span> <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza ELPD (vero - candidato): 0.2014</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"KL(p || q): %.4f\n"</span>, <span class="va">kl_pq</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; KL(p || q): 0.2014</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Cosa stiamo verificando?</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)\)</span> è <em>più basso</em> (più negativo) del valore ottenuto dal modello vero <span class="math inline">\(\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)\)</span>. → Il modello con <span class="math inline">\(q=0.6\)</span> è <em>più predittivo</em> di quello con <span class="math inline">\(q=0.5\)</span>.</p></li>
<li><p>La <em>differenza</em> tra i due ELPD è <em>uguale</em> (vicina numericamente) alla <em>divergenza di Kullback–Leibler</em>:</p></li>
</ol>
<p><span class="math display">\[
\mathrm{ELPD}(p)-\mathrm{ELPD}(q)
= \sum_y p(y)\big[\log p(y)-\log q(y)\big]
= D_{\mathrm{KL}}(p\|q)\;&gt;\;0.
\]</span></p>
<p>→ Questo mostra <em>algebricamente e numericamente</em> il legame: <em>massimizzare l’ELPD equivale a minimizzare la KL</em>.</p>
<blockquote class="blockquote">
<p>Nota sul log: nel codice usiamo il log naturale (unità in <strong>nat</strong>). Se si preferisce il log in base 2 (unità in <em>bit</em>), basta sostituire <code><a href="https://rdrr.io/r/base/Log.html">log()</a></code> con <code><a href="https://rdrr.io/r/base/Log.html">log2()</a></code>; tutte le quantità cambiano di una costante di scala, ma i <em>confronti</em> tra modelli restano identici.</p>
</blockquote>
<p><strong>In pratica.</strong></p>
<p>In questo esempio abbiamo potuto calcolare l’ELPD <em>vero</em> perché conoscevamo l’intera distribuzione generatrice <span class="math inline">\(p(y)\)</span> e potevamo integrare esattamente. Nella realtà, <span class="math inline">\(p(y)\)</span> è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD <em>empiricamente</em>, ad esempio con la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che sostituisce l’aspettativa rispetto a <span class="math inline">\(p\)</span> con una media sui dati raccolti, lasciando fuori una osservazione alla volta. Questa procedura ci consente di avvicinarci al calcolo ideale della KL, anche senza conoscere <span class="math inline">\(p(y)\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Collegamento chiave</strong><br>
L’ELPD è una stima empirica (con segno cambiato) della divergenza di Kullback–Leibler.<br>
Più alto è l’ELPD, migliore è la capacità predittiva del modello.</p>
</div>
</div>
</div>
</section></section><section id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="level2" data-number="75.3"><h2 data-number="75.3" class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica">
<span class="header-section-number">75.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</h2>
<p>Poiché la distribuzione vera dei dati futuri è inaccessibile, dobbiamo usare metodi indiretti per stimare quanto bene il nostro modello prevede nuove osservazioni. La validazione incrociata Leave-One-Out (<em>LOO-CV</em>) è uno di questi metodi e, se combinata con l’uso dell’<em>Expected Log Predictive Density</em> (<em>ELPD</em>), diventa uno strumento potente per il confronto tra modelli.</p>
<p>Abbiamo visto che l’ELPD è la misura ideale della capacità predittiva di un modello su dati futuri. Il problema è che, per definizione, richiede di calcolare un’aspettativa rispetto alla <em>vera</em> distribuzione generatrice <span class="math inline">\(p(\tilde{y})\)</span>, che non conosciamo.</p>
<p><strong>Come possiamo stimarla in pratica?</strong> Usando la LOO-CV, che simula la previsione di nuovi dati sfruttando solo le informazioni presenti nei dati osservati.</p>
<section id="cosè-la-loo-cv" class="level3" data-number="75.3.1"><h3 data-number="75.3.1" class="anchored" data-anchor-id="cosè-la-loo-cv">
<span class="header-section-number">75.3.1</span> Cos’è la LOO-CV</h3>
<p>La LOO-CV è un esperimento concettuale semplice:</p>
<ol type="1">
<li>Scegli un’osservazione <span class="math inline">\(y_i\)</span> dal dataset.</li>
<li>Escludila dal set di addestramento.</li>
<li>Adatta il modello ai dati rimanenti <span class="math inline">\(y_{-i}\)</span>.</li>
<li>Calcola la densità predittiva del modello per l’osservazione esclusa: <span class="math inline">\(p(y_i \mid y_{-i})\)</span>.</li>
<li>Ripeti per ogni osservazione e somma i logaritmi ottenuti.</li>
</ol>
<p>Formalmente:</p>
<p><span id="eq-loo-def"><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}),
\tag{75.7}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset senza l’osservazione <span class="math inline">\(i\)</span>.<br>
La struttura è identica a quella dell’ELPD “ideale”, ma ogni termine è calcolato <em>fuori campione</em>, escludendo il dato che viene valutato.</p>
<p>Un’analogia: è come escludere uno studente dall’allenamento e verificare se il modello riesce a predire il suo punteggio d’esame; ripetendo questo processo per tutti gli studenti otteniamo una misura diretta della capacità di generalizzazione.</p>
</section><section id="perché-loo-cv-funziona" class="level3" data-number="75.3.2"><h3 data-number="75.3.2" class="anchored" data-anchor-id="perché-loo-cv-funziona">
<span class="header-section-number">75.3.2</span> Perché LOO-CV funziona</h3>
<p>L’ELPD può essere scritto come:</p>
<p><span id="eq-loo-def2"><span class="math display">\[
\mathbb{E}_p[\log q(\tilde{y} \mid y)],
\tag{75.8}\]</span></span></p>
<p>dove <span class="math inline">\(q(\tilde{y} \mid y)\)</span> è la distribuzione predittiva del modello.<br>
Non possiamo calcolare l’aspettativa rispetto a <span class="math inline">\(p(\tilde{y})\)</span>, ma possiamo trattare ogni osservazione <span class="math inline">\(y_i\)</span> come “nuovo dato” generato da <span class="math inline">\(p\)</span> e usare la media empirica sulle osservazioni reali come stima dell’aspettativa:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>In altre parole: LOO-CV misura <em>quanto bene il modello predirebbe ciascun dato se non lo avesse mai visto</em>.</p>
</section><section id="legame-con-la-divergenza-kl" class="level3" data-number="75.3.3"><h3 data-number="75.3.3" class="anchored" data-anchor-id="legame-con-la-divergenza-kl">
<span class="header-section-number">75.3.3</span> Legame con la divergenza KL</h3>
<p>La divergenza di Kullback–Leibler è definita come:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p[\log p(\tilde{y})] - \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Il primo termine, l’entropia di <span class="math inline">\(p\)</span>, è lo stesso per tutti i modelli e scompare nel confronto.<br>
Ne segue che, per due modelli <span class="math inline">\(q_1\)</span> e <span class="math inline">\(q_2\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) =
\mathbb{E}_p[\log q\_2(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_1(\tilde{y} \mid y)].
\]</span></p>
<p><em>Vince il modello con ELPD più alto</em>, perché corrisponde alla minore divergenza KL dalla distribuzione vera.</p>
</section><section id="confrontare-i-modelli-con-loo-cv" class="level3" data-number="75.3.4"><h3 data-number="75.3.4" class="anchored" data-anchor-id="confrontare-i-modelli-con-loo-cv">
<span class="header-section-number">75.3.4</span> Confrontare i modelli con LOO-CV</h3>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, sostituiamo l’aspettativa teorica con la stima empirica via LOO:</p>
<p><span id="eq-delta-elpd-def"><span class="math display">\[
\Delta\text{ELPD} = \text{ELPD}*{\text{LOO}}(M_1) - \text{ELPD}*{\text{LOO}}(M_2) .
\tag{75.9}\]</span></span></p>
<p><span class="math inline">\(\Delta\text{ELPD}\)</span> approssima la differenza tra le divergenze KL dei modelli.<br>
Oltre alla differenza, possiamo stimare un <em>errore standard</em> per capire se la superiorità di un modello è robusta o dovuta al caso.</p>
</section><section id="punti-chiave" class="level3" data-number="75.3.5"><h3 data-number="75.3.5" class="anchored" data-anchor-id="punti-chiave">
<span class="header-section-number">75.3.5</span> Punti chiave</h3>
<ul>
<li>
<strong>Problema:</strong> L’ELPD teorico richiede <span class="math inline">\(p(\tilde{y})\)</span>, che è sconosciuta.<br>
</li>
<li>
<strong>Soluzione:</strong> LOO-CV fornisce una stima empirica out-of-sample.<br>
</li>
<li>
<strong>Teoria:</strong> L’ELPD è direttamente collegato alla parte “accuratezza” della KL-divergence.<br>
</li>
<li>
<strong>Pratica:</strong> Massimizzare l’ELPD stimato equivale a scegliere il modello più vicino alla distribuzione vera.</li>
</ul>
<p>Direi che l’esempio che hai scritto è già molto chiaro e in linea con il testo precedente, ma per integrarlo meglio nel capitolo e mantenere continuità con la sezione teorica, potremmo:</p>
<ol type="1">
<li>
<strong>Aggiungere un’introduzione contestuale</strong> per collegarlo subito alla discussione ELPD–LOO–KL.</li>
<li>
<strong>Rendere più esplicito il parallelismo con la teoria</strong> (ELPD come somma delle log-predittive fuori campione).</li>
<li>
<strong>Sintetizzare il codice</strong> con commenti chiave, così che lo studente possa leggerlo senza perdersi nei dettagli secondari.</li>
<li>
<strong>Chiarire il senso della tabella</strong> subito dopo l’esecuzione del codice.</li>
</ol>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: confronto ELPD-LOO tra due modelli">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: confronto ELPD-LOO tra due modelli
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Questo mini-esempio mostra come passare <em>dalla definizione teorica</em> dell’ELPD alla <em>stima pratica via Leave-One-Out</em>, usando un caso elementare Beta–Bernoulli.</p>
<p><strong>Dati.</strong> Cinque prove indipendenti: <span class="math inline">\(y=\{1,1,1,0,1\}\)</span> (quattro “successi”, un “insuccesso”).</p>
<p><strong>Modello A (Bayesiano adattato ai dati).</strong> Bernoulli<span class="math inline">\((\theta)\)</span> con prior <span class="math inline">\(\theta\sim \text{Beta}(1,1)\)</span> (uninformativa). Per LOO:</p>
<ul>
<li>per ogni <span class="math inline">\(i\)</span>, escludiamo <span class="math inline">\(y_i\)</span>;</li>
<li>calcoliamo la posteriore <span class="math inline">\(\theta \mid y_{-i} \sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})\)</span>, dove <span class="math inline">\(s_{-i}\)</span> è il numero di successi tra i <span class="math inline">\(n-1\)</span> rimanenti;</li>
<li>calcoliamo la probabilità predittiva per <span class="math inline">\(y_i\)</span>.</li>
</ul>
<p><strong>Modello B (di confronto).</strong> Moneta equa fissa (<span class="math inline">\(q=0.5\)</span>): la predittiva è sempre <span class="math inline">\(0.5\)</span>, indipendentemente dai dati.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-predittiva LOO per Modello A (Beta(1,1) + Bernoulli)</span></span>
<span><span class="va">loo_log_pred_beta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span>, <span class="va">y</span>, <span class="va">a0</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">b0</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">yi</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">s_minus</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">yi</span></span>
<span>  <span class="va">n_minus</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="va">alpha</span> <span class="op">&lt;-</span> <span class="va">a0</span> <span class="op">+</span> <span class="va">s_minus</span></span>
<span>  <span class="va">beta</span>  <span class="op">&lt;-</span> <span class="va">b0</span> <span class="op">+</span> <span class="op">(</span><span class="va">n_minus</span> <span class="op">-</span> <span class="va">s_minus</span><span class="op">)</span></span>
<span>  <span class="va">p1</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="op">(</span><span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span><span class="op">)</span></span>
<span>  <span class="va">p</span>  <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="va">yi</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="va">p1</span> <span class="kw">else</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Log-predittive punto-per-punto</span></span>
<span><span class="va">lp_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="va">loo_log_pred_beta</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">lp_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ELPD-LOO</span></span>
<span><span class="va">elpd_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_beta</span><span class="op">)</span></span>
<span><span class="va">elpd_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_fixed</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Differenza e SE</span></span>
<span><span class="va">diff_pt</span> <span class="op">&lt;-</span> <span class="va">lp_beta</span> <span class="op">-</span> <span class="va">lp_fixed</span></span>
<span><span class="va">se_diff</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">diff_pt</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Tabella riassuntiva</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>, y <span class="op">=</span> <span class="va">y</span>,</span>
<span>  lp_beta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_beta</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  lp_fixed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_fixed</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  diff <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">diff_pt</span>, <span class="fl">6</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt;   i y lp_beta lp_fixed    diff</span></span>
<span><span class="co">#&gt; 1 1 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 2 2 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 3 3 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 4 4 0 -1.7918  -0.6931 -1.0986</span></span>
<span><span class="co">#&gt; 5 5 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"\nELPD-LOO Modello A: %.6f\n"</span>, <span class="va">elpd_beta</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ELPD-LOO Modello A: -3.413620</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD-LOO Modello B: %.6f\n"</span>, <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD-LOO Modello B: -3.465736</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza (A-B)  : %.6f\n"</span>, <span class="va">elpd_beta</span> <span class="op">-</span> <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza (A-B)  : 0.052116</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"SE differenza     : %.6f\n"</span>, <span class="va">se_diff</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; SE differenza     : 1.386294</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Interpretazione.</strong></p>
<ul>
<li>Ogni riga della tabella mostra la log-predittiva fuori campione per entrambi i modelli.</li>
<li>In un campione con 4 successi su 5, il Modello A assegna <em>più di 0.5</em> di probabilità ai successi, e meno di 0.5 all’unico insuccesso.</li>
<li>L’ELPD-LOO di A può risultare leggermente più alto di quello di B, ma l’errore standard è grande perché <span class="math inline">\(n\)</span> è piccolo.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Regola pratica:</strong> una differenza <span class="math inline">\(|\Delta \text{ELPD}|\)</span> di almeno 2 volte l’SE fornisce un’indicazione più affidabile di superiorità del modello. In esempi così piccoli l’obiettivo è puramente didattico: capire <em>come</em> si calcola e <em>cosa</em> significa.</p>
</blockquote>
</div>
</div>
</div>
</section><section id="elpd-loo-e-il-problema-delloverfitting" class="level3" data-number="75.3.6"><h3 data-number="75.3.6" class="anchored" data-anchor-id="elpd-loo-e-il-problema-delloverfitting">
<span class="header-section-number">75.3.6</span> ELPD-LOO e il problema dell’overfitting</h3>
<p>Valutare un modello sugli stessi dati usati per addestrarlo tende a <em>gonfiare</em> le stime della sua capacità predittiva (<em>overfitting</em>). È come se uno studente ottenesse un punteggio perfetto ripetendo esercizi già svolti: non sappiamo se saprebbe risolverne di nuovi.</p>
<p>La <em>Leave-One-Out Cross-Validation (LOO-CV)</em> aggira il problema valutando ciascuna osservazione <span class="math inline">\(y_i\)</span> usando <em>solo</em> i dati rimanenti (<span class="math inline">\(y_{-i}\)</span>). Il punteggio ottenuto (ELPD-LOO) è quindi una stima <em>out-of-sample</em> della bontà predittiva, meno sensibile all’overfitting.</p>
<p>Grazie a metodi come il <em>Pareto-smoothed importance sampling (PSIS)</em>, oggi è possibile calcolare l’ELPD-LOO <em>senza riadattare il modello <span class="math inline">\(n\)</span> volte</em>. In R, la funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> del pacchetto <em>loo</em> (integrata in <code>brms</code> e <code>rstanarm</code>) rende questa procedura rapida e diretta anche per modelli complessi.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: ELPD atteso vs ELPD-LOO stimato">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: ELPD atteso vs ELPD-LOO stimato
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Quando conosci la distribuzione vera dei dati (<span class="math inline">\(p\)</span>), puoi calcolare l’<em>ELPD atteso</em> in modo esatto. Quando invece hai solo i dati osservati, lo stimi tramite <em>Leave-One-Out</em> (PSIS-LOO), come mostrato di seguito.</p>
<p><strong>ELPD atteso (p noto)</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span></span>
<span><span class="va">y_vals</span>   <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">p_y</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">log_q_y</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">elpd</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD atteso (modello q = 0.5): %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD atteso (modello q = 0.5): -2.0549</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>ELPD-LOO stimato da dati simulati</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">20</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>,</span>
<span>  n <span class="op">=</span> <span class="va">n</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html">brm</a></span><span class="op">(</span><span class="va">k</span> <span class="op">|</span> <span class="fu">trials</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">df</span>,</span>
<span>           family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>           prior <span class="op">=</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html">prior</a></span><span class="op">(</span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/constant.html">constant</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>, class <span class="op">=</span> <span class="st">"Intercept"</span><span class="op">)</span>,</span>
<span>           iter <span class="op">=</span> <span class="fl">2000</span>, chains <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">loo_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">loo_res</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’oggetto <code>loo_res</code> fornisce l’ELPD-LOO, il suo errore standard, e le statistiche <em>Pareto k</em> per la diagnostica. Con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code> puoi confrontare due modelli sulla base della differenza di ELPD-LOO e del relativo SE.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="In pratica: stimare e confrontare l'ELPD-LOO">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In pratica: stimare e confrontare l’ELPD-LOO
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Concetto chiave</strong></p>
<ul>
<li>L’ELPD valuta la capacità predittiva su dati non visti.</li>
<li>La LOO-CV lo stima in modo efficiente con PSIS-LOO.</li>
</ul>
<p><strong>Strumenti</strong></p>
<ul>
<li>Funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> del pacchetto <em>loo</em>, integrata in <code>brms</code> e <code>rstanarm</code>.</li>
<li>Diagnostica con <em>Pareto k</em>, confronto con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ul>
<p><strong>Workflow tipico in R</strong></p>
<ol type="1">
<li>Adattare ogni modello (<code><a href="https://paulbuerkner.com/brms/reference/brm.html">brm()</a></code> o <code>stan_glm()</code>).</li>
<li>Estrarre <code><a href="https://mc-stan.org/rstantools/reference/log_lik.html">log_lik()</a></code> e calcolare <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code>.</li>
<li>Confrontare modelli con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ol>
<p><strong>Decisione</strong></p>
<ul>
<li>Preferire l’ELPD-LOO più alto.</li>
<li>Differenza ≥ 2×SE → indicazione di vantaggio sostanziale.</li>
<li>Valutare anche semplicità e interpretabilità.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="level2" data-number="75.4"><h2 data-number="75.4" class="anchored" data-anchor-id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl">
<span class="header-section-number">75.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>
</h2>
<p>Oltre alla <em>Leave-One-Out Cross-Validation</em>, esistono altri strumenti per stimare la qualità predittiva di un modello senza dover conoscere la distribuzione vera dei dati. Molti di questi metodi derivano, in modo più o meno diretto, dalla <em>divergenza di Kullback–Leibler</em> <span class="math inline">\(D_{\text{KL}}\)</span>, che — come visto — misura la distanza tra la distribuzione reale e quella stimata dal modello.</p>
<p>L’idea di base è sempre la stessa:</p>
<ul>
<li>valutare quanto bene il modello spiega i dati (<em>bontà di adattamento</em>);</li>
<li>penalizzare la <em>complessità</em> del modello, per ridurre il rischio di <em>overfitting</em>.</li>
</ul>
<p>Questa logica si traduce in <em>criteri di informazione</em> che combinano due componenti:</p>
<ol type="1">
<li>
<em>termine di fit</em>: misura di quanto bene il modello si adatta ai dati osservati (es. log-verosimiglianza, MSE);</li>
<li>
<em>termine di penalizzazione</em>: aumenta con il numero di parametri o con la flessibilità del modello.</li>
</ol>
<p>Tra i criteri più usati troviamo:</p>
<ul>
<li>
<strong>MSE</strong> (Mean Squared Error) – semplice e intuitivo, basato sugli errori di previsione;</li>
<li>
<strong>AIC</strong> (Akaike Information Criterion) – approssima <span class="math inline">\(D_{\text{KL}}\)</span> tra il modello e la verità, penalizzando il numero di parametri;</li>
<li>
<strong>BIC</strong> (Bayesian Information Criterion) – simile all’AIC, ma con penalizzazione più forte per modelli complessi, proporzionale al numero di osservazioni;</li>
<li>
<strong>WAIC</strong> (Widely Applicable Information Criterion) – versione pienamente bayesiana, basata sulle previsioni del modello integrate sull’intera distribuzione a posteriori.</li>
</ul>
<p>Nelle sezioni seguenti vedremo come ciascun criterio si calcola, quali assunzioni richiede e in quali situazioni è preferibile rispetto agli altri.</p>
<section id="errore-quadratico-medio-mse" class="level3" data-number="75.4.1"><h3 data-number="75.4.1" class="anchored" data-anchor-id="errore-quadratico-medio-mse">
<span class="header-section-number">75.4.1</span> Errore Quadratico Medio (MSE)</h3>
<p>L’<em>Errore Quadratico Medio</em> misura la media delle differenze al quadrato tra valori osservati e previsti:</p>
<p><span id="eq-mse-def"><span class="math display">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
\tag{75.10}\]</span></span></p>
<ul>
<li>Valori più bassi indicano previsioni più vicine ai dati osservati.</li>
<li>Non tiene conto della complessità del modello, quindi può favorire modelli eccessivamente flessibili (<em>overfitting</em>).</li>
</ul>
<p>Utile per valutare l’accuratezza, ma da solo non è adatto a scegliere tra modelli con diversa complessità.</p>
</section><section id="akaike-information-criterion-aic" class="level3" data-number="75.4.2"><h3 data-number="75.4.2" class="anchored" data-anchor-id="akaike-information-criterion-aic">
<span class="header-section-number">75.4.2</span> Akaike Information Criterion (AIC)</h3>
<p>L’<em>AIC</em> è un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e stima quanta informazione si perde usando un modello per descrivere i dati:</p>
<p><span id="eq-aic-def"><span class="math display">\[
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k,
\tag{75.11}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(\hat{\theta}_{\text{MLE}}\)</span>: stima dei parametri ottenuta massimizzando la verosimiglianza;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri del modello.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine valuta l’adattamento del modello ai dati.</li>
<li>Il secondo penalizza la complessità per evitare overfitting.</li>
<li>Un AIC più basso indica un miglior equilibrio tra accuratezza e semplicità.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Basato su assunzioni asintotiche (funziona meglio con campioni grandi).</li>
<li>Usa solo stime puntuali, ignorando l’incertezza dei parametri.</li>
<li>Non è pienamente coerente con l’approccio bayesiano.</li>
</ul></section><section id="bayesian-information-criterion-bic" class="level3" data-number="75.4.3"><h3 data-number="75.4.3" class="anchored" data-anchor-id="bayesian-information-criterion-bic">
<span class="header-section-number">75.4.3</span> Bayesian Information Criterion (BIC)</h3>
<p>Il <em>BIC</em> valuta il compromesso tra <em>adattamento ai dati</em> e <em>complessità del modello</em>, applicando una penalizzazione più severa rispetto all’AIC — soprattutto quando il numero di osservazioni <span class="math inline">\(n\)</span> è grande.</p>
<p><span id="eq-bic-def"><span class="math display">\[
BIC = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k,
\tag{75.12}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(y \mid \hat{\theta})\)</span>: massima verosimiglianza del modello (o MAP con prior piatti);</li>
<li>
<span class="math inline">\(n\)</span>: numero di osservazioni indipendenti;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri stimati.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine misura l’adattamento ai dati.</li>
<li>Il secondo penalizza la complessità in modo crescente con <span class="math inline">\(n\)</span> e <span class="math inline">\(k\)</span>.</li>
<li>Un BIC più basso indica un compromesso migliore tra accuratezza e parsimonia.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Tende a favorire modelli più semplici quando <span class="math inline">\(n\)</span> è elevato.</li>
<li>Ha una giustificazione teorica bayesiana: in certe condizioni, approssima il log della <em>marginal likelihood</em>.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Si basa su assunzioni forti (indipendenza, modelli regolari, prior deboli).</li>
<li>Può sottoselezionare modelli utili con campioni piccoli o strutture complesse.</li>
</ul></section><section id="widely-applicable-information-criterion-waic" class="level3" data-number="75.4.4"><h3 data-number="75.4.4" class="anchored" data-anchor-id="widely-applicable-information-criterion-waic">
<span class="header-section-number">75.4.4</span> Widely Applicable Information Criterion (WAIC)</h3>
<p>Il <em>WAIC</em> è una versione <em>pienamente bayesiana</em> dell’AIC:</p>
<ul>
<li>utilizza <em>tutta la distribuzione a posteriori</em> dei parametri;</li>
<li>fornisce una stima diretta della <em>capacità predittiva</em> del modello.</li>
</ul>
<p><span id="eq-waic-def"><span class="math display">\[
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \mathrm{Var}_{\theta^{(s)}} \big( \log p(y_i \mid \theta^{(s)}) \big)
\right],
\tag{75.13}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(S\)</span> = numero di campioni dalla distribuzione a posteriori;</li>
<li>
<span class="math inline">\(\theta^{(s)}\)</span> = <span class="math inline">\(s\)</span>-esimo campione;</li>
<li>il secondo termine stima il <em>numero effettivo di parametri</em> basato sulla variabilità della log-verosimiglianza.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Adatto anche a modelli complessi o non regolari.</li>
<li>Usa direttamente i campioni MCMC.</li>
<li>Migliore dell’AIC per modelli bayesiani, perché incorpora l’incertezza dei parametri.</li>
</ul>
<p><strong>Nota</strong> Il WAIC è strettamente collegato all’ELPD: è una sua stima approssimata ottenuta dalla posteriori, senza bisogno di eseguire la LOO-CV.</p>
<p><strong>Riepilogo comparativo.</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead><tr class="header">
<th>Criterio</th>
<th>Tipo</th>
<th>Penalizza la complessità?</th>
<th>Usa stime puntuali?</th>
<th>Supporta Bayesian MCMC?</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSE</td>
<td>Frequentista</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>AIC</td>
<td>Frequentista</td>
<td>✅ (modesta)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="odd">
<td>BIC</td>
<td>Frequentista/Bayesiano</td>
<td>✅ (forte)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>WAIC</td>
<td>Bayesiano</td>
<td>✅ (effettiva)</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr class="odd">
<td>LOO-CV</td>
<td>Bayesiano</td>
<td>✅ (empirica)</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table></section></section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>La selezione del modello, in ottica bayesiana, ruota attorno a una domanda essenziale: <em>quanto bene il modello predice dati che non ha mai visto?</em></p>
<p>Il riferimento teorico è l’<em>Expected Log Predictive Density (ELPD)</em>, che misura quanto la distribuzione predittiva del modello si avvicina alla vera (e ignota) distribuzione dei dati. In termini matematici, massimizzare l’ELPD equivale a minimizzare la <em>divergenza di Kullback–Leibler</em> rispetto alla vera generatrice: due facce dello stesso obiettivo, rappresentare al meglio la realtà sottostante.</p>
<p>Poiché <span class="math inline">\(p_{\text{vera}}(y)\)</span> è sconosciuta, l’ELPD va stimato. Le principali approssimazioni sono:</p>
<ul>
<li>
<strong>LOO-CV</strong> (Leave-One-Out Cross-Validation): oggi lo strumento più affidabile, valuta ogni osservazione come “nuova” e stima la capacità di generalizzazione del modello.</li>
<li>
<strong>WAIC</strong>: alternativa completamente bayesiana, calcolata direttamente dai campioni della posteriori.</li>
<li>
<strong>AIC</strong> e <strong>BIC</strong>: criteri frequenstisti più rapidi ma basati su stime puntuali; utili in contesti semplici.</li>
<li>
<strong>MSE</strong>: misura l’accuratezza sulle osservazioni note, ma non penalizza la complessità e quindi non è adatto alla selezione del modello.</li>
</ul>
<p>Nel confronto tra modelli, la <em>differenza di ELPD</em> (stimata con LOO-CV o WAIC) andrebbe interpretata insieme al relativo <em>errore standard</em>: una regola pratica è considerare rilevante una differenza almeno doppia rispetto all’errore standard.</p>
<p><strong>In sintesi:</strong></p>
<ul>
<li>la buona statistica non si limita a spiegare il passato: sa <em>anticipare il futuro</em>;</li>
<li>la <em>divergenza KL</em> fornisce la misura teorica della distanza tra modello e realtà;</li>
<li>l’<em>ELPD</em>, stimato via LOO-CV o WAIC, traduce questa misura in una valutazione pratica della capacità predittiva;</li>
<li>la scelta del modello ottimale richiede un equilibrio tra accuratezza, generalizzazione e parsimonia.</li>
</ul>
<p>Con questi strumenti possiamo individuare modelli che colgono i veri pattern nei dati, evitando di farsi ingannare dal rumore e garantendo previsioni solide anche in contesti complessi.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’ambiente di sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] pillar_1.11.0         tinytable_0.11.0      patchwork_1.3.1      </span></span>
<span><span class="co">#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.13.0     </span></span>
<span><span class="co">#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.0     </span></span>
<span><span class="co">#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         </span></span>
<span><span class="co">#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           </span></span>
<span><span class="co">#&gt; [16] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    </span></span>
<span><span class="co">#&gt; [19] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          </span></span>
<span><span class="co">#&gt; [22] tidyr_1.3.1           rio_1.2.3             here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] svUnit_1.0.6         tidyselect_1.2.1     farver_2.1.2        </span></span>
<span><span class="co">#&gt;  [4] fastmap_1.2.0        TH.data_1.1-3        tensorA_0.36.2.1    </span></span>
<span><span class="co">#&gt;  [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    </span></span>
<span><span class="co">#&gt; [10] lifecycle_1.0.4      processx_3.8.6       survival_3.8-3      </span></span>
<span><span class="co">#&gt; [13] magrittr_2.0.3       compiler_4.5.1       rlang_1.1.6         </span></span>
<span><span class="co">#&gt; [16] tools_4.5.1          yaml_2.3.10          data.table_1.17.8   </span></span>
<span><span class="co">#&gt; [19] knitr_1.50           bridgesampling_1.1-2 htmlwidgets_1.6.4   </span></span>
<span><span class="co">#&gt; [22] pkgbuild_1.4.8       curl_6.4.0           cmdstanr_0.9.0      </span></span>
<span><span class="co">#&gt; [25] RColorBrewer_1.1-3   abind_1.4-8          multcomp_1.4-28     </span></span>
<span><span class="co">#&gt; [28] withr_3.0.2          purrr_1.1.0          grid_4.5.1          </span></span>
<span><span class="co">#&gt; [31] stats4_4.5.1         xtable_1.8-4         colorspace_2.1-1    </span></span>
<span><span class="co">#&gt; [34] inline_0.3.21        emmeans_1.11.2       scales_1.4.0        </span></span>
<span><span class="co">#&gt; [37] MASS_7.3-65          cli_3.6.5            mvtnorm_1.3-3       </span></span>
<span><span class="co">#&gt; [40] rmarkdown_2.29       generics_0.1.4       RcppParallel_5.1.10 </span></span>
<span><span class="co">#&gt; [43] cachem_1.1.0         stringr_1.5.1        splines_4.5.1       </span></span>
<span><span class="co">#&gt; [46] parallel_4.5.1       vctrs_0.6.5          V8_6.0.5            </span></span>
<span><span class="co">#&gt; [49] Matrix_1.7-3         sandwich_3.1-1       jsonlite_2.0.0      </span></span>
<span><span class="co">#&gt; [52] arrayhelpers_1.1-0   glue_1.8.0           ps_1.9.1            </span></span>
<span><span class="co">#&gt; [55] codetools_0.2-20     distributional_0.5.0 lubridate_1.9.4     </span></span>
<span><span class="co">#&gt; [58] stringi_1.8.7        gtable_0.3.6         QuickJSR_1.8.0      </span></span>
<span><span class="co">#&gt; [61] htmltools_0.5.8.1    Brobdingnag_1.2-9    R6_2.6.1            </span></span>
<span><span class="co">#&gt; [64] rprojroot_2.1.0      evaluate_1.0.4       lattice_0.22-7      </span></span>
<span><span class="co">#&gt; [67] backports_1.5.0      memoise_2.0.1        broom_1.0.9         </span></span>
<span><span class="co">#&gt; [70] snakecase_0.11.1     rstantools_2.4.0     coda_0.19-4.1       </span></span>
<span><span class="co">#&gt; [73] gridExtra_2.3        nlme_3.1-168         checkmate_2.3.2     </span></span>
<span><span class="co">#&gt; [76] xfun_0.52            zoo_1.8-14           pkgconfig_2.0.3</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/formal_models/introduction.html" class="pagination-link" aria-label="Introduzione">
        <span class="nav-page-text">Introduzione</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>