# Flusso di lavoro riproducibile {#sec-r-workflow}

::: callout-note
## In questo capitolo imparerai a

- organizzazione un progetto di analisi dei dati
- conoscere le funzionalità di `targets` e di Pixi
:::

::: callout-tip
## Prerequisiti

- Leggere "1,500 scientists lift the lid on reproducibility" [@baker20161].
:::

::: callout-important
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(readr)
```
:::


## Introduzione

In questo capitolo esploreremo gli strumenti essenziali per garantire che il flusso di lavoro di un progetto di analisi dei dati in R sia pienamente riproducibile.

## Analisi e Flussi di Lavoro Riproducibili

La possibilità di confermare ripetutamente i risultati scientifici attraverso la replica è un principio fondamentale della scienza. Questo concetto si basa sull'idea che una verità scientifica debba resistere a ulteriori indagini da parte di altri osservatori. In ambito scientifico, è utile distinguere due aspetti legati alla replica: **replicabilità** e **riproducibilità**.

- **Replicabilità**: si riferisce alla capacità di ottenere risultati simili con dati diversi, ma seguendo lo stesso protocollo sperimentale.
- **Riproducibilità**: indica la capacità di ottenere gli stessi risultati utilizzando gli stessi dati e lo stesso metodo di analisi, sia dalla stessa persona che da altri.

### Riproducibilità dei Dati

La replicazione di esperimenti fisici può presentare difficoltà pratiche significative. Tuttavia, anche riprodurre semplicemente un’analisi dei dati, che sembra un compito più semplice, è spesso problematico per molteplici ragioni. Tradizionalmente, i ricercatori annotavano scrupolosamente i dettagli sperimentali nei loro taccuini di laboratorio, consentendo di replicare l’esperimento. Oggi, strumenti software moderni permettono di applicare lo stesso principio alla riproduzione dei dati: tutto il necessario per rifare l’analisi deve essere documentato in modo chiaro e centralizzato.

Questi strumenti non solo facilitano la ripetizione dell’analisi, ma permettono anche di migliorarla e applicarla facilmente a nuovi dati. Tuttavia, per essere riproducibile, l’analisi deve essere scritta in modo appropriato, utilizzando ambienti di programmazione statistica come R o Python, che permettono l’automazione del processo. Al contrario, software come i fogli di calcolo non sono adatti a garantire riproducibilità, perché legano i comandi a celle specifiche, rendendo l’adattamento a nuovi dati complesso e soggetto a errori.

## La Crisi della Replicazione e la Riproducibilità

La crisi della replicazione evidenzia un problema crescente nella scienza moderna: molti studi pubblicati, anche sottoposti a peer review, non sono replicabili. Studi come quello di @ioannidis2005most e @baker20161 denunciano che molte ricerche sperimentali e statistiche non resistono alla verifica di altri ricercatori. Questo significa che, nonostante l'apparente validità dei risultati, spesso non è possibile raggiungere le stesse conclusioni ripetendo lo studio.

Tra le cause di questa crisi figurano problematiche complesse come la molteplicità e i percorsi analitici alternativi (il cosiddetto *garden of forking paths*). Tuttavia, indipendentemente da queste difficoltà, un’analisi riproducibile è un requisito minimo per garantire la validità dei risultati.

::: {.callout-note}

Un problema che compromette la solidità delle conclusioni di molti studi è stato definito da [Andrew Gelman](https://statmodeling.stat.columbia.edu/), della Columbia University, come il [garden of forking paths](https://en.wikipedia.org/wiki/Forking_paths_problem) (giardino dei sentieri che si biforcano). La maggior parte delle analisi richiede una serie di decisioni su come codificare i dati, identificare i fattori rilevanti e formulare (e successivamente rivedere) i modelli prima di arrivare alle analisi finali. Questo processo implica spesso l'esame dei dati per costruire una rappresentazione parsimoniosa. Ad esempio, un predittore continuo potrebbe essere suddiviso arbitrariamente in gruppi per valutare la relazione con l'esito, oppure alcune variabili potrebbero essere incluse o escluse da un modello di regressione durante una fase esplorativa.

Questo approccio tende a favorire risultati dei test di ipotesi che sono distorti verso il rigetto dell'ipotesi nulla, poiché le decisioni prese durante il processo possono privilegiare segnali più forti (o p-value più piccoli) rispetto ad altre alternative. Nella maggior parte dei problemi di data science, questo rappresenta una sfida importante che solleva dubbi sulla riproducibilità dei risultati.

:::

## Elementi chiave per un workflow riproducibile

Un flusso di lavoro riproducibile si compone di tre elementi fondamentali:

1. **Ambienti di programmazione statistica scriptabili**: software come R o Python consentono di automatizzare le analisi e ridurre gli errori manuali.
2. **Analisi riproducibili**: basate sull’approccio della *literate programming*, dove codice e documentazione sono integrati per garantire trasparenza e comprensione.
3. **Controllo di versione**: sistemi come Git e piattaforme come GitHub permettono di monitorare e documentare i cambiamenti, favorendo la collaborazione e la tracciabilità.

Integrare questi componenti nella pratica quotidiana non solo migliora la qualità delle analisi, ma contribuisce a rafforzare la fiducia nella scienza.

## Gestione del Workflow

L’utilizzo di R per scrivere script che documentano le analisi dei dati è un importante passo avanti verso la riproducibilità, ma non garantisce automaticamente che il progetto possa essere replicato con successo da altri ricercatori. A conferma di ciò, uno studio condotto da @obels2020analysis ha analizzato la condivisione di dati e codice per articoli pubblicati come *Registered Reports* nella letteratura psicologica tra il 2014 e il 2018. Tentando di riprodurre i risultati principali di ciascun articolo, hanno osservato quanto segue:

> "Abbiamo esaminato dati e script condivisi per i Registered Reports pubblicati nella letteratura psicologica dal 2014 al 2018 e tentato di riprodurre computazionalmente i risultati principali di ciascun articolo. Dei 62 articoli che soddisfacevano i nostri criteri di inclusione, 41 mettevano a disposizione i dati e 37 gli script di analisi. Dati e codice erano condivisi per 36 articoli. Siamo riusciti a eseguire gli script per 31 analisi e a riprodurre i risultati principali di 21 articoli. Sebbene la percentuale di articoli con dati e codice condivisi (58%) e quella degli articoli riproducibili computazionalmente (58%) siano relativamente alte rispetto ad altri studi, c'è un evidente margine di miglioramento."

Questi risultati evidenziano come, anche con dati e codice disponibili, la riproducibilità non sia garantita. Gli script possono essere incompleti o l'ambiente di sviluppo originale potrebbe non essere replicabile da altri ricercatori.

### Strumenti per la Gestione del Workflow

Per migliorare la riproducibilità, esistono diversi strumenti per la gestione del workflow, che permettono di strutturare i progetti in modo chiaro e ripetibile:

- **Make**: uno strumento storico per sistemi Unix, noto per la sua portabilità e la presenza predefinita su molti sistemi.
- **Snakemake**: popolare in biologia, offre flessibilità nella gestione di workflow complessi.
- **targets**: specifico per R, consente una gestione semplice ed efficace di workflow riproducibili in progetti di analisi dati.
- **Pixi**: un workflow manager innovativo basato sull'ecosistema **Conda**.

Questi strumenti aiutano a coordinare i diversi passaggi di un'analisi, mantenendo traccia dei file e garantendo che ogni fase del processo sia ripetibile.

#### Limiti degli Strumenti di Workflow Management

L’adozione di strumenti per la gestione del workflow richiede un certo investimento iniziale. Oltre a scrivere gli script per l’analisi dei dati, è necessario sviluppare ulteriori script per definire il workflow, riorganizzando il progetto per garantire riproducibilità. Questo processo può rendere più complesso il debug e le modifiche successive, motivo per cui alcuni ricercatori possono trovare impegnativo integrare tali strumenti.

## Funzioni e Astrazione

Il pacchetto **`targets`** è uno strumento per R che semplifica la gestione dei workflow riproducibili. La sua utilità emerge soprattutto nei progetti complessi, dove il codice può diventare lungo e difficile da gestire. 

### Gestione di codice lungo e complesso

Nel contesto di analisi dati, il codice può includere diverse sezioni che svolgono compiti specifici e spesso ripetuti, come l'importazione dei dati, la pulizia, l'analisi e la generazione di report. Man mano che il progetto cresce, tenere traccia delle diverse parti del codice e farle funzionare in sequenza può diventare complicato.

Un codice ben scritto deve essere chiaro e comunicare il suo scopo in modo efficace. La trasparenza aumenta la possibilità che il codice sia compreso, sia dagli altri ricercatori sia da te stesso in futuro.

Un modo efficace per affrontare la complessità è suddividere il codice in **funzioni**, utilizzando il concetto di **astrazione** per ridurre la complessità e migliorare la leggibilità.

### Funzioni: Gestire la Complessità

Una **funzione** è un blocco di codice che realizza un compito specifico. Le funzioni permettono di evitare ripetizioni, rendendo il codice più compatto e leggibile. In R esistono molte funzioni integrate, come `mean()` per calcolare la media aritmetica, ma, come abbiamo visto in precedenza, è possibile scrivere funzioni personalizzate per compiti specifici.

Le funzioni devono essere il più generali possibile per favorirne il riutilizzo in diversi progetti.

### Il Concetto di Astrazione

**L’astrazione** consiste nel suddividere il codice complesso in compiti più piccoli e specifici, delegando i dettagli a funzioni. Questo approccio semplifica la struttura del codice principale (main script), rendendolo più leggibile e autoesplicativo [@filazzola2022call].

Nel main script, le funzioni sono richiamate in sequenza, mentre i dettagli sono "nascosti" nei file che contengono le definizioni delle funzioni. Ad esempio:

#### Main Script:

```r
library(tidyverse)
library(here)

source("R/functions.R")

# Importa e pulisci i dati
raw_data <- read_csv(here("data/raw_data.csv"))
cleaned_data <- clean_data(raw_data)

# Modello
my_model <- fit_model(data = cleaned_data, response = "Value", predictor = "Gradient")
summary(my_model)

# Grafico
my_plot <- make_plot(cleaned_data)
```

#### Script delle Funzioni:

```r
clean_data <- function(data){
  data |> 
    filter(!is.na(Value)) |> 
    mutate(Gradient = recode(Gradient, "C" = "Control", "B" = "Treatment")) |> 
    filter(Taxon == "SpeciesA")
}

fit_model <- function(data, response, predictor){
  lm(as.formula(paste(response, "~", predictor)), data = data)
}

make_plot <- function(data){
  ggplot(data, aes(x = Gradient, y = Value)) +
    geom_boxplot()
}
```

Questa separazione migliora la leggibilità, la modularità e la riutilizzabilità del codice.

## Introduzione a `targets`

`targets` è uno strumento per la gestione di pipeline in R, progettato per coordinare i diversi passaggi di un’analisi dati. Permette di automatizzare e ottimizzare il workflow, gestire le dipendenze tra i vari step, e tenere traccia degli oggetti obsoleti che necessitano di essere aggiornati. Una delle sue caratteristiche chiave è la capacità di evitare inutili ripetizioni, riducendo i tempi di esecuzione.

In una pipeline di `targets`, ogni passaggio rappresenta un "target", che può essere un oggetto R come un dataset, un modello o una figura. Ogni target è generato da una funzione e la pipeline viene orchestrata da uno script principale chiamato `_targets.R`, che tiene traccia delle dipendenze tra i target e ne garantisce l’esecuzione nell’ordine corretto.

Questa struttura è coerente con il concetto di **astrazione**: suddividere il codice complesso in compiti più semplici e ben definiti.

### Quando utilizzare `targets`?

- **Codice complesso o con lunghi tempi di esecuzione**: Quando il codice richiede molto tempo per essere elaborato, `targets` evita di rieseguire le parti già aggiornate e supporta l'elaborazione in parallelo, ottimizzando i tempi di calcolo.  
- **Workflow con dipendenze tra i passaggi**: Se il flusso di lavoro include step interconnessi, `targets` semplifica la gestione delle dipendenze, garantendo che ogni step sia eseguito nell'ordine corretto.  
- **Riproducibilità dell’analisi**: Automatizza il controllo dell’allineamento tra codice, dati e risultati, assicurando che il workflow sia sempre coerente e riproducibile.  

In breve, `targets` consente di creare pipeline di analisi dati riproducibili e scalabili, migliorando l’efficienza e la gestione del workflow in R. Per un approfondimento pratico, è possibile consultare il tutorial disponibile nella pagina web [The {targets} R package user manual](https://books.ropensci.org/targets/walkthrough.html).

## Pixi

Oltre a `targets`, vale la pena citare **Pixi**, un moderno workflow manager progettato per semplificare la gestione dei flussi di lavoro nei progetti di analisi dati. Pixi si distingue per la sua semplicità d’uso e la capacità di affrontare le sfide legate alla riproducibilità e all’organizzazione di progetti complessi.

Pixi è un gestore di pacchetti rapido e versatile, basato sull'ecosistema **Conda**, che facilita la creazione e la gestione di ambienti di sviluppo su piattaforme come Windows, macOS e Linux. Supporta un’ampia gamma di linguaggi di programmazione, tra cui **Python**, **R**, **C/C++**, **Rust** e **Ruby**, offrendo una grande flessibilità per progetti interdisciplinari. Una delle sue caratteristiche principali è la possibilità di creare ambienti riproducibili senza la complessità aggiuntiva di strumenti come Docker, riducendo così i tempi e gli sforzi necessari per configurare e mantenere il progetto.

Con Pixi, una configurazione iniziale ben organizzata permette di concentrare le energie sull'analisi dei dati e sulla produzione di risultati, garantendo al contempo un workflow riproducibile e ben strutturato. Questo lo rende uno strumento ideale per migliorare l'efficienza e l’affidabilità dei progetti di analisi dati.

### Installazione di Pixi

Per utilizzare Pixi, è necessario avere installati **R**, **RStudio** e Pixi stesso. L'installazione di Pixi è semplice seguendo le [indicazioni ufficiali](https://github.com/prefix-dev/pixi). Per installare Pixi, è sufficiente eseguire il seguente comando nel terminale:

```bash
curl -fsSL https://pixi.sh/install.sh | bash
```

### Struttura del Progetto

Immaginiamo che il progetto segua questa struttura organizzativa:

```
data-analysis-project/
│
├── pixi.toml               # File di configurazione Pixi
├── pixi.lock               # Lockfile per le dipendenze
│
├── data/                   # Directory per i dati
│   ├── raw/                # Dati grezzi
│   └── processed/          # Dati processati
│
├── src/                    # Codice sorgente in R
│   ├── data_cleaning.R     # Script per pulizia dei dati
│   ├── analysis.R          # Script per analisi
│   └── visualization.R     # Script per visualizzazioni
│
├── reports/                # Report e output
│   ├── figures/            # Figure generate
│   └── report.Rmd          # Report in R Markdown
│
└── README.md               # Documentazione del progetto
```

---

### Configurazione di Pixi

Per configurare Pixi, si crea un file `pixi.toml` nella directory principale del progetto:

```toml
[project]
name = "r-data-analysis"
description = "Progetto di analisi dati con R"
authors = ["Tuo Nome <tua.email@example.com>"]
channels = ["conda-forge"]
platforms = ["osx-64", "linux-64","win-64"]

[dependencies]
r-base = ">=4.3,<5"
r-tidyverse = "*"
r-rio= "*"
r-knitr = "*"
r-rmarkdown = "*"
r-here = "*"

[tasks]
clean = "rm -rf reports/figures/* data/processed/*"
data-prep = "Rscript src/data_cleaning.R"
analysis = "Rscript src/analysis.R"
report = "Rscript -e 'rmarkdown::render(\"reports/report.Rmd\")'"
all = { depends-on = ["clean", "data-prep", "analysis", "report"] }
```


### Script di Pulizia Dati (`src/data_cleaning.R`)

Uno script per pulire e organizzare i dati grezzi:

```r
library(readr)
library(dplyr)

# Caricamento dati grezzi
raw_data <- read_csv("data/raw/dati_esempio.csv")

# Pulizia e trasformazione dei dati
cleaned_data <- raw_data %>%
  filter(!is.na(valore)) %>%
  mutate(categoria = factor(categoria)) %>%
  group_by(categoria) %>%
  summarise(media = mean(valore, na.rm = TRUE))

# Salvataggio dei dati processati
write_csv(cleaned_data, "data/processed/dati_puliti.csv")
```


### Script di Analisi (`src/analysis.R`)

Uno script per eseguire analisi statistiche e creare visualizzazioni:

```r
library(readr)
library(dplyr)
library(ggplot2)

# Caricamento dei dati processati
dati <- read_csv("data/processed/dati_puliti.csv")

# Analisi statistica
risultati_analisi <- dati %>%
  group_by(categoria) %>%
  summarise(
    media = mean(media),
    deviazione_standard = sd(media)
  )

# Salvataggio dei risultati
write_csv(risultati_analisi, "reports/risultati_analisi.csv")

# Creazione di un grafico
grafico <- ggplot(dati, aes(x = categoria, y = media)) +
  geom_bar(stat = "identity") +
  ggtitle("Media per Categoria")

# Salvataggio del grafico
ggsave("reports/figures/media_categoria.png", plot = grafico)
```

### Creazione di un Report (`reports/report.Rmd`)

Un report generato con R Markdown per presentare i risultati dell'analisi:

````
---
title: "Report di Analisi dei Dati"
output: html_document
---

## Risultati dell'Analisi

```
library(readr)
library(knitr)
risultati <- read_csv("reports/risultati_analisi.csv")
kable(risultati)
```

## Grafico delle Medie per Categoria

![Media per Categoria](figures/media_categoria.png)
````

### Esecuzione del Workflow con Pixi

Una volta installato Pixi, puoi inizializzare un nuovo progetto con:

```bash
# Inizializzazione del progetto
pixi init
```

Per installare le dipendenze specificate in `in pixi.toml`, esegui:

```bash
# Installazione delle dipendenze specificate in pixi.toml
pixi install
```

Questo comando crea un file `pixi.lock` che elenca tutte le dipendenze del progetto, garantendo che l'ambiente possa essere ricreato in modo identico su diverse macchine.

Per eseguire le attività definite, utilizza:

```bash
pixi run data-prep
pixi run analysis
pixi run report
```

Oppure, per eseguire tutte le attività in sequenza:

```bash
pixi run all
```

Il codice utilizzato in questo tutorial è disponibile nel [repository GitHub dedicato](https://github.com/ccaudek/pixi-tutorial).

## Riflessioni Conclusive

Strumenti di workflow management come `targets` e Pixi offrono soluzioni efficaci per semplificare e ottimizzare la gestione dei progetti di analisi dati in R. Automatizzando i flussi di lavoro, questi strumenti garantiscono riproducibilità, coerenza e organizzazione, riducendo la possibilità di errori. Una configurazione iniziale ben strutturata consente di focalizzarsi sull’analisi e sull’interpretazione dei dati, massimizzando l’efficienza e migliorando la qualità complessiva del processo.


## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

