<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>77&nbsp; Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/formal_models/introduction.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-52ea30a691a390814f50c81eb384824c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c35ff93b8eba83909532149344eaeead.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><style>html{ scroll-behavior: smooth; }</style>
<script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
</script><script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
// Suggerimento CSS: vedi sezione 3 per gli spazi attorno a display math
</script><script>
window.MathJax = {
  tex: {
    packages: {'[+]': ['boldsymbol']},
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: { fontCache: 'global' }
};
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../style/styles.css">
<link rel="stylesheet" href="../../style/_typography-extras.css">
<link rel="stylesheet" href="../../style/_code-extras.css">
<link rel="stylesheet" href="../../style/_math-extras.css">
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Il ciclo di vita di un progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Introduzione pratica a Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_odds_ratio_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Modello di Poisson (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04a_stan_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Regressione lineare in Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni con la regressione logistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_logistic_process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Dal GLM a un modello processuale per dati binari</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Teorie formali dei fenomeni psicologici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Il modello di Rescorla–Wagner</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Decisioni</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/decision_analysis/01_study_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Analisi delle decisioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Missing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/missing/01_mnar_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Dati mancanti in psicologia: identificare e modellare i casi MNAR con un approccio Bayesiano in Stan</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#distribuzione-predittiva-posteriore" id="toc-distribuzione-predittiva-posteriore" class="nav-link active" data-scroll-target="#distribuzione-predittiva-posteriore"><span class="header-section-number">77.1</span> Distribuzione predittiva posteriore</a></li>
  <li><a href="#sec-logscore" id="toc-sec-logscore" class="nav-link" data-scroll-target="#sec-logscore"><span class="header-section-number">77.2</span> Il log-score: accuratezza predittiva punto per punto</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" id="toc-leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica"><span class="header-section-number">77.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</a></li>
  <li><a href="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" id="toc-criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="nav-link" data-scroll-target="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl"><span class="header-section-number">77.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span></a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="epigraph">
<blockquote class="blockquote">
<p>“Se uno scienziato è costretto a scegliere fra due ipotesi, il suo istinto sarà quello di scegliere la più semplice. Questa inclinazione non è un capriccio, ma una necessità logica.”</p>
<p>– <strong>E.T. Jaynes</strong>, Fisico e Statistico</p>
</blockquote>
</div>
<section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<div class="lead">
<p>Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’<em>entropia</em>, che misura l’incertezza insita in una distribuzione, e la <em>divergenza di Kullback–Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per <em>valutare e confrontare modelli statistici</em> nel contesto bayesiano.</p>
</div>
<p>Il punto di partenza è una domanda cruciale: <em>quanto bene il modello riesce a prevedere nuovi dati?</em> Un buon modello non deve solo adattarsi bene ai dati già osservati, ma anche saper <em>generalizzare</em> a situazioni future o a campioni mai visti. Questa distinzione — adattamento vs.&nbsp;generalizzazione — è il cuore della valutazione predittiva.</p>
<p>Per rendere concreta questa idea, immaginiamo di aver sviluppato un test psicologico per prevedere il livello di ansia degli studenti alla vigilia di un esame. Non basta sapere che il modello descrive bene i dati del campione che abbiamo usato per costruirlo: vogliamo anche essere ragionevolmente sicuri che le stesse previsioni funzionino per studenti che non hanno partecipato allo studio. In psicologia, scegliere tra due modelli non è diverso dal decidere quale test usare per prevedere un disturbo: entrambi mirano a capire quale strumento fornisce previsioni più affidabili sui dati futuri.</p>
<p>In questo capitolo esploreremo gli strumenti fondamentali per la valutazione e il confronto di modelli nell’ambito dell’inferenza bayesiana.</p>
<p><strong>1. La distribuzione predittiva posteriore</strong><br>
Introdurremo la <em>distribuzione predittiva posteriore</em>, che incorpora l’incertezza sui parametri per generare previsioni coerenti con lo stato di conoscenza del modello. Questo strumento rappresenta il ponte naturale tra stima e previsione, garantendo una quantificazione probabilistica completa dell’incertezza.</p>
<p><strong>2. Misure di accuratezza predittiva</strong><br>
Discuteremo il <em>log-score</em>, una metrica punto per punto che valuta la qualità delle previsioni, e due sue sintesi fondamentali:</p>
<ul>
<li>la <em>LPPD</em> (<em>Log Pointwise Predictive Density</em>), che misura la bontà di adattamento su dati osservati;<br>
</li>
<li>l’<em>ELPD</em> (<em>Expected Log Predictive Density</em>), che stima l’abilità predittiva attesa su nuove osservazioni.</li>
</ul>
<p><strong>3. Validazione empirica e confronto tra modelli</strong><br>
Presenteremo la tecnica <em>Leave-One-Out Cross-Validation</em> (LOO-CV), un approccio efficiente per stimare l’ELPD senza bisogno di nuovi dati, dimostrando come questa metodologia fornisca una valutazione robusta delle prestazioni predittive.</p>
<p><strong>4. Fondamenti teorici e interpretazione</strong><br>
Approfondiremo il legame tra ELPD e <em>divergenza di Kullback-Leibler</em>, che consente di interpretare il confronto tra modelli come una ricerca del modello più vicino alla vera distribuzione generatrice dei dati. Questa connessione teorica fornisce una solida giustificazione informazionale per le procedure di selezione bayesiana.</p>
<p>L’obiettivo del capitolo è offrire una panoramica completa e operativa, che unisca principi teorici a strumenti applicativi, guidando il lettore nella scelta razionale del modello più adatto al problema in esame.</p>
<section id="panoramica-del-capitolo" class="level3 unnumbered unlisted"><h3 class="unnumbered unlisted anchored" data-anchor-id="panoramica-del-capitolo">Panoramica del capitolo</h3>
<ul>
<li>Cos’è la distribuzione predittiva posteriore e come si costruisce.</li>
<li>Cosa misura il log-score e come si calcola nella pratica.</li>
<li>Distinzione tra LPPD ed ELPD e il loro significato;</li>
<li>Come LOO-CV fornisce una stima dell’ELPD;</li>
<li>Il confronto tra modelli alla divergenza di Kullback-Leibler.</li>
</ul>
<hr>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="Preparazione del Notebook">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gt.rstudio.com">gt</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://conflicted.r-lib.org/">conflicted</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mc-stan.org/loo/">loo</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://conflicted.r-lib.org/reference/conflicts_prefer.html">conflicts_prefer</a></span><span class="op">(</span><span class="fu">rstan</span><span class="fu">::</span><span class="va"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section><section id="distribuzione-predittiva-posteriore" class="level2" data-number="77.1"><h2 data-number="77.1" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore">
<span class="header-section-number">77.1</span> Distribuzione predittiva posteriore</h2>
<p>Nel capitolo precedente abbiamo usato la <em>divergenza di Kullback–Leibler (KL)</em> come misura teorica della distanza tra realtà e modello. Qui ci chiediamo: <em>come stimiamo questa distanza quando la “vera” distribuzione generatrice è ignota?</em> Un tassello fondamentale è la <em>distribuzione predittiva posteriore</em>.</p>
<p>Nel capitolo sul modello <em>beta–binomiale</em> l’abbiamo già incontrata: è lo strumento che, nell’approccio bayesiano, consente di prevedere nuovi dati incorporando sia la struttura del modello sia l’incertezza sui parametri.</p>
<p>In sintesi: dopo aver osservato i dati <span class="math inline">\(y\)</span>, non otteniamo un singolo “miglior” valore dei parametri, ma una <em>distribuzione posteriore</em> <span class="math inline">\(p(\theta \mid y)\)</span> che quantifica i valori plausibili di <span class="math inline">\(\theta\)</span> e la nostra incertezza.</p>
<blockquote class="blockquote">
<p><strong>Esempio.</strong> Uno psicologo che stima il livello medio di ansia in una popolazione, invece di affermare “la media è 4.7”, dirà: “il valore più plausibile è 4.7, <strong>ma</strong> è ragionevole che sia tra 4.2 e 5.1”, riflettendo la variabilità posteriore.</p>
</blockquote>
<p>Per prevedere un nuovo dato <span class="math inline">\(\tilde y\)</span>, non fissiamo <span class="math inline">\(\theta\)</span>. <em>Media</em>mo invece tutte le previsioni condizionate <span class="math inline">\(p(\tilde y \mid \theta)\)</span> pesandole con la posteriore <span class="math inline">\(p(\theta\mid y)\)</span>:</p>
<p><span class="math display">\[
q(\tilde{y} \mid y)
\;=\;
\int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta .
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Intuizione">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuizione
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Se conoscessimo il valore vero di <span class="math inline">\(\theta\)</span>, potremmo prevedere i dati futuri usando la distribuzione predittiva condizionata:</p>
<p><span class="math display">\[
p(\tilde y \mid \theta).
\]</span></p>
<p>Il problema è che <span class="math inline">\(\theta\)</span> non lo conosciamo: abbiamo soltanto la distribuzione a posteriori <span class="math inline">\(p(\theta\mid y)\)</span>. Perciò, la distribuzione predittiva posteriore si costruisce combinando le previsioni condizionate per ogni valore possibile di <span class="math inline">\(\theta\)</span>, pesandole con quanto ciascun valore è plausibile a posteriori:</p>
<p><span class="math display">\[
p(\tilde y\mid y) = \int p(\tilde y\mid \theta)\,p(\theta\mid y)\,d\theta.
\]</span></p>
<p>Per fare un esempio concreto, consideriamo il caso binomiale. Supponiamo che i dati futuri siano generati da una Binomiale con <span class="math inline">\(m\)</span> prove e parametro <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
p(\tilde y = x \mid \theta) = \binom{m}{x}\,\theta^x(1-\theta)^{m-x}.
\]</span></p>
<p>La distribuzione predittiva posteriore diventa:</p>
<p><span class="math display">\[
p(\tilde y = x \mid y) = \int \binom{m}{x}\,\theta^x(1-\theta)^{m-x}\,p(\theta\mid y)\,d\theta.
\]</span></p>
<p>In alcuni casi particolari (per esempio con prior Beta e dati binomiali) questo integrale si può risolvere analiticamente, ottenendo la <em>Beta–Binomiale</em>. Ma in generale non c’è una formula chiusa e serve un’approssimazione numerica.</p>
<p><strong>Approssimazione numerica con il metodo su griglia.</strong> L’idea è semplice: sostituire l’integrale con una somma pesata su una griglia di valori possibili di <span class="math inline">\(\theta\)</span>. I passaggi algoritmici sono i seguenti.</p>
<ol type="1">
<li>
<p><strong>Definire una griglia di valori di <span class="math inline">\(\theta\)</span></strong>, ad esempio 1000 punti equispaziati tra 0 e 1:</p>
<p><span class="math display">\[
\theta_1, \theta_2, \dots, \theta_J.
\]</span></p>
</li>
<li>
<p><strong>Calcolare la posteriore su ciascun punto della griglia</strong>. Nel caso Beta–Binomiale:</p>
<p><span class="math display">\[
p(\theta_j \mid y) \propto \theta_j^{\,k+a-1}(1-\theta_j)^{n-k+b-1}.
\]</span></p>
<p>Poi normalizzare per avere somme che valgono 1:</p>
<p><span class="math display">\[
w_j = \frac{p(\theta_j \mid y)}{\sum_{\ell=1}^J p(\theta_\ell \mid y)}.
\]</span></p>
</li>
<li>
<p><strong>Combinare le predizioni condizionate</strong>. Per ogni valore futuro <span class="math inline">\(x=0,\dots,m\)</span>, si calcola:</p>
<p><span class="math display">\[
p(\tilde y = x \mid y) \approx \sum_{j=1}^J w_j \, \binom{m}{x}\theta_j^x(1-\theta_j)^{m-x}.
\]</span></p>
</li>
<li>
<p><strong>Interpretazione</strong>: la pmf ottenuta è la nostra approssimazione numerica della distribuzione predittiva posteriore. Da essa possiamo:</p>
<ul>
<li>calcolare probabilità,</li>
<li>generare campioni di <span class="math inline">\(\tilde y\)</span>,</li>
<li>confrontare la predizione con i dati osservati.</li>
</ul>
</li>
</ol>
<p><strong>Da ricordare:</strong></p>
<ul>
<li>La predittiva non si ottiene facendo la media dei valori di <span class="math inline">\(\tilde y\)</span>, ma costruendo <strong>un’intera distribuzione di probabilità</strong>.</li>
<li>Il metodo su griglia è il più semplice: discretizza <span class="math inline">\(\theta\)</span>, pesa ogni valore con la sua plausibilità a posteriori, e combina le predizioni condizionate.</li>
<li>In problemi più complessi, la stessa logica viene implementata tramite <strong>MCMC</strong>: invece di usare una griglia fissa, si usano campioni <span class="math inline">\(\theta^{(s)}\)</span> dalla posteriore.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio numerico">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio numerico
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Esaminiamo ora uno script in R che implementa passo per passo l’approssimazione della distribuzione predittiva posteriore binomiale con il metodo su griglia.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># ESEMPIO DIDATTICO: predittiva posteriore per Binomiale con metodo su griglia</span></span>
<span><span class="co"># Dati e prior</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">10</span>     <span class="co"># successi osservati</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">50</span>     <span class="co"># prove osservate</span></span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fl">1</span>      <span class="co"># prior Beta(a, b)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fl">10</span>     <span class="co"># numero di prove future per la predizione (scelta didattica)</span></span>
<span><span class="va">J</span> <span class="op">&lt;-</span> <span class="fl">2000</span>   <span class="co"># numero di punti griglia su theta in [0,1]</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 1: Griglia su theta</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">J</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 2: Densità posteriore non normalizzata su ogni punto di griglia</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="co"># Posteriore ~ Beta(a + k, b + n - k)  -&gt; densità proporzionale a theta^(a+k-1) (1-theta)^(b+n-k-1)</span></span>
<span><span class="va">post_unnorm</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">^</span><span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">k</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">b</span> <span class="op">+</span> <span class="va">n</span> <span class="op">-</span> <span class="va">k</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Normalizzazione per ottenere pesi che sommano a 1</span></span>
<span><span class="va">w</span> <span class="op">&lt;-</span> <span class="va">post_unnorm</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">post_unnorm</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 3: combinare le predittive condizionate p(tilde y | theta)</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># Obiettivo: costruire la pmf predittiva p(tilde y = x | y) </span></span>
<span><span class="co"># per ogni x = 0,...,m come media pesata (sulla griglia di θ) </span></span>
<span><span class="co"># delle pmf condizionate binomiali.</span></span>
<span></span>
<span><span class="co"># 1) Definiamo i valori futuri possibili di tilde y</span></span>
<span><span class="va">x_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">m</span></span>
<span></span>
<span><span class="co"># 2) Inizializziamo una matrice vuota: </span></span>
<span><span class="co">#    - J righe (una per ciascun θ_j della griglia)</span></span>
<span><span class="co">#    - (m+1) colonne (una per ogni valore possibile di x)</span></span>
<span><span class="va">px_given_theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="cn">NA_real_</span>, nrow <span class="op">=</span> <span class="va">J</span>, ncol <span class="op">=</span> <span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Riempiamo la matrice: per ogni θ_j (riga j) e per ogni x (colonna i)</span></span>
<span><span class="co">#    calcoliamo P(tilde y = x | θ_j) = Binomiale(x | m, θ_j)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">J</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x_vals</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">px_given_theta</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, size <span class="op">=</span> <span class="va">m</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># 4) Combinazione pesata:</span></span>
<span><span class="co">#    p(tilde y = x | y) ≈ somma_j w_j * P(tilde y = x | θ_j).</span></span>
<span><span class="co">#    Per ciascun valore di x (colonna i), facciamo la somma pesata.</span></span>
<span><span class="va">pred_pmf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">pred_pmf</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">w</span> <span class="op">*</span> <span class="va">px_given_theta</span><span class="op">[</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Nota didattica:</span></span>
<span><span class="co"># - Ogni colonna della matrice px_given_theta contiene le probabilità condizionate </span></span>
<span><span class="co">#   P(tilde y = x_i | θ_j) per tutti i valori di griglia θ_j.</span></span>
<span><span class="co"># - Moltiplicando riga per riga queste probabilità per i pesi posteriori w_j </span></span>
<span><span class="co">#   e sommando, otteniamo la probabilità predittiva p(tilde y = x_i | y).</span></span>
<span><span class="co"># - In questo modo l’integrale viene approssimato da una somma pesata.</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 4: Risultato: una pmf su {0,1,...,m}</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="va">pred_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x_vals</span>, p <span class="op">=</span> <span class="va">pred_pmf</span><span class="op">)</span></span>
<span><span class="va">pred_df</span></span>
<span><span class="co">#&gt;     x         p</span></span>
<span><span class="co">#&gt; 1   0 1.139e-01</span></span>
<span><span class="co">#&gt; 2   1 2.506e-01</span></span>
<span><span class="co">#&gt; 3   2 2.762e-01</span></span>
<span><span class="co">#&gt; 4   3 1.995e-01</span></span>
<span><span class="co">#&gt; 5   4 1.040e-01</span></span>
<span><span class="co">#&gt; 6   5 4.069e-02</span></span>
<span><span class="co">#&gt; 7   6 1.206e-02</span></span>
<span><span class="co">#&gt; 8   7 2.662e-03</span></span>
<span><span class="co">#&gt; 9   8 4.178e-04</span></span>
<span><span class="co">#&gt; 10  9 4.200e-05</span></span>
<span><span class="co">#&gt; 11 10 2.049e-06</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span>  <span class="co"># dovrebbe essere ~1</span></span>
<span><span class="co">#&gt; [1] 1</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># (Opzionale) campionamento dalla predittiva posteriore approssimata</span></span>
<span><span class="co"># Estrae N valori da {0,...,m} con le probabilità 'p'</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">5000</span></span>
<span><span class="va">tilde_y_samples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">x</span>, size <span class="op">=</span> <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Controllo: istogramma delle simulazioni vs pmf teorica approssimata</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">tilde_y_samples</span><span class="op">)</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    binwidth <span class="op">=</span> <span class="fl">1</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="va">m</span> <span class="op">+</span> <span class="fl">0.5</span>, by <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, fill <span class="op">=</span> <span class="st">"skyblue"</span>, </span>
<span>    color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">pred_df</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">p</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">pred_df</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">p</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Posterior Predictive (grid) — m=10"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">tilde</span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Density"</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="03_model_comparison_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Nota:</strong> il vettore <code>pred_df$p</code> è la <em>pmf</em> della predittiva posteriore approssimata; da qui si leggono probabilità, si calcolano quantità riassuntive e si può estrarre <span class="math inline">\(\tilde y\)</span>.</p>
</blockquote>
<p><strong>Verifica quando esiste la formula chiusa.</strong> Quando prior e likelihood sono coniugate (Beta + Binomiale), la predittiva è <em>Beta–Binomiale</em>. Possiamo usarla solo come verifica didattica:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Confronto con Beta-Binomiale (se applicabile)</span></span>
<span><span class="va">a_post</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">k</span></span>
<span><span class="va">b_post</span> <span class="op">&lt;-</span> <span class="va">b</span> <span class="op">+</span> <span class="va">n</span> <span class="op">-</span> <span class="va">k</span></span>
<span></span>
<span><span class="co"># pmf beta-binomial (con funzione base: dbetabinom in VGAM, altrimenti la implementiamo)</span></span>
<span><span class="va">dbetabinom</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">m</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Beta-Binomiale: choose(m, x) * Beta(x+a, m-x+b) / Beta(a, b)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Special.html">choose</a></span><span class="op">(</span><span class="va">m</span>, <span class="va">x</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">beta</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="va">a</span>, <span class="va">m</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">beta</a></span><span class="op">(</span><span class="va">a</span>, <span class="va">b</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">bb_pmf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="va">m</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">dbetabinom</span><span class="op">(</span><span class="va">x</span>, <span class="va">m</span>, <span class="va">a_post</span>, <span class="va">b_post</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>grid <span class="op">=</span> <span class="va">pred_df</span><span class="op">$</span><span class="va">p</span>, beta_binom <span class="op">=</span> <span class="va">bb_pmf</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="op">]</span>  <span class="co"># prime 6 righe a confronto</span></span>
<span><span class="co">#&gt;         grid beta_binom</span></span>
<span><span class="co">#&gt; [1,] 0.11391    0.11391</span></span>
<span><span class="co">#&gt; [2,] 0.25061    0.25061</span></span>
<span><span class="co">#&gt; [3,] 0.27618    0.27618</span></span>
<span><span class="co">#&gt; [4,] 0.19946    0.19946</span></span>
<span><span class="co">#&gt; [5,] 0.10398    0.10398</span></span>
<span><span class="co">#&gt; [6,] 0.04069    0.04069</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span> <span class="op">-</span> <span class="va">bb_pmf</span><span class="op">)</span><span class="op">)</span>   <span class="co"># lo scarto massimo (dovrebbe essere ~0 con J grande)</span></span>
<span><span class="co">#&gt; [1] 1.679e-15</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p><strong>Notazione.</strong> Useremo talvolta la forma compatta <span class="math inline">\(q(\cdot \mid y)\)</span> per indicare la predittiva posteriore del modello. Quando ci servirà evidenziare la previsione <em>marginale</em> per una singola osservazione <span class="math inline">\(y_i\)</span>, scriveremo:</p>
<p><span class="math display">\[
p(y_i \mid y)
\;=\;
\int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta,
\]</span></p>
<p>cioè la verosimiglianza <span class="math inline">\(p(y_i\mid\theta)\)</span> integrata rispetto alla posteriore <span class="math inline">\(p(\theta\mid y)\)</span>.</p>
<p><strong>Idea chiave:</strong> la predittiva posteriore propaga l’incertezza sui parametri alle previsioni. È questo passaggio a rendere le valutazioni predittive coerenti con il principio bayesiano, e quindi utilizzabili nel confronto tra modelli e nella stima di quantità legate alla “distanza” dal generatore dei dati.</p>
<section id="il-problema-della-valutazione-predittiva" class="level3" data-number="77.1.1"><h3 data-number="77.1.1" class="anchored" data-anchor-id="il-problema-della-valutazione-predittiva">
<span class="header-section-number">77.1.1</span> Il problema della valutazione predittiva</h3>
<p>Il nostro obiettivo è capire quanto la distribuzione predittiva posteriore <span class="math inline">\(q(\tilde{y} \mid y)\)</span> si avvicini alla vera distribuzione generatrice dei dati futuri, <span class="math inline">\(p(\tilde{y})\)</span>. In teoria, questa distanza si misura con la divergenza di Kullback–Leibler (KL):</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) \;=\; \mathbb{E}_p\!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right].
\]</span></p>
<p>Qui però incontriamo subito un problema concettuale: <em>non conosciamo <span class="math inline">\(p(\tilde{y})\)</span></em>. Per superare questo ostacolo, possiamo ricorrere a misure surrogate che, pur non avendo accesso diretto a <span class="math inline">\(p(\tilde{y})\)</span>, permettono di stimare la qualità predittiva del modello utilizzando in modo ingegnoso i dati osservati. Tra queste, vedremo il <em>log-score</em>, la <em>LPPD</em> e l’<em>ELPD</em>, che forniscono stime indirette della bontà predittiva.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Mappa concettuale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 39%">
<col style="width: 34%">
</colgroup>
<thead><tr class="header">
<th>Quantità</th>
<th>Significato</th>
<th>Uso principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid \theta)\)</span></td>
<td>Verosimiglianza</td>
<td>Calcolo predittivo</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(\theta \mid y)\)</span></td>
<td>Distribuzione posteriore</td>
<td>Ponderazione</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid y)\)</span></td>
<td>Predizione bayesiana media</td>
<td>Log-score, LPPD</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(y_i \mid y_{-i})\)</span></td>
<td>Predizione LOO (<em>leave-one-out</em>)</td>
<td>ELPD</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(q(\tilde{y} \mid y)\)</span></td>
<td>Distribuzione predittiva complessiva</td>
<td>Divergenza KL, confronto modelli</td>
</tr>
</tbody>
</table>
</div>
</div>
</section></section><section id="sec-logscore" class="level2" data-number="77.2"><h2 data-number="77.2" class="anchored" data-anchor-id="sec-logscore">
<span class="header-section-number">77.2</span> Il log-score: accuratezza predittiva punto per punto</h2>
<p>Abbiamo definito la distribuzione predittiva posteriore. Ora chiediamoci: <em>quanto bene il modello ha previsto ciascun dato osservato?</em> Il <em>log-score</em> risponde proprio a questa domanda: per ogni osservazione <span class="math inline">\(y_i\)</span> misura quanto il modello la considerava plausibile, cioè quanto avrebbe scommesso su quel dato.</p>
<p>Formalmente,</p>
<p><span id="eq-log-score-def"><span class="math display">\[
\log p(y_i \mid y)
\;=\;
\log \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta .
\tag{77.1}\]</span></span></p>
<p>Se il modello assegna <em>alta probabilità</em> a <span class="math inline">\(y_i\)</span>, <span class="math inline">\(\log p(y_i \mid y)\)</span> è vicino a 0 (buono). Se assegna <em>bassa probabilità</em>, il log-score è molto negativo (scarso).</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Perché il logaritmo?</strong><br>
Il log trasforma prodotti di probabilità in somme. Così possiamo sommare contributi <em>punto per punto</em> dei dati invece di moltiplicarli; inoltre stabilizza i numeri molto piccoli tipici delle verosimiglianze.</p>
</div>
</div>
</div>
<section id="dal-singolo-dato-al-punteggio-totale" class="level3" data-number="77.2.1"><h3 data-number="77.2.1" class="anchored" data-anchor-id="dal-singolo-dato-al-punteggio-totale">
<span class="header-section-number">77.2.1</span> Dal singolo dato al punteggio totale</h3>
<p>Per avere una visione complessiva, sommiamo i contributi su tutte le osservazioni:</p>
<p><span id="eq-log-score-sum-def"><span class="math display">\[
S
\;=\;
\sum_{i=1}^n \log p(y_i \mid y) .
\tag{77.2}\]</span></span></p>
<p>Più <span class="math inline">\(S\)</span> è alto, più il modello “scommette” bene sui dati osservati (<em>in-sample</em>).</p>
</section><section id="parametri-fissati-vs.-parametri-incerti" class="level3" data-number="77.2.2"><h3 data-number="77.2.2" class="anchored" data-anchor-id="parametri-fissati-vs.-parametri-incerti">
<span class="header-section-number">77.2.2</span> Parametri fissati vs.&nbsp;parametri incerti</h3>
<p>Ci sono due modi concettualmente distinti per valutare il log-score.</p>
<p><strong>Parametri fissati (impostazione classica).</strong> Usiamo una stima puntuale dei parametri (ad es. Massima Verosimiglianza o MAP) e ignoriamo l’incertezza:</p>
<p><span class="math display">\[
\log p(y_i \mid \hat{\theta}) .
\]</span></p>
<p><strong>Parametri incerti (impostazione bayesiana).</strong> Non fissiamo <span class="math inline">\(\theta\)</span>, ma lo trattiamo come incerto e “mescoliamo” le verosimiglianze pesandole per la plausibilità a posteriori:</p>
<p><span id="eq-integral-p-h-mid-yi"><span class="math display">\[
p(y_i \mid y)
\;=\;
\int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta .
\tag{77.3}\]</span></span></p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Differenza chiave.</strong><br>
Il frequentista chiede: “Quanto sono plausibili i dati se i parametri valgono esattamente <span class="math inline">\(\hat{\theta}\)</span>?”<br>
Il bayesiano chiede: “Quanto sono plausibili i dati <strong>in media</strong>, considerando tutti i valori di <span class="math inline">\(\theta\)</span> compatibili con i dati?”</p>
</div>
</div>
</div>
</section><section id="come-stimare-lintegrale-in-pratica-campioni-mcmc" class="level3" data-number="77.2.3"><h3 data-number="77.2.3" class="anchored" data-anchor-id="come-stimare-lintegrale-in-pratica-campioni-mcmc">
<span class="header-section-number">77.2.3</span> Come stimare l’integrale in pratica: campioni MCMC</h3>
<p>L’integrale nell’<a href="#eq-integral-p-h-mid-yi" class="quarto-xref">Equazione&nbsp;<span>77.3</span></a> raramente è calcolabile in forma chiusa. Con i campioni MCMC <span class="math inline">\(\theta^{(1)},\dots,\theta^{(S)} \sim p(\theta\mid y)\)</span> possiamo approssimarlo così:</p>
<ol type="1">
<li><p><strong>Per ciascun campione <span class="math inline">\(\theta^{(s)}\)</span></strong> calcoliamo la verosimiglianza del dato <span class="math inline">\(y_i\)</span>: <span class="math display">\[
p\bigl(y_i \mid \theta^{(s)}\bigr).
\]</span> Questo produce <em>una collezione di valori</em> <span class="math display">\[
\bigl\{\, p(y_i \mid \theta^{(1)}),\; p(y_i \mid \theta^{(2)}),\; \dots,\; p(y_i \mid \theta^{(S)}) \,\bigr\},
\]</span> che rappresenta come la plausibilità di <span class="math inline">\(y_i\)</span> varia al variare dei parametri plausibili.</p></li>
<li><p><strong>Media sui campioni (mixing).</strong><br>
La probabilità predittiva <em>puntuale</em> di <span class="math inline">\(y_i\)</span>, che useremo nel log-score, è la media di quella collezione: <span id="eq-mcmc-posterior-parameter-distr"><span class="math display">\[
p(y_i \mid y)
\;\approx\;
\frac{1}{S}\sum_{s=1}^S p\bigl(y_i \mid \theta^{(s)}\bigr).
\tag{77.4}\]</span></span> Questa media è <em>uno scalare</em>: condensa l’incertezza sui parametri in un’unica previsione probabilistica per <span class="math inline">\(y_i\)</span>.</p></li>
</ol>
<blockquote class="blockquote">
<p>Mini-illustrazione: se per tre campioni otteniamo <span class="math inline">\(\{0.40, 0.50, 0.60\}\)</span>, la media è <span class="math inline">\(0.50\)</span>. Questo numero è <span class="math inline">\(p(y_i \mid y)\)</span> da inserire nel log.</p>
</blockquote>
</section><section id="sec-lppd" class="level3" data-number="77.2.4"><h3 data-number="77.2.4" class="anchored" data-anchor-id="sec-lppd">
<span class="header-section-number">77.2.4</span> La LPPD: il log-score bayesiano complessivo</h3>
<p>Ripetiamo i passi precedenti per <strong>ogni osservazione</strong> <span class="math inline">\(y_i\)</span>:</p>
<ol type="a">
<li>calcoliamo la probabilità predittiva media <span class="math inline">\(p(y_i\mid y)\)</span> con l’<a href="#eq-mcmc-posterior-parameter-distr" class="quarto-xref">Equazione&nbsp;<span>77.4</span></a>;<br>
</li>
<li>ne prendiamo il logaritmo;<br>
</li>
<li>
<em>sommiamo</em> su tutte le osservazioni.</li>
</ol>
<p>Il risultato è la <em>Log Pointwise Predictive Density (LPPD)</em>:</p>
<p><span id="eq-lppd-def"><span class="math display">\[
\text{LPPD}
\;=\;
\sum_{i=1}^n
\log \left[
\frac{1}{S}
\sum_{s=1}^S
p\bigl(y_i \mid \theta^{(s)}\bigr)
\right].
\tag{77.5}\]</span></span></p>
<p>In sintesi, il <em>log-score classico</em> usa un solo valore dei parametri <span class="math inline">\((\hat{\theta})\)</span>; la <em>LPPD</em> compie lo stesso calcolo ma tiene conto dell’incertezza, mediando su tutti i valori plausibili secondo la posterior.</p>
</section><section id="attenzione-alloverfitting" class="level3" data-number="77.2.5"><h3 data-number="77.2.5" class="anchored" data-anchor-id="attenzione-alloverfitting">
<span class="header-section-number">77.2.5</span> Attenzione all’overfitting</h3>
<p>La LPPD è calcolata sugli stessi dati usati per stimare il modello: modelli molto flessibili possono “scommettere bene” anche sul rumore, gonfiando la LPPD <em>in-sample</em>. Per valutare la capacità di generalizzazione, serve una stima <em>out-of-sample</em>. Nelle prossime sezioni introdurremo la validazione incrociata <em>leave-one-out</em> (LOO-CV) e l’<em>ELPD</em> (<em>Expected Log Pointwise Predictive Density</em>), che forniscono una versione “fuori campione” della LPPD per il confronto predittivo tra modelli.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consideriamo un singolo dato <span class="math inline">\(y_i = 3\)</span> successi su <span class="math inline">\(n=5\)</span> tentativi (Binomiale). Abbiamo tre valori plausibili per <span class="math inline">\(\theta\)</span> dalla posterior, con pesi didattici <span class="math inline">\(w^{(s)}\)</span> (nella pratica MCMC i pesi sono uguali):</p>
<ul>
<li>
<span class="math inline">\(\theta^{(1)}=0.3\)</span> con <span class="math inline">\(w^{(1)}=0.2\)</span><br>
</li>
<li>
<span class="math inline">\(\theta^{(2)}=0.5\)</span> con <span class="math inline">\(w^{(2)}=0.5\)</span><br>
</li>
<li>
<span class="math inline">\(\theta^{(3)}=0.7\)</span> con <span class="math inline">\(w^{(3)}=0.3\)</span>
</li>
</ul>
<p>Per ogni campione <span class="math inline">\(\theta^{(s)}\)</span> calcoliamo <span class="math inline">\(p(y_i \mid \theta^{(s)})\)</span>, otteniamo la <em>collezione di likelihood</em> <span class="math inline">\(\{p(y_i\mid \theta^{(s)})\}_{s=1}^S\)</span>, poi facciamo la media pesata (<a href="#eq-mcmc-posterior-parameter-distr" class="quarto-xref">Equazione&nbsp;<span>77.4</span></a>) per ottenere <span class="math inline">\(p(y_i\mid y)\)</span>, e infine il <em>log-score</em> <span class="math inline">\(\log p(y_i\mid y)\)</span> (<a href="#eq-log-score-def" class="quarto-xref">Equazione&nbsp;<span>77.1</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dato osservato (un solo punto)</span></span>
<span><span class="va">y_i</span>  <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">n_i</span>  <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span></span>
<span><span class="co"># "Campioni" posteriori (qui pochi e con pesi espliciti per chiarezza didattica)</span></span>
<span><span class="va">theta_vals</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span>      <span class="co"># θ^(1), θ^(2), θ^(3)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span>      <span class="co"># w^(1), w^(2), w^(3); in MCMC tipicamente uguali</span></span>
<span></span>
<span><span class="co"># (1) Likelihood punto-per-punto: p(y_i | θ^(s))</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_i</span>, size <span class="op">=</span> <span class="va">n_i</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span><span class="va">likelihoods</span>  <span class="co"># questa è la collezione { p(y_i | θ^(s)) }_s</span></span>
<span><span class="co">#&gt; [1] 0.1323 0.3125 0.3087</span></span>
<span></span>
<span><span class="co"># (2) Media (pesata) sulle likelihood ⇒ p(y_i | y) ≈ Σ_s w^(s) p(y_i | θ^(s))</span></span>
<span><span class="va">p_yi_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">posterior_weights</span> <span class="op">*</span> <span class="va">likelihoods</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># (3) Log-score (per un solo dato coincide con la LPPD del singolo punto)</span></span>
<span><span class="va">log_score_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_yi_given_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stampa riassuntiva con notazione coerente</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Campioni θ^{(s)}:        "</span>, <span class="va">theta_vals</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Campioni θ^{(s)}:         0.3 0.5 0.7</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Likelihood p(y_i|θ^{(s)}):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">likelihoods</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Likelihood p(y_i|θ^{(s)}): 0.1323 0.3125 0.3087</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p(y_i|y) (media pesata):  "</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_yi_given_y</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p(y_i|y) (media pesata):   0.2753</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"log p(y_i|y):             "</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">log_score_i</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; log p(y_i|y):              -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Nota didattica.</strong> Nella pratica con MCMC i pesi sono uguali, <span class="math inline">\(w^{(s)}=\tfrac{1}{S}\)</span>, quindi <span class="math inline">\(p(y_i\mid y) \approx \tfrac{1}{S}\sum_{s=1}^S p(y_i\mid \theta^{(s)})\)</span> (<a href="#eq-mcmc-posterior-parameter-distr" class="quarto-xref">Equazione&nbsp;<span>77.4</span></a>). Con più osservazioni <span class="math inline">\(\{y_i\}_{i=1}^n\)</span>, la <em>LPPD</em> è la somma dei log-score punto-per-punto (<a href="#eq-lppd-def" class="quarto-xref">Equazione&nbsp;<span>77.5</span></a>).</p>
</div>
</div>
</div>
</section><section id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" class="level3" data-number="77.2.6"><h3 data-number="77.2.6" class="anchored" data-anchor-id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati">
<span class="header-section-number">77.2.6</span> Expected Log Predictive Density (ELPD): guardare oltre i dati osservati</h3>
<p>Se vogliamo valutare la capacità di generalizzazione di un modello, la domanda chiave è: quanto bene predirebbe dati che non ha mai visto? L’<em>ELPD</em> (<em>Expected Log Predictive Density</em>) risponde a questa domanda con la stessa logica della LPPD, ma introduce una differenza fondamentale: la previsione di <span class="math inline">\(y_i\)</span> viene calcolata escludendo <span class="math inline">\(y_i\)</span> dall’adattamento del modello (<em>Leave-One-Out</em>, LOO):</p>
<p><span id="eq-elpd-def"><span class="math display">\[
\text{ELPD} \;=\; \sum_{i=1}^n \log p(y_i \mid y_{-i}),
\tag{77.6}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset a cui è stata rimossa l’osservazione <span class="math inline">\(i\)</span>.</p>
<p><strong>Esempio</strong> Nel caso di un test sull’ansia:</p>
<ul>
<li>
<em>LPPD</em> → misura quanto bene il modello predice i punteggi di ansia degli studenti <em>già presenti</em> nel campione osservato.</li>
<li>
<em>ELPD</em> → misura quanto bene predirebbe il punteggio di un <em>nuovo</em> studente, usando solo i dati degli altri.</li>
</ul>
<p>In sostanza, l’ELPD è una <em>stima empirica</em> (con segno cambiato) della divergenza di Kullback–Leibler tra la vera distribuzione dei dati futuri e la distribuzione predittiva del modello. Ci fornisce quindi un indicatore diretto di quanto le previsioni del modello si avvicinano a ciò che accadrà davvero, senza richiedere di conoscere la distribuzione reale.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Interpretazione:</strong> l’ELPD è un <em>log-score out-of-sample</em>: per ogni <span class="math inline">\(y_i\)</span>, lo escludiamo, adattiamo il modello agli altri dati, e valutiamo la probabilità predittiva di <span class="math inline">\(y_i\)</span>. Più alto è l’ELPD, migliore è la capacità del modello di generalizzare a dati nuovi.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere tre osservazioni <span class="math inline">\(y_1, y_2, y_3\)</span> e che il modello stimi:</p>
<p><span class="math display">\[
p(y_1 \mid y_2,y_3)=0.6,\quad p(y_2 \mid y_1,y_3)=0.7,\quad p(y_3 \mid y_1,y_2)=0.5.
\]</span></p>
<p>L’ELPD è:</p>
<p><span class="math display">\[
\log 0.6 + \log 0.7 + \log 0.5 \; \approx\; -0.5108 -0.3567 -0.6931 = -1.5606.
\]</span></p>
<p>Un valore meno negativo indica maggiore capacità predittiva fuori campione.</p>
</div>
</div>
</div>
</section><section id="lppd-vs.-elpd-in-sintesi" class="level3" data-number="77.2.7"><h3 data-number="77.2.7" class="anchored" data-anchor-id="lppd-vs.-elpd-in-sintesi">
<span class="header-section-number">77.2.7</span> LPPD vs.&nbsp;ELPD in sintesi</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 39%">
<col style="width: 25%">
<col style="width: 26%">
</colgroup>
<thead><tr class="header">
<th>Misura</th>
<th>Dati usati per predire <span class="math inline">\(y_i\)</span>
</th>
<th>Valuta</th>
<th>Limite principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>LPPD</strong></td>
<td>Tutti i dati, incluso <span class="math inline">\(y_i\)</span>
</td>
<td>Adattamento in-sample</td>
<td>Rischio di overfitting</td>
</tr>
<tr class="even">
<td><strong>ELPD</strong></td>
<td>Tutti i dati tranne <span class="math inline">\(y_i\)</span> (<em>LOO</em>)</td>
<td>Generalizzazione</td>
<td>—</td>
</tr>
</tbody>
</table>
<p><strong>Metafora</strong> In un esperimento di riconoscimento di volti, mostriamo a un partecipante 100 fotografie e lo alleniamo a riconoscerle:</p>
<ul>
<li>
<em>LPPD</em> → misura quanto bene riconosce quelle stesse foto, già viste in fase di addestramento (<em>in-sample</em>).</li>
<li>
<em>ELPD</em> → misura quanto bene riconosce nuove foto, mai viste prima, cioè immagini fuori dall’insieme di addestramento (<em>out-of-sample</em>).</li>
</ul>
<p>Se il punteggio LPPD è alto ma l’ELPD è basso, significa che il partecipante — o il modello — ha memorizzato i casi specifici, senza aver appreso regole generali utili per nuovi dati.</p>
</section><section id="il-collegamento-con-la-divergenza-kl" class="level3" data-number="77.2.8"><h3 data-number="77.2.8" class="anchored" data-anchor-id="il-collegamento-con-la-divergenza-kl">
<span class="header-section-number">77.2.8</span> Il collegamento con la divergenza KL</h3>
<p>La divergenza di Kullback–Leibler <span class="math inline">\(D_{\text{KL}}\)</span> misura teoricamente la distanza tra la distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, e la distribuzione predittiva del modello, <span class="math inline">\(q(\tilde{y} \mid y)\)</span>.</p>
<p>Nel confronto tra due modelli <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span>, la differenza nelle loro <span class="math inline">\(D_{\text{KL}}\)</span> equivale alla differenza nelle rispettive accuratezze predittive medie rispetto a <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, non possiamo calcolare direttamente la KL. L’<em>ELPD</em> fornisce una stima empirica di questa accuratezza predittiva: un valore più alto implica un modello più “vicino” alla distribuzione vera.</p>
<p><span class="math display">\[
\text{Massimizzare ELPD} \;\; \approx \;\; \text{Minimizzare la divergenza KL}.
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Perché ELPD ≈ - KL?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Perché ELPD ≈ - KL?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Per definizione:</p>
<p><span class="math display">\[
D_{\text{KL}}\big(p \parallel q\big)
= \mathbb{E}_{p} \!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
\]</span></p>
<ul>
<li>Il primo termine <span class="math inline">\(\mathbb{E}_{p}[\log p(\tilde{y})]\)</span> non dipende dal modello (è fisso per tutti).</li>
<li>Confrontare due modelli equivale quindi a confrontare solo il secondo termine, che è <span class="math inline">\(-\)</span>ELPD.</li>
</ul>
<p>Ecco perché <em>massimizzare l’ELPD equivale a minimizzare la divergenza KL</em>: si sta massimizzando la media log-predittiva che il modello assegna ai dati futuri.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Vogliamo confrontare due modelli predittivi per il numero di “teste” in <span class="math inline">\(n=10\)</span> lanci.</p>
<ul>
<li>La <strong>distribuzione vera</strong> è <span class="math inline">\(p(y)=\text{Binom}(n=10,\;p=0.6)\)</span>.</li>
<li>Il <strong>modello candidato</strong> prevede <span class="math inline">\(q(y)=\text{Binom}(n=10,\;q=0.5)\)</span>.</li>
</ul>
<p>L’<em>ELPD</em> di un modello è l’aspettativa, rispetto alla distribuzione vera <span class="math inline">\(p\)</span>, del <em>log-score</em> del modello: <span class="math inline">\(\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]\)</span>. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori <span class="math inline">\(y=0,\dots,n\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri del problema</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># probabilità vera di "testa"</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità ipotizzata dal modello candidato</span></span>
<span></span>
<span><span class="co"># 1) Supporto dei possibili esiti</span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span></span>
<span><span class="co"># 2) Distribuzione vera p(y) su tutto il supporto</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Log-predittiva del modello candidato q su tutto il supporto</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)</span></span>
<span><span class="va">elpd_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 5) "Modello vero": usa q = p. Log-predittiva del modello vero</span></span>
<span><span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)</span></span>
<span><span class="va">elpd_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]</span></span>
<span><span class="va">kl_pq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="op">(</span><span class="va">log_p_y</span> <span class="op">-</span> <span class="va">log_q_y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello candidato (q=0.5): %.4f\n"</span>, <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello candidato (q=0.5): -2.0549</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello vero      (q=0.6): %.4f\n"</span>, <span class="va">elpd_p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello vero      (q=0.6): -1.8536</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza ELPD (vero - candidato): %.4f\n"</span>, <span class="va">elpd_p</span> <span class="op">-</span> <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza ELPD (vero - candidato): 0.2014</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"KL(p || q): %.4f\n"</span>, <span class="va">kl_pq</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; KL(p || q): 0.2014</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Cosa stiamo verificando?</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)\)</span> è <em>più basso</em> (più negativo) del valore ottenuto dal modello vero <span class="math inline">\(\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)\)</span>. → Il modello con <span class="math inline">\(q=0.6\)</span> è <em>più predittivo</em> di quello con <span class="math inline">\(q=0.5\)</span>.</p></li>
<li><p>La differenza tra i due ELPD è <em>uguale</em> (vicina numericamente) alla divergenza di Kullback–Leibler:</p></li>
</ol>
<p><span class="math display">\[
\mathrm{ELPD}(p)-\mathrm{ELPD}(q)
= \sum_y p(y)\big[\log p(y)-\log q(y)\big]
= D_{\mathrm{KL}}(p\|q)\;&gt;\;0.
\]</span></p>
<p>→ Questo mostra algebricamente e numericamente il legame: <em>massimizzare l’ELPD equivale a minimizzare la KL</em>.</p>
<blockquote class="blockquote">
<p>Nota sul log: nel codice usiamo il log naturale (unità in <strong>nat</strong>). Se si preferisce il log in base 2 (unità in <em>bit</em>), basta sostituire <code><a href="https://rdrr.io/r/base/Log.html">log()</a></code> con <code><a href="https://rdrr.io/r/base/Log.html">log2()</a></code>; tutte le quantità cambiano di una costante di scala, ma i confronti tra modelli restano identici.</p>
</blockquote>
<p><strong>In pratica.</strong></p>
<p>In questo esempio abbiamo potuto calcolare l’ELPD <em>vero</em> perché conoscevamo l’intera distribuzione generatrice <span class="math inline">\(p(y)\)</span> e potevamo integrare esattamente. Nella realtà, <span class="math inline">\(p(y)\)</span> è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD <em>empiricamente</em>, ad esempio con la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che sostituisce l’aspettativa rispetto a <span class="math inline">\(p\)</span> con una media sui dati raccolti, lasciando fuori una osservazione alla volta. Questa procedura ci consente di avvicinarci al calcolo ideale della KL, anche senza conoscere <span class="math inline">\(p(y)\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Collegamento chiave</strong><br>
L’ELPD è una stima empirica (con segno cambiato) della divergenza di Kullback–Leibler.<br>
Più alto è l’ELPD, migliore è la capacità predittiva del modello.</p>
</div>
</div>
</div>
</section></section><section id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="level2" data-number="77.3"><h2 data-number="77.3" class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica">
<span class="header-section-number">77.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</h2>
<p>Poiché la distribuzione vera dei dati futuri è inaccessibile, dobbiamo usare metodi indiretti per stimare quanto bene il nostro modello prevede nuove osservazioni. La validazione incrociata Leave-One-Out (<em>LOO-CV</em>) è uno di questi metodi e, se combinata con l’uso dell’<em>Expected Log Predictive Density</em> (<em>ELPD</em>), diventa uno strumento potente per il confronto tra modelli.</p>
<p>Abbiamo visto che l’ELPD è la misura ideale della capacità predittiva di un modello su dati futuri. Il problema è che, per definizione, richiede di calcolare un’aspettativa rispetto alla vera distribuzione generatrice <span class="math inline">\(p(\tilde{y})\)</span>, che non conosciamo.</p>
<p><strong>Come possiamo stimarla in pratica?</strong> Usando la LOO-CV, che simula la previsione di nuovi dati sfruttando solo le informazioni presenti nei dati osservati.</p>
<section id="cosè-la-loo-cv" class="level3" data-number="77.3.1"><h3 data-number="77.3.1" class="anchored" data-anchor-id="cosè-la-loo-cv">
<span class="header-section-number">77.3.1</span> Cos’è la LOO-CV</h3>
<p>La LOO-CV è un esperimento concettuale semplice:</p>
<ol type="1">
<li>Scegli un’osservazione <span class="math inline">\(y_i\)</span> dal dataset.</li>
<li>Escludila dal set di addestramento.</li>
<li>Adatta il modello ai dati rimanenti <span class="math inline">\(y_{-i}\)</span>.</li>
<li>Calcola la densità predittiva del modello per l’osservazione esclusa: <span class="math inline">\(p(y_i \mid y_{-i})\)</span>.</li>
<li>Ripeti per ogni osservazione e somma i logaritmi ottenuti.</li>
</ol>
<p>Formalmente:</p>
<p><span id="eq-loo-def"><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}),
\tag{77.7}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset senza l’osservazione <span class="math inline">\(i\)</span>.<br>
La struttura è identica a quella dell’ELPD “ideale”, ma ogni termine è calcolato <em>fuori campione</em>, escludendo il dato che viene valutato.</p>
<p>Un’analogia: è come escludere uno studente dall’allenamento e verificare se il modello riesce a predire il suo punteggio d’esame; ripetendo questo processo per tutti gli studenti otteniamo una misura diretta della capacità di generalizzazione.</p>
</section><section id="perché-loo-cv-funziona" class="level3" data-number="77.3.2"><h3 data-number="77.3.2" class="anchored" data-anchor-id="perché-loo-cv-funziona">
<span class="header-section-number">77.3.2</span> Perché LOO-CV funziona</h3>
<p>L’ELPD può essere scritto come:</p>
<p><span id="eq-loo-def2"><span class="math display">\[
\mathbb{E}_p[\log q(\tilde{y} \mid y)],
\tag{77.8}\]</span></span></p>
<p>dove <span class="math inline">\(q(\tilde{y} \mid y)\)</span> è la distribuzione predittiva del modello.<br>
Non possiamo calcolare l’aspettativa rispetto a <span class="math inline">\(p(\tilde{y})\)</span>, ma possiamo trattare ogni osservazione <span class="math inline">\(y_i\)</span> come “nuovo dato” generato da <span class="math inline">\(p\)</span> e usare la media empirica sulle osservazioni reali come stima dell’aspettativa:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>In altre parole: LOO-CV misura quanto bene il modello predirebbe ciascun dato se non lo avesse mai visto.</p>
</section><section id="legame-con-la-divergenza-kl" class="level3" data-number="77.3.3"><h3 data-number="77.3.3" class="anchored" data-anchor-id="legame-con-la-divergenza-kl">
<span class="header-section-number">77.3.3</span> Legame con la divergenza KL</h3>
<p>La divergenza di Kullback–Leibler è definita come:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p[\log p(\tilde{y})] - \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Il primo termine, l’entropia di <span class="math inline">\(p\)</span>, è lo stesso per tutti i modelli e scompare nel confronto.<br>
Ne segue che, per due modelli <span class="math inline">\(q_1\)</span> e <span class="math inline">\(q_2\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) =
\mathbb{E}_p[\log q\_2(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_1(\tilde{y} \mid y)].
\]</span></p>
<p><em>Vince il modello con ELPD più alto</em>, perché corrisponde alla minore divergenza KL dalla distribuzione vera.</p>
</section><section id="confrontare-i-modelli-con-loo-cv" class="level3" data-number="77.3.4"><h3 data-number="77.3.4" class="anchored" data-anchor-id="confrontare-i-modelli-con-loo-cv">
<span class="header-section-number">77.3.4</span> Confrontare i modelli con LOO-CV</h3>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, sostituiamo l’aspettativa teorica con la stima empirica via LOO:</p>
<p><span id="eq-delta-elpd-def"><span class="math display">\[
\Delta\text{ELPD} = \text{ELPD}*{\text{LOO}}(M_1) - \text{ELPD}*{\text{LOO}}(M_2) .
\tag{77.9}\]</span></span></p>
<p><span class="math inline">\(\Delta\text{ELPD}\)</span> approssima la differenza tra le divergenze KL dei modelli.<br>
Oltre alla differenza, possiamo stimare un errore standard per capire se la superiorità di un modello è robusta o dovuta al caso.</p>
</section><section id="punti-chiave" class="level3" data-number="77.3.5"><h3 data-number="77.3.5" class="anchored" data-anchor-id="punti-chiave">
<span class="header-section-number">77.3.5</span> Punti chiave</h3>
<ul>
<li>
<strong>Problema:</strong> L’ELPD teorico richiede <span class="math inline">\(p(\tilde{y})\)</span>, che è sconosciuta.<br>
</li>
<li>
<strong>Soluzione:</strong> LOO-CV fornisce una stima empirica out-of-sample.<br>
</li>
<li>
<strong>Teoria:</strong> L’ELPD è direttamente collegato alla parte “accuratezza” della KL-divergence.<br>
</li>
<li>
<strong>Pratica:</strong> Massimizzare l’ELPD stimato equivale a scegliere il modello più vicino alla distribuzione vera.</li>
</ul>
<p>Direi che l’esempio che hai scritto è già molto chiaro e in linea con il testo precedente, ma per integrarlo meglio nel capitolo e mantenere continuità con la sezione teorica, potremmo:</p>
<ol type="1">
<li>
<strong>Aggiungere un’introduzione contestuale</strong> per collegarlo subito alla discussione ELPD–LOO–KL.</li>
<li>
<strong>Rendere più esplicito il parallelismo con la teoria</strong> (ELPD come somma delle log-predittive fuori campione).</li>
<li>
<strong>Sintetizzare il codice</strong> con commenti chiave, così che lo studente possa leggerlo senza perdersi nei dettagli secondari.</li>
<li>
<strong>Chiarire il senso della tabella</strong> subito dopo l’esecuzione del codice.</li>
</ol>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: confronto ELPD-LOO tra due modelli">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: confronto ELPD-LOO tra due modelli
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Questo mini-esempio mostra come passare dalla definizione teorica dell’ELPD alla stima pratica via Leave-One-Out, usando un caso elementare Beta–Bernoulli.</p>
<p><strong>Dati.</strong> Cinque prove indipendenti: <span class="math inline">\(y=\{1,1,1,0,1\}\)</span> (quattro “successi”, un “insuccesso”).</p>
<p><strong>Modello A (Bayesiano adattato ai dati).</strong> Bernoulli<span class="math inline">\((\theta)\)</span> con prior <span class="math inline">\(\theta\sim \text{Beta}(1,1)\)</span> (uninformativa). Per LOO:</p>
<ul>
<li>per ogni <span class="math inline">\(i\)</span>, escludiamo <span class="math inline">\(y_i\)</span>;</li>
<li>calcoliamo la posteriore <span class="math inline">\(\theta \mid y_{-i} \sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})\)</span>, dove <span class="math inline">\(s_{-i}\)</span> è il numero di successi tra i <span class="math inline">\(n-1\)</span> rimanenti;</li>
<li>calcoliamo la probabilità predittiva per <span class="math inline">\(y_i\)</span>.</li>
</ul>
<p><strong>Modello B (di confronto).</strong> Moneta equa fissa (<span class="math inline">\(q=0.5\)</span>): la predittiva è sempre <span class="math inline">\(0.5\)</span>, indipendentemente dai dati.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-predittiva LOO per Modello A (Beta(1,1) + Bernoulli)</span></span>
<span><span class="va">loo_log_pred_beta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span>, <span class="va">y</span>, <span class="va">a0</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">b0</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">yi</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">s_minus</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">yi</span></span>
<span>  <span class="va">n_minus</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="va">alpha</span> <span class="op">&lt;-</span> <span class="va">a0</span> <span class="op">+</span> <span class="va">s_minus</span></span>
<span>  <span class="va">beta</span>  <span class="op">&lt;-</span> <span class="va">b0</span> <span class="op">+</span> <span class="op">(</span><span class="va">n_minus</span> <span class="op">-</span> <span class="va">s_minus</span><span class="op">)</span></span>
<span>  <span class="va">p1</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="op">(</span><span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span><span class="op">)</span></span>
<span>  <span class="va">p</span>  <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="va">yi</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="va">p1</span> <span class="kw">else</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Log-predittive punto-per-punto</span></span>
<span><span class="va">lp_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="va">loo_log_pred_beta</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">lp_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ELPD-LOO</span></span>
<span><span class="va">elpd_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_beta</span><span class="op">)</span></span>
<span><span class="va">elpd_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_fixed</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Differenza e SE</span></span>
<span><span class="va">diff_pt</span> <span class="op">&lt;-</span> <span class="va">lp_beta</span> <span class="op">-</span> <span class="va">lp_fixed</span></span>
<span><span class="va">se_diff</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">diff_pt</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Tabella riassuntiva</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>, y <span class="op">=</span> <span class="va">y</span>,</span>
<span>  lp_beta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_beta</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  lp_fixed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_fixed</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  diff <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">diff_pt</span>, <span class="fl">6</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt;   i y lp_beta lp_fixed    diff</span></span>
<span><span class="co">#&gt; 1 1 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 2 2 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 3 3 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 4 4 0 -1.7918  -0.6931 -1.0986</span></span>
<span><span class="co">#&gt; 5 5 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"\nELPD-LOO Modello A: %.6f\n"</span>, <span class="va">elpd_beta</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ELPD-LOO Modello A: -3.413620</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD-LOO Modello B: %.6f\n"</span>, <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD-LOO Modello B: -3.465736</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza (A-B)  : %.6f\n"</span>, <span class="va">elpd_beta</span> <span class="op">-</span> <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza (A-B)  : 0.052116</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"SE differenza     : %.6f\n"</span>, <span class="va">se_diff</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; SE differenza     : 1.386294</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Interpretazione.</strong></p>
<ul>
<li>Ogni riga della tabella mostra la log-predittiva fuori campione per entrambi i modelli.</li>
<li>In un campione con 4 successi su 5, il Modello A assegna più di 0.5 di probabilità ai successi, e meno di 0.5 all’unico insuccesso.</li>
<li>L’ELPD-LOO di A può risultare leggermente più alto di quello di B, ma l’errore standard è grande perché <span class="math inline">\(n\)</span> è piccolo.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Regola pratica:</strong> una differenza <span class="math inline">\(|\Delta \text{ELPD}|\)</span> di almeno 2 volte l’SE fornisce un’indicazione più affidabile di superiorità del modello. In esempi così piccoli l’obiettivo è puramente didattico: capire come si calcola e cosa significa.</p>
</blockquote>
</div>
</div>
</div>
</section><section id="elpd-loo-e-il-problema-delloverfitting" class="level3" data-number="77.3.6"><h3 data-number="77.3.6" class="anchored" data-anchor-id="elpd-loo-e-il-problema-delloverfitting">
<span class="header-section-number">77.3.6</span> ELPD-LOO e il problema dell’overfitting</h3>
<p>Valutare un modello sugli stessi dati usati per addestrarlo tende a <em>gonfiare</em> le stime della sua capacità predittiva (<em>overfitting</em>). È come se uno studente ottenesse un punteggio perfetto ripetendo esercizi già svolti: non sappiamo se saprebbe risolverne di nuovi.</p>
<p>La <em>Leave-One-Out Cross-Validation (LOO-CV)</em> aggira il problema valutando ciascuna osservazione <span class="math inline">\(y_i\)</span> usando <em>solo</em> i dati rimanenti (<span class="math inline">\(y_{-i}\)</span>). Il punteggio ottenuto (ELPD-LOO) è quindi una stima <em>out-of-sample</em> della bontà predittiva, meno sensibile all’overfitting.</p>
<p>Grazie a metodi come il <em>Pareto-smoothed importance sampling (PSIS)</em>, oggi è possibile calcolare l’ELPD-LOO senza riadattare il modello <span class="math inline">\(n\)</span> volte. In R, la funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> del pacchetto <em>loo</em> (integrata in <code>brms</code> e <code>rstanarm</code>) rende questa procedura rapida e diretta anche per modelli complessi.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: ELPD atteso vs ELPD-LOO stimato">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: ELPD atteso vs ELPD-LOO stimato
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Quando conosci la distribuzione vera dei dati (<span class="math inline">\(p\)</span>), puoi calcolare l’ELPD atteso in modo esatto. Quando invece hai solo i dati osservati, lo stimi tramite <em>Leave-One-Out</em> (PSIS-LOO), come mostrato di seguito.</p>
<p><strong>ELPD atteso (p noto)</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span></span>
<span><span class="va">y_vals</span>   <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">p_y</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">log_q_y</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">elpd</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD atteso (modello q = 0.5): %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD atteso (modello q = 0.5): -2.0549</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>ELPD-LOO stimato da dati simulati</strong></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">20</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>,</span>
<span>  n <span class="op">=</span> <span class="va">n</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html">brm</a></span><span class="op">(</span><span class="va">k</span> <span class="op">|</span> <span class="fu">trials</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">df</span>,</span>
<span>           family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>           prior <span class="op">=</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html">prior</a></span><span class="op">(</span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/constant.html">constant</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>, class <span class="op">=</span> <span class="st">"Intercept"</span><span class="op">)</span>,</span>
<span>           iter <span class="op">=</span> <span class="fl">2000</span>, chains <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">loo_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">loo_res</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’oggetto <code>loo_res</code> fornisce l’ELPD-LOO, il suo errore standard, e le statistiche <em>Pareto k</em> per la diagnostica. Con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code> puoi confrontare due modelli sulla base della differenza di ELPD-LOO e del relativo SE.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="In pratica: stimare e confrontare l'ELPD-LOO">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In pratica: stimare e confrontare l’ELPD-LOO
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Concetto chiave</strong></p>
<ul>
<li>L’ELPD valuta la capacità predittiva su dati non visti.</li>
<li>La LOO-CV lo stima in modo efficiente con PSIS-LOO.</li>
</ul>
<p><strong>Strumenti</strong></p>
<ul>
<li>Funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> del pacchetto <em>loo</em>, integrata in <code>brms</code> e <code>rstanarm</code>.</li>
<li>Diagnostica con <em>Pareto k</em>, confronto con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ul>
<p><strong>Workflow tipico in R</strong></p>
<ol type="1">
<li>Adattare ogni modello (<code><a href="https://paulbuerkner.com/brms/reference/brm.html">brm()</a></code> o <code>stan_glm()</code>).</li>
<li>Estrarre <code><a href="https://mc-stan.org/rstantools/reference/log_lik.html">log_lik()</a></code> e calcolare <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code>.</li>
<li>Confrontare modelli con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ol>
<p><strong>Decisione</strong></p>
<ul>
<li>Preferire l’ELPD-LOO più alto.</li>
<li>Differenza ≥ 2×SE → indicazione di vantaggio sostanziale.</li>
<li>Valutare anche semplicità e interpretabilità.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="level2" data-number="77.4"><h2 data-number="77.4" class="anchored" data-anchor-id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl">
<span class="header-section-number">77.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>
</h2>
<p>Oltre alla <em>Leave-One-Out Cross-Validation</em>, esistono altri strumenti per stimare la qualità predittiva di un modello senza dover conoscere la distribuzione vera dei dati. Molti di questi metodi derivano, in modo più o meno diretto, dalla divergenza di Kullback–Leibler <span class="math inline">\(D_{\text{KL}}\)</span>, che — come visto — misura la distanza tra la distribuzione reale e quella stimata dal modello.</p>
<p>L’idea di base è sempre la stessa:</p>
<ul>
<li>valutare quanto bene il modello spiega i dati (<em>bontà di adattamento</em>);</li>
<li>penalizzare la complessità del modello, per ridurre il rischio di <em>overfitting</em>.</li>
</ul>
<p>Questa logica si traduce in <em>criteri di informazione</em> che combinano due componenti:</p>
<ol type="1">
<li>
<em>termine di fit</em>: misura di quanto bene il modello si adatta ai dati osservati (es. log-verosimiglianza, MSE);</li>
<li>
<em>termine di penalizzazione</em>: aumenta con il numero di parametri o con la flessibilità del modello.</li>
</ol>
<p>Tra i criteri più usati troviamo:</p>
<ul>
<li>
<strong>MSE</strong> (Mean Squared Error) – semplice e intuitivo, basato sugli errori di previsione;</li>
<li>
<strong>AIC</strong> (Akaike Information Criterion) – approssima <span class="math inline">\(D_{\text{KL}}\)</span> tra il modello e la verità, penalizzando il numero di parametri;</li>
<li>
<strong>BIC</strong> (Bayesian Information Criterion) – simile all’AIC, ma con penalizzazione più forte per modelli complessi, proporzionale al numero di osservazioni;</li>
<li>
<strong>WAIC</strong> (Widely Applicable Information Criterion) – versione pienamente bayesiana, basata sulle previsioni del modello integrate sull’intera distribuzione a posteriori.</li>
</ul>
<p>Nelle sezioni seguenti vedremo come ciascun criterio si calcola, quali assunzioni richiede e in quali situazioni è preferibile rispetto agli altri.</p>
<section id="errore-quadratico-medio-mse" class="level3" data-number="77.4.1"><h3 data-number="77.4.1" class="anchored" data-anchor-id="errore-quadratico-medio-mse">
<span class="header-section-number">77.4.1</span> Errore Quadratico Medio (MSE)</h3>
<p>L’<em>Errore Quadratico Medio</em> misura la media delle differenze al quadrato tra valori osservati e previsti:</p>
<p><span id="eq-mse-def"><span class="math display">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
\tag{77.10}\]</span></span></p>
<ul>
<li>Valori più bassi indicano previsioni più vicine ai dati osservati.</li>
<li>Non tiene conto della complessità del modello, quindi può favorire modelli eccessivamente flessibili (<em>overfitting</em>).</li>
</ul>
<p>Utile per valutare l’accuratezza, ma da solo non è adatto a scegliere tra modelli con diversa complessità.</p>
</section><section id="akaike-information-criterion-aic" class="level3" data-number="77.4.2"><h3 data-number="77.4.2" class="anchored" data-anchor-id="akaike-information-criterion-aic">
<span class="header-section-number">77.4.2</span> Akaike Information Criterion (AIC)</h3>
<p>L’<em>AIC</em> è un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e stima quanta informazione si perde usando un modello per descrivere i dati:</p>
<p><span id="eq-aic-def"><span class="math display">\[
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k,
\tag{77.11}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(\hat{\theta}_{\text{MLE}}\)</span>: stima dei parametri ottenuta massimizzando la verosimiglianza;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri del modello.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine valuta l’adattamento del modello ai dati.</li>
<li>Il secondo penalizza la complessità per evitare overfitting.</li>
<li>Un AIC più basso indica un miglior equilibrio tra accuratezza e semplicità.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Basato su assunzioni asintotiche (funziona meglio con campioni grandi).</li>
<li>Usa solo stime puntuali, ignorando l’incertezza dei parametri.</li>
<li>Non è pienamente coerente con l’approccio bayesiano.</li>
</ul></section><section id="bayesian-information-criterion-bic" class="level3" data-number="77.4.3"><h3 data-number="77.4.3" class="anchored" data-anchor-id="bayesian-information-criterion-bic">
<span class="header-section-number">77.4.3</span> Bayesian Information Criterion (BIC)</h3>
<p>Il <em>BIC</em> valuta il compromesso tra adattamento ai dati e complessità del modello, applicando una penalizzazione più severa rispetto all’AIC — soprattutto quando il numero di osservazioni <span class="math inline">\(n\)</span> è grande.</p>
<p><span id="eq-bic-def"><span class="math display">\[
BIC = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k,
\tag{77.12}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(y \mid \hat{\theta})\)</span>: massima verosimiglianza del modello (o MAP con prior piatti);</li>
<li>
<span class="math inline">\(n\)</span>: numero di osservazioni indipendenti;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri stimati.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine misura l’adattamento ai dati.</li>
<li>Il secondo penalizza la complessità in modo crescente con <span class="math inline">\(n\)</span> e <span class="math inline">\(k\)</span>.</li>
<li>Un BIC più basso indica un compromesso migliore tra accuratezza e parsimonia.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Tende a favorire modelli più semplici quando <span class="math inline">\(n\)</span> è elevato.</li>
<li>Ha una giustificazione teorica bayesiana: in certe condizioni, approssima il log della <em>marginal likelihood</em>.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Si basa su assunzioni forti (indipendenza, modelli regolari, prior deboli).</li>
<li>Può sottoselezionare modelli utili con campioni piccoli o strutture complesse.</li>
</ul></section><section id="widely-applicable-information-criterion-waic" class="level3" data-number="77.4.4"><h3 data-number="77.4.4" class="anchored" data-anchor-id="widely-applicable-information-criterion-waic">
<span class="header-section-number">77.4.4</span> Widely Applicable Information Criterion (WAIC)</h3>
<p>Il <em>WAIC</em> è una versione pienamente bayesiana dell’AIC:</p>
<ul>
<li>utilizza tutta la distribuzione a posteriori dei parametri;</li>
<li>fornisce una stima diretta della capacità predittiva del modello.</li>
</ul>
<p><span id="eq-waic-def"><span class="math display">\[
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \mathrm{Var}_{\theta^{(s)}} \big( \log p(y_i \mid \theta^{(s)}) \big)
\right],
\tag{77.13}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(S\)</span> = numero di campioni dalla distribuzione a posteriori;</li>
<li>
<span class="math inline">\(\theta^{(s)}\)</span> = <span class="math inline">\(s\)</span>-esimo campione;</li>
<li>il secondo termine stima il <em>numero effettivo di parametri</em> basato sulla variabilità della log-verosimiglianza.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Adatto anche a modelli complessi o non regolari.</li>
<li>Usa direttamente i campioni MCMC.</li>
<li>Migliore dell’AIC per modelli bayesiani, perché incorpora l’incertezza dei parametri.</li>
</ul>
<p><strong>Nota.</strong> Il WAIC è strettamente collegato all’ELPD: è una sua stima approssimata ottenuta dalla distribuzione a posteriori, senza bisogno di eseguire la LOO-CV.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="ofzfwdvofy" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#ofzfwdvofy table {
  font-family: Palatino, sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ofzfwdvofy thead, #ofzfwdvofy tbody, #ofzfwdvofy tfoot, #ofzfwdvofy tr, #ofzfwdvofy td, #ofzfwdvofy th {
  border-style: none;
}

#ofzfwdvofy p {
  margin: 0;
  padding: 0;
}

#ofzfwdvofy .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: 100%;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ofzfwdvofy .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ofzfwdvofy .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ofzfwdvofy .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ofzfwdvofy .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ofzfwdvofy .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ofzfwdvofy .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ofzfwdvofy .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ofzfwdvofy .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ofzfwdvofy .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ofzfwdvofy .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ofzfwdvofy .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ofzfwdvofy .gt_spanner_row {
  border-bottom-style: hidden;
}

#ofzfwdvofy .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ofzfwdvofy .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ofzfwdvofy .gt_from_md > :first-child {
  margin-top: 0;
}

#ofzfwdvofy .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ofzfwdvofy .gt_row {
  padding-top: 3px;
  padding-bottom: 3px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ofzfwdvofy .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ofzfwdvofy .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ofzfwdvofy .gt_row_group_first td {
  border-top-width: 2px;
}

#ofzfwdvofy .gt_row_group_first th {
  border-top-width: 2px;
}

#ofzfwdvofy .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ofzfwdvofy .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ofzfwdvofy .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ofzfwdvofy .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ofzfwdvofy .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ofzfwdvofy .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ofzfwdvofy .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ofzfwdvofy .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ofzfwdvofy .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ofzfwdvofy .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ofzfwdvofy .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ofzfwdvofy .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ofzfwdvofy .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ofzfwdvofy .gt_left {
  text-align: left;
}

#ofzfwdvofy .gt_center {
  text-align: center;
}

#ofzfwdvofy .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ofzfwdvofy .gt_font_normal {
  font-weight: normal;
}

#ofzfwdvofy .gt_font_bold {
  font-weight: bold;
}

#ofzfwdvofy .gt_font_italic {
  font-style: italic;
}

#ofzfwdvofy .gt_super {
  font-size: 65%;
}

#ofzfwdvofy .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ofzfwdvofy .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ofzfwdvofy .gt_indent_1 {
  text-indent: 5px;
}

#ofzfwdvofy .gt_indent_2 {
  text-indent: 10px;
}

#ofzfwdvofy .gt_indent_3 {
  text-indent: 15px;
}

#ofzfwdvofy .gt_indent_4 {
  text-indent: 20px;
}

#ofzfwdvofy .gt_indent_5 {
  text-indent: 25px;
}

#ofzfwdvofy .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#ofzfwdvofy div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead>
<tr class="gt_heading header">
<th colspan="5" class="gt_heading gt_title gt_font_normal gt_bottom_border"><strong>Riepilogo comparativo dei criteri di valutazione del modello</strong></th>
</tr>
<tr class="gt_col_headings even">
<th id="Criterio" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Criterio</th>
<th id="Tipo" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">Tipo</th>
<th id="Penalizza-la-complessità" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Penalizza la complessità</th>
<th id="Usa-stime-puntuali" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Usa stime puntuali</th>
<th id="Basato-su-campioni-a-posteriori-(es.-MCMC)" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Basato su campioni a posteriori (es. MCMC)</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="Criterio">MSE</td>
<td class="gt_row gt_left" headers="Tipo">Frequentista</td>
<td class="gt_row gt_center" headers="Penalizza la complessità">No</td>
<td class="gt_row gt_center" headers="Usa stime puntuali">Sì</td>
<td class="gt_row gt_center" headers="Basato su campioni a posteriori (es. MCMC)">No</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Criterio">AIC</td>
<td class="gt_row gt_left" headers="Tipo">Frequentista</td>
<td class="gt_row gt_center" headers="Penalizza la complessità">Sì (modesta)</td>
<td class="gt_row gt_center" headers="Usa stime puntuali">Sì</td>
<td class="gt_row gt_center" headers="Basato su campioni a posteriori (es. MCMC)">No</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Criterio">BIC</td>
<td class="gt_row gt_left" headers="Tipo">Frequentista/Bayesiano</td>
<td class="gt_row gt_center" headers="Penalizza la complessità">Sì (forte)</td>
<td class="gt_row gt_center" headers="Usa stime puntuali">Sì</td>
<td class="gt_row gt_center" headers="Basato su campioni a posteriori (es. MCMC)">No</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="Criterio">WAIC</td>
<td class="gt_row gt_left" headers="Tipo">Bayesiano</td>
<td class="gt_row gt_center" headers="Penalizza la complessità">Sì (effettiva)</td>
<td class="gt_row gt_center" headers="Usa stime puntuali">No</td>
<td class="gt_row gt_center" headers="Basato su campioni a posteriori (es. MCMC)">Sì</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="Criterio">LOO-CV</td>
<td class="gt_row gt_left" headers="Tipo">Bayesiano</td>
<td class="gt_row gt_center" headers="Penalizza la complessità">Sì (empirica)</td>
<td class="gt_row gt_center" headers="Usa stime puntuali">No</td>
<td class="gt_row gt_center" headers="Basato su campioni a posteriori (es. MCMC)">Sì</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section></section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>La selezione del modello, in ottica bayesiana, ruota attorno a una domanda essenziale: quanto bene il modello predice dati che non ha mai visto?</p>
<p>Il riferimento teorico è l’<em>Expected Log Predictive Density (ELPD)</em>, che misura quanto la distribuzione predittiva del modello si avvicina alla vera (e ignota) distribuzione dei dati. In termini matematici, massimizzare l’ELPD equivale a minimizzare la divergenza di Kullback–Leibler rispetto alla vera generatrice: due facce dello stesso obiettivo, rappresentare al meglio la realtà sottostante.</p>
<p>Poiché <span class="math inline">\(p_{\text{vera}}(y)\)</span> è sconosciuta, l’ELPD va stimato. Le principali approssimazioni sono:</p>
<ul>
<li>
<strong>LOO-CV</strong> (Leave-One-Out Cross-Validation): oggi lo strumento più affidabile, valuta ogni osservazione come “nuova” e stima la capacità di generalizzazione del modello.</li>
<li>
<strong>WAIC</strong>: alternativa completamente bayesiana, calcolata direttamente dai campioni della posteriori.</li>
<li>
<strong>AIC</strong> e <strong>BIC</strong>: criteri frequenstisti più rapidi ma basati su stime puntuali; utili in contesti semplici.</li>
<li>
<strong>MSE</strong>: misura l’accuratezza sulle osservazioni note, ma non penalizza la complessità e quindi non è adatto alla selezione del modello.</li>
</ul>
<p>Nel confronto tra modelli, la <em>differenza di ELPD</em> (stimata con LOO-CV o WAIC) andrebbe interpretata insieme al relativo <em>errore standard</em>: una regola pratica è considerare rilevante una differenza almeno doppia rispetto all’errore standard.</p>
<p><strong>In sintesi:</strong></p>
<ul>
<li>la buona statistica non si limita a spiegare il passato: sa <em>anticipare il futuro</em>;</li>
<li>la <em>divergenza KL</em> fornisce la misura teorica della distanza tra modello e realtà;</li>
<li>l’<em>ELPD</em>, stimato via LOO-CV o WAIC, traduce questa misura in una valutazione pratica della capacità predittiva;</li>
<li>la scelta del modello ottimale richiede un equilibrio tra accuratezza, generalizzazione e parsimonia.</li>
</ul>
<p>Con questi strumenti possiamo individuare modelli che colgono i veri pattern nei dati, evitando di farsi ingannare dal rumore e garantendo previsioni solide anche in contesti complessi.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Informazioni sull'ambiente di sviluppo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Informazioni sull’ambiente di sviluppo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] gt_1.0.0              pillar_1.11.0         tinytable_0.11.0     </span></span>
<span><span class="co">#&gt;  [4] patchwork_1.3.1       ggdist_3.3.3          tidybayes_3.0.7      </span></span>
<span><span class="co">#&gt;  [7] bayesplot_1.13.0      ggplot2_3.5.2         reliabilitydiag_0.2.1</span></span>
<span><span class="co">#&gt; [10] priorsense_1.1.0      posterior_1.6.1       loo_2.8.0            </span></span>
<span><span class="co">#&gt; [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          </span></span>
<span><span class="co">#&gt; [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     </span></span>
<span><span class="co">#&gt; [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        </span></span>
<span><span class="co">#&gt; [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          </span></span>
<span><span class="co">#&gt; [25] rio_1.2.3             here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] gridExtra_2.3        inline_0.3.21        sandwich_3.1-1      </span></span>
<span><span class="co">#&gt;  [4] rlang_1.1.6          magrittr_2.0.3       multcomp_1.4-28     </span></span>
<span><span class="co">#&gt;  [7] snakecase_0.11.1     compiler_4.5.1       systemfonts_1.2.3   </span></span>
<span><span class="co">#&gt; [10] vctrs_0.6.5          stringr_1.5.1        pkgconfig_2.0.3     </span></span>
<span><span class="co">#&gt; [13] arrayhelpers_1.1-0   fastmap_1.2.0        backports_1.5.0     </span></span>
<span><span class="co">#&gt; [16] labeling_0.4.3       cmdstanr_0.9.0       rmarkdown_2.29      </span></span>
<span><span class="co">#&gt; [19] markdown_2.0         ps_1.9.1             ragg_1.4.0          </span></span>
<span><span class="co">#&gt; [22] purrr_1.1.0          xfun_0.52            cachem_1.1.0        </span></span>
<span><span class="co">#&gt; [25] litedown_0.7         jsonlite_2.0.0       broom_1.0.9         </span></span>
<span><span class="co">#&gt; [28] parallel_4.5.1       R6_2.6.1             stringi_1.8.7       </span></span>
<span><span class="co">#&gt; [31] RColorBrewer_1.1-3   lubridate_1.9.4      estimability_1.5.1  </span></span>
<span><span class="co">#&gt; [34] knitr_1.50           zoo_1.8-14           base64enc_0.1-3     </span></span>
<span><span class="co">#&gt; [37] Matrix_1.7-3         splines_4.5.1        timechange_0.3.0    </span></span>
<span><span class="co">#&gt; [40] tidyselect_1.2.1     abind_1.4-8          yaml_2.3.10         </span></span>
<span><span class="co">#&gt; [43] codetools_0.2-20     processx_3.8.6       curl_6.4.0          </span></span>
<span><span class="co">#&gt; [46] pkgbuild_1.4.8       lattice_0.22-7       withr_3.0.2         </span></span>
<span><span class="co">#&gt; [49] bridgesampling_1.1-2 coda_0.19-4.1        evaluate_1.0.4      </span></span>
<span><span class="co">#&gt; [52] survival_3.8-3       RcppParallel_5.1.10  xml2_1.3.8          </span></span>
<span><span class="co">#&gt; [55] tensorA_0.36.2.1     checkmate_2.3.2      stats4_4.5.1        </span></span>
<span><span class="co">#&gt; [58] distributional_0.5.0 generics_0.1.4       rprojroot_2.1.0     </span></span>
<span><span class="co">#&gt; [61] commonmark_2.0.0     rstantools_2.4.0     scales_1.4.0        </span></span>
<span><span class="co">#&gt; [64] xtable_1.8-4         glue_1.8.0           emmeans_1.11.2      </span></span>
<span><span class="co">#&gt; [67] tools_4.5.1          data.table_1.17.8    mvtnorm_1.3-3       </span></span>
<span><span class="co">#&gt; [70] grid_4.5.1           QuickJSR_1.8.0       colorspace_2.1-1    </span></span>
<span><span class="co">#&gt; [73] nlme_3.1-168         cli_3.6.5            textshaping_1.0.1   </span></span>
<span><span class="co">#&gt; [76] svUnit_1.0.6         Brobdingnag_1.2-9    V8_6.0.5            </span></span>
<span><span class="co">#&gt; [79] gtable_0.3.6         sass_0.4.10          digest_0.6.37       </span></span>
<span><span class="co">#&gt; [82] TH.data_1.1-3        htmlwidgets_1.6.4    farver_2.1.2        </span></span>
<span><span class="co">#&gt; [85] memoise_2.0.1        htmltools_0.5.8.1    lifecycle_1.0.4     </span></span>
<span><span class="co">#&gt; [88] MASS_7.3-65</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/formal_models/introduction.html" class="pagination-link" aria-label="Teorie formali dei fenomeni psicologici">
        <span class="nav-page-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Teorie formali dei fenomeni psicologici</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>