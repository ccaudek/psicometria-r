# Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV  {#sec-div-kl-lppd-elpd}


::: callout-important
## Obiettivi di apprendimento

Alla fine di questo capitolo, sarai in grado di:

* comprendere cos'è la distribuzione predittiva posteriore e come si costruisce;
* spiegare cosa misura il log-score e come si calcola nella pratica;
* distinguere tra LPPD ed ELPD e comprendere il loro significato;
* capire come LOO-CV fornisca una stima dell’ELPD;
* collegare il confronto tra modelli alla divergenza di Kullback-Leibler.
:::

::: callout-tip
## Prerequisiti

- Per comprendere appieno questo capitolo è utile leggere il capitolo 7 *Ulysses’ Compass* di *Statistical Rethinking* (@McElreath_rethinking).
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

library(conflicted)
library(brms)
library(loo)
conflicts_prefer(rstan::loo)
```
:::


## Introduzione {.unnumbered .unlisted}

::: {.lead}
Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’*entropia*, che misura l’incertezza insita in una distribuzione, e la *divergenza di Kullback–Leibler* ($D_{\text{KL}}$), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per *valutare e confrontare modelli statistici* nel contesto bayesiano.
:::

Il punto di partenza è una domanda cruciale: *quanto bene il modello riesce a prevedere nuovi dati?* Un buon modello non deve solo adattarsi bene ai dati già osservati, ma anche saper *generalizzare* a situazioni future o a campioni mai visti. Questa distinzione — adattamento vs. generalizzazione — è il cuore della valutazione predittiva.

Per rendere concreta questa idea, immaginiamo di aver sviluppato un test psicologico per prevedere il livello di ansia degli studenti alla vigilia di un esame. Non basta sapere che il modello descrive bene i dati del campione che abbiamo usato per costruirlo: vogliamo anche essere ragionevolmente sicuri che le stesse previsioni funzionino per studenti che non hanno partecipato allo studio. In psicologia, scegliere tra due modelli non è diverso dal decidere quale test usare per prevedere un disturbo: entrambi mirano a capire quale strumento fornisce previsioni più affidabili sui dati futuri.

In questo capitolo esploreremo gli strumenti fondamentali per la valutazione e il confronto di modelli nell’ambito dell’inferenza bayesiana.  

**1. La distribuzione predittiva posteriore**  
Introdurremo la *distribuzione predittiva posteriore*, che incorpora l’incertezza sui parametri per generare previsioni coerenti con lo stato di conoscenza del modello. Questo strumento rappresenta il ponte naturale tra stima e previsione, garantendo una quantificazione probabilistica completa dell’incertezza.  

**2. Misure di accuratezza predittiva**  
Discuteremo il *log-score*, una metrica punto per punto che valuta la qualità delle previsioni, e due sue sintesi fondamentali:  

- la *LPPD* (*Log Pointwise Predictive Density*), che misura la bontà di adattamento su dati osservati;  
- l’*ELPD* (*Expected Log Predictive Density*), che stima l’abilità predittiva attesa su nuove osservazioni.  

**3. Validazione empirica e confronto tra modelli**  
Presenteremo la tecnica *Leave-One-Out Cross-Validation* (LOO-CV), un approccio efficiente per stimare l’ELPD senza bisogno di nuovi dati, dimostrando come questa metodologia fornisca una valutazione robusta delle prestazioni predittive.  

**4. Fondamenti teorici e interpretazione**  
Approfondiremo il legame tra ELPD e *divergenza di Kullback-Leibler*, che consente di interpretare il confronto tra modelli come una ricerca del modello più vicino alla vera distribuzione generatrice dei dati. Questa connessione teorica fornisce una solida giustificazione informazionale per le procedure di selezione bayesiana.  

L’obiettivo del capitolo è offrire una panoramica completa e operativa, che unisca principi teorici a strumenti applicativi, guidando il lettore nella scelta razionale del modello più adatto al problema in esame.  


## Distribuzione predittiva posteriore

Nel capitolo precedente abbiamo usato la *divergenza di Kullback–Leibler (KL)* come misura teorica della distanza tra realtà e modello. Qui ci chiediamo: *come stimiamo questa distanza quando la “vera” distribuzione generatrice è ignota?* Un tassello fondamentale è la *distribuzione predittiva posteriore*.

Nel capitolo sul modello *beta–binomiale* l’abbiamo già incontrata: è lo strumento che, nell’approccio bayesiano, consente di prevedere nuovi dati incorporando **sia la struttura del modello sia l’incertezza sui parametri**.

In sintesi: dopo aver osservato i dati $y$, non otteniamo un singolo “miglior” valore dei parametri, ma una *distribuzione posteriore* $p(\theta \mid y)$ che quantifica i valori plausibili di $\theta$ e la nostra incertezza.

> Esempio. Uno psicologo che stima il livello medio di ansia in una popolazione, invece di affermare “la media è 4.7”, dirà: “il valore più plausibile è 4.7, **ma** è ragionevole che sia tra 4.2 e 5.1”, riflettendo la variabilità posteriore.

Per prevedere un nuovo dato $\tilde y$, non fissiamo $\theta$. *Media*mo invece tutte le previsioni condizionate $p(\tilde y \mid \theta)$ pesandole con la posteriore $p(\theta\mid y)$:

$$
q(\tilde{y} \mid y)
\;=\;
\int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta .
$$

::: {.callout-note collapse=true title="Intuizione"}
Se conoscessimo $\theta$, useremmo direttamente $p(\tilde y\mid \theta)$. Poiché non lo conosciamo, *combiniamo* le previsioni per tutti i $\theta$ possibili, pesandole in base a quanto ciascun $\theta$ è plausibile a posteriori—come chiedere il parere a più esperti e fare una media ponderata della loro opinione.
:::

**Notazione.** Useremo talvolta la forma compatta $q(\cdot \mid y)$ per indicare la predittiva posteriore del modello. Quando ci servirà evidenziare la previsione *marginale* per una singola osservazione $y_i$, scriveremo:

$$
p(y_i \mid y)
\;=\;
\int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta,
$$

cioè la verosimiglianza $p(y_i\mid\theta)$ integrata rispetto alla posteriore $p(\theta\mid y)$.

**Idea chiave:** la predittiva posteriore *propaga l’incertezza sui parametri alle previsioni*. È questo passaggio a rendere le valutazioni predittive coerenti con il principio bayesiano, e quindi utilizzabili nel *confronto tra modelli* e nella stima di quantità legate alla “distanza” dal generatore dei dati.


### Il problema della valutazione predittiva

Il nostro obiettivo è capire *quanto* la distribuzione predittiva posteriore $q(\tilde{y} \mid y)$ si avvicini alla *vera distribuzione generatrice* dei dati futuri, $p(\tilde{y})$. In teoria, questa distanza si misura con la *divergenza di Kullback–Leibler (KL)*:

$$
D_{\text{KL}}(p \parallel q) \;=\; \mathbb{E}_p\!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right].
$$

Qui però incontriamo subito un problema concettuale: *non conosciamo $p(\tilde{y})$*. È come voler giudicare la precisione di una mappa senza poter vedere il territorio reale.

Per superare questo ostacolo, possiamo ricorrere a *misure surrogate* che, pur non avendo accesso diretto a $p(\tilde{y})$, permettono di stimare la qualità predittiva del modello utilizzando in modo ingegnoso i dati osservati. Tra queste, vedremo il *log-score*, la *LPPD* e l’*ELPD*, che forniscono stime indirette ma utili della bontà predittiva.


::: {.callout-note title="Mappa concettuale"}

| Quantità                 | Significato                          | Uso principale                   |
| ------------------------ | ------------------------------------ | -------------------------------- |
| $p(y_i \mid \theta)$  | Verosimiglianza                      | Calcolo predittivo               |
| $p(\theta \mid y)$     | Distribuzione posteriore             | Ponderazione                     |
| $p(y_i \mid y)$       | Predizione bayesiana media           | Log-score, LPPD                  |
| $p(y_i \mid y_{-i})$ | Predizione LOO (*leave-one-out*)     | ELPD                             |
| $q(\tilde{y} \mid y)$  | Distribuzione predittiva complessiva | Divergenza KL, confronto modelli |
:::


## Il log-score: accuratezza predittiva punto per punto {#sec-logscore}

Definita la *distribuzione predittiva posteriore*, possiamo chiederci: *quanto bene il modello ha “previsto” ciascun dato osservato?*
Il *log-score* risponde proprio a questa domanda: per ogni osservazione $y_i$ calcola il logaritmo della probabilità predittiva che il modello le assegna:

$$
\log p(y_i \mid y) \;=\; \log \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta .
$$ {#eq-log-score-def}

Un valore alto (cioè meno negativo) indica che l’osservazione era plausibile per il modello; un valore basso che era improbabile.

Il *log-score totale* si ottiene sommando i contributi di tutte le osservazioni:

$$
S \;=\; \sum_{i=1}^n \log p(y_i \mid y) .
$$ {#eq-log-score-sum-def}

Più il punteggio è alto, maggiore è la probabilità che il modello attribuisce ai dati realmente osservati. In altre parole, è un *indice di fiducia*: se ciò che accade era atteso dal modello, il punteggio cresce.


### Frequentista vs. Bayesiano

* **Versione frequentista**
  Si valuta $p(y_i \mid \hat{\theta})$ usando una *stima puntuale* dei parametri (ad esempio MLE o MAP), senza considerare l’incertezza sui parametri:

  $$
  \log p(y_i \mid \hat{\theta}) .
  $$

* **Versione bayesiana**
  Si usa la *densità predittiva puntuale*, integrando la verosimiglianza sulla distribuzione a posteriori dei parametri:

  $$
  p(y_i \mid y) \;=\; \int p(y_i \mid \theta) \, p(\theta \mid y) \, d\theta ,
  $$

  dove $\theta^{(s)}$ sono campioni MCMC dalla posteriore.


### La LPPD: log-score “bayesiano” {#sec-lppd}

Calcolando il log-score in versione bayesiana e sommando su tutte le osservazioni si ottiene la *Log Pointwise Predictive Density (LPPD)*:

$$
\text{LPPD} \;=\; \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right] .
$$ {#eq-lppd-def}

**In breve:**

* *Log-score totale (frequentista)* → usa una sola stima dei parametri.
* *LPPD (bayesiana)* → stessa idea, ma integra l’incertezza sui parametri usando la posteriore.


#### Calcolo pratico con MCMC

Nella formula teorica dell’@eq-lppd-def, la quantità $p(y_i \mid y)$ è una *media predittiva* ottenuta integrando:

* $p(y_i \mid \theta)$: la verosimiglianza condizionata, cioè la distribuzione dei dati futuri se i parametri fossero $\theta$;
* $p(\theta \mid y)$: la distribuzione posteriore dei parametri, che rappresenta la nostra incertezza residua dopo aver osservato i dati.

Poiché questo integrale raramente è calcolabile in forma chiusa, lo *stimiamo usando i campioni MCMC* dalla posteriore:

$$
p(y_i \mid y) \;\approx\; \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) ,
$$ {#eq-mcmc-posterior-parameter-distr}

e quindi:

$$
\text{Log-score} \;\approx\; \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right] .
$$ {#eq-mcmc-log-score}

Il log-score così calcolato fornisce *una misura complessiva di accuratezza predittiva sui dati osservati* (*in-sample*).

**Limite importante:** essendo calcolato sugli stessi dati usati per stimare i parametri, tende a favorire modelli più complessi, rischiando di sopravvalutarne la capacità di generalizzazione (*overfitting*). Per valutazioni più affidabili, serviranno tecniche di validazione incrociata che misurino la performance *out-of-sample*.


::: {.callout-note collapse=true title="Esempio."}
Supponiamo di avere tre valori posteriori di $\theta$: 0.3 (peso 0.2), 0.5 (peso 0.5) e 0.7 (peso 0.3). Se la nuova osservazione è $y = 3$ su $n = 5$ tentativi:

```{r}
theta_vals <- c(0.3, 0.5, 0.7)
posterior_weights <- c(0.2, 0.5, 0.3)
likelihoods <- dbinom(3, size = 5, prob = theta_vals)
p_y_given_y <- sum(likelihoods * posterior_weights)
log_score <- log(p_y_given_y)
log_score
```

Il log-score è circa -1.29. Un valore meno negativo indica una previsione migliore.
:::



### Expected Log Predictive Density (ELPD): guardare oltre i dati osservati

Se vogliamo valutare la *capacità di generalizzazione* di un modello, la domanda chiave è: *quanto bene predirebbe dati che non ha mai visto?* L’*ELPD* (*Expected Log Predictive Density*) risponde a questa domanda con la stessa logica della LPPD, ma introduce una differenza fondamentale: la previsione di $y_i$ viene calcolata *escludendo $y_i$ dall’adattamento del modello* (*Leave-One-Out*, LOO):

$$
\text{ELPD} \;=\; \sum_{i=1}^n \log p(y_i \mid y_{-i}),
$$ {#eq-elpd-def}

dove $y_{-i}$ indica il dataset a cui è stata rimossa l’osservazione $i$.

**Esempio**
Nel caso di un test sull’ansia:

* *LPPD* → misura quanto bene il modello predice i punteggi di ansia degli studenti *già presenti* nel campione osservato.
* *ELPD* → misura quanto bene predirebbe il punteggio di un *nuovo* studente, usando solo i dati degli altri.

In sostanza, l’ELPD è una *stima empirica* (con segno cambiato) della *divergenza di Kullback–Leibler* tra la vera distribuzione dei dati futuri e la distribuzione predittiva del modello. Ci fornisce quindi un indicatore diretto di *quanto* le previsioni del modello si avvicinano a ciò che accadrà davvero, senza richiedere di conoscere la distribuzione reale.


::: callout-note
**Interpretazione:** l’ELPD è un *log-score out-of-sample*: per ogni $y_i$, lo escludiamo, adattiamo il modello agli altri dati, e valutiamo la probabilità predittiva di $y_i$.
Più alto è l’ELPD, migliore è la capacità del modello di generalizzare a dati nuovi.
:::


::: {.callout-note collapse=true title="Esempio."}
Supponiamo di avere tre osservazioni $y_1, y_2, y_3$ e che il modello stimi:

$$
p(y_1 \mid y_2,y_3)=0.6,\quad p(y_2 \mid y_1,y_3)=0.7,\quad p(y_3 \mid y_1,y_2)=0.5.
$$

L’ELPD è:

$$
\log 0.6 + \log 0.7 + \log 0.5 \; \approx\; -0.5108 -0.3567 -0.6931 = -1.5606.
$$

Un valore meno negativo indica maggiore capacità predittiva fuori campione.
:::


### LPPD vs. ELPD in sintesi

| Misura   | Dati usati per predire $y_i$      | Valuta                | Limite principale      |
| -------- | --------------------------------- | --------------------- | ---------------------- |
| **LPPD** | Tutti i dati, incluso $y_i$       | Adattamento in-sample | Rischio di overfitting |
| **ELPD** | Tutti i dati tranne $y_i$ (*LOO*) | Generalizzazione      | —                      |


**Metafora**
In un esperimento di riconoscimento di volti, mostriamo a un partecipante 100 fotografie e lo alleniamo a riconoscerle:

* *LPPD* → misura quanto bene riconosce *quelle stesse foto*, già viste in fase di addestramento (*in-sample*).
* *ELPD* → misura quanto bene riconosce *nuove foto*, mai viste prima, cioè immagini fuori dall’insieme di addestramento (*out-of-sample*).

Se il punteggio LPPD è alto ma l’ELPD è basso, significa che il partecipante — o il modello — ha *memorizzato* i casi specifici, senza aver appreso regole generali utili per nuovi dati.


### Il collegamento con la divergenza KL

La *divergenza di Kullback–Leibler* $D_{\text{KL}}$ misura teoricamente la distanza tra la distribuzione vera dei dati, $p(\tilde{y})$, e la distribuzione predittiva del modello, $q(\tilde{y} \mid y)$.

Nel confronto tra due modelli $A$ e $B$, la differenza nelle loro $D_{\text{KL}}$ equivale alla differenza nelle rispettive *accuratezze predittive medie* rispetto a $p(\tilde{y})$.

Poiché $p(\tilde{y})$ è sconosciuta, non possiamo calcolare direttamente la KL. L’*ELPD* fornisce una stima empirica di questa accuratezza predittiva: un valore più alto implica un modello più “vicino” alla distribuzione vera.

$$
\text{Massimizzare ELPD} \;\; \approx \;\; \text{Minimizzare la divergenza KL}.
$$

::: {.callout-note title="Perché ELPD ≈ - KL?"}
Per definizione:

$$
D_{\text{KL}}\big(p \parallel q\big)
= \mathbb{E}_{p} \!\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right]
= \mathbb{E}_{p}[\log p(\tilde{y})] - \mathbb{E}_{p}[\log q(\tilde{y} \mid y)].
$$

* Il primo termine $\mathbb{E}_{p}[\log p(\tilde{y})]$ *non dipende dal modello* (è fisso per tutti).
* Confrontare due modelli equivale quindi a confrontare *solo il secondo termine*, che è $-$ELPD.

Ecco perché *massimizzare l’ELPD equivale a minimizzare la divergenza KL*: si sta massimizzando la media log-predittiva che il modello assegna ai dati futuri.
:::


::: {.callout-note collapse=true title="Esempio."}
Vogliamo confrontare due modelli predittivi per il numero di “teste” in $n=10$ lanci.

- La **distribuzione vera** è $p(y)=\text{Binom}(n=10,\;p=0.6)$.
- Il **modello candidato** prevede $q(y)=\text{Binom}(n=10,\;q=0.5)$.

L’*ELPD* di un modello è l’aspettativa, rispetto alla distribuzione vera $p$, del *log-score* del modello: $\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]$. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori $y=0,\dots,n$.

```{r}
# Parametri del problema
n <- 10          # numero di lanci
p <- 0.6         # probabilità vera di "testa"
q <- 0.5         # probabilità ipotizzata dal modello candidato

# 1) Supporto dei possibili esiti
y_vals <- 0:n

# 2) Distribuzione vera p(y) su tutto il supporto
p_y <- dbinom(y_vals, size = n, prob = p)

# 3) Log-predittiva del modello candidato q su tutto il supporto
log_q_y <- log(dbinom(y_vals, size = n, prob = q))

# 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)
elpd_q <- sum(p_y * log_q_y)

# 5) "Modello vero": usa q = p. Log-predittiva del modello vero
log_p_y <- log(dbinom(y_vals, size = n, prob = p))

# 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)
elpd_p <- sum(p_y * log_p_y)

# 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]
kl_pq <- sum(p_y * (log_p_y - log_q_y))

cat(sprintf("ELPD modello candidato (q=0.5): %.4f\n", elpd_q))
cat(sprintf("ELPD modello vero      (q=0.6): %.4f\n", elpd_p))
cat(sprintf("Differenza ELPD (vero - candidato): %.4f\n", elpd_p - elpd_q))
cat(sprintf("KL(p || q): %.4f\n", kl_pq))
```

**Cosa stiamo verificando?**

1. $\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)$ è *più basso* (più negativo) del valore ottenuto dal modello vero $\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)$.
   → Il modello con $q=0.6$ è *più predittivo* di quello con $q=0.5$.

2. La *differenza* tra i due ELPD è *uguale* (vicina numericamente) alla *divergenza di Kullback–Leibler*:

$$
\mathrm{ELPD}(p)-\mathrm{ELPD}(q)
= \sum_y p(y)\big[\log p(y)-\log q(y)\big]
= D_{\mathrm{KL}}(p\|q)\;>\;0.
$$

→ Questo mostra *algebricamente e numericamente* il legame: *massimizzare l’ELPD equivale a minimizzare la KL*.

> Nota sul log: nel codice usiamo il log naturale (unità in **nat**). Se si preferisce il log in base 2 (unità in *bit*), basta sostituire `log()` con `log2()`; tutte le quantità cambiano di una costante di scala, ma i *confronti* tra modelli restano identici.

**In pratica.**

In questo esempio abbiamo potuto calcolare l’ELPD *vero* perché conoscevamo l’intera distribuzione generatrice $p(y)$ e potevamo integrare esattamente. Nella realtà, $p(y)$ è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD *empiricamente*, ad esempio con la *Leave-One-Out Cross-Validation* (LOO-CV), che sostituisce l’aspettativa rispetto a $p$ con una media sui dati raccolti, lasciando fuori una osservazione alla volta. Questa procedura ci consente di avvicinarci al calcolo ideale della KL, anche senza conoscere $p(y)$.
:::

::: {.callout-note}
**Collegamento chiave**  
L’ELPD è una stima empirica (con segno cambiato) della divergenza di Kullback–Leibler.  
Più alto è l’ELPD, migliore è la capacità predittiva del modello.
:::


## Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica

Poiché la distribuzione vera dei dati futuri è inaccessibile, dobbiamo usare metodi indiretti per stimare quanto bene il nostro modello prevede nuove osservazioni.
La validazione incrociata Leave-One-Out (*LOO-CV*) è uno di questi metodi e, se combinata con l’uso dell’*Expected Log Predictive Density* (*ELPD*), diventa uno strumento potente per il confronto tra modelli.

Abbiamo visto che l’ELPD è la misura ideale della capacità predittiva di un modello su dati futuri. Il problema è che, per definizione, richiede di calcolare un’aspettativa rispetto alla *vera* distribuzione generatrice $p(\tilde{y})$, che non conosciamo.

**Come possiamo stimarla in pratica?**
Usando la LOO-CV, che simula la previsione di nuovi dati sfruttando solo le informazioni presenti nei dati osservati.


### Cos’è la LOO-CV

La LOO-CV è un esperimento concettuale semplice:

1. Scegli un’osservazione $y_i$ dal dataset.
2. Escludila dal set di addestramento.
3. Adatta il modello ai dati rimanenti $y_{-i}$.
4. Calcola la densità predittiva del modello per l’osservazione esclusa: $p(y_i \mid y_{-i})$.
5. Ripeti per ogni osservazione e somma i logaritmi ottenuti.

Formalmente:

$$
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}),
$$ {#eq-loo-def}

dove $y_{-i}$ indica il dataset senza l’osservazione $i$.  
La struttura è identica a quella dell’ELPD “ideale”, ma ogni termine è calcolato *fuori campione*, escludendo il dato che viene valutato.

Un’analogia: è come escludere uno studente dall’allenamento e verificare se il modello riesce a predire il suo punteggio d’esame; ripetendo questo processo per tutti gli studenti otteniamo una misura diretta della capacità di generalizzazione.


### Perché LOO-CV funziona

L’ELPD può essere scritto come:

$$
\mathbb{E}_p[\log q(\tilde{y} \mid y)],
$$ {#eq-loo-def2}

dove $q(\tilde{y} \mid y)$ è la distribuzione predittiva del modello.  
Non possiamo calcolare l’aspettativa rispetto a $p(\tilde{y})$, ma possiamo trattare ogni osservazione $y_i$ come “nuovo dato” generato da $p$ e usare la media empirica sulle osservazioni reali come stima dell’aspettativa:

$$
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$

In altre parole: LOO-CV misura *quanto bene il modello predirebbe ciascun dato se non lo avesse mai visto*.

### Legame con la divergenza KL

La divergenza di Kullback–Leibler è definita come:

$$
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p[\log p(\tilde{y})] - \mathbb{E}_p[\log q(\tilde{y} \mid y)].
$$

Il primo termine, l’entropia di $p$, è lo stesso per tutti i modelli e scompare nel confronto.  
Ne segue che, per due modelli $q_1$ e $q_2$:

$$
D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) =
\mathbb{E}_p[\log q\_2(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_1(\tilde{y} \mid y)].
$$

*Vince il modello con ELPD più alto*, perché corrisponde alla minore divergenza KL dalla distribuzione vera.


### Confrontare i modelli con LOO-CV

Poiché $p(\tilde{y})$ è sconosciuta, sostituiamo l’aspettativa teorica con la stima empirica via LOO:

$$
\Delta\text{ELPD} = \text{ELPD}*{\text{LOO}}(M_1) - \text{ELPD}*{\text{LOO}}(M_2) .
$$ {#eq-delta-elpd-def}

$\Delta\text{ELPD}$ approssima la differenza tra le divergenze KL dei modelli.  
Oltre alla differenza, possiamo stimare un *errore standard* per capire se la superiorità di un modello è robusta o dovuta al caso.


### Punti chiave

- **Problema:** L’ELPD teorico richiede $p(\tilde{y})$, che è sconosciuta.  
- **Soluzione:** LOO-CV fornisce una stima empirica out-of-sample.  
- **Teoria:** L’ELPD è direttamente collegato alla parte “accuratezza” della KL-divergence.  
- **Pratica:** Massimizzare l’ELPD stimato equivale a scegliere il modello più vicino alla distribuzione vera.

Direi che l’esempio che hai scritto è già molto chiaro e in linea con il testo precedente, ma per integrarlo meglio nel capitolo e mantenere continuità con la sezione teorica, potremmo:

1. **Aggiungere un’introduzione contestuale** per collegarlo subito alla discussione ELPD–LOO–KL.
2. **Rendere più esplicito il parallelismo con la teoria** (ELPD come somma delle log-predittive fuori campione).
3. **Sintetizzare il codice** con commenti chiave, così che lo studente possa leggerlo senza perdersi nei dettagli secondari.
4. **Chiarire il senso della tabella** subito dopo l’esecuzione del codice.


::: {.callout-note collapse=true title="Esempio: confronto ELPD-LOO tra due modelli"}
Questo mini-esempio mostra come passare *dalla definizione teorica* dell’ELPD alla *stima pratica via Leave-One-Out*, usando un caso elementare Beta–Bernoulli.

**Dati.**
Cinque prove indipendenti: $y=\{1,1,1,0,1\}$ (quattro “successi”, un “insuccesso”).

**Modello A (Bayesiano adattato ai dati).**
Bernoulli$(\theta)$ con prior $\theta\sim \text{Beta}(1,1)$ (uninformativa).
Per LOO:

* per ogni $i$, escludiamo $y_i$;
* calcoliamo la posteriore $\theta \mid y_{-i} \sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})$,
  dove $s_{-i}$ è il numero di successi tra i $n-1$ rimanenti;
* calcoliamo la probabilità predittiva per $y_i$.

**Modello B (di confronto).**
Moneta equa fissa ($q=0.5$): la predittiva è sempre $0.5$, indipendentemente dai dati.

```{r}
# Dati
y <- c(1, 1, 1, 0, 1)
n <- length(y)

# Log-predittiva LOO per Modello A (Beta(1,1) + Bernoulli)
loo_log_pred_beta <- function(i, y, a0 = 1, b0 = 1) {
  yi <- y[i]
  s_minus <- sum(y) - yi
  n_minus <- n - 1
  alpha <- a0 + s_minus
  beta  <- b0 + (n_minus - s_minus)
  p1 <- alpha / (alpha + beta)
  p  <- if (yi == 1) p1 else (1 - p1)
  log(p)
}

# Log-predittive punto-per-punto
lp_beta  <- sapply(seq_along(y), loo_log_pred_beta, y = y)
lp_fixed <- rep(log(0.5), n)

# ELPD-LOO
elpd_beta  <- sum(lp_beta)
elpd_fixed <- sum(lp_fixed)

# Differenza e SE
diff_pt <- lp_beta - lp_fixed
se_diff <- sqrt(n * var(diff_pt))

# Tabella riassuntiva
res <- data.frame(
  i = 1:n, y = y,
  lp_beta = round(lp_beta, 6),
  lp_fixed = round(lp_fixed, 6),
  diff = round(diff_pt, 6)
)
print(res)
cat(sprintf("\nELPD-LOO Modello A: %.6f\n", elpd_beta))
cat(sprintf("ELPD-LOO Modello B: %.6f\n", elpd_fixed))
cat(sprintf("Differenza (A-B)  : %.6f\n", elpd_beta - elpd_fixed))
cat(sprintf("SE differenza     : %.6f\n", se_diff))
```

**Interpretazione.**

* Ogni riga della tabella mostra la log-predittiva fuori campione per entrambi i modelli.
* In un campione con 4 successi su 5, il Modello A assegna *più di 0.5* di probabilità ai successi, e meno di 0.5 all’unico insuccesso.
* L’ELPD-LOO di A può risultare leggermente più alto di quello di B, ma l’errore standard è grande perché $n$ è piccolo.

> **Regola pratica:** una differenza $|\Delta \text{ELPD}|$ di almeno 2 volte l’SE fornisce un’indicazione più affidabile di superiorità del modello. In esempi così piccoli l’obiettivo è puramente didattico: capire *come* si calcola e *cosa* significa.
:::


### ELPD-LOO e il problema dell’overfitting

Valutare un modello sugli stessi dati usati per addestrarlo tende a *gonfiare* le stime della sua capacità predittiva (*overfitting*). È come se uno studente ottenesse un punteggio perfetto ripetendo esercizi già svolti: non sappiamo se saprebbe risolverne di nuovi.

La *Leave-One-Out Cross-Validation (LOO-CV)* aggira il problema valutando ciascuna osservazione $y_i$ usando *solo* i dati rimanenti ($y_{-i}$). Il punteggio ottenuto (ELPD-LOO) è quindi una stima *out-of-sample* della bontà predittiva, meno sensibile all’overfitting.

Grazie a metodi come il *Pareto-smoothed importance sampling (PSIS)*, oggi è possibile calcolare l’ELPD-LOO *senza riadattare il modello $n$ volte*. In R, la funzione `loo()` del pacchetto *loo* (integrata in `brms` e `rstanarm`) rende questa procedura rapida e diretta anche per modelli complessi.


::: {.callout-note collapse=true title="Esempio: ELPD atteso vs ELPD-LOO stimato"}
Quando conosci la distribuzione vera dei dati ($p$), puoi calcolare l’*ELPD atteso* in modo esatto.
Quando invece hai solo i dati osservati, lo stimi tramite *Leave-One-Out* (PSIS-LOO), come mostrato di seguito.

**ELPD atteso (p noto)**

```{r}
n <- 10
p <- 0.6
q <- 0.5

y_vals   <- 0:n
p_y      <- dbinom(y_vals, size = n, prob = p)
log_q_y  <- log(dbinom(y_vals, size = n, prob = q))
elpd     <- sum(p_y * log_q_y)

cat(sprintf("ELPD atteso (modello q = 0.5): %.4f\n", elpd))
```

**ELPD-LOO stimato da dati simulati**

```{r}
#| output: false

set.seed(123)
df <- data.frame(
  k = rbinom(20, size = n, prob = p),
  n = n
)

fit <- brm(k | trials(n) ~ 1, data = df,
           family = binomial(),
           prior = prior(constant(0), class = "Intercept"),
           iter = 2000, chains = 2)

loo_res <- loo(fit)
print(loo_res)
```

L’oggetto `loo_res` fornisce l’ELPD-LOO, il suo errore standard, e le statistiche *Pareto k* per la diagnostica.
Con `loo_compare()` puoi confrontare due modelli sulla base della differenza di ELPD-LOO e del relativo SE.
:::

::: {.callout-note collapse=true title="In pratica: stimare e confrontare l'ELPD-LOO"}
**Concetto chiave**

* L’ELPD valuta la capacità predittiva su dati non visti.
* La LOO-CV lo stima in modo efficiente con PSIS-LOO.

**Strumenti**

* Funzione `loo()` del pacchetto *loo*, integrata in `brms` e `rstanarm`.
* Diagnostica con *Pareto k*, confronto con `loo_compare()`.

**Workflow tipico in R**

1. Adattare ogni modello (`brm()` o `stan_glm()`).
2. Estrarre `log_lik()` e calcolare `loo()`.
3. Confrontare modelli con `loo_compare()`.

**Decisione**

* Preferire l’ELPD-LOO più alto.
* Differenza ≥ 2×SE → indicazione di vantaggio sostanziale.
* Valutare anche semplicità e interpretabilità.
:::


## Criteri di informazione come approssimazioni della divergenza $D_{\text{KL}}$

Oltre alla *Leave-One-Out Cross-Validation*, esistono altri strumenti per stimare la qualità predittiva di un modello senza dover conoscere la distribuzione vera dei dati.
Molti di questi metodi derivano, in modo più o meno diretto, dalla *divergenza di Kullback–Leibler* $D_{\text{KL}}$, che — come visto — misura la distanza tra la distribuzione reale e quella stimata dal modello.

L’idea di base è sempre la stessa:

* valutare quanto bene il modello spiega i dati (*bontà di adattamento*);
* penalizzare la *complessità* del modello, per ridurre il rischio di *overfitting*.

Questa logica si traduce in *criteri di informazione* che combinano due componenti:

1. *termine di fit*: misura di quanto bene il modello si adatta ai dati osservati (es. log-verosimiglianza, MSE);
2. *termine di penalizzazione*: aumenta con il numero di parametri o con la flessibilità del modello.

Tra i criteri più usati troviamo:

* **MSE** (Mean Squared Error) – semplice e intuitivo, basato sugli errori di previsione;
* **AIC** (Akaike Information Criterion) – approssima $D_{\text{KL}}$ tra il modello e la verità, penalizzando il numero di parametri;
* **BIC** (Bayesian Information Criterion) – simile all’AIC, ma con penalizzazione più forte per modelli complessi, proporzionale al numero di osservazioni;
* **WAIC** (Widely Applicable Information Criterion) – versione pienamente bayesiana, basata sulle previsioni del modello integrate sull’intera distribuzione a posteriori.

Nelle sezioni seguenti vedremo come ciascun criterio si calcola, quali assunzioni richiede e in quali situazioni è preferibile rispetto agli altri.


### Errore Quadratico Medio (MSE)

L’*Errore Quadratico Medio* misura la media delle differenze al quadrato tra valori osservati e previsti:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
$$ {#eq-mse-def}

* Valori più bassi indicano previsioni più vicine ai dati osservati.
* Non tiene conto della complessità del modello, quindi può favorire modelli eccessivamente flessibili (*overfitting*).

Utile per valutare l’accuratezza, ma da solo non è adatto a scegliere tra modelli con diversa complessità.


### Akaike Information Criterion (AIC)

L’*AIC* è un’approssimazione della divergenza $D_{\text{KL}}$ e stima quanta informazione si perde usando un modello per descrivere i dati:

$$
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k,
$$ {#eq-aic-def}

dove:

* $\hat{\theta}_{\text{MLE}}$: stima dei parametri ottenuta massimizzando la verosimiglianza;
* $k$: numero di parametri del modello.

**Interpretazione**

* Il primo termine valuta l’adattamento del modello ai dati.
* Il secondo penalizza la complessità per evitare overfitting.
* Un AIC più basso indica un miglior equilibrio tra accuratezza e semplicità.

**Limiti**

* Basato su assunzioni asintotiche (funziona meglio con campioni grandi).
* Usa solo stime puntuali, ignorando l’incertezza dei parametri.
* Non è pienamente coerente con l’approccio bayesiano.


### Bayesian Information Criterion (BIC)

Il *BIC* valuta il compromesso tra *adattamento ai dati* e *complessità del modello*, applicando una penalizzazione più severa rispetto all’AIC — soprattutto quando il numero di osservazioni $n$ è grande.

$$
BIC = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k,
$$ {#eq-bic-def}

dove:

* $p(y \mid \hat{\theta})$: massima verosimiglianza del modello (o MAP con prior piatti);
* $n$: numero di osservazioni indipendenti;
* $k$: numero di parametri stimati.

**Interpretazione**

* Il primo termine misura l’adattamento ai dati.
* Il secondo penalizza la complessità in modo crescente con $n$ e $k$.
* Un BIC più basso indica un compromesso migliore tra accuratezza e parsimonia.

**Vantaggi**

* Tende a favorire modelli più semplici quando $n$ è elevato.
* Ha una giustificazione teorica bayesiana: in certe condizioni, approssima il log della *marginal likelihood*.

**Limiti**

* Si basa su assunzioni forti (indipendenza, modelli regolari, prior deboli).
* Può sottoselezionare modelli utili con campioni piccoli o strutture complesse.


### Widely Applicable Information Criterion (WAIC)

Il *WAIC* è una versione *pienamente bayesiana* dell’AIC:

* utilizza *tutta la distribuzione a posteriori* dei parametri;
* fornisce una stima diretta della *capacità predittiva* del modello.

$$
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \mathrm{Var}_{\theta^{(s)}} \big( \log p(y_i \mid \theta^{(s)}) \big)
\right],
$$ {#eq-waic-def}

dove:

* $S$ = numero di campioni dalla distribuzione a posteriori;
* $\theta^{(s)}$ = $s$-esimo campione;
* il secondo termine stima il *numero effettivo di parametri* basato sulla variabilità della log-verosimiglianza.

**Vantaggi**

* Adatto anche a modelli complessi o non regolari.
* Usa direttamente i campioni MCMC.
* Migliore dell’AIC per modelli bayesiani, perché incorpora l’incertezza dei parametri.

**Nota**
Il WAIC è strettamente collegato all’ELPD: è una sua stima approssimata ottenuta dalla posteriori, senza bisogno di eseguire la LOO-CV.


**Riepilogo comparativo.**

| Criterio | Tipo                   | Penalizza la complessità? | Usa stime puntuali? | Supporta Bayesian MCMC? |
| -------- | ---------------------- | ------------------------- | ------------------- | ----------------------- |
| MSE      | Frequentista           | ❌                         | ✅                   | ❌                       |
| AIC      | Frequentista           | ✅ (modesta)               | ✅                   | ❌                       |
| BIC      | Frequentista/Bayesiano | ✅ (forte)                 | ✅                   | ❌                       |
| WAIC     | Bayesiano              | ✅ (effettiva)             | ❌                   | ✅                       |
| LOO-CV   | Bayesiano              | ✅ (empirica)              | ❌                   | ✅                       |


## Riflessioni conclusive {.unnumbered .unlisted}

La selezione del modello, in ottica bayesiana, ruota attorno a una domanda essenziale: *quanto bene il modello predice dati che non ha mai visto?*

Il riferimento teorico è l’*Expected Log Predictive Density (ELPD)*, che misura quanto la distribuzione predittiva del modello si avvicina alla vera (e ignota) distribuzione dei dati. In termini matematici, massimizzare l’ELPD equivale a minimizzare la *divergenza di Kullback–Leibler* rispetto alla vera generatrice: due facce dello stesso obiettivo, rappresentare al meglio la realtà sottostante.

Poiché $p_{\text{vera}}(y)$ è sconosciuta, l’ELPD va stimato. Le principali approssimazioni sono:

* **LOO-CV** (Leave-One-Out Cross-Validation): oggi lo strumento più affidabile, valuta ogni osservazione come “nuova” e stima la capacità di generalizzazione del modello.
* **WAIC**: alternativa completamente bayesiana, calcolata direttamente dai campioni della posteriori.
* **AIC** e **BIC**: criteri frequenstisti più rapidi ma basati su stime puntuali; utili in contesti semplici.
* **MSE**: misura l’accuratezza sulle osservazioni note, ma non penalizza la complessità e quindi non è adatto alla selezione del modello.

Nel confronto tra modelli, la *differenza di ELPD* (stimata con LOO-CV o WAIC) andrebbe interpretata insieme al relativo *errore standard*: una regola pratica è considerare rilevante una differenza almeno doppia rispetto all’errore standard.

**In sintesi:**

* la buona statistica non si limita a spiegare il passato: sa *anticipare il futuro*;
* la *divergenza KL* fornisce la misura teorica della distanza tra modello e realtà;
* l’*ELPD*, stimato via LOO-CV o WAIC, traduce questa misura in una valutazione pratica della capacità predittiva;
* la scelta del modello ottimale richiede un equilibrio tra accuratezza, generalizzazione e parsimonia.

Con questi strumenti possiamo individuare modelli che colgono i veri pattern nei dati, evitando di farsi ingannare dal rumore e garantendo previsioni solide anche in contesti complessi.


## Informazioni sull'ambiente di sviluppo {.unnumbered .unlisted}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered .unlisted}

