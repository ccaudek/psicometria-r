# La quantificazione dell'incertezza {#sec-bayes-inference-intro}

::: callout-important
## In questo capitolo approfondirai i seguenti concetti fondamentali:  

- *L'incertezza come distribuzione di probabilità:* comprensione di come quantificare e rappresentare matematicamente l'incertezza attraverso le distribuzioni di densità.
- *Aggiornamento bayesiano:* il processo di integrazione delle nuove evidenze con le conoscenze preesistenti.
- *Il meccanismo generatore dei dati:* come i parametri sconosciuti determinano i dati osservati attraverso processi probabilistici. 
::: 

::: callout-tip
## Prerequisiti

- Leggere [Bayesian statistics for clinical research](https://www.sciencedirect.com/science/article/pii/S0140673624012959) di @Goligher2024.
- Leggere [Dicing with the unknown](http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2004.00050.x/abstract) di Tony O'Hagan, per una descrizione chiara della distinzione tra incertezza aleatoria e incertezza epistemica.
- Leggere il capitolo *Estimation* [@schervish2014probability].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


## Introduzione {.unnumbered .unlisted}

> “Quindi non avete una sola risposta alle vostre domande?”  
> “Adson, se l'avessi insegnerei teologia a Parigi.”  
> “A Parigi hanno sempre la risposta vera?”  
> “Mai,” disse Guglielmo, “ma sono molto sicuri dei loro errori.”  
>
> — *Umberto Eco* (Il Nome della Rosa)


::: {.lead}
Nel @sec-uncertainty abbiamo visto come l'incertezza sia un aspetto inevitabile della ricerca psicologica e più in generale della conoscenza scientifica. Abbiamo distinto diversi tipi di incertezza e sottolineato la necessità di strumenti per gestirla in maniera coerente. Questo ci porta naturalmente al cuore dell'approccio bayesiano: come quantificare matematicamente l'incertezza e come aggiornarla quando osserviamo nuove evidenze.
:::

Se nel @sec-uncertainty abbiamo definito che cosa intendiamo per incertezza, in questo capitolo vediamo come rappresentarla matematicamente e come aggiornarla sistematicamente quando raccogliamo nuovi dati.


## L'incertezza come distribuzione di probabilità

Prima di addentrarci nei meccanismi dell'aggiornamento bayesiano, è fondamentale comprendere come l'approccio bayesiano concettualizza l'incertezza. A differenza del pensiero quotidiano, che spesso tratta l'incertezza come una semplice mancanza di informazione, la statistica bayesiana la rappresenta attraverso distribuzioni di probabilità che catturano non solo cosa non sappiamo, ma anche quanto siamo incerti riguardo a ciò che non sappiamo.


### L'esempio di Sherlock Holmes

Per illustrare questo concetto, consideriamo un esempio ispirato ai racconti di Arthur Conan Doyle [@doing_bayesian_data_an]. Immaginiamo che Sherlock Holmes stia indagando su un caso di furto in una villa londinese. Il detective sa che il colpevole è una delle quattro persone presenti quella sera: il maggiordomo, la cuoca, il giardiniere e un ospite. Inizialmente, senza alcuna evidenza particolare, Holmes potrebbe assegnare la stessa probabilità a ciascun sospetto: 25% per ognuno.

Questa distribuzione uniforme dell'incertezza rappresenta uno stato di "ignoranza simmetrica" - non abbiamo ragioni per sospettare una persona più delle altre. Tuttavia, man mano che Holmes raccoglie indizi, la sua incertezza si trasforma e si concentra. Supponiamo che scopra impronte di terra fresca nel corridoio che porta alla cassaforte. Questa evidenza aumenta la probabilità che il colpevole sia il giardiniere (che lavora con la terra) e diminuisce quella degli altri sospetti.
La distribuzione della probabilità potrebbe ora diventare: giardiniere 60%, maggiordomo 20%, cuoca 15%, ospite 5%. Successivamente, Holmes potrebbe scoprire che il giardiniere ha un alibi solido per l'ora del furto. Questa nuova informazione ridistribuisce completamente la probabilità: giardiniere 5%, maggiordomo 50%, cuoca 35%, ospite 10%.


### La natura dinamica dell'incertezza bayesiana

L'esempio relativo a Sherlock Holmes illustra tre aspetti cruciali dell'approccio bayesiano all'incertezza. Primo, l'incertezza non è semplicemente "non sapere qualcosa", ma una distribuzione strutturata di plausibilità tra diverse possibilità. Secondo, questa distribuzione cambia sistematicamente man mano che nuove evidenze diventano disponibili. Terzo, in ogni momento possiamo quantificare precisamente il nostro grado di certezza o incertezza riguardo a ciascuna ipotesi.

Nel contesto della ricerca psicologica, possiamo applicare lo stesso principio ai parametri dei nostri modelli statistici. Invece di cercare un singolo valore "vero" per un parametro come la media di un gruppo o la forza di una correlazione, trattiamo questi parametri come variabili la cui incertezza può essere rappresentata attraverso distribuzioni di probabilità. Una distribuzione stretta e appuntita indica alta certezza riguardo al valore del parametro, mentre una distribuzione ampia e piatta riflette maggiore incertezza.[^1]

[^1]: Un esempio semplice è il modello binomiale, che descrive il numero di successi $y$ in $n$ prove indipendenti con probabilità di successo $\theta$. Scriviamo $y \sim \text{Binomiale}(n, \theta)$. In questo caso $\theta$ è il parametro che rappresenta la probabilità di successo in ciascuna prova. Nella pratica, non conosciamo il valore “vero” di $\theta$ (per esempio, la probabilità che una persona risponda correttamente a un item di un test), ma possiamo rappresentare la nostra incertezza con una distribuzione di probabilità su $\theta$ e aggiornarla man mano che raccogliamo dati.


## Dalla certezza puntuale alla distribuzione dell'incertezza

La differenza tra l'approccio tradizionale e quello bayesiano può essere illustrata pensando a come risponderemmo alla domanda "Qual è l'intelligenza media degli studenti di psicologia?" L'approccio tradizionale tenterebbe di fornire una singola risposta numerica, come "Il QI medio è 110". L'approccio bayesiano, invece, riconoscerebbe l'incertezza inerente in questa stima e fornirebbe una distribuzione: "Siamo piuttosto sicuri che il QI medio sia tra 105 e 115, con 110 come valore più plausibile, ma valori tra 100 e 120 sono ancora ragionevolmente possibili".

Questa rappresentazione probabilistica dell'incertezza non è semplicemente più onesta intellettualmente - è anche più utile praticamente. Ci permette di prendere decisioni informate che tengono conto dell'incertezza, di pianificare studi futuri che riducano l'incertezza dove è più necessario, e di comunicare i risultati della ricerca in modo che rifletta accuratamente il nostro grado di conoscenza.


## Le fondamenta concettuali dell'inferenza bayesiana

Come è stato discusso nei capitoli introduttivi, l'approccio bayesiano alla statistica si fonda su una visione particolare della probabilità che differisce sostanzialmente dall'interpretazione frequentista tradizionale. Mentre il frequentismo interpreta la probabilità come una proprietà oggettiva del mondo legata alla frequenza di eventi ripetibili, il bayesianesimo la intende come una misura del nostro grado di credenza o certezza riguardo a proposizioni incerte.

### Interpretazione bayesiana della probabilità

Nel frequentismo, dire che una moneta ha probabilità 0.5 di dare testa significa che, in un numero infinito di lanci, la frequenza relativa di teste convergerà a 0.5. Questa definizione, pur elegante matematicamente, presenta difficoltà pratiche evidenti: raramente possiamo ripetere un esperimento infinite volte, e molti eventi di interesse sono intrinsecamente irripetibili.
Il bayesianesimo offre un'alternativa concettualmente diversa: la probabilità 0.5 rappresenta il nostro grado di credenza che il prossimo lancio darà testa, basato sulle informazioni attualmente disponibili. Questa interpretazione soggettiva della probabilità non la rende arbitraria o non scientifica, purché seguiamo le regole della teoria della probabilità per garantire coerenza logica nelle nostre inferenze.

### Il teorema di Bayes come strumento di aggiornamento

Il cuore dell'inferenza bayesiana è il teorema di Bayes, che fornisce una formula matematica per aggiornare le nostre credenze alla luce di nuove evidenze:

$$
p(\theta \mid D) = \frac{p(D \mid \theta)\, p(\theta)}{p(D)},
$$

Questa equazione apparentemente semplice cattura un processo cognitivo fondamentale: come integrare nuove informazioni con conoscenze preesistenti. Il termine $p(θ)$ rappresenta le nostre credenze iniziali sul parametro $\theta$  (distribuzione a priori), $(D \mid \theta)$ descrive quanto sono compatibili i dati osservati $D$ con diversi valori di $\theta$ (verosimiglianza), e $p(\theta \mid D)$ è il risultato dell'aggiornamento: le nostre credenze aggiornate dopo aver osservato i dati (distribuzione a posteriori).

Il denominatore $p(D)$ è spesso chiamato "evidenza" o "probabilità marginale" dei dati, e serve come costante di normalizzazione per garantire che la distribuzione a posteriori integri a 1.


::: {.callout-note}
## La verosimiglianza come funzione di θ

La stessa formula che usiamo per descrivere la probabilità dei dati dati i parametri può essere “letta al contrario”.  Se fissiamo i dati osservati $y$, la funzione

$$
L(\theta \mid y) \propto p(y \mid \theta)
$$

si chiama *funzione di verosimiglianza*.  Non è una probabilità vera e propria (non si somma a 1 su $\theta$), ma ci dice quali valori di $\theta$ rendono i dati più plausibili.

Questa distinzione è cruciale:  

- *Probabilità* → dati variabili, parametri fissi.  
- *Verosimiglianza* → dati fissi, parametri variabili.
:::


## Il processo generatore dei dati: dal parametro all'osservazione

Un concetto centrale nell'inferenza statistica, che spesso rimane implicito nei corsi introduttivi, è quello del processo generatore dei dati. Questo concetto rappresenta il meccanismo teorico attraverso cui i parametri sconosciuti di interesse producono i dati che osserviamo nel nostro campione.


### Comprendere il legame parametro-dati

Quando conduciamo una ricerca psicologica, osserviamo dati specifici: punteggi a test, tempi di reazione, risposte a questionari. Tuttavia, quello che veramente ci interessa sono i parametri sottostanti che hanno generato questi dati: la vera media della popolazione, la vera forza di una correlazione, la vera differenza tra gruppi sperimentali. Il processo generatore dei dati è il modello teorico che specifica come questi parametri latenti si traducono nelle osservazioni concrete.

Consideriamo un esempio concreto. Supponiamo di essere interessati al livello medio di ansia in una popolazione di studenti universitari. Il parametro di interesse è $\mu$, la vera media della popolazione. Tuttavia, quando raccogliamo dati, otteniamo solo un campione di osservazioni: $x_1,x_2, \dots,x_n$. Il processo generatore dei dati specifica che ogni osservazione $x_i$ è estratta da una distribuzione normale con media $\mu$ e deviazione standard $\sigma$: $x_i \sim \mathcal{N}(\mu, \sigma)$. 


### La verosimiglianza come ponte concettuale

La verosimiglianza $p(D \mid \theta)$ formalizza matematicamente il processo generatore dei dati (si veda il @sec-prob-likelihood). Essa esprime la probabilità di osservare esattamente i dati che abbiamo raccolto, dato un particolare valore del parametro $\theta$. In altre parole, per ogni possibile valore di $\theta$, la verosimiglianza ci dice quanto sono "probabili" o "compatibili" i nostri dati osservati.

Questo concetto può essere controintuitivo inizialmente, perché stiamo calcolando la probabilità dei dati (che abbiamo già osservato e quindi sappiamo essere "veri") condizionata su parametri (che sono sconosciuti). Tuttavia, la verosimiglianza ci fornisce uno strumento potente per confrontare diversi valori possibili del parametro: valori di $\theta$ che rendono i nostri dati più probabili sono più plausibili di valori che li rendono meno probabili.


### L'importanza delle assunzioni sul processo generatore

La scelta del processo generatore dei dati è cruciale e riflette le nostre assunzioni teoriche sul fenomeno studiato. Assumere che i dati seguano una distribuzione normale implica che la variabilità osservata è simmetrica attorno alla media e che valori estremi sono rari ma possibili. Assumere una distribuzione di Poisson suggerirebbe che stiamo modellando conteggi di eventi rari. Assumere una distribuzione Beta indicherebbe che stiamo lavorando con proporzioni o probabilità.

Queste scelte non sono neutre: influenzano profondamente le inferenze che traiamo dai dati. Un vantaggio dell'approccio bayesiano è che rende esplicite queste assunzioni, permettendoci di valutarne la plausibilità e di esplorare la robustezza delle nostre conclusioni a assunzioni alternative.


## L'aggiornamento bayesiano in azione: l'esempio del globo

Per illustrare concretamente come funziona l'aggiornamento bayesiano, consideriamo l'esempio classico proposto da McElreath nel suo testo "Statistical Rethinking" [@McElreath_rethinking]. Immaginiamo di voler stimare la proporzione della superficie terrestre coperta d'acqua utilizzando un metodo sperimentale semplice: lanciare un globo terrestre in aria, afferrarlo casualmente, e osservare se il punto sotto il nostro dito indice è acqua o terra.


### Il setup sperimentale

Chiamiamo $p$ la vera proporzione di superficie coperta d'acqua. Questo è il nostro parametro di interesse - quello che vogliamo stimare. Ogni lancio del globo ci dà un'osservazione: "W" se tocchiamo acqua, "L" se tocchiamo terra. Il nostro processo generatore dei dati assume che ogni lancio sia indipendente e che la probabilità di osservare acqua in ogni singolo lancio sia esattamente $p$.

Inizialmente, non abbiamo conoscenze specifiche sulla proporzione d'acqua sulla Terra. Potremmo quindi iniziare con una distribuzione a priori uniforme: ogni valore di $p$ tra 0 e 1 è ugualmente plausibile. Questa scelta riflette uno stato di "ignoranza informativa" - non abbiamo ragioni per favorire alcuni valori di $p$ rispetto ad altri.

### La dinamica dell'apprendimento

Supponiamo che il primo lancio risulti in "W" (acqua). Come dovrebbe cambiare la nostra credenza su $p$? Intuitivamente, osservare acqua dovrebbe aumentare la plausibilità di valori di pp
p più alti e diminuire quella di valori più bassi. Il teorema di Bayes formalizza esattamente questa intuizione.

Prima dell'osservazione, tutti i valori di $p$ avevano la stessa probabilità. Dopo aver osservato "W", valori di $p$ vicini a 0 diventano molto implausibili (se $p$ fosse davvero vicino a 0, sarebbe stato molto improbabile osservare acqua al primo lancio), mentre valori più alti di $p$ diventano più credibili. La distribuzione a posteriori rifletterà questo cambiamento, concentrando più probabilità sui valori più alti di $p$.

Il secondo lancio risulta in "L" (terra). Questa nuova osservazione fornisce evidenza nella direzione opposta: ora valori molto alti di $p$ diventano meno plausibili. La distribuzione a posteriori si aggiusterà di nuovo, bilanciando l'evidenza di entrambe le osservazioni. Il pattern risultante sarà una distribuzione che favorisce valori intermedi di $p$, coerenti con l'aver osservato sia acqua che terra.


### L'accumulo progressivo dell'evidenza

Man mano che continuiamo a raccogliere dati - diciamo che osserviamo la sequenza "W", "L", "W", "W", "L", "W", "L", "W", "W" - la distribuzione a posteriori evolve continuamente. Ogni nuova osservazione aggiorna le nostre credenze, integrando la nuova evidenza con tutta l'informazione precedentemente raccolta.

Un aspetto cruciale di questo processo è che la distribuzione a posteriori dopo $n$ osservazioni diventa automaticamente la distribuzione a priori per la (n+1)-esima osservazione. Questo riflette un principio fondamentale dell'apprendimento bayesiano: ogni pezzo di informazione è integrato cumulativamente, e non c'è perdita di informazione nel processo di aggiornamento.


### L'evoluzione dell'incertezza

Oltre a tracciare come cambia la nostra stima "migliore" di $p$ (ad esempio, la moda della distribuzione), è altrettanto importante osservare come evolve la nostra incertezza. Nelle prime fasi dell'esperimento, quando abbiamo pochi dati, la distribuzione a posteriori sarà relativamente ampia, riflettendo la nostra incertezza sostanziale sul valore di $p$.

Man mano che raccogliamo più osservazioni, la distribuzione diventa progressivamente più stretta e concentrata. Questo restringimento rappresenta la riduzione dell'incertezza: con più dati, diventiamo più sicuri del valore di $p$. Tuttavia, la forma precisa di questa evoluzione dipende dai dati specifici osservati. Se i dati sono molto consistenti (ad esempio, molti successi consecutivi), l'incertezza si ridurrà rapidamente. Se i dati sono più variabili, la riduzione dell'incertezza sarà più graduale.


```{r fig.width=8, fig.asp=1, out.width="80%", echo=TRUE}
#| echo: false
#| tags: [hide-input]
#| message: false
#| warning: false

library(ggplot2)

define_beta_posterior <- function(W, L, p) {
  dbeta(p, W + 1, L + 1)
}

plot_beta_update_ggplot <- function(observations) {
  W <- cumsum(observations == "W")
  L <- cumsum(observations == "L")
  p_grid <- seq(0, 1, length.out = 100)
  
  data_list <- list()
  
  for (i in seq_along(observations)) {
    post <- define_beta_posterior(W[i], L[i], p_grid)
    prior <- if (i == 1) dbeta(p_grid, 1, 1) else define_beta_posterior(W[i-1], L[i-1], p_grid)
    
    df_post <- data.frame(p = p_grid, density = post, 
                          type = "Posterior", trial = factor(i, levels = seq_along(observations)))
    df_prior <- data.frame(p = p_grid, density = prior, 
                           type = "Prior", trial = factor(i, levels = seq_along(observations)))
    
    data_list[[i]] <- rbind(df_prior, df_post)
  }
  
  data_combined <- do.call(rbind, data_list)
  
  trial_labels <- sapply(seq_along(observations), function(i) 
    paste0("Lancio ", i, "\n(", W[i], " W, ", L[i], " L)"))
  
  ggplot(data_combined, aes(x = p, y = density, color = type, group = interaction(trial, type))) +
    geom_line() +
    scale_color_manual(values = c("Prior" = "gray", "Posterior" = "blue")) +
    labs(title = "Aggiornamento Bayesiano della Distribuzione Beta",
         x = "Proporzione d'acqua (p)",
         y = "Densità") +
    facet_wrap(~trial, ncol = 3, labeller = labeller(trial = function(x) trial_labels[as.numeric(x)])) +
    scale_x_continuous(labels = scales::number_format(accuracy = 0.1))
}

# Eseguire l'esperimento
osservazioni <- c("W", "L", "W", "W", "L", "W", "L", "W", "W")
plot_beta_update_ggplot(osservazioni)
```

Il grafico illustra visivamente questo processo di aggiornamento bayesiano. In ogni pannello, la linea grigia rappresenta la distribuzione a priori (le nostre credenze prima della nuova osservazione), mentre la linea blu mostra la distribuzione a posteriori (le credenze aggiornate dopo l'osservazione). Si può notare come ogni distribuzione a posteriori diventi la nuova distribuzione a priori per il passo successivo, creando una catena continua di aggiornamento dell'incertezza.


::: {.callout-note}
## Posteriori diverse a confronto

Lo stesso insieme di dati può dare origine a posteriori molto diverse a seconda del prior.  
Questo non è un “difetto” del Bayes, ma un aspetto centrale: rende esplicita l’influenza delle ipotesi di partenza.

Il vantaggio è che possiamo discutere apertamente *quanto* le conclusioni dipendano dal prior e, se necessario, confrontare più specificazioni (analisi di sensibilità).
:::


### Lezioni dall'esempio del globo

Questo esempio illustra diversi principi fondamentali dell'inferenza bayesiana. Primo, l'aggiornamento delle credenze è sistematico e quantificabile: non ci basiamo su intuizioni vaghe, ma su calcoli matematici precisi. Secondo, l'informazione si accumula in modo ottimale: ogni osservazione contribuisce alla nostra conoscenza complessiva senza perdita di informazione precedente. Terzo, il processo esplicita chiaramente la distinzione tra ciò che sappiamo (la distribuzione a posteriori corrente) e ciò che non sappiamo (l'ampiezza di questa distribuzione). Quarto, il metodo fornisce previsioni calibrate: possiamo usare la distribuzione a posteriori non solo per stimare $p$, ma anche per predire l'esito di futuri lanci del globo.


## Implicazioni per la ricerca psicologica

L'approccio bayesiano all'inferenza ha profonde implicazioni per come conduciamo e interpretiamo la ricerca psicologica. A differenza dei metodi frequentisti tradizionali, che forniscono risposte dicotomiche (significativo/non significativo) basate su criteri arbitrari (come $p$ < 0.05), l'inferenza bayesiana offre un quadro più ricco e sfumato per comprendere e comunicare i risultati della ricerca.


### Dai test di ipotesi all'aggiornamento delle credenze

Il paradigma tradizionale dei test di ipotesi null'ipotesi (NHST) struttura la ricerca come un processo di decisione binaria: rifiutiamo o non rifiutiamo l'ipotesi nulla. Questo approccio, pur avendo servito la psicologia per decenni, presenta limitazioni ben documentate. Non ci dice quanto è plausibile l'ipotesi alternativa, non quantifica l'incertezza nelle nostre stime, e non fornisce un meccanismo naturale per integrare risultati di studi multipli.

L'approccio bayesiano ricontestualizza la ricerca come un processo continuo di aggiornamento delle credenze. Invece di chiedere "Possiamo rifiutare l'ipotesi nulla?", chiediamo "Come cambiano le nostre credenze riguardo al fenomeno di interesse alla luce di questi nuovi dati?". Questa formulazione è più naturale e informativamente ricca: invece di una decisione binaria, otteniamo una quantificazione completa dell'incertezza e un aggiornamento calibrato delle nostre conoscenze.
L'integrazione di conoscenze preesistenti

Uno dei vantaggi più significativi dell'approccio bayesiano è la sua capacità di integrare formalmente conoscenze preesistenti nell'analisi dei dati. Nella ricerca psicologica, raramente partiamo da zero: abbiamo teorie esistenti, risultati di studi precedenti, e conoscenze accumulate nel campo. L'inferenza frequentista tradizionale tratta ogni studio come se fosse condotto in un vuoto informativo, ignorando sistematicamente queste conoscenze pregresse.

L'inferenza bayesiana, attraverso la distribuzione a priori, permette di incorporare esplicitamente queste informazioni preesistenti. Questo non significa che i risultati sono "soggettivi" o "non scientifici" - significa semplicemente che riconosciamo che la scienza è un'attività cumulativa dove ogni nuovo studio si costruisce sulle fondamenta degli studi precedenti.


### La comunicazione dell'incertezza

I risultati bayesiani si prestano naturalmente a una comunicazione più onesta e informativa dell'incertezza. Invece di riportare che "la differenza tra gruppi è significativa ($p$ < 0.05)", possiamo dire che "siamo al 95% sicuri che la vera differenza tra gruppi sia tra 0.3 e 0.8 punti, con 0.55 come valore più plausibile". Questa formulazione comunica non solo la direzione e la magnitudine dell'effetto, ma anche il grado di certezza che possiamo avere in questa stima.

Inoltre, l'approccio bayesiano fornisce strumenti naturali per rispondere a domande praticamente rilevanti. Invece di limitarci a stabilire se un effetto è "significativo", possiamo calcolare la probabilità che l'effetto sia abbastanza grande da essere praticamente importante, o la probabilità che un intervento terapeutico sia superiore a trattamenti esistenti.


## Considerazioni pratiche e limitazioni

Pur offrendo vantaggi concettuali significativi, l'approccio bayesiano presenta anche sfide pratiche che devono essere riconosciute e affrontate. La scelta delle distribuzioni a priori, in particolare, può essere controversa e richiede giustificazione teorica. Distribuzioni a priori troppo informative possono dominare i dati, mentre distribuzioni troppo vaghe possono portare a problemi computazionali.

La complessità computazionale rappresenta un'altra sfida. Mentre esempi semplici come quello del globo ammettono soluzioni analitiche, la maggior parte dei modelli bayesiani realistici richiede metodi computazionali intensivi come il campionamento Monte Carlo via catene di Markov (MCMC). Questo può rendere l'analisi bayesiana meno accessibile per ricercatori senza formazione computazionale specializzata.

Tuttavia, lo sviluppo di software user-friendly come Stan, PyMC, e interfacce R come `rstanarm` e `brms` stanno rendendo l'analisi bayesiana sempre più accessibile. Inoltre, la crescente disponibilità di potenza computazionale e il miglioramento degli algoritmi stanno riducendo progressivamente questi ostacoli pratici.


::: {.callout-note}
## La predittiva posteriore

Oltre a stimare i parametri, possiamo chiederci: *quali dati futuri ci aspettiamo?* Questa distribuzione si chiama *predittiva posteriore* (si veda il @sec-bayesian-inference-post-pred-distr):

$$
p(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta.
$$

In pratica:  

1. estraiamo valori di $\theta$ dalla posterior;  
2. per ciascun valore simuliamo nuovi dati;  
3. otteniamo così la distribuzione delle possibili osservazioni future.

Esempio: dopo aver osservato 8 successi su 10, la probabilità che il prossimo lancio sia testa non è fissata, ma distribuita: circa il 73% in media, con incertezza esplicitata dalla posterior.
:::


## Riflessioni conclusive {.unnumbered .unlisted}

L'approccio bayesiano alla statistica rappresenta più di un semplice insieme di tecniche analitiche alternative: offre un paradigma fondamentalmente diverso per pensare all'incertezza, all'evidenza, e al processo di scoperta scientifica. Trattando i parametri come quantità incerte caratterizzate da distribuzioni di probabilità, fornendo meccanismi sistematici per l'aggiornamento delle credenze, e integrando naturalmente conoscenze preesistenti con nuove osservazioni, l'inferenza bayesiana si allinea più strettamente con il modo in cui gli scienziati pensano realmente al processo di ricerca.

Nella psicologia contemporanea, dove questioni di riproducibilità e comunicazione dell'incertezza sono al centro del dibattito metodologico, l'approccio bayesiano offre strumenti particolarmente rilevanti. Non è una panacea per tutti i problemi metodologici della disciplina, ma fornisce un quadro concettuale più ricco e strumenti analitici più flessibili per affrontare la complessità intrinseca dei fenomeni psicologici.

L'obiettivo di questo capitolo non è stato quello di fornire ricette procedurali per l'analisi bayesiana - quello verrà nei capitoli successivi - ma di costruire le fondamenta concettuali necessarie per apprezzare perché l'approccio bayesiano sta diventando sempre più importante nella ricerca psicologica contemporanea. Comprendere l'incertezza come distribuzione di probabilità, il meccanismo dell'aggiornamento bayesiano, e il ruolo del processo generatore dei dati ci prepara per esplorare applicazioni più specifiche e tecniche di questi principi fondamentali.


::: {.callout-note title="Approfondimento" collapse="true"}
A una prima lettura, è sufficiente focalizzarsi sul significato dell'aggiornamento bayesiano e sulle conseguenze che questo produce rispetto alle nostre credenze sui parametri, man mano che vengono osservati nuovi dati. Per il momento, il meccanismo dettagliato attraverso cui l'aggiornamento bayesiano viene realizzato non è ancora stato esplicitato, e quindi gli studenti possono inizialmente tralasciare la spiegazione approfondita contenuta in questo riquadro.

Dopo aver letto il contenuto relativo alle famiglie coniugate nei capitoli successivi, sarà possibile tornare sull'esempio discusso qui e comprenderne appieno l'aggiornamento bayesiano, interpretandolo alla luce delle proprietà delle famiglie coniugate. Questo consentirà di cogliere non solo il significato generale dell'aggiornamento, ma anche i dettagli tecnici che lo rendono particolarmente efficiente in contesti come quello descritto.

L'esempio del globo che abbiamo discusso utilizza quello che i statistici chiamano modello "Beta-Binomiale". In questo modello, la distribuzione a priori per la proporzione pp
p è una distribuzione Beta, e la verosimiglianza per le osservazioni è una distribuzione Binomiale. Questa combinazione ha una proprietà matematica elegante chiamata "coniugazione": quando combiniamo una distribuzione a priori Beta con una verosimiglianza Binomiale usando il teorema di Bayes, la distribuzione a posteriori risultante è anch'essa una Beta.

*Primo Pannello: l'inizio dell'esperimento*

1. *Osservazione iniziale*: Abbiamo il primo dato, un successo ("W").
2. *A priori*: La distribuzione *a priori* iniziale è una distribuzione Beta(1, 1). Questa rappresenta una conoscenza iniziale non informativa, ovvero l'ipotesi che qualsiasi proporzione di successi ($p$) sia ugualmente probabile.
3. *A posteriori*: Con un successo su una prova:
   $$
   \text{Posterior} \sim \mathcal{Beta}(1 + 1, 1 + 0) = \mathcal{Beta}(2, 1).
   $$
   La distribuzione risultante è concentrata verso i valori più alti di $p$, riflettendo il successo osservato.

*Secondo Pannello: bilanciamento dell'evidenza.*

1. *Osservazioni*: Ora abbiamo due dati, "W" e "L", quindi un successo su due prove.
2. *A priori*: La distribuzione *a priori* per questo passo è il *posterior* del pannello precedente, ovvero $\mathcal{Beta}(2, 1)$.
3. *A posteriori*: Con un successo ($W = 1$) e un insuccesso ($L = 1$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(2 + 1, 1 + 1) = \mathcal{Beta}(3, 2).
   $$
   La nuova distribuzione riflette un aggiornamento che tiene conto sia del successo che dell’insuccesso.
   

::: {.callout-tip}
## Effetto del prior sulla distribuzione a posteriori

Cambiare il prior significa cambiare l’aggiornamento:  
- con un **prior uniforme**, la posterior riflette quasi solo la verosimiglianza;  
- con un **prior informativo**, la posterior “media” l’informazione dei dati con quella preesistente.

Esempio: osservando 8 successi su 10 lanci di moneta:  
- con un prior $Beta(1,1)$ la posterior è $Beta(9,3)$;  
- con un prior $Beta(20,20)$ la posterior è $Beta(28,22)$, più “conservativa” verso 0.5.

Il grafico mostra come due priors diversi possano produrre posteriori con varianze molto differenti.
:::

*Terzo Pannello: accumulo progressivo.*

1. *Osservazioni*: Ora abbiamo tre dati, "W", "L", "W", quindi due successi su tre prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(3, 2)$.
3. *A posteriori*: Con due successi ($W = 2$) e un insuccesso ($L = 1$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(3 + 1, 2 + 0) = \mathcal{Beta}(4, 2).
   $$

*Quarto Pannello.*

1. *Osservazioni*: Ora abbiamo quattro dati, "W", "L", "W", "W", quindi tre successi su quattro prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(4, 2)$.
3. *A posteriori*: Con tre successi ($W = 3$) e un insuccesso ($L = 1$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(4 + 1, 2 + 0) = \mathcal{Beta}(5, 2).
   $$

*Quinto Pannello.*

1. *Osservazioni*: Ora abbiamo cinque dati, "W", "L", "W", "W", "L", quindi tre successi su cinque prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(5, 2)$.
3. *A posteriori*: Con tre successi ($W = 3$) e due insuccessi ($L = 2$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(5 + 0, 2 + 1) = \mathcal{Beta}(5, 3).
   $$

*Sesto Pannello.*

1. *Osservazioni*: Ora abbiamo sei dati, "W", "L", "W", "W", "L", "W", quindi quattro successi su sei prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(5, 3)$.
3. *A posteriori*: Con quattro successi ($W = 4$) e due insuccessi ($L = 2$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(5 + 1, 3 + 0) = \mathcal{Beta}(6, 3).
   $$

*Settimo Pannello.*

1. *Osservazioni*: Ora abbiamo sette dati, "W", "L", "W", "W", "L", "W", "L", quindi quattro successi su sette prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(6, 3)$.
3. *A posteriori*: Con quattro successi ($W = 4$) e tre insuccessi ($L = 3$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(6 + 0, 3 + 1) = \mathcal{Beta}(6, 4).
   $$

*Ottavo Pannello.*

1. *Osservazioni*: Ora abbiamo otto dati, "W", "L", "W", "W", "L", "W", "L", "W", quindi cinque successi su otto prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(6, 4)$.
3. *A posteriori*: Con cinque successi ($W = 5$) e tre insuccessi ($L = 3$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(6 + 1, 4 + 0) = \mathcal{Beta}(7, 4).
   $$

*Nono Pannello.*

1. *Osservazioni*: Ora abbiamo nove dati, "W", "L", "W", "W", "L", "W", "L", "W", "W", quindi sei successi su nove prove.
2. *A priori*: La distribuzione *a priori* è il *posterior* del pannello precedente, $\mathcal{Beta}(7, 4)$.
3. *A posteriori*: Con sei successi ($W = 6$) e tre insuccessi ($L = 3$):
   $$
   \text{Posterior} \sim \mathcal{Beta}(7 + 1, 4 + 0) = \mathcal{Beta}(8, 4).
   $$
   
**In sintesi,** man mano che raccogliamo più dati, la distribuzione Beta diventa più concentrata attorno al valore di $p$ che meglio spiega le osservazioni accumulate, riducendo progressivamente l'incertezza e fornendo stime sempre più precise della vera proporzione di superficie coperta d'acqua.
:::


## Informazioni sull'Ambiente di Sviluppo {.unnumbered .unlisted}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered .unlisted}

