# Distribuzioni di v.c. discrete {#sec-prob-discrete-prob-distr}

::: callout-important
## In questo capitolo imparerai a:

- comprendere le principali distribuzioni di massa di probabilità;
- utilizzare R per manipolare e analizzare queste distribuzioni.
::: 

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Random variables and their distributions* del testo di @blitzstein2019introduction.
- Leggere il capitolo *Special Distributions* [@schervish2014probability].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione

È importante distinguere tra variabili casuali discrete e continue, perché le distribuzioni di probabilità associate sono molto diverse nei due casi.

In questo capitolo ci focalizzeremo sulle **distribuzioni di probabilità discrete**, strumenti fondamentali per modellare fenomeni aleatori che generano un numero finito o numerabile di possibili esiti. Queste distribuzioni risultano particolarmente efficaci per descrivere eventi che si verificano in contesti discreti, come il numero di successi in un esperimento, l'occorrenza di un evento, o la selezione casuale da un insieme di opzioni finite. 

### Definizione di una Variabile Casuale Discreta

::: {#def-}
Una variabile casuale $X$ si dice avere una **distribuzione discreta** se assume valori in un insieme finito o numerabile di punti $\{x_1, x_2, \dots\}$, e per ciascun valore $x_i$ nell'insieme, la probabilità $P(X = x_i) > 0$. Inoltre, la somma delle probabilità associate a tutti i possibili valori di $X$ deve essere uguale a 1, ovvero:

$$
\sum_{i} P(X = x_i) = 1.
$$

La **funzione di massa di probabilità** (o **probability mass function**, PMF) di $X$, denotata con $f(x)$, è definita come:

$$
f(x) = P(X = x),
$$

dove $f(x) \geq 0$ per ogni $x$ e $f(x) = 0$ per tutti i valori $x$ che non appartengono all'insieme dei valori possibili di $X$.
:::

### Caratteristiche delle distribuzioni discrete

Una distribuzione discreta è caratterizzata da:

1. un insieme finito o infinito numerabile di valori possibili per $X$;
2. una probabilità positiva associata a ciascun valore possibile;
3. la proprietà che la somma delle probabilità su tutti i valori possibili è pari a 1.

La funzione $f(x)$ rappresenta quindi la probabilità che $X$ assuma esattamente il valore $x$. Per esempio, se $X$ rappresenta il numero di teste ottenute lanciando una moneta tre volte, i possibili valori di $X$ sono $\{0, 1, 2, 3\}$, e la funzione $f(x)$ specifica la probabilità di ciascuno di questi risultati. 

In sintesi, una distribuzione discreta descrive fenomeni in cui le variabili casuali possono assumere solo valori isolati, e la funzione di massa di probabilità fornisce una descrizione completa della distribuzione di probabilità di $X$.

### Panoramica delle Distribuzioni Discrete

Di seguito, vengono presentate alcune delle principali distribuzioni discrete utilizzate in statistica e nella ricerca psicologica Ogni distribuzione è descritta in termini di caratteristiche fondamentali, applicazioni pratiche e importanza teorica.


#### Distribuzione di Bernoulli

- **Descrizione**: La distribuzione di Bernoulli modella esperimenti con due possibili esiti, generalmente etichettati come "successo" (con probabilità $p$) e "fallimento" (con probabilità $1-p$).
- **Applicazioni**: Si applica a situazioni binarie, come il lancio di una moneta (testa/croce), la risposta a domande dicotomiche (sì/no), o l'esito di un evento che può verificarsi o meno.
- **Parametro**: 
  - $p$: probabilità di successo.
- **Importanza**: Costituisce la base per molte altre distribuzioni discrete, come la distribuzione binomiale e geometrica. È fondamentale per comprendere fenomeni con esiti dichotomici.

#### Distribuzione Binomiale

- **Descrizione**: La distribuzione binomiale descrive il numero totale di successi in un numero fisso $n$ di prove indipendenti, ciascuna governata da una distribuzione di Bernoulli con probabilità di successo $p$.
- **Applicazioni**: Viene utilizzata per analizzare processi ripetuti con esiti binari, ad esempio:
  - Il numero di voti favorevoli in un campione di opinione.
  - Il numero di sintomi osservati in un gruppo di pazienti.
  - Il conteggio di errori in un test di accuratezza.
- **Parametri**:
  - $n$: numero di prove.
  - $p$: probabilità di successo in ogni prova.
- **Importanza**: Fornisce uno strumento essenziale per modellare fenomeni ripetuti in condizioni identiche, consentendo analisi probabilistiche avanzate e previsioni statistiche.

#### Distribuzione di Poisson

- **Descrizione**: La distribuzione di Poisson modella il numero di eventi che si verificano in un intervallo fissato di tempo o spazio, quando tali eventi sono rari, indipendenti e accadono a un tasso medio costante $\lambda$.
- **Applicazioni**: Trova impiego in contesti dove gli eventi sono sporadici ma prevedibili, ad esempio:
  - Il numero di episodi di ansia riportati in una settimana.
  - Il numero di interazioni sociali spontanee di un bambino con disturbo dello spettro autistico durante una sessione di osservazione.
  - La frequenza di lapsus verbali durante una presentazione pubblica.
  - Il numero di sogni vividi riportati durante una serie di notti consecutive in uno studio sul sonno.
- **Parametro**:
  - $\lambda$: tasso medio di eventi per unità di tempo o spazio.
- **Importanza**: È cruciale per analizzare fenomeni psicologici o comportamentali rari ma significativi. Aiuta a comprendere i meccanismi sottostanti e a modellare la variabilità osservata in contesti clinici, sperimentali o quotidiani.

#### Distribuzione Uniforme Discreta

- **Descrizione**: La distribuzione uniforme discreta rappresenta situazioni in cui tutti gli eventi all'interno di un insieme finito hanno la stessa probabilità di verificarsi.
- **Applicazioni**: Si applica in contesti di scelta casuale equiprobabile, come:
  - La selezione casuale di uno stimolo da una lista di parole in un esperimento di memoria.
  - L'assegnazione casuale di partecipanti a gruppi sperimentali in uno studio di psicologia sociale.
  - La scelta di un'immagine tra un insieme di stimoli visivi in una ricerca sull'attenzione.
  - La probabilità uniforme che un partecipante scelga una delle opzioni in un questionario a risposte multiple, in assenza di preferenze o conoscenze specifiche.
- **Parametri**:
  - Intervallo di supporto: l'insieme finito di valori possibili (ad esempio, $\{1, 2, \dots, k\}$).
- **Importanza**: Funziona come modello di riferimento in situazioni di massima incertezza o mancanza di preferenze. È utile per definire un punto di partenza in analisi più complesse e per studiare comportamenti casuali.

In conclusione, le distribuzioni discrete sopra descritte rappresentano strumenti fondamentali per modellare una vasta gamma di fenomeni osservati in ambito scientifico, psicologico e applicativo. Ciascuna distribuzione offre una cornice teorica ben definita per interpretare e analizzare situazioni caratterizzate da variabili aleatorie discrete, fornendo così le basi per inferenze statistiche robuste e previsioni quantitative affidabili.

## Distribuzioni in R

In R, per ogni distribuzione sono disponibili quattro funzioni principali, i cui nomi iniziano con le lettere:

- **d** (*density*): per calcolare i valori teorici relativi alla distribuzione,  
- **p** (*probability*): per ottenere la probabilità cumulativa,  
- **q** (*quantile*): per determinare i quantili,  
- **r** (*random*): per generare campioni casuali.  

Il pacchetto di base `stats` include numerose funzioni dedicate alle principali distribuzioni statistiche, permettendo di calcolare valori teorici e simulare dati in modo semplice e flessibile. Per ulteriori dettagli sulle distribuzioni disponibili e sull'uso delle relative funzioni, è possibile consultare la documentazione con il comando `?Distributions`.

## Distribuzione di Bernoulli

{{< include ./distributions/binomial_distr.qmd >}}

## Distribuzione Binomiale

La distribuzione binomiale è una distribuzione di probabilità discreta che modella il numero di successi $y$ in un numero fissato $n$ di prove di Bernoulli indipendenti e identiche, dove ciascuna prova ha solo due esiti possibili: "successo" (rappresentato da "1") con probabilità $p$ o "insuccesso" (rappresentato da "0") con probabilità $1 - p$. La notazione utilizzata è la seguente:

$$
Y \sim \mathcal{Binom}(n, p).
$$

::: {#def-}
La distribuzione binomiale descrive la probabilità di osservare esattamente $y$ successi in $n$ prove di Bernoulli indipendenti:

$$
P(Y = y) = \binom{n}{y} p^{y} (1 - p)^{n - y} = \frac{n!}{y!(n - y)!} p^{y} (1 - p)^{n - y},
$$ {#eq-binom-distr}

dove $\binom{n}{y}$, noto come coefficiente binomiale, rappresenta il numero di modi possibili per ottenere $y$ successi in $n$ prove, e $p$ è la probabilità di successo in ciascuna prova.
:::

La distribuzione binomiale si presta bene a esempi classici come il lancio ripetuto di una moneta o l'estrazione di biglie da un'urna. Ad esempio, nel caso del lancio di una moneta, questa distribuzione descrive la probabilità di ottenere un determinato numero di "teste" in un certo numero di lanci, con ogni lancio che segue una distribuzione di Bernoulli con probabilità di successo $p$.

Una caratteristica interessante della distribuzione binomiale è la sua *proprietà di riproducibilità*: se due variabili casuali indipendenti, $y_1$ e $y_2$, seguono entrambe distribuzioni binomiali con lo stesso parametro $p$, ma con un diverso numero di prove ($n_1$ e $n_2$), la loro somma, $y = y_1 + y_2$, sarà ancora distribuita binomialmente, con parametri $n_1 + n_2$ e $p$.

### Calcolo delle Probabilità

Per chiarire il calcolo delle probabilità nella distribuzione binomiale, consideriamo una serie di prove di Bernoulli. Supponiamo di avere $n$ prove, con $y$ successi. La configurazione di questi risultati può essere rappresentata come:

$$
\overbrace{SS\dots S}^\text{$y$ successi} \overbrace{II\dots I}^\text{$n - y$ insuccessi}
$$

La probabilità di ottenere esattamente $y$ successi in una sequenza specifica di prove è pari a:

$$
p^y \cdot (1 - p)^{n - y},
$$

dove $p^y$ è la probabilità di ottenere $y$ successi, e $(1 - p)^{n - y}$ è la probabilità di ottenere $n - y$ insuccessi.

Tuttavia, siamo interessati alla probabilità complessiva di ottenere esattamente $y$ successi in *qualsiasi* ordine. Il numero di modi in cui ciò può avvenire è dato dal coefficiente binomiale $\binom{n}{y}$, che rappresenta tutte le possibili disposizioni dei successi e degli insuccessi nelle $n$ prove.

Quindi, moltiplicando la probabilità di una singola sequenza per il numero di sequenze possibili, otteniamo la probabilità di osservare esattamente $y$ successi:

$$
P(Y = y) = \binom{n}{y} p^y (1 - p)^{n - y}.
$$

Questo risultato corrisponde alla formula della distribuzione binomiale.

### Caso particolare $n = 1$

Ora consideriamo il caso particolare in cui $n = 1$. Quando $n = 1$, il coefficiente binomiale diventa:

$$
\binom{1}{k} = \frac{1!}{k! (1-k)!}.
$$

Espandiamo i fattoriali per i due possibili valori di $k$, che può assumere solo 0 o 1 (poiché $k \in \{0, 1, \dots, n\}$).

**Caso 1: $k = 0$**

$$
\binom{1}{0} = \frac{1!}{0! (1-0)!} = \frac{1}{1 \cdot 1} = 1.
$$

Quindi, per $k = 0$:
$$
P(X = 0) = \binom{1}{0} p^0 (1-p)^{1-0} = 1 \cdot 1 \cdot (1-p) = 1-p.
$$

**Caso 2: $k = 1$**

$$
\binom{1}{1} = \frac{1!}{1! (1-1)!} = \frac{1}{1 \cdot 1} = 1.
$$

Quindi, per $k = 1$:
$$
P(X = 1) = \binom{1}{1} p^1 (1-p)^{1-1} = 1 \cdot p \cdot 1 = p.
$$

In conclusione, la PMF per la distribuzione binomiale con $n = 1$ diventa:

$$
P(X = k) =
\begin{cases}
1-p, & \text{se } k = 0, \\
p, & \text{se } k = 1.
\end{cases}
$$

Questa è esattamente la PMF della distribuzione di Bernoulli con parametro $p$:

$$
P(X = x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}.
$$ 

Pertanto, la distribuzione binomiale con $n = 1$ è equivalente alla distribuzione di Bernoulli con parametro $p$.

### Applicazioni Pratiche della Distribuzione Binomiale

Consideriamo un esempio pratico per illustrare l'applicazione della distribuzione binomiale. Supponiamo di osservare 2 successi in 4 prove Bernoulliane, dove la probabilità di successo in ogni prova è $p = 0.2$. La probabilità di ottenere questo risultato specifico è calcolata utilizzando l'eq. {eq}`eq-binom-distr`:

$$
P(Y=2) = \frac{4!}{2!(4-2)!} \cdot 0.2^{2} \cdot (1-0.2)^{4-2} = 0.1536.
$$

Questo calcolo può essere replicato in Python. Utilizzando il modulo `math`, possiamo calcolare direttamente:

```{r}
# Parametri
n <- 4
p <- 0.2
y <- 2

# Probabilità di ottenere esattamente y successi
prob <- choose(n, y) * p^y * (1 - p)^(n - y)
print(prob)
```

In alternativa, possiamo sfruttare la libreria SciPy per eseguire calcoli analoghi. SciPy offre una vasta gamma di funzioni per la gestione delle distribuzioni statistiche, tra cui la distribuzione binomiale.

```{r}
# Probabilità di ottenere esattamente y successi
prob <- choose(n, y) * p^y * (1 - p)^(n - y)
print(prob)
```

Utilizzando `dbinom(y, n, p)`, possiamo trovare le probabilità per ogni possibile valore $y$ in una distribuzione binomiale di parametri $n = 4$ e $\theta = 0.2$:

```{r}
# Usando la funzione dbinom
prob <- dbinom(x = y, size = n, prob = p)
print(prob)
```

Visualizziamo la distribuzione di massa di probabilità:

```{r}
y <- 0:n  # Numero di successi
probabilities <- dbinom(y, size = n, prob = p)  # Probabilità associate

# Preparare i dati in un data frame
df <- data.frame(Successi = y, Probabilità = probabilities)

df |> 
  ggplot(aes(x = Successi, y = Probabilità)) +
    geom_segment(aes(xend = Successi, yend = 0), lwd = 1.2, color = "blue") +
    geom_point(size = 3, color = "blue") +
    labs(
      x = "Numero di Successi y",
      y = "Probabilità",
      title = paste("Distribuzione Binomiale: n =", n, ", p =", p)
  )
```

Un campione casuale si ottiene con `rbinom()`:

```{r}
set.seed(42)
samples <- rbinom(n = 30, size = 5, prob = 0.5)
print(samples)
```

Per esplorare ulteriormente, consideriamo la distribuzione di probabilità di diverse distribuzioni binomiali per due valori di $n$ e $\theta$. La seguente visualizzazione mostra come cambia la distribuzione al variare di $\theta$:

```{r}
# Parametri
n <- 20
p_values <- seq(0.3, 0.9, by = 0.3) # Valori di probabilità
y <- 0:25 # Numero di successi

# Creazione di un data frame per tutte le distribuzioni
df <- data.frame()

for (p in p_values) {
  binom_dist <- dbinom(y, size = n, prob = p)
  df <- rbind(df, data.frame(y = y, Prob = binom_dist, p = factor(p)))
}

# Grafico con ggplot2
df |> 
  ggplot(aes(x = y, y = Prob, color = p)) +
    geom_point() +
    geom_line() +
    labs(
      x = "Numero di successi y", 
      y = "Probabilità",
      title = "Distribuzione binomiale al variare di p",
      color = expression(theta)
  )
```

Consideriamo un altro esempio. Lanciando $5$ volte una moneta onesta, qual è la probabilità che esca testa almeno due volte? Troviamo la soluzione usando `stats.binom.pmf()`.

```{r}
# Calcolo della somma delle probabilità
result <- dbinom(2, size = 5, prob = 0.5) +
          dbinom(3, size = 5, prob = 0.5) +
          dbinom(4, size = 5, prob = 0.5) +
          dbinom(5, size = 5, prob = 0.5)

print(result)
```

Oppure, in modo più compatto:

```{r}
# Valori di interesse
result <- sum(dbinom(2:5, size = 5, prob = 0.5))
print(result)
```

Rappresentiamo graficamente la funzione di ripartizione per una Binomiale di ordine $n$ = 5 e $\theta$ = 0.5.

```{r}
# Parametri
n <- 5
p <- 0.5
y <- 0:n

# Calcolo della funzione di ripartizione cumulativa
cdf_values <- pbinom(y, size = n, prob = p)

# Creazione del data frame per ggplot2
df <- data.frame(y = y, cdf = cdf_values)

# Grafico
df |> 
  ggplot(aes(x = y, y = cdf)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = 1, color = "black", alpha = 0.7, linetype = "dashed") +
    labs(
      title = paste("Funzione di ripartizione binomiale: n =", n, ", p =", p),
      x = "y",
      y = "Probabilità"
    )
```

Un'altra funzione utile è quella che permette di trovare il numero di successi associato a una data probabilità cumulativa nella coda sinistra di una distribuzione binomiale. Questo si ottiene utilizzando la funzione `qbinom`, che rappresenta l'inversa della funzione di distribuzione cumulativa (CDF). 

Ad esempio, consideriamo una distribuzione binomiale con $n = 5$ prove e probabilità di successo $p = 0.5$. Supponiamo di voler calcolare il numero minimo di successi per cui la probabilità cumulativa è almeno $1 - 0.8125 = 0.1875$. Possiamo farlo nel seguente modo:

```{r}
# Probabilità target
target_probability <- 1 - 0.8125

# Numero di successi corrispondente alla probabilità target
result <- qbinom(target_probability, size = 5, prob = 0.5)

print(result)
```

In questo esempio, il valore restituito è $1$, che indica che almeno 1 successo soddisfa la condizione di una probabilità cumulativa di $0.1875$.

### Altro esempio

Consideriamo ora una distribuzione binomiale con $n = 10$ prove e probabilità di successo $p = 0.2$. Per calcolare la probabilità cumulativa $P(Y \leq 4)$, ovvero la probabilità di ottenere al massimo 4 successi su 10 tentativi, possiamo utilizzare la funzione `pbinom`:

```{r}
# Calcolo della probabilità cumulativa
target_probability <- pbinom(4, size = 10, prob = 0.2)
print(target_probability)
```

Il risultato rappresenta la probabilità cumulativa associata a 4 o meno successi.

Se invece vogliamo determinare il numero di successi corrispondente a questa probabilità cumulativa, possiamo utilizzare la funzione inversa `qbinom`:

```{r}
# Calcolo del numero di successi associato alla probabilità cumulativa
result <- qbinom(target_probability, size = 10, prob = 0.2)
print(result)
```

In questo caso, il valore restituito rappresenta il numero massimo di successi $Y$ per cui la probabilità cumulativa è uguale o inferiore a $target\_probability$. Questo è particolarmente utile per interpretare i risultati di una distribuzione binomiale in termini di successi associati a determinate probabilità cumulative.

### Valore atteso e deviazione standard

La media (numero atteso di successi in $n$ prove) e la deviazione standard di una distribuzione binomiale si calcolano nel seguente modo:

$$
\begin{align}
\mu    &= np,  \notag \\
\sigma &= \sqrt{np(1-p)}.
\end{align}
$$ {#eq-mean-var-binomial}

*Dimostrazione.* Dato che $Y$ rappresenta la somma di $n$ prove di Bernoulli indipendenti $Y_i$, possiamo scrivere:

$$
\begin{align}
\mathbb{E}(Y) &= \mathbb{E}\left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \mathbb{E}(Y_i) = np, \\
\mathbb{V}(Y) &= \mathbb{V} \left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \mathbb{V}(Y_i) = np(1-p).
\end{align}
$$

Pertanto, la deviazione standard è data da $\sigma = \sqrt{np(1-p)}$.

Per esempio, prendiamo in considerazione il caso di un esperimento in cui vengono lanciate quattro monete, ciascuna con una probabilità di ottenere testa (successo) pari a $p = 0.2$. Calcoliamo il valore atteso e la varianza per questo esperimento.

Il valore atteso, $\mu$, rappresenta il numero medio di teste che ci aspettiamo di ottenere in ciascun lancio. Per la distribuzione binomiale, questo è dato da $\mu = n p$, dove $n$ è il numero di prove (lanci di monete). Nel nostro caso, con $n = 4$ e $p = 0.2$, abbiamo:

$$
\mu = n p = 4 \times 0.2 = 0.8.
$$

Questo significa che, in media, ci aspettiamo di ottenere circa 0.8 teste per ogni serie di quattro lanci.

Per quanto riguarda la varianza, che misura quanto i risultati individuali tendono a differire dalla media, nella distribuzione binomiale è calcolata come $n p (1-p)$. Pertanto, per il nostro esperimento:

$$
\text{Varianza} = n p (1-p) = 4 \times 0.2 \times (1 - 0.2) = 0.64.
$$

La varianza di 0.64 suggerisce una certa dispersione intorno al valore medio di 0.8 teste.

Per confermare queste aspettative teoriche, possiamo eseguire una simulazione. Creiamo una serie di esperimenti simulati in cui lanciamo quattro monete per un gran numero di volte, registrando il numero di teste ottenute in ogni serie. Calcoliamo poi la media e la varianza dei risultati ottenuti per vedere quanto si avvicinano ai valori teorici calcolati.

```{r}
set.seed(42)

# Genera un campione di 1.000.000 di valori dalla distribuzione binomiale
x <- rbinom(n = 1000000, size = 4, prob = 0.2)
```

```{r}
mean(x)
```

```{r}
var(x)
```

### Funzioni R associate alle distribuzioni di probabilità

La seguente tabella riassume le funzioni di R utilizzate per manipolare le distribuzioni di probabilità, illustrando i casi della distribuzione Binomiale e della Normale.

| Tipo                              | Esempio: Binomiale ($y \mid n, p$)         | Esempio: Normale ($y \mid \mu, \sigma$)        |
|:----------------------------------|:----------------------------------------------|:--------------------------------------------------|
| Funzione di verosimiglianza       | `dbinom(y, size = n, prob = p)`               | `dnorm(y, mean = μ, sd = σ)`                      |
| Prob $Y = y$                  | `dbinom(y, size = n, prob = p)`               | Non definita (variabili continue hanno pdf, non pmf) |
| Prob $Y \geq y, Y \leq y, y_1 < Y < y_2$ | `pbinom(y, size = n, prob = p)` o `1 - pbinom(y - 1, ...)` | `pnorm(y, mean = μ, sd = σ)` o `1 - pnorm(y, ...)` |
| Inversa della CDF                 | `qbinom(q, size = n, prob = p)`               | `qnorm(q, mean = μ, sd = σ)`                      |
| Generazione di dati simulati      | `rbinom(n, size = trials, prob = p)`          | `rnorm(n, mean = μ, sd = σ)`                      |

In seguito, useremo altre distribuzioni come Uniforme, Beta, ecc., ognuna delle quali ha un proprio insieme di funzioni disponibili in R. La sintassi segue uno schema generale comune:

- **`d*`**: Calcola la funzione di densità di probabilità (per distribuzioni continue) o di massa (per distribuzioni discrete). Esempi: `dbinom`, `dnorm`.
- **`p*`**: Calcola la funzione di ripartizione cumulativa (CDF). Esempi: `pbinom`, `pnorm`.
- **`q*`**: Calcola l'inversa della funzione di ripartizione cumulativa (quantile function). Esempi: `qbinom`, `qnorm`.
- **`r*`**: Genera campioni casuali secondo una determinata distribuzione. Esempi: `rbinom`, `rnorm`.


::: {#exm-}

1. Calcolare la probabilità di esattamente $y = 3$ successi su $n = 5$ prove con $p = 0.5$:

```{r}
dbinom(3, size = 5, prob = 0.5)
```

2. Calcolare la probabilità cumulativa $P(Y \leq 3)$:
   
```{r}
pbinom(3, size = 5, prob = 0.5)
```

3. Calcolare il valore minimo $y$ tale che $P(Y \leq y) \geq 0.9$:

```{r}
qbinom(0.9, size = 5, prob = 0.5)
```

4. Generare un campione di 100 numeri casuali da una distribuzione binomiale:

```{r}
rbinom(100, size = 5, prob = 0.5)
```
:::

## Distribuzione Discreta Uniforme

La distribuzione discreta uniforme è un tipo particolare di distribuzione di probabilità, dove ogni risultato in un insieme finito e discreto $S$ ha la stessa probabilità $p$ di verificarsi. Questa distribuzione è caratterizzata dalla sua semplicità e dalla sua proprietà fondamentale di equiprobabilità.

Consideriamo un esempio pratico con una variabile casuale discreta $X$, che può assumere valori nell'insieme $\{1, 2, \dots, N\}$. Un'istanza classica di questa distribuzione si verifica quando si sceglie casualmente un numero intero tra 1 e $N$, inclusi. Se $X$ rappresenta il numero selezionato, allora la somma delle probabilità di tutti i possibili valori di $X$ deve totalizzare 1, come indicato dalla formula di normalizzazione:

$$
\sum_{i=1}^N P(X_i) = Np = 1.
$$

Di conseguenza, la probabilità che $X$ assuma un valore specifico $x$ è uniformemente distribuita:

$$
P(X = x) = \frac{1}{N},
$$

indicando che ogni evento ha la stessa probabilità di verificarsi.

Il valore atteso, o la media, di $X$ ci dà un'idea del risultato medio atteso e si calcola come:

$$
\mathbb{E}(X) = \sum_{x=1}^N x \cdot \frac{1}{N} = \frac{1}{N} \cdot \sum_{x=1}^N x.
$$

A questo punto, dobbiamo calcolare la somma $\sum_{x=1}^{N} x$, che è la somma dei primi $N$ numeri naturali. Questa somma è data dalla formula:

$$
\sum_{x=1}^{N} x = \frac{N(N + 1)}{2}.
$$

Sostituendo questa formula nel nostro calcolo del valore atteso, otteniamo:

$$
\mathbb{E}(X) = \frac{1}{N} \cdot \frac{N(N + 1)}{2} = \frac{N + 1}{2}.
$$

Quindi, abbiamo dimostrato che il valore atteso $ \mathbb{E}(X) $ per una variabile casuale $X$ che assume valori interi uniformemente distribuiti da 1 a $N$ è $\frac{N + 1}{2}$. 

Per determinare quanto i valori di $X$ si disperdono attorno al valore medio, calcoliamo la varianza. Il primo passo è calcolare $\mathbb{E}(X^2)$, il valore atteso del quadrato di $X$. Per una variabile casuale discreta uniforme, questo si ottiene moltiplicando ogni valore al quadrato per la sua probabilità (che è $1/N$ per tutti i valori) e sommando i risultati:

$$
\mathbb{E}(X^2) = \frac{1}{N} \cdot \sum_{x=1}^N x^2
$$

Usando l'identità per la somma dei quadrati dei primi $N$ numeri naturali:

$$
1^2 + 2^2 + \dots + N^2 = \frac{N(N + 1)(2N + 1)}{6}
$$

possiamo sostituirla per trovare $\mathbb{E}(X^2)$:

$$
\mathbb{E}(X^2) = \frac{1}{N} \cdot \frac{N(N + 1)(2N + 1)}{6} = \frac{(N + 1)(2N + 1)}{6}
$$

La varianza di $X$, denotata con $\mathbb{V}(X)$, si calcola usando la formula:

$$
\mathbb{V}(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2
$$

Abbiamo già stabilito che $\mathbb{E}(X) = \frac{N + 1}{2}$ e $\mathbb{E}(X^2) = \frac{(N + 1)(2N + 1)}{6}$. Sostituendo questi valori nella formula della varianza, otteniamo:

$$
\mathbb{V}(X) = \frac{(N + 1)(2N + 1)}{6} - \left(\frac{N + 1}{2}\right)^2
$$

Per semplicare l'espressione della varianza, dobbiamo sottrarre il quadrato di $\mathbb{E}(X)$ da $\mathbb{E}(X^2)$:

$$
\begin{align*}
\mathbb{V}(X) &= \frac{(N + 1)(2N + 1)}{6} - \left(\frac{N + 1}{2}\right)^2 \\
&= \frac{(N + 1)(2N + 1)}{6} - \frac{(N + 1)^2}{4} \\
&= \frac{2(N + 1)(2N + 1)}{12} - \frac{3(N + 1)^2}{12} \\
&= \frac{(N + 1)(2(2N + 1) - 3(N + 1))}{12} \\
&= \frac{(N + 1)(4N + 2 - 3N - 3)}{12} \\
&= \frac{(N + 1)(N - 1)}{12}
\end{align*}
$$

Quindi, la varianza $\mathbb{V}(X)$ di una variabile casuale uniforme discreta $X$ che assume valori da 1 a $N$ è $\frac{(N + 1)(N - 1)}{12}$, il che mostra come la dispersione dei valori attorno al loro valore medio dipenda dalla grandezza di $N$. Questa formula fornisce la varianza di una variabile casuale in una distribuzione discreta uniforme, offrendo una misura quantitativa della dispersione dei valori attorno al loro valore medio. 


## Distribuzione di Poisson 

La **distribuzione di Poisson** è utilizzata per modellare il numero di eventi che si verificano in un determinato intervallo di tempo o spazio, con eventi indipendenti e un tasso costante di occorrenza.

La funzione di massa di probabilità (PMF) è data da:

$$
P(Y = y \mid \lambda) = \frac{\lambda^y \cdot e^{-\lambda}}{y!}, \quad y = 0, 1, 2, \ldots
$$

dove $\lambda$ rappresenta il tasso medio di eventi e $y$ è il numero di eventi.

La distribuzione di Poisson può essere derivata come il limite di una distribuzione binomiale quando il numero di prove, $n$, tende all'infinito e la probabilità di successo in ciascuna prova, $p$, tende a zero, in modo tale che $np = \lambda$. 

::: callout-note
## Dimostrazione

Partiamo dalla funzione di probabilità binomiale:

$$
p(k) = \frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}.
$$

Impostiamo $np = \lambda$, il che implica che $p = \frac{\lambda}{n}$. Sostituendo $p$ con $\frac{\lambda}{n}$ nella formula binomiale, otteniamo:

$$
p(k) = \frac{n!}{k!(n - k)!} \left(\frac{\lambda}{n}\right)^k \left(1 - \frac{\lambda}{n}\right)^{n - k}.
$$

Ora, separiamo i termini per rendere più chiara la semplificazione. Possiamo riscrivere $\left(\frac{\lambda}{n}\right)^k$ come $\frac{\lambda^k}{n^k}$, e $\left(1 - \frac{\lambda}{n}\right)^{n - k}$ come $\left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}$. Quindi, l'espressione diventa:

$$
p(k) = \frac{n!}{k!(n - k)!} \cdot \frac{\lambda^k}{n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}.
$$

Ora, separiamo ulteriormente i termini:

$$
p(k) = \frac{\lambda^k}{k!} \cdot \frac{n!}{(n - k)! n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-k}.
$$

Questo passaggio mostra come la funzione di probabilità binomiale, sotto le condizioni $np = \lambda$ e $n \to \infty$, si trasformi gradualmente nella forma che conduce alla distribuzione di Poisson.

Quando $n \to \infty$:

$$
\frac{\lambda}{n} \to 0
$$

$$
\frac{n!}{(n - k)! n^k} \to 1
$$

$$
\left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda}
$$

$$
\left(1 - \frac{\lambda}{n}\right)^{-k} \to 1
$$

Si ottiene quindi:

$$
p(k) \to \frac{\lambda^k e^{-\lambda}}{k!}
$$

che è la funzione di Poisson.
:::

::: callout-note
## Dimostrazione

Analizziamo il limite:

$$
\left(1 - \frac{\lambda}{n}\right)^n \to e^{-\lambda} \quad \text{quando} \quad n \to \infty.
$$

Un limite fondamentale in analisi matematica è:

$$
\lim_{n \to \infty} \left(1 + \frac{a}{n}\right)^n = e^a,
$$

dove $e$ è la base del logaritmo naturale ($e \approx 2.71828$) e $a$ è una costante. Questo limite è alla base della definizione della funzione esponenziale.

Nel nostro caso, abbiamo l'espressione:

$$
\left(1 - \frac{\lambda}{n}\right)^n .
$$

Notiamo che questa è molto simile al limite notevole, ma con un segno negativo. Possiamo riscriverla come:

$$
\left(1 - \frac{\lambda}{n}\right)^n = \left(1 + \frac{-\lambda}{n}\right)^n.
$$

Applicando il limite notevole con $a = -\lambda$, otteniamo:

$$
\lim_{n \to \infty} \left(1 + \frac{-\lambda}{n}\right)^n = e^{-\lambda}.
$$

Quindi, quando $n$ diventa molto grande, l'espressione $\left(1 - \frac{\lambda}{n}\right)^n$ si avvicina sempre di più a $e^{-\lambda}$. 
:::

### Proprietà principali

- **Media**: $\mathbb{E}[Y] = \lambda$
- **Varianza**: $\text{Var}(Y) = \lambda$

Di seguito, presentiamo esempi di calcolo e simulazione con R.


### Grafico della distribuzione di Poisson con $\lambda = 2$

```{r}
# Parametro lambda
lambda <- 2

# Valori di y (numero di eventi)
y <- 0:10

# Calcolo delle probabilità
probabilities <- dpois(y, lambda = lambda)

# Creazione di un dataframe per ggplot
data <- data.frame(
  Numero_eventi = y,
  Probabilita = probabilities
)

# Grafico della funzione di massa di probabilità con ggplot
ggplot(data, aes(x = Numero_eventi, y = Probabilita)) +
  geom_col(fill = "blue") +  # Usa colonne verticali per rappresentare le probabilità
  labs(
    title = "Distribuzione di Massa di Probabilità di Poisson",
    x = "Numero di eventi (k)",
    y = "Probabilità"
  ) 
```

### Calcolo della probabilità per un numero specifico di eventi

Per calcolare la probabilità di osservare esattamente 3 eventi con $\lambda = 2$:

```{r}
prob <- dpois(3, lambda = 2)
print(prob)
```

### Calcolo della probabilità cumulativa $P(Y \leq 3)$

Per calcolare $P(Y \leq 3)$, la probabilità cumulativa:

```{r}
cum_prob <- ppois(3, lambda = 2)
print(cum_prob)
```

### Trovare il quantile corrispondente a una probabilità data

Per trovare il numero massimo di eventi per cui la probabilità cumulativa è al massimo $0.8125$:

```{r}
quantile <- qpois(0.8125, lambda = 2)
print(quantile)
```

### Generazione di numeri casuali

Per generare un campione di 1.000.000 di osservazioni da una distribuzione di Poisson con $\lambda = 2$:

```{r}
set.seed(42)
sample <- rpois(1000000, lambda = 2)

# Calcolo di media e varianza del campione
mean_sample <- mean(sample)
var_sample <- var(sample)

print(mean_sample)
print(var_sample)
```

::: {#exm-}
Consideriamo un ospedale con una media storica di 4.5 nascite al giorno. Qual è la probabilità che nascano esattamente 6 bambini in un giorno?

```{r}
# Calcolo della probabilità
lambda <- 4.5
prob <- dpois(6, lambda = lambda)
print(prob)
```

Simuliamo 365 giorni di nascite e confrontiamo la proporzione di giorni con esattamente 6 nascite:

```{r}
set.seed(42)
n_days <- 365
simulated_births <- rpois(n_days, lambda = lambda)

# Proporzione di giorni con esattamente 6 nascite
proportion_six_births <- mean(simulated_births == 6)
print(proportion_six_births)
```

Istogramma delle nascite simulate: 

```{r}
# Creare un dataframe con i dati simulati
data <- data.frame(Nascite = simulated_births)

# Creare l'istogramma con ggplot
ggplot(data, aes(x = Nascite)) +
  geom_histogram(
    breaks = seq(-0.5, max(simulated_births) + 0.5, by = 1), 
    fill = "blue", 
    color = "black"
  ) +
  labs(
    title = "365 nascite simulate (Poisson)",
    x = "Numero di nascite per giorno",
    y = "Frequenza"
  )
```

Probabilità di più di 6 nascite in un giorno. Per calcolare la probabilità teorica $P(Y > 6)$:

```{r}
prob_more_than_six <- 1 - ppois(6, lambda = lambda)
print(prob_more_than_six)
```

Proporzione simulata di più di 6 nascite:

```{r}
proportion_more_than_six <- mean(simulated_births > 6)
print(proportion_more_than_six)
```
:::

::: {#exm-}
Questo esempio classico è tratto da von Bortkiewicz (1898). Furono registrate le morti causate da calci di cavalli all'interno di 10 squadroni della cavalleria prussiana, osservando i dati per un periodo di 20 anni, il che corrisponde a un totale di 200 "squadroni-anni" di dati.

I dati e le probabilità ottenute da un modello di Poisson con parametro $\lambda = 0.61$ sono mostrati nella tabella seguente. La prima colonna della tabella indica il numero di decessi annui, che varia da 0 a 4. La seconda colonna elenca quante volte è stato osservato quel particolare numero di decessi. Per esempio, in 65 dei 200 squadroni-anni ci fu un solo decesso. Nella terza colonna, i numeri osservati vengono convertiti in frequenze relative dividendo ciascun valore per 200. La quarta colonna mostra le probabilità di Poisson calcolate con il parametro $\lambda = 0.61$. Il valore $\lambda = 0.61$ è stato scelto in modo da corrispondere al numero medio di decessi annui.

| Numero di decessi annui | Frequenza osservata | Frequenza relativa | Probabilità di Poisson |
|-------------------------|---------------------|--------------------|------------------------|
| 0                       | 109                 | 0.545              | 0.543                  |
| 1                       | 65                  | 0.325              | 0.331                  |
| 2                       | 22                  | 0.110              | 0.101                  |
| 3                       | 3                   | 0.015              | 0.021                  |
| 4                       | 1                   | 0.005              | 0.003                  |

**Note:**

- **Frequenza osservata**: Indica quante volte un determinato numero di decessi è stato registrato nei 200 squadroni-anni.
- **Frequenza relativa**: È ottenuta dividendo la frequenza osservata per il totale dei dati (200).
- **Probabilità di Poisson**: Valori teorici calcolati usando il modello di Poisson con $\lambda = 0.61$, che rappresenta il tasso medio di decessi annui.

Questo esempio dimostra come il modello di Poisson possa essere utilizzato per descrivere fenomeni rari ma prevedibili, come le morti accidentali in questo caso.
:::

## Distribuzione Beta-Binomiale

La distribuzione beta-binomiale rappresenta una estensione della distribuzione binomiale che tiene conto della variabilità nella probabilità di successo tra i vari tentativi. Viene descritta da tre parametri principali: $N$, $\alpha$ e $\beta$.

Nel dettaglio, la funzione di massa di probabilità per la distribuzione beta-binomiale è data da:

$$
\text{BetaBinomiale}(y | N, \alpha, \beta) = \binom{N}{y} \cdot \frac{B(y + \alpha, N - y + \beta)}{B(\alpha, \beta)},
$$ {#eq-beta-binom-formula}

dove:

- $y$ indica il numero di successi osservati.
- $N$ rappresenta il numero totale di tentativi.
- $\alpha$ e $\beta$ sono i parametri della distribuzione beta, che modellano la variabilità nella probabilità di successo tra i tentativi.

La funzione $B(u, v)$, nota come funzione beta, è definita tramite l'uso della funzione gamma $\Gamma$, secondo la formula:

$$
B(u, v) = \frac{\Gamma(u) \Gamma(v)}{\Gamma(u + v)},
$$

dove la funzione gamma $\Gamma$ generalizza il concetto di fattoriale a numeri reali e complessi.

L'importanza della distribuzione beta-binomiale deriva dalla sua capacità di modellare situazioni in cui la probabilità di successo non è fissa, ma segue una distribuzione di probabilità, specificatamente una distribuzione beta. Ciò la rende particolarmente adatta per applicazioni in cui le probabilità di successo cambiano in maniera incerta da un tentativo all'altro, come può avvenire in contesti di ricerca clinica o in studi comportamentali. Rispetto alla distribuzione binomiale, che assume una probabilità di successo costante per tutti i tentativi, la beta-binomiale offre una rappresentazione più realistica e flessibile per dati empirici che presentano variabilità nelle probabilità di successo.

## Riflessioni Conclusive

In questo capitolo, abbiamo approfondito alcune delle distribuzioni discrete più importanti, ognuna con caratteristiche uniche e campi di applicazione specifici. Abbiamo iniziato con la **distribuzione di Bernoulli**, che modella esperimenti con due soli esiti possibili, per poi passare alla **distribuzione Binomiale**, che generalizza la Bernoulli considerando un numero fisso di prove indipendenti. Successivamente, abbiamo esaminato la **distribuzione di Poisson**, utile per descrivere eventi rari in un intervallo di tempo o spazio, e la **distribuzione Beta-Binomiale**, un'estensione della Binomiale che incorpora la variabilità nella probabilità di successo, rendendola particolarmente adatta per modellare situazioni in cui tale probabilità non è fissa. Infine, abbiamo discusso la **distribuzione Discreta Uniforme**, che assegna la stessa probabilità a ciascun evento in un insieme finito e discreto.

Queste distribuzioni rappresentano il fondamento dell'analisi statistica discreta e trovano applicazione in numerosi ambiti. In particolare, nel contesto dell'**inferenza bayesiana**, la comprensione della distribuzione Binomiale e della sua estensione Beta-Binomiale è essenziale. Queste distribuzioni, infatti, forniscono gli strumenti necessari per l'**aggiornamento bayesiano**, un processo chiave che permette di rivedere le nostre credenze iniziali alla luce di nuovi dati. Questo concetto sarà ulteriormente esplorato nei capitoli successivi, dove approfondiremo come le distribuzioni a priori e a posteriori interagiscono nel quadro bayesiano.

## Esercizi

::: {#exr-discr-rv-1}

Per ciascuna delle distribuzioni di massa di probabilità discusse, utilizzare R per:

- creare un grafico della funzione, scegliendo opportunamente i parametri;
- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;
- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;
- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;
- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;
- scegliendo un valore della distribuzione pari alla media più una deviazione standard, calcolare la probabilità che la variabile aleatoria assuma un valore minore o uguale a questo valore.

:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

