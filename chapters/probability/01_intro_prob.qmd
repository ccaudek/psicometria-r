---
execute:
  freeze: auto
---

# Interpretazione della probabilità {#sec-prob-interpretation}

::: callout-important
## In questo capitolo imparerai a

- a comprendere le diverse interpretazioni della probabilità.
:::

::: callout-tip
## Prerequisiti

- Leggere *Why probability probably doesn't exist (but it is useful to act like it does* [@spiegelhalter2024probability].
- Leggere il capitolo *Introduction to Probability* [@Probability and statistics].
- Leggere @sec-apx-sets.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione 

Nel corso di questo capitolo, esploreremo varie concezioni della probabilità, tra cui la visione classica, frequentista e bayesiana. Inoltre, introdurremo la simulazione con R per una migliore comprensione della legge dei grandi numeri, un concetto fondamentale nell'ambito della probabilità. Iniziamo introducendo il concetto di causalità.

## Il Concetto di Casualità e la Teoria della Probabilità  

David Spiegelhalter, in un recente articolo pubblicato su *Nature*, introduce il concetto di probabilità partendo dall’idea di incertezza:  

> Life is uncertain. None of us know what is going to happen. We know little of what has happened in the past, or is happening now outside our immediate experience. Uncertainty has been called the ‘conscious awareness of ignorance’ — be it of the weather tomorrow, the next Premier League champions, the climate in 2100 or the identity of our ancient ancestors [@spiegelhalter2024probability]. 

L’incertezza può essere interpretata come una manifestazione della casualità, che non rappresenta solo un fenomeno, ma anche un **modello concettuale** utile per affrontare l’imprevedibilità della realtà. Attraverso il concetto di casualità, è possibile gestire e quantificare eventi che, pur essendo imprevedibili singolarmente, seguono schemi regolari e riconoscibili. Questo rende la casualità uno strumento cruciale per comprendere il mondo e prendere decisioni.  

> Attempts to put numbers on chance and uncertainty take us into the mathematical realm of probability, which today is used confidently in any number of fields [@spiegelhalter2024probability].

La teoria della probabilità offre una risposta matematica all’incertezza, permettendo di esprimere in termini numerici il grado di possibilità associato a un evento. Grazie a questa formalizzazione, possiamo modellare fenomeni complessi e applicare il concetto di probabilità in ambiti che spaziano dalla ricerca scientifica alla vita quotidiana.  

### L’Urna come Modello della Casualità  

Uno degli esempi più classici per rappresentare la casualità è il modello dell’urna. Immaginiamo un’urna contenente palline identiche, ciascuna numerata in modo univoco. Se ogni pallina ha la stessa probabilità di essere estratta, l’estrazione è considerata casuale. Sebbene non possiamo prevedere quale pallina verrà estratta, sappiamo che tutte hanno uguali possibilità di essere selezionate.  

Questo modello, nella sua semplicità, incarna l’essenza della casualità. La sua applicazione non si limita all’estrazione di palline, ma si estende a scenari più complessi. Ad esempio, il modello dell’urna rappresenta un concetto di base utilizzato per comprendere processi che si ritrovano in statistica, scienze naturali e psicologia.  

#### Applicazioni del Modello della Casualità  

- **Indagini statistiche**: Il campionamento casuale è essenziale per ottenere campioni rappresentativi di una popolazione più ampia, riducendo il rischio di bias di selezione e aumentando la generalizzabilità delle conclusioni.  

- **Sperimentazione scientifica**: La randomizzazione è uno strumento fondamentale per controllare le variabili confondenti, consentendo di attribuire le differenze osservate tra gruppi al trattamento, minimizzando l’influenza di fattori esterni.  

> Open any science journal, for example, and you’ll find papers liberally sprinkled with P values, confidence intervals and possibly Bayesian posterior distributions, all of which are dependent on probability [@spiegelhalter2024probability].

- **Simulazioni**: In fisica, psicologia e altre discipline, le simulazioni basate sulla casualità permettono di analizzare sistemi complessi e formulare previsioni, offrendo strumenti indispensabili per la ricerca scientifica.  

Il modello dell’urna, sebbene semplice, costituisce un pilastro per comprendere e applicare il concetto di casualità in una vasta gamma di contesti.  

### Dalla Casualità alla Teoria della Probabilità  

La teoria della probabilità si sviluppa a partire dal concetto di casualità, fornendo strumenti matematici per analizzare e quantificare rigorosamente l’incertezza. Questo approccio consente di tradurre intuizioni intuitive in un linguaggio formale e preciso, attraverso cui è possibile:  

1. **Quantificare l’incertezza**: Assegnare valori numerici agli esiti possibili permette di esprimere in modo preciso la probabilità di ciascun evento.  

2. **Combinare informazioni**: Utilizzando regole come la somma e il prodotto delle probabilità, possiamo calcolare la probabilità di eventi complessi partendo da eventi più semplici.  

3. **Aggiornare le credenze**: La teoria della probabilità, in particolare nella sua formulazione bayesiana, offre metodi per aggiornare le stime di probabilità alla luce di nuove informazioni.  

4. **Prendere decisioni informate**: Valutando rischi e benefici attesi, la probabilità guida scelte razionali in condizioni di incertezza.  

In sintesi, casualità e probabilità rappresentano strumenti essenziali per affrontare fenomeni non deterministici caratterizzati dall’imprevedibilità.  

### L'Incertezza nei Fenomeni Non Deterministici  

L'incertezza, caratteristica fondamentale dei fenomeni non deterministici, può essere ricondotta a due principali fonti:  

1. **Incertezza epistemica**  
   Questa forma di incertezza deriva dalla nostra conoscenza limitata o incompleta. Ad esempio, in un esperimento scientifico complesso, l’impossibilità di controllare o osservare tutte le variabili coinvolte può introdurre incertezza nei risultati. In questi casi, l’incertezza non è una proprietà intrinseca del fenomeno studiato, ma riflette i limiti della nostra capacità di misurare, modellare o comprendere pienamente la realtà.  

2. **Incertezza ontologica**  
   Questa forma di incertezza è intrinseca al fenomeno stesso, indipendentemente dal livello di conoscenza o controllo che possiamo esercitare. Ad esempio, in fisica quantistica, il principio di indeterminazione di Heisenberg ci ricorda che l'indeterminazione è una caratteristica fondamentale della realtà fisica. Un esempio intuitivo è il lancio di un dado: anche se conoscessimo tutte le condizioni iniziali (forza applicata, angolo di lancio, ecc.), non potremmo mai prevedere con assoluta certezza il risultato finale, poiché il sistema stesso è intrinsecamente casuale.  

Il fisico danese Niels Bohr ha espresso una visione illuminante su questo tema: secondo Bohr, l'obiettivo della fisica non è tanto quello di rivelare una verità assoluta sulla natura, quanto piuttosto di comprendere ciò che possiamo dire su di essa. Questa prospettiva mette in luce il fatto che l'incertezza, sia epistemica che ontologica, rappresenta i limiti del nostro linguaggio, dei nostri strumenti di misura e delle nostre conoscenze.  

### Probabilità e Interpretazione Soggettiva  

Questo approccio si allinea bene con l'interpretazione soggettiva della probabilità. Secondo questa visione, la probabilità non è una proprietà oggettiva degli eventi, ma rappresenta il grado di fiducia o convinzione di un individuo riguardo al verificarsi di un evento, sulla base delle informazioni di cui dispone. Ad esempio, stimare che ci sia il 70% di probabilità che piova domani riflette non una certezza oggettiva, ma il livello di confidenza che attribuiamo a tale previsione, considerati i dati disponibili.  

## La Storia della Probabilità  

### La Dualità della Probabilità  

Come sottolinea @hacking2006emergence, il concetto di probabilità si sviluppa attorno a due principali dimensioni:  

1. **Epistemologico**: La probabilità come misura della credibilità o plausibilità di una proposizione, basata sulle evidenze disponibili. In questa accezione, la probabilità è uno strumento per quantificare la nostra conoscenza e per ragionare in condizioni di incertezza.  

2. **Frequenziale**: La probabilità come tendenza osservabile nei fenomeni aleatori, in cui frequenze relative stabili emergono nel lungo termine. Questo approccio descrive la probabilità come un fenomeno naturale oggettivo, osservabile attraverso esperimenti ripetuti.  

Questa dualità — probabilità come misura della conoscenza e probabilità come proprietà del mondo naturale — è stata formalizzata solo a partire dal XVII secolo, grazie al lavoro pionieristico di Blaise Pascal e Pierre de Fermat.  

### Le Origini della Probabilità Moderna  

Il gioco d’azzardo, una pratica antica e universale, ha rappresentato per secoli uno dei pochi ambiti in cui venivano esplorati concetti rudimentali di probabilità. Resti di dadi e altri strumenti di gioco sono stati rinvenuti in siti sumeri, assiri ed egizi, a testimonianza di una lunga tradizione di interazione con l’alea. Tuttavia, fu solo nel 1654 che il matematico Antoine Gombaud, conosciuto come Chevalier de Méré, sollevò una questione che portò allo sviluppo della probabilità moderna.  

L’aneddoto più celebre riguarda la sua domanda a Blaise Pascal:  

> Due giocatori, A e B, stanno partecipando a un gioco in cui il primo a vincere sei round consecutivi ottiene il premio. Dopo sei round, A ha vinto cinque round e B uno. Poiché il gioco viene interrotto, come si dovrebbe dividere il premio in modo equo?  

Questo problema spinse Pascal e Fermat a sviluppare i primi strumenti matematici per calcolare la probabilità degli eventi futuri, dando vita a un metodo rigoroso per affrontare l’incertezza. Stimando, ad esempio, che A avesse il 97% di probabilità di vincere e B il 3%, sembrava equo dividere il premio nella stessa proporzione.  

La soluzione di questo problema ispirò Christian Huygens, che nel 1657 pubblicò il trattato *De Ratiociniis in Ludo Aleae*. Quest’opera, fondamentale per lo studio della probabilità, rimase il riferimento principale per almeno cinquant’anni.  

### Contributi Successivi  

Parallelamente, altri studiosi iniziarono a sviluppare idee legate ai due aspetti della probabilità:  

- **Christian Huygens**: concentrò il suo lavoro su problemi legati al gioco d’azzardo e alla casualità.  
- **Leibniz**: esplorò la probabilità come misura epistemologica, applicandola alla logica e al diritto.  
- **John Graunt (1662)**: studiò le frequenze stabili nella demografia, introducendo concetti proto-statistici legati alla probabilità frequenziale.  
- **Antoine Arnauld e il gruppo di Port-Royal**: approfondirono la probabilità come strumento per valutare la credibilità e il ragionamento.  

Nel 1713, Jacob Bernoulli pubblicò postumo *L’Arte della Congettura*, introducendo la legge dei grandi numeri. Quest’opera formalizzò il legame tra la probabilità epistemologica e quella frequenziale, ampliandone le applicazioni a contesti quali la mortalità, la giustizia penale e le decisioni economiche.  

### La Dualità nella Filosofia della Probabilità  

La dualità intrinseca del concetto di probabilità non è solo una questione terminologica o tecnica, ma riflette due prospettive fondamentali sul modo in cui interpretiamo e modelliamo l'incertezza.  

Da un lato, la **probabilità epistemologica** si focalizza su ciò che sappiamo o possiamo sapere, descrivendo il grado di fiducia o credibilità di una proposizione data l’evidenza disponibile. Dall’altro, la **probabilità frequenziale** cerca di catturare l’aleatorietà intrinseca del mondo naturale, identificando schemi ricorrenti e frequenze stabili nei fenomeni osservati.  

Questa dualità emerge chiaramente anche nel dibattito filosofico che ha coinvolto figure come Poisson, Cournot, Carnap e altri. Ad esempio, distinguendo tra "chance" e "probabilità", Poisson e Cournot hanno sottolineato come la probabilità possa essere usata sia per descrivere eventi aleatori oggettivi sia per quantificare il grado di conoscenza soggettiva su tali eventi.  

Nella seconda metà del XX secolo, i filosofi della scienza come Carnap hanno approfondito ulteriormente questa distinzione, proponendo un modello biforcato di probabilità:  

1. **Probabilità induttiva**: un modo per ragionare su ipotesi ed evidenze, essenzialmente epistemologico.  
2. **Probabilità statistica**: uno strumento per descrivere i fenomeni aleatori, principalmente frequenziale.  

La tensione tra questi due approcci ha portato allo sviluppo di teorie e metodi distinti, spesso con implicazioni pratiche e metodologiche profondamente diverse.  

### La Dualità Riflessa nei Metodi Bayesiani e Frequentisti  

La distinzione tra probabilità epistemologica e frequenziale si riflette chiaramente nei due principali paradigmi della statistica moderna: l’approccio **bayesiano** e quello **frequentista**.  

#### Approccio Bayesiano  

L'approccio bayesiano, fondato su principi epistemologici, considera la probabilità come una misura soggettiva del grado di fiducia. Attraverso il **teorema di Bayes**, le stime di probabilità vengono aggiornate iterativamente, integrando nuove evidenze con le conoscenze a priori. Questo approccio consente di modellare l'incertezza in modo dinamico e adattabile, rendendolo particolarmente utile in contesti dove i dati disponibili sono limitati o dove l’incertezza è intrinsecamente complessa.  

Ad esempio, nel contesto di una diagnosi medica, un medico può utilizzare informazioni preliminari (probabilità a priori) sul paziente e aggiornare questa stima alla luce dei risultati di test clinici (evidenza).  

#### Approccio Frequentista  

Al contrario, l'approccio frequentista si concentra sull'aspetto oggettivo della probabilità, definendola come la frequenza relativa di un evento in una serie infinita (o molto ampia) di prove ripetute. In questo contesto, la probabilità non dipende da credenze o conoscenze soggettive, ma è una proprietà osservabile del fenomeno studiato.  

Questo approccio si applica spesso in scenari sperimentali, dove si assume che i dati siano il risultato di un processo ripetibile. Ad esempio, un frequentista può stimare la probabilità di successo di un farmaco basandosi esclusivamente sui risultati osservati in una sperimentazione clinica controllata.  

### Differenze, Complementarità e Critiche  

Gli approcci bayesiano e frequentista presentano differenze fondamentali, ma non devono essere necessariamente considerati in conflitto. In determinati contesti applicativi, possono persino essere visti come complementari:  

- **Approccio bayesiano**: si distingue per la sua capacità di affrontare situazioni caratterizzate da elevata incertezza o dati limitati. Grazie all'integrazione esplicita di conoscenze pregresse mediante distribuzioni a priori, il paradigma bayesiano è particolarmente efficace nell'analisi di problemi complessi che richiedono un continuo aggiornamento delle stime alla luce di nuove evidenze.  
- **Approccio frequentista**: offre strumenti consolidati e più semplici da applicare, risultando particolarmente adatto in situazioni in cui sono disponibili grandi quantità di dati indipendenti e si richiedono procedure standardizzate per l'inferenza statistica.  

Tuttavia, entrambi gli approcci sono stati oggetto di critiche:  

- **Critiche all'approccio bayesiano**: la scelta delle distribuzioni a priori viene talvolta considerata un elemento soggettivo che potrebbe influenzare i risultati dell'analisi. Tuttavia, questa critica perde rilevanza nell'approccio bayesiano moderno, che utilizza prior debolmente informativi per minimizzare l'impatto delle scelte iniziali e garantire inferenze più robuste. Sebbene la complessità computazionale del metodo bayesiano sia stata notevolmente ridotta dai progressi tecnologici, essa rappresenta ancora un ostacolo in alcuni contesti applicativi, specialmente quando si utilizzano modelli particolarmente sofisticati.  
- **Critiche all'approccio frequentista**: l'inferenza frequentista è stata criticata per la sua dipendenza dal test dell'ipotesi nulla (NHST), un metodo che è stato spesso interpretato in modo meccanico e fuori contesto. Il NHST è stato inoltre identificato come una delle principali cause della crisi di replicabilità che ha colpito la ricerca scientifica, in particolare in psicologia. Un altro limite dell'approccio frequentista è la difficoltà di gestire modelli complessi o situazioni in cui le ipotesi fondamentali, come l'indipendenza delle osservazioni o la normalità delle distribuzioni, non sono realistiche.

Negli ultimi anni, l'approccio bayesiano ha guadagnato una crescente popolarità, soprattutto per la sua capacità di affrontare modelli complessi e di fornire inferenze più flessibili rispetto alle procedure frequentiste. La forza del paradigma bayesiano risiede nella sua natura adattabile, che consente di incorporare conoscenze pregresse e di aggiornare continuamente le stime in base a nuove informazioni. Questo lo rende particolarmente utile in ambiti come l'intelligenza artificiale, la data science e la psicologia, dove l'analisi richiede spesso un'interpretazione dinamica e multidimensionale dell'incertezza. 

In conclusione, sebbene l'approccio frequentista abbia costituito per decenni il pilastro dell'analisi statistica tradizionale, il paradigma bayesiano si è affermato come una soluzione più avanzata e flessibile, capace di rispondere alle esigenze della ricerca contemporanea. Questa evoluzione rappresenta non solo un progresso metodologico, ma anche un cambiamento culturale e scientifico, che richiede strumenti più solidi e versatili per affrontare la complessità intrinseca dei fenomeni oggetto di studio.

### Implicazioni Filosofiche e Pratiche  

La dualità della probabilità, tra epistemologia e frequenza, riflette una tensione fondamentale nel pensiero scientifico e filosofico: il tentativo di bilanciare una descrizione oggettiva della realtà con la soggettività inevitabile del processo interpretativo. Questa tensione non è solo teorica, ma influenza profondamente il modo in cui i metodi statistici vengono applicati nei diversi ambiti del sapere. 

Con questo in mente, esploriamo ora come il concetto di probabilità si sia evoluto storicamente e come continui a influenzare il progresso scientifico.

## L'Evoluzione del Concetto di Probabilità

La storia della probabilità offre un contesto ricco per comprendere le diverse interpretazioni e applicazioni del concetto di probabilità. Le definizioni classiche e frequentiste, sebbene centrali nello sviluppo iniziale della statistica, hanno rivelato limiti concettuali e pratici che hanno spinto verso approcci più generali e flessibili.

### Interpretazione Classica

La definizione classica di probabilità fu proposta da Pierre-Simon Laplace (1749-1827), che basò il concetto sul calcolo combinatorio. Secondo Laplace, la probabilità di un evento è data dal rapporto tra i casi favorevoli e il numero totale di casi possibili, assumendo che tutti siano equiprobabili. Ad esempio, la probabilità di ottenere un “3” lanciando un dado è $\frac{1}{6}$, poiché solo uno dei sei risultati è favorevole. 

Questo approccio, pur utile per scenari semplici, è limitato dalla sua dipendenza da ipotesi spesso irrealistiche (es., l’assunzione che ogni evento sia equiprobabile) e da una formulazione implicitamente circolare (poiché presuppone una conoscenza implicita del concetto di probabilità).

### Interpretazione Frequentista

Per superare tali limiti, il frequentismo definisce la probabilità come il limite della frequenza relativa con cui un evento si verifica in una serie infinita di prove. Per esempio, la probabilità di ottenere "testa" in un lancio di moneta può essere stimata come la frequenza relativa di “testa” sul totale dei lanci, quando il numero di lanci tende all’infinito. Questa definizione è utile, ma impraticabile in molte situazioni, poiché richiede un numero infinito di ripetizioni e assume che gli eventi futuri siano identici a quelli passati.

La figura seguente illustra la proporzione di risultati "testa" in una sequenza di lanci di una moneta equa. Si può osservare come la frequenza relativa dei risultati "testa" converga progressivamente verso il valore della probabilità teorica.

```{r}
#| echo: false
coin_flips <- function(n, run_label) {
  # Genera un vettore di 0 e 1 dove 1 rappresenta "testa" e 0 "croce"
  # usando una distribuzione binomiale.
  heads <- rbinom(n, 1, 0.5)
  
  # Calcola la proporzione cumulativa di teste.
  flips <- seq(1, n)
  proportion_heads <- cumsum(heads) / flips
  
  # Crea un data frame per un facile accesso e visualizzazione dei dati.
  df <- data.frame(
    flips = flips, 
    proportion_heads = proportion_heads, 
    run = run_label
  )
  
  return(df)
}

n <- 1000

df <- do.call(rbind, lapply(1:4, function(i) coin_flips(n, paste0("run", i))))

ggplot(df, aes(x = flips, y = proportion_heads, color = run)) +
  geom_line()
```


#### La Legge dei Grandi Numeri

Un fondamento cruciale dell’approccio frequentista è la Legge dei Grandi Numeri, che garantisce la convergenza delle frequenze empiriche ai valori teorici con un numero crescente di osservazioni. 

La simulazione precedente offre una chiara illustrazione della **Legge dei Grandi Numeri**, un principio fondamentale della probabilità. Questo teorema afferma che, con l'aumentare del numero di esperimenti casuali ripetuti, la stima empirica della probabilità di un evento $P(Y = y)$ tende a convergere al valore teorico.

In termini semplici, la Legge dei Grandi Numeri garantisce che, al crescere del numero di prove, la media dei risultati osservati si avvicina progressivamente al valore atteso della variabile casuale. Questo significa che, anche se i risultati individuali possono variare in modo casuale, la media dei risultati su un gran numero di esperimenti rifletterà con sempre maggiore precisione la probabilità teorica.

Questo principio è fondamentale perché assicura che:

- Con un numero sufficiente di prove, le stime empiriche delle probabilità siano affidabili.
- I modelli probabilistici possano essere utilizzati per descrivere e prevedere fenomeni reali, nonostante la variabilità intrinseca delle osservazioni singole.

Formalmente, se consideriamo una serie di variabili casuali indipendenti $X_1, X_2, \ldots, X_n$, tutte con la stessa media teorica $\mu$, la Legge dei Grandi Numeri si esprime come:

$$
\lim_{{n \to \infty}} P\left(\left|\frac{1}{n} \sum_{i=1}^n X_i - \mu\right| < \epsilon\right) = 1,
$$

dove $\epsilon$ è un numero positivo arbitrariamente piccolo e $P(\cdot)$ rappresenta la probabilità. In altre parole:

- Man mano che $n$ (il numero di prove) diventa molto grande, la media campionaria $\frac{1}{n} \sum_{i=1}^n X_i$ sarà sempre più vicina alla media teorica $\mu$, con una probabilità che si avvicina a 1.

Questo teorema ha implicazioni importanti nella pratica:

- Permette di stimare probabilità con crescente precisione al crescere delle osservazioni.
- Offre un fondamento teorico per l'utilizzo di medie campionarie in statistica e in molte applicazioni scientifiche.

In conclusione, la Legge dei Grandi Numeri è essenziale per comprendere come, nonostante la variabilità casuale nei risultati individuali, la regolarità emerge quando si considera un numero sufficientemente grande di osservazioni. Questo principio stabilisce il legame tra la teoria della probabilità e le applicazioni pratiche, garantendo che le stime empiriche, nel lungo periodo, riflettano i valori teorici con elevata precisione. Questo principio, tuttavia, non elimina la necessità di considerare l’incertezza residua nei casi di campioni piccoli o dati non rappresentativi.

#### Collegamento tra Probabilità e Statistica

La probabilità intesa come frequenza relativa costituisce il fondamento del framework teorico per l'inferenza statistica proposto durante gli anni '20 del Novecento da Ronald A. Fisher. Fisher introdusse concetti chiave come la massima verosimiglianza, i test di significatività, i metodi di campionamento, l'analisi della varianza e il disegno sperimentale.

Fisher assunse una prospettiva critica nei confronti della "probabilità inversa" (ossia, i metodi bayesiani), nonostante questa fosse stata la metodologia predominante per l'inferenza statistica per quasi un secolo e mezzo. Il suo approccio frequentista ebbe un profondo impatto sullo sviluppo della statistica sia teorica che sperimentale, contribuendo a un decremento nell'utilizzo dell'inferenza basata sul metodo della probabilità inversa, originariamente proposto da Laplace.

Negli anni '30, Jerzy Neyman ed Egon Pearson fecero ulteriori progressi nel campo con lo sviluppo di una teoria della decisione statistica, basata sul principio della verosimiglianza e sull'interpretazione frequentista della probabilità. Definirono due tipologie di errori decisionali e utilizzarono il test di significatività di Fisher, interpretando i valori-$p$ come indicatori dei tassi di errore a lungo termine.

#### Limiti

L'approccio frequentista è robusto in contesti ripetibili, ma si scontra con difficoltà concettuali e pratiche nell'analisi di eventi singolari e non ripetibili.

Ad esempio, la probabilità che un atleta vinca una specifica competizione, o che un determinato evento atmosferico si verifichi in un giorno particolare, non può essere rigorosamente quantificata dal punto di vista frequentista, in quanto il frequentismo richiede un esperimento ripetibile per definire la probabilità. Questo limite ha alimentato l'interesse verso l'approccio bayesiano, che consente di esprimere la probabilità come una misura soggettiva del grado di fiducia basato su informazioni disponibili.

### Il Rinascimento Bayesiano

Nel 1939, il libro di @jeffreys1998theory intitolato "Theory of Probability" rappresentò una delle prime esposizioni moderne dei metodi bayesiani. Tuttavia, la rinascita del framework bayesiano fu rinviata fino alla scoperta dei metodi Monte Carlo Markov chain alla fine degli anni '80. Questi metodi hanno reso fattibile il calcolo di risultati precedentemente non ottenibili, consentendo un rinnovato interesse e sviluppo nei metodi bayesiani. Per una storia dell'approccio bayesiano, si veda [Bayesian Methods: General Background](https://bayes.wustl.edu/etj/articles/general.background.pdf) oppure [Philosophy of Statistics](https://plato.stanford.edu/entries/statistics/).

#### Interpretazione Soggettivista della Probabilità

Bruno de Finetti sintetizzò l’essenza della probabilità bayesiana con l’affermazione provocatoria: *"La probabilità non esiste"*. Questo approccio concepisce la probabilità non come una proprietà intrinseca degli eventi, ma come una misura del grado di fiducia razionale basata su informazioni incomplete e soggette a revisione.

La definizione soggettivista, tuttavia, non implica arbitrarietà. Attraverso il teorema di Bayes, la probabilità soggettiva è costantemente aggiornata per incorporare nuove evidenze, garantendo coerenza logica e rigore matematico. Tale prospettiva rende la probabilità uno strumento dinamico e adattabile per descrivere l’incertezza.

Un recente articolo su *Nature* ne ribadisce la rilevanza contemporanea, sottolineando l’essenza soggettiva della probabilità:

> [...] any numerical probability, I will argue — whether in a scientific paper, as part of weather forecasts, predicting the outcome of a sports competition or quantifying a health risk — is not an objective property of the world, but a construction based on personal or collective judgements and (often doubtful) assumptions. Furthermore, in most circumstances, it is not even estimating some underlying ‘true’ quantity. Probability, indeed, can only rarely be said to ‘exist’ at all [@spiegelhalter2024probability].

#### Origini e Sviluppo della Prospettiva Soggettivista

Le radici dell’interpretazione soggettivista risalgono al lavoro pionieristico di Frank P. Ramsey nel 1926, che definì la probabilità come il **grado di credenza individuale** [@ramsey1926truth]. Questa concezione, inizialmente relegata a una posizione marginale, ha gettato le basi per un approccio profondamente innovativo alla teoria della probabilità.

Successivamente, una rigorosa formalizzazione matematica degli assiomi della probabilità soggettiva è stata proposta da Fishburn [@fishburn1986axioms], che ha fornito una struttura teorica robusta per il trattamento della probabilità come costruzione soggettiva. Approfondimenti metodologici e applicativi si trovano nei lavori di Press [@press2009subjective], che hanno ampliato l’ambito di applicazione della prospettiva soggettivista, consolidandola come uno strumento essenziale per affrontare l’incertezza in ambito scientifico.

Questa interpretazione sposta il focus dalla realtà oggettiva alla **costruzione umana** della probabilità, enfatizzando il ruolo di giudizi, ipotesi e informazioni disponibili. La probabilità non è dunque una proprietà intrinseca del mondo, ma una misura del grado di fiducia razionale attribuito da un soggetto idealizzato.

#### La Probabilità come Grado di Credenza Razionale

Secondo l’interpretazione soggettivista bayesiana, la probabilità rappresenta una misura del **grado di fiducia** che un soggetto razionale assegna alla validità di un’affermazione, basandosi su informazioni disponibili, spesso incomplete. Questo concetto si applica non a un individuo specifico, ma a un’idealizzazione di razionalità: un agente che opera esclusivamente sulla base della logica e delle evidenze, privo di emozioni, pregiudizi o bias cognitivi.

E.T. Jaynes, nel suo *Probability Theory: The Logic of Science* [@jaynes2003probability], propone un esperimento mentale che esemplifica questa concezione:

- Si immagini un **robot razionale** dotato di un insieme di informazioni $I$, ritenute vere e complete.
- Si presenti un’affermazione $A$, che nella realtà può essere solo vera o falsa.
- Il robot deve quantificare l’incertezza sulla validità di $A$, basandosi esclusivamente sulle informazioni $I$.

In questo contesto, la probabilità di $A$ dato $I$, espressa come $P(A \mid I)$, è definita secondo i seguenti principi fondamentali:

1. **Intervallo numerico**: $P(A \mid I)$ è un numero reale compreso tra 0 e 1, dove:
   - $P(A \mid I) = 0$ rappresenta la certezza che $A$ sia falsa.
   - $P(A \mid I) = 1$ rappresenta la certezza che $A$ sia vera.

2. **Coerenza logica**: La probabilità deve rispettare i postulati della teoria della probabilità, garantendo un quadro logico rigoroso e internamente consistente.

In conclusione, l’interpretazione soggettivista della probabilità rappresenta un cambio di paradigma, trasformando la probabilità da una proprietà oggettiva a uno **strumento flessibile e razionale** per affrontare l’incertezza. Essa fornisce un quadro teorico che combina rigore matematico e adattabilità pratica, rendendo la probabilità uno strumento essenziale per descrivere e modellare il mondo in presenza di informazioni incomplete.

::: {.callout-note}
# Terminologia

Il termine *probabilità soggettiva* può suggerire una connotazione di imprecisione o mancanza di rigore scientifico. Per evitare tali fraintendimenti, sono state proposte alternative terminologiche:

- @lindley2013understanding suggerisce il termine *probabilità personale*, per enfatizzare l’aspetto individuale ma razionale di questa concezione.
- @howson2006scientific propone invece *probabilità epistemica*, evidenziando il legame con la conoscenza e l’incertezza derivanti da informazioni incomplete.

Queste alternative, adottate da autori come @kaplan2023bayesian, offrono una descrizione più neutrale, particolarmente utile nei contesti scientifici.
:::

::: {.callout-tip}
Per chi desidera approfondire, il primo capitolo del testo *Bernoulli's Fallacy* [@clayton2021bernoulli] offre un'introduzione molto leggibile alle tematiche della definizione della probabilità nella storia della scienza.
:::

## Riflessioni Conclusive

In questo capitolo è stata condotta un’analisi filosofica del concetto di probabilità, esplorandone le principali interpretazioni: da un lato, come proprietà intrinseca e oggettiva degli eventi, e dall’altro, come espressione di convinzioni soggettive in condizioni di incertezza. Questa duplice prospettiva ha messo in luce come la probabilità possa essere concepita sia come un elemento del mondo reale, sia come uno strumento epistemico per rappresentare il grado di fiducia razionale.

Inoltre, è stato approfondito il ruolo della simulazione come strumento metodologico essenziale per approssimare probabilità empiriche in contesti complessi, dove soluzioni analitiche risultano impraticabili. Questa tecnica si è dimostrata particolarmente rilevante nei campi di ricerca più avanzati, in cui la crescente complessità dei modelli matematici rende indispensabile l’utilizzo di algoritmi numerici sofisticati. La simulazione non solo amplia le possibilità di analisi, ma funge anche da ponte tra le teorie probabilistiche e le loro applicazioni pratiche.

Sulla base di queste premesse, i capitoli successivi saranno dedicati a un’analisi matematica della probabilità. Verranno esaminati i teoremi e le leggi fondamentali che costituiscono il nucleo formale della disciplina, evidenziando come tali strumenti possano estendere l’applicabilità del concetto di probabilità oltre le speculazioni teoriche, verso ambiti pratici. Questo approccio quantitativo fornirà un quadro più preciso e affidabile per quantificare e gestire l’incertezza, gettando le basi per una comprensione più completa e rigorosa delle dinamiche probabilistiche.

## Informazioni sull'Ambiente di Sviluppo

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

