<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>30&nbsp; Il teorema di Bayes – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/probability/07_random_var.html" rel="next">
<link href="../../chapters/probability/05_conditional_prob.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9ac0fa60fcd2ae55f2ddd9f231fc218f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2388ef3e6d0c54e2df1accbdf581d6e2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/06_bayes_theorem.html"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La riforma metodologica in psicologia: dalla crisi alla rivoluzione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelli cognitivi</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Science e R: un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions in R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione in R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti in R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione in R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI per la programmazione</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/16_likelihood_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">La verosimiglianza gaussiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/17_likelihood_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Il rapporto di verosimiglianze</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/18_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Simulazioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Calcolo della distribuzione a posteriori gaussiana tramite metodo a griglia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata: un approccio frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_prediction_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Predizione e inferenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Disegno della ricerca e potere statistico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">30.1</span> Introduzione</a></li>
  <li><a href="#una-rivoluzione-nel-pensiero-probabilistico" id="toc-una-rivoluzione-nel-pensiero-probabilistico" class="nav-link" data-scroll-target="#una-rivoluzione-nel-pensiero-probabilistico"><span class="header-section-number">30.2</span> Una Rivoluzione nel Pensiero Probabilistico</a></li>
  <li><a href="#la-regola-di-bayes" id="toc-la-regola-di-bayes" class="nav-link" data-scroll-target="#la-regola-di-bayes"><span class="header-section-number">30.3</span> La Regola di Bayes</a></li>
  <li><a href="#applicazioni-della-regola-di-bayes" id="toc-applicazioni-della-regola-di-bayes" class="nav-link" data-scroll-target="#applicazioni-della-regola-di-bayes"><span class="header-section-number">30.4</span> Applicazioni della Regola di Bayes</a></li>
  <li><a href="#test-medici" id="toc-test-medici" class="nav-link" data-scroll-target="#test-medici"><span class="header-section-number">30.5</span> Test medici</a></li>
  <li><a href="#la-fallacia-del-procuratore-e-il-teorema-di-bayes" id="toc-la-fallacia-del-procuratore-e-il-teorema-di-bayes" class="nav-link" data-scroll-target="#la-fallacia-del-procuratore-e-il-teorema-di-bayes"><span class="header-section-number">30.6</span> La Fallacia del Procuratore e il Teorema di Bayes</a></li>
  <li><a href="#probabilit%C3%A0-inversa-dal-problema-classico-allinferenza-bayesiana" id="toc-probabilità-inversa-dal-problema-classico-allinferenza-bayesiana" class="nav-link" data-scroll-target="#probabilit%C3%A0-inversa-dal-problema-classico-allinferenza-bayesiana"><span class="header-section-number">30.7</span> Probabilità Inversa: Dal Problema Classico all’Inferenza Bayesiana</a></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">30.8</span> Riflessioni Conclusive</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  <li><a href="#bibliografia" id="toc-bibliografia" class="nav-link" data-scroll-target="#bibliografia">Bibliografia</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/06_bayes_theorem.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/06_bayes_theorem.html"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-prob-bayes-theorem" class="quarto-section-identifier"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In questo capitolo imparerai a
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>capire in profondità il teorema di Bayes e la sua importanza;</li>
<li>utilizzare il teorema di Bayes per analizzare e interpretare i test diagnostici, tenendo in considerazione la prevalenza della malattia in questione;</li>
<li>affrontare e risolvere problemi di probabilità discreta che necessitano dell’applicazione del teorema di Bayes.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Leggere <em>Everything is Predictable: How Bayesian Statistics Explain Our World</em> <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>. Questo libro offre una descrizione chiara e accessibile dell’impatto che il teorema di Bayes ha avuto sulla vita moderna.</li>
<li>Leggere <a href="https://oecs.mit.edu/pub/lwxmte1p/release/2">Bayesian Models of Cognition</a> di Thomas L. Griffiths, una voce della <a href="https://oecs.mit.edu">Open Encyclopedia of Cognitive Science</a>.</li>
<li>Leggere il capitolo <em>Conditional Probability</em> <span class="citation" data-cites="schervish2014probability">(<a href="#ref-schervish2014probability" role="doc-biblioref">Schervish &amp; DeGroot, 2014</a>)</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>“It is, without exaggeration, perhaps the most important single equation in history.”<br>
— <strong>Tom Chivers (2024)</strong></p>
</blockquote>
<section id="introduzione" class="level2" data-number="30.1"><h2 data-number="30.1" class="anchored" data-anchor-id="introduzione">
<span class="header-section-number">30.1</span> Introduzione</h2>
<p>Il teorema di Bayes offre una soluzione ottimale ai problemi induttivi, che spaziano dall’identificazione della struttura tridimensionale del mondo basata su dati sensoriali limitati <span class="citation" data-cites="ma2023bayesian domini20033 caudek2024fenomeni">(<a href="#ref-caudek2024fenomeni" role="doc-biblioref">Caudek &amp; Bruno, 2024</a>; <a href="#ref-domini20033" role="doc-biblioref">Domini &amp; Caudek, 2003</a>; <a href="#ref-ma2023bayesian" role="doc-biblioref">Ma et al., 2023</a>)</span>, all’inferenza dei pensieri altrui a partire dal loro comportamento <span class="citation" data-cites="baker2011bayesian">(<a href="#ref-baker2011bayesian" role="doc-biblioref">Baker et al., 2011</a>)</span>. Questa regola si rivela particolarmente utile in situazioni in cui i dati osservati sono insufficienti per distinguere in modo definitivo tra diverse ipotesi.</p>
<p>Nonostante ciò, tutte le previsioni basate su questo metodo mantengono un certo grado di incertezza. Anche se l’universo fosse completamente deterministico, la nostra conoscenza di esso rimarrebbe imperfetta: non possiamo conoscere la posizione e lo stato di ogni singola particella che lo compone. Le informazioni a nostra disposizione sono inevitabilmente parziali e imprecise, ottenute attraverso i nostri sensi limitati.</p>
<p>La vita reale non è paragonabile a una partita di scacchi, un gioco con informazioni perfette che può essere “risolto” in linea di principio. Assomiglia piuttosto al poker, dove le decisioni vengono prese utilizzando le informazioni limitate a disposizione dei giocatori <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>. Questo capitolo si concentra sull’equazione che ci permette di fare proprio questo: il teorema di Bayes. Esso descrive come modifichiamo le nostre convinzioni riguardo a un’ipotesi in una situazione in cui i dati disponibili non consentono una decisione certa.</p>
<p>Questo processo è noto come inferenza induttiva, che ci permette di trarre conclusioni generali da dati specifici e limitati. Il teorema di Bayes fornisce quindi un framework matematico per aggiornare le nostre credenze alla luce di nuove evidenze, permettendoci di prendere decisioni informate in un mondo caratterizzato dall’incertezza.</p>
</section><section id="una-rivoluzione-nel-pensiero-probabilistico" class="level2" data-number="30.2"><h2 data-number="30.2" class="anchored" data-anchor-id="una-rivoluzione-nel-pensiero-probabilistico">
<span class="header-section-number">30.2</span> Una Rivoluzione nel Pensiero Probabilistico</h2>
<p>Nel cuore del XVIII secolo, un ecclesiastico presbiteriano di nome Thomas Bayes pose le fondamenta per una delle più importanti rivoluzioni nel campo della statistica e del calcolo delle probabilità. Il suo contributo, noto oggi come <strong>teorema di Bayes</strong>, non solo trasformò radicalmente l’interpretazione della probabilità, ma continua a influenzare profondamente la scienza moderna, la tecnologia e persino l’intelligenza artificiale <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>.</p>
<p>Nato in una famiglia benestante, Thomas Bayes ricevette un’educazione raffinata, studiando teologia a Edimburgo per prepararsi alla vita da ecclesiastico. Come sottolinea il biografo David Bellhouse, Bayes “non incarnava l’archetipo dell’accademico moderno. Era piuttosto un erudito, un libero pensatore: perseguiva il sapere per il proprio piacere personale, non per seguire un rigido programma di ricerca.”</p>
<p>Bayes pubblicò due opere principali durante la sua vita:</p>
<ul>
<li>
<strong>Un trattato teologico</strong>, <em>Divine Benevolence: Or, an Attempt to Prove that the Principal End of the Divine Providence and Government is the Happiness of His Creatures</em> (1731), in cui argomentò una teodicea basata sull’ottimizzazione del benessere universale attraverso le leggi naturali <span class="citation" data-cites="bellhouse2004">(<a href="#ref-bellhouse2004" role="doc-biblioref">Bellhouse, 2004</a>)</span>.</li>
<li>
<em><strong>Un’apologia del calcolo newtoniano</strong>, </em>An Introduction to the Doctrine of Fluxions* (1736), difesa metodologica contro le critiche di George Berkeley ai fondamenti degli infinitesimi, allora ritenuti logicamente inconsistenti <span class="citation" data-cites="jesseph1993berkeley">(<a href="#ref-jesseph1993berkeley" role="doc-biblioref">Jesseph, 1993</a>)</span>.</li>
</ul>
<p>Tuttavia, fu il suo lavoro postumo, <em>An Essay towards Solving a Problem in the Doctrine of Chances</em> (1763), pubblicato nelle <em>Philosophical Transactions of the Royal Society</em>, a cambiare per sempre il corso della teoria della probabilità. Questo lavoro formalizzò per la prima volta un metodo per aggiornare probabilisticamente ipotesi alla luce di dati empirici, ponendo le basi per l’inferenza bayesiana <span class="citation" data-cites="stigler1990history">(<a href="#ref-stigler1990history" role="doc-biblioref">Stigler, 1990</a>)</span>.</p>
<p>Come evidenziato dallo storico David Bellhouse, l’interesse di Bayes per la matematica era emblematico del ruolo culturale della scienza tra le élite colte nel XVIII secolo: “Per i ricchi del tempo, impegnarsi in discipline scientifiche era un segno di status, analogo all’odierna passione per attività come gli sport esclusivi” <span class="citation" data-cites="bellhouse2004">(<a href="#ref-bellhouse2004" role="doc-biblioref">Bellhouse, 2004</a>)</span>.</p>
<p>Il contributo rivoluzionario dell’approccio bayesiano non risiede tanto nella sua formulazione matematica quanto nella sua <strong>portata epistemologica</strong>. Come sottolinea David Spiegelhalter, statistico di fama internazionale e già presidente della <em>Royal Statistical Society</em>, per Bayes «la probabilità incarna l’espressione quantificabile della nostra ignoranza sul mondo». Questa definizione sovverte radicalmente la visione classica – legata a frequenze osservative e ripetibilità – trasformando la probabilità da proprietà oggettiva degli eventi a <strong>strumento soggettivo di conoscenza</strong>.</p>
<p>Nel paradigma bayesiano, assegnare una probabilità diventa un atto intrinsecamente <em>contestuale</em>: non riflette dinamiche “esterne”, ma sintetizza il grado di fiducia razionale di un osservatore, inevitabilmente filtrato dal suo bagaglio di conoscenze, esperienze e persino pregiudizi. Il teorema di Bayes formalizza questo processo dialettico, strutturando l’inferenza come <strong>revisione critica delle credenze</strong> alla luce di nuovi dati. La probabilità si trasforma così in una misura dinamica, continuamente aggiornabile, che modella statistiche e convinzioni in un unico atto cognitivo <span class="citation" data-cites="spiegelhalter2019art">(<a href="#ref-spiegelhalter2019art" role="doc-biblioref">Spiegelhalter, 2019</a>)</span>.</p>
<p>Per illustrare questo concetto, Bayes utilizzò l’esempio di un esperimento mentale che coinvolge sei palline lanciate casualmente su un tavolo da biliardo.</p>
<blockquote class="blockquote">
<p>Supponiamo che una pallina bianca venga lanciata a caso sul tavolo, la sua posizione lungo il tavolo sia segnata con una linea, e poi la pallina bianca venga rimossa. Successivamente, un certo numero di palline rosse viene lanciato casualmente sul tavolo, e ti viene comunicato solo quante di esse si trovano a sinistra e quante a destra della linea. Dove pensi che possa trovarsi la linea, e quale dovrebbe essere la probabilità che la prossima pallina rossa cada alla sua sinistra?</p>
</blockquote>
<p>La soluzione proposta da Bayes utilizza non solo i dati osservati (il numero di palline rosse a sinistra e a destra della linea), ma anche le <strong>convinzioni iniziali</strong> (il “prior”).</p>
<p><strong>Richard Price (1723-1791)</strong>, altro ecclesiastico nonconformista, fu determinante per la diffusione dell’opera di Thomas Bayes. Ben più noto dell’amico, Price godeva di una solida reputazione nei circoli intellettuali del tempo. Intratteneva rapporti con numerosi Padri Fondatori della Rivoluzione Americana, tra cui <strong>Benjamin Franklin</strong>, <strong>Thomas Jefferson</strong> e <strong>John Adams</strong> – futuro secondo presidente degli Stati Uniti –, ai cui ideali rivoluzionari aderì con fervore. Nel 1776, pubblicò un opuscolo di grande impatto, <em>Observations on the Nature of Civil Liberty, the Principles of Government, and the Justice and Policy of the War with America</em>, divenuto manifesto del sostegno britannico all’indipendenza americana. La sua rete di contatti includeva anche filosofi del calibro di <strong>David Hume</strong> e <strong>Adam Smith</strong>, con cui discuteva di etica ed economia.</p>
<p>Il contributo più rilevante di Price in ambito scientifico fu però la valorizzazione del lavoro di Bayes. Dopo la morte di quest’ultimo, nel 1761 sottopose il saggio inedito al fisico <strong>John Canton</strong> e ne curò la pubblicazione nelle <em>Philosophical Transactions</em> della Royal Society, avvenuta due anni più tardi. Il ritardo non fu casuale: Price non si limitò a una semplice revisione di refusi o punteggiatura. Rivisitò profondamente l’opera, aggiungendovi una seconda metà interamente dedicata alle applicazioni pratiche del teorema. Mentre Bayes si era concentrato sulla dimensione teorica – come del resto in tutti i suoi scritti, privi di qualsiasi riferimento a casi concreti –, Price ampliò il testo con esempi probabilistici, trasformandolo in uno strumento utilizzabile in pratica. Non a caso, lo storico della statistica Stephen Stigler lo definisce <em>«il primo bayesiano della storia»</em>.</p>
<p>Sebbene l’opera di Bayes rimase pressoché ignorata per oltre mezzo secolo - oscurata dal contributo pionieristico di <strong>Pierre-Simon Laplace</strong>, che nel 1774 formulò indipendentemente principi analoghi per poi sistematizzarli nella monumentale <em>Théorie analytique des probabilités</em> (1812) - il teorema bayesiano rappresenta oggi il fondamento epistemologico della statistica moderna. Questo strumento matematico, che formalizza il processo di aggiornamento delle credenze probabilistiche alla luce di nuove evidenze, incarna una visione dinamica della conoscenza dove ogni dato modifica iterativamente lo spazio delle ipotesi possibili.</p>
<p>Nell’era digitale, la rivoluzione bayesiana ha permeato ogni ambito scientifico: dalla genomica alle scienze cognitive, dalla fisica delle particelle all’econometria. Il suo impatto risulta particolarmente importante nell’intelligenza artificiale, dove costituisce l’architettura logica di sistemi di apprendimento automatico. Modelli linguistici avanzati come ChatGPT e Claude implementano versioni sofisticate di inferenza bayesiana per ottimizzare previsioni, generare testi coerenti e adattarsi contestualmente agli input dell’utente.</p>
<p>La parabola storica di questo teorema - nato dalle speculazioni teologiche di un pastore presbiteriano settecentesco - illustra il potere trasformativo delle idee matematiche. Come sottolinea Tom Chivers in <em>Everything Is Predictable: How Bayesian Statistics Explain Our World</em>, la statistica bayesiana è diventata una <strong>“grammatica universale”</strong> per decifrare la realtà, permettendoci non solo di interpretare fenomeni complessi ma di prevederne l’evoluzione in condizioni d’incertezza <span class="citation" data-cites="chivers2024everything">(<a href="#ref-chivers2024everything" role="doc-biblioref">Chivers, 2024</a>)</span>.</p>
</section><section id="la-regola-di-bayes" class="level2" data-number="30.3"><h2 data-number="30.3" class="anchored" data-anchor-id="la-regola-di-bayes">
<span class="header-section-number">30.3</span> La Regola di Bayes</h2>
<p>L’inferenza bayesiana si basa su un principio fondamentale della teoria delle probabilità: la <strong>regola di Bayes</strong>. Questa formula consente di aggiornare le credenze alla luce di nuove evidenze, combinando informazioni a priori con dati osservati.</p>
<p>Per comprenderla, consideriamo due variabili casuali, <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span>. La probabilità congiunta di questi due eventi, ovvero la probabilità che entrambi accadano simultaneamente, può essere espressa in due modi diversi.</p>
<p><strong>1. Applicazione della regola della catena.</strong><br>
La <strong>regola della catena</strong> ci permette di esprimere la probabilità congiunta come il prodotto tra una probabilità condizionata e una probabilità marginale. In particolare, possiamo scrivere:</p>
<p><span class="math display">\[
P(A, B) = P(A \mid B) P(B).
\]</span></p>
<p>Questa espressione indica che la probabilità congiunta <span class="math inline">\(P(A, B)\)</span> si ottiene moltiplicando la probabilità condizionata <span class="math inline">\(P(A \mid B)\)</span> (cioè la probabilità che <span class="math inline">\(A\)</span> si verifichi dato che <span class="math inline">\(B\)</span> è accaduto) per la probabilità marginale <span class="math inline">\(P(B)\)</span> (cioè la probabilità che <span class="math inline">\(B\)</span> si verifichi indipendentemente da <span class="math inline">\(A\)</span>).</p>
<p><strong>2. Simmetria della probabilità congiunta.</strong><br>
Poiché la probabilità congiunta è simmetrica, possiamo anche esprimerla invertendo l’ordine delle variabili:</p>
<p><span class="math display">\[
P(A, B) = P(B \mid A) P(A).
\]</span></p>
<p>Questa formula è ottenuta applicando la stessa regola della catena, ma considerando prima <span class="math inline">\(B\)</span> condizionato a <span class="math inline">\(A\)</span>.</p>
<p><strong>3. Eguaglianza delle due espressioni.</strong><br>
Poiché entrambi i modi di scrivere la probabilità congiunta rappresentano la stessa quantità, possiamo eguagliarli:</p>
<p><span class="math display">\[
P(A \mid B) P(B) = P(B \mid A) P(A).
\]</span></p>
<p><strong>4. Derivazione della regola di Bayes.</strong><br>
Ora possiamo risolvere questa equazione per <span class="math inline">\(P(B \mid A)\)</span>. Dividendo entrambi i lati per <span class="math inline">\(P(A)\)</span>, otteniamo:</p>
<p><span class="math display">\[
P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
\]</span></p>
<p>Questa è la <strong>regola di Bayes</strong>. Essa ci permette di calcolare la probabilità che <span class="math inline">\(B\)</span> si verifichi dato che sappiamo che <span class="math inline">\(A\)</span> è accaduto, utilizzando:</p>
<ul>
<li>
<span class="math inline">\(P(A \mid B)\)</span>, cioè la probabilità di osservare <span class="math inline">\(A\)</span> se <span class="math inline">\(B\)</span> fosse vero.</li>
<li>
<span class="math inline">\(P(B)\)</span>, la probabilità a priori di <span class="math inline">\(B\)</span> (cioè prima di considerare l’evidenza di <span class="math inline">\(A\)</span>).</li>
<li>
<span class="math inline">\(P(A)\)</span>, la probabilità marginale di <span class="math inline">\(A\)</span>, che funge da termine di normalizzazione per assicurare che le probabilità condizionate si sommino a 1.</li>
</ul>
<p>Questa formula è il cuore dell’inferenza bayesiana, poiché consente di aggiornare la nostra conoscenza su un evento alla luce di nuove osservazioni.</p>
<p>Nel contesto dell’inferenza bayesiana:</p>
<ul>
<li>
<strong><span class="math inline">\(P(B)\)</span></strong> rappresenta la nostra conoscenza iniziale (<strong>prior</strong>),</li>
<li>
<strong><span class="math inline">\(P(A \mid B)\)</span></strong> è la <strong>verosimiglianza</strong>, ovvero la probabilità di osservare i dati supponendo che l’ipotesi sia vera,</li>
<li>
<strong><span class="math inline">\(P(B \mid A)\)</span></strong> è la probabilità aggiornata (<strong>posterior</strong>), che incorpora l’evidenza fornita dai dati.</li>
</ul></section><section id="applicazioni-della-regola-di-bayes" class="level2" data-number="30.4"><h2 data-number="30.4" class="anchored" data-anchor-id="applicazioni-della-regola-di-bayes">
<span class="header-section-number">30.4</span> Applicazioni della Regola di Bayes</h2>
<p>La <strong>regola di Bayes</strong> è fondamentale per l’inferenza probabilistica, permettendo di aggiornare le credenze alla luce di nuove informazioni. Supponiamo di voler determinare quale processo abbia generato un insieme di dati osservati, denotato con <span class="math inline">\(D\)</span>. Definiamo uno <strong>spazio delle ipotesi</strong> <span class="math inline">\(\mathcal{H}\)</span>, che contiene tutte le possibili ipotesi sui processi che potrebbero aver generato <span class="math inline">\(D\)</span>. Indichiamo con <span class="math inline">\(H\)</span> una specifica ipotesi appartenente a <span class="math inline">\(\mathcal{H}\)</span>, mentre con <span class="math inline">\(H'\)</span> denotiamo genericamente altre ipotesi alternative appartenenti allo stesso spazio <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>L’agente esprime il proprio grado di credenza in <span class="math inline">\(H\)</span> attraverso la <strong>probabilità a priori</strong> <span class="math inline">\(P(H)\)</span>, che riflette il livello di fiducia nell’ipotesi prima di osservare i dati. Dopo aver raccolto nuovi dati <span class="math inline">\(D\)</span>, questa credenza viene aggiornata calcolando la <strong>probabilità a posteriori</strong> <span class="math inline">\(P(H \mid D)\)</span>, ovvero la probabilità dell’ipotesi dopo aver considerato l’evidenza.</p>
<p>La regola di Bayes fornisce il modo formale per eseguire questo aggiornamento:</p>
<p><span id="eq-bayes-rule-2"><span class="math display">\[
P(H \mid D) = \frac{P(D \mid H) P(H)}{P(D)} ,
\tag{30.1}\]</span></span></p>
<p>dove:</p>
<ul>
<li><p><strong><span class="math inline">\(P(D \mid H)\)</span></strong> è la <strong>verosimiglianza</strong>, ovvero la probabilità di osservare i dati <span class="math inline">\(D\)</span> supponendo che l’ipotesi <span class="math inline">\(H\)</span> sia vera. Indica quanto l’ipotesi spiega i dati osservati.</p></li>
<li><p><strong><span class="math inline">\(P(H)\)</span></strong> è la <strong>probabilità a priori</strong>, ovvero la conoscenza preesistente su <span class="math inline">\(H\)</span> prima dell’osservazione dei dati.</p></li>
<li>
<p><strong><span class="math inline">\(P(D)\)</span></strong> è la <strong>probabilità marginale</strong> dei dati, ottenuta considerando tutte le ipotesi possibili:</p>
<p><span class="math display">\[
P(D) = \sum_{H' \in \mathcal{H}} P(D \mid H') P(H').
\]</span></p>
</li>
</ul>
<p>Questa quantità, detta <strong>evidenza</strong> o <strong>fattore di normalizzazione</strong>, garantisce che la somma delle probabilità a posteriori su tutte le ipotesi sia pari a 1.</p>
<section id="la-marginalizzazione" class="level3" data-number="30.4.1"><h3 data-number="30.4.1" class="anchored" data-anchor-id="la-marginalizzazione">
<span class="header-section-number">30.4.1</span> La Marginalizzazione</h3>
<p>Il calcolo della probabilità marginale <span class="math inline">\(P(D)\)</span> è fondamentale per normalizzare la distribuzione a posteriori. Questo avviene considerando tutte le ipotesi possibili.</p>
<ul>
<li>
<p><strong>Caso discreto:</strong> la probabilità marginale si ottiene sommando su tutte le possibili ipotesi <span class="math inline">\(H' \in \mathcal{H}\)</span>:</p>
<p><span class="math display">\[
P(D) = \sum_{H' \in \mathcal{H}} P(D \mid H') P(H').
\]</span></p>
<p>Sostituendo questa espressione nella regola di Bayes, otteniamo la formula esplicita della distribuzione a posteriori:</p>
<p><span id="eq-marginal-prob"><span class="math display">\[
P(H \mid D) = \frac{P(D \mid H) P(H)}{\sum_{H' \in \mathcal{H}} P(D \mid H') P(H')}.
\tag{30.2}\]</span></span></p>
</li>
<li>
<p><strong>Caso continuo:</strong> se lo spazio delle ipotesi è continuo, la somma viene sostituita da un <strong>integrale</strong>:</p>
<p><span class="math display">\[
P(D) = \int_{\mathcal{H}} P(D \mid H') P(H') \, dH'.
\]</span></p>
<p>La distribuzione a posteriori si esprime quindi come:</p>
<p><span id="eq-bayes-rule-cont"><span class="math display">\[
P(H \mid D) = \frac{P(D \mid H) P(H)}{\int_{\mathcal{H}} P(D \mid H') P(H') \, dH'}.
\tag{30.3}\]</span></span></p>
</li>
</ul>
<p>L’evidenza <span class="math inline">\(P(D)\)</span> può essere computazionalmente onerosa da calcolare, specialmente in spazi ad alta dimensionalità. Per questo motivo, si utilizzano tecniche di approssimazione come il <strong>campionamento Monte Carlo</strong> o i <strong>metodi variazionali</strong>.</p>
</section><section id="componenti-chiave-della-formula-di-bayes" class="level3" data-number="30.4.2"><h3 data-number="30.4.2" class="anchored" data-anchor-id="componenti-chiave-della-formula-di-bayes">
<span class="header-section-number">30.4.2</span> Componenti Chiave della Formula di Bayes</h3>
<p>La regola di Bayes si basa su tre elementi essenziali:</p>
<ol type="1">
<li>
<strong>Probabilità a priori <span class="math inline">\(P(H)\)</span></strong>: la credenza iniziale sull’ipotesi <span class="math inline">\(H\)</span> prima di osservare i dati.</li>
<li>
<strong>Verosimiglianza <span class="math inline">\(P(D \mid H)\)</span></strong>: la probabilità dei dati osservati, dato che <span class="math inline">\(H\)</span> sia vera. Misura quanto l’ipotesi è compatibile con l’evidenza.</li>
<li>
<strong>Probabilità a posteriori <span class="math inline">\(P(H \mid D)\)</span></strong>: la credenza aggiornata in <span class="math inline">\(H\)</span> dopo aver osservato i dati.</li>
</ol>
<p>L’aggiornamento delle credenze attraverso la <strong>regola di Bayes</strong> è un processo iterativo: ogni volta che vengono raccolti nuovi dati, la distribuzione a posteriori diventa la nuova distribuzione a priori per il successivo aggiornamento. Questo meccanismo consente di adattare continuamente le proprie credenze alla luce di nuove informazioni, rendendo l’approccio bayesiano estremamente utile per modellare il ragionamento umano in condizioni di incertezza.</p>
</section><section id="applicazioni-in-psicologia" class="level3" data-number="30.4.3"><h3 data-number="30.4.3" class="anchored" data-anchor-id="applicazioni-in-psicologia">
<span class="header-section-number">30.4.3</span> Applicazioni in Psicologia</h3>
<p>Negli ultimi anni, i <strong>modelli bayesiani</strong> hanno acquisito un ruolo centrale nello studio della cognizione umana, fornendo una struttura formale per comprendere come il cervello costruisca rappresentazioni del mondo e prenda decisioni sulla base di dati incerti. Come discusso da <span class="citation" data-cites="griffiths2024bayesian">Griffiths et al. (<a href="#ref-griffiths2024bayesian" role="doc-biblioref">2024</a>)</span>, questi modelli sono stati applicati a una vasta gamma di processi cognitivi, tra cui:</p>
<ul>
<li>
<strong>Apprendimento e generalizzazione</strong>: i modelli bayesiani descrivono come gli individui apprendano nuove categorie e concetti sulla base di dati limitati e rumorosi (Tenenbaum, Griffiths, &amp; Kemp, 2006).</li>
<li>
<strong>Percezione e interpretazione sensoriale</strong>: la percezione visiva e il riconoscimento di oggetti possono essere spiegati come un’inferenza bayesiana sulla base di segnali sensoriali ambigui <span class="citation" data-cites="yuille2006vision domini20033">(<a href="#ref-domini20033" role="doc-biblioref">Domini &amp; Caudek, 2003</a>; <a href="#ref-yuille2006vision" role="doc-biblioref">Yuille &amp; Kersten, 2006</a>)</span>.</li>
<li>
<strong>Controllo motorio</strong>: il sistema motorio umano sembra ottimizzare i movimenti attraverso una combinazione di modelli interni e aggiornamenti bayesiani (Kording &amp; Wolpert, 2006).</li>
<li>
<strong>Memoria e recupero delle informazioni</strong>: i processi mnemonici, come il richiamo della memoria semantica, possono essere modellati come inferenze bayesiane basate su conoscenze pregresse (Steyvers, Griffiths, &amp; Dennis, 2006).</li>
<li>
<strong>Acquisizione del linguaggio</strong>: l’apprendimento del linguaggio nei bambini può essere descritto attraverso processi probabilistici che permettono di inferire le strutture grammaticali sulla base di dati linguistici limitati (Chater &amp; Manning, 2006; Xu &amp; Tenenbaum, in press).</li>
<li>
<strong>Apprendimento causale</strong>: la capacità di inferire relazioni causali dagli eventi osservati è coerente con un modello bayesiano, in cui la mente valuta la probabilità di una relazione causale sulla base dell’evidenza disponibile (Griffiths &amp; Tenenbaum, 2005, 2007).</li>
<li>
<strong>Ragionamento e decisione</strong>: il ragionamento simbolico e il processo decisionale possono essere formalizzati come un aggiornamento bayesiano delle credenze sulla base di nuove informazioni (Oaksford &amp; Chater, 2001).</li>
<li>
<strong>Cognizione sociale</strong>: le inferenze sulle intenzioni e credenze altrui possono essere modellate attraverso processi bayesiani, permettendo di spiegare come le persone comprendano il comportamento altrui (Baker, Tenenbaum, &amp; Saxe, 2007).</li>
</ul></section><section id="linferenza-bayesiana-nella-cognizione-umana" class="level3" data-number="30.4.4"><h3 data-number="30.4.4" class="anchored" data-anchor-id="linferenza-bayesiana-nella-cognizione-umana">
<span class="header-section-number">30.4.4</span> L’Inferenza Bayesiana nella Cognizione Umana</h3>
<p>Un tema centrale che emerge da questi programmi di ricerca è la seguente domanda: <strong>come fa la mente umana ad andare oltre i dati dell’esperienza?</strong> In altre parole, come riesce il cervello a costruire modelli complessi del mondo a partire da informazioni limitate e spesso ambigue?</p>
<p>L’approccio bayesiano propone che il cervello utilizzi un processo di <strong>inferenza probabilistica</strong> per aggiornare continuamente le proprie credenze, combinando informazioni pregresse con nuove osservazioni per affinare le proprie rappresentazioni mentali. Questo meccanismo consente di spiegare molte delle capacità cognitive umane, dall’apprendimento rapido di nuove categorie alla capacità di adattarsi a un ambiente mutevole, fino alla formulazione di inferenze sociali e alla presa di decisioni in condizioni di incertezza.</p>
<p>L’adozione dei modelli bayesiani nella psicologia cognitiva ha portato a una nuova comprensione della mente come <strong>sistema predittivo</strong>, in grado di formulare ipotesi probabilistiche sugli eventi futuri e di correggerle dinamicamente sulla base dell’esperienza. Questo approccio ha profonde implicazioni per lo studio del comportamento umano e per lo sviluppo di nuove tecniche di modellizzazione nei campi della psicologia, delle neuroscienze e dell’intelligenza artificiale.</p>
</section></section><section id="test-medici" class="level2" data-number="30.5"><h2 data-number="30.5" class="anchored" data-anchor-id="test-medici">
<span class="header-section-number">30.5</span> Test medici</h2>
<p>Uno degli esempi più comuni per comprendere il teorema di Bayes riguarda i test diagnostici.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 30.1</strong></span> Consideriamo un test di mammografia utilizzato per diagnosticare il cancro al seno che abbiamo già discusso nel <a href="05_conditional_prob.html" class="quarto-xref"><span>Capitolo 29</span></a>. Definiamo le seguenti ipotesi:</p>
<ul>
<li>
<strong><span class="math inline">\(M^+\)</span></strong>: la persona ha il cancro al seno;</li>
<li>
<strong><span class="math inline">\(M^-\)</span></strong>: la persona non ha il cancro al seno.</li>
</ul>
<p>L’evidenza è il risultato positivo del test, indicato con <span class="math inline">\(T^+\)</span>. Il nostro obiettivo è calcolare la probabilità che una persona abbia il cancro al seno, dato un risultato positivo al test, ovvero <span class="math inline">\(P(M^+ \mid T^+)\)</span>.</p>
<p><strong>Definizione dei termini nella regola di Bayes.</strong><br>
Il teorema di Bayes afferma che:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) P(M^+)}{P(T^+)} ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<strong><span class="math inline">\(P(T^+ \mid M^+)\)</span></strong> è la <strong>sensibilità</strong> del test, cioè la probabilità che il test risulti positivo se la persona ha effettivamente il cancro. Nel nostro caso, <span class="math inline">\(P(T^+ \mid M^+) = 0.90\)</span>.</li>
<li>
<strong><span class="math inline">\(P(M^+)\)</span></strong> è la <strong>probabilità a priori</strong> di avere il cancro al seno, ovvero la prevalenza della malattia nella popolazione. Supponiamo che sia <span class="math inline">\(P(M^+) = 0.01\)</span> (1%).</li>
<li>
<strong><span class="math inline">\(P(T^+ \mid M^-)\)</span></strong> è la <strong>probabilità di un falso positivo</strong>, cioè la probabilità che il test risulti positivo anche in assenza di malattia. Questa è complementare alla specificità del test:</li>
</ul>
<p><span class="math display">\[
  P(T^+ \mid M^-) = 1 - \text{Specificità} = 1 - 0.90 = 0.10.
\]</span></p>
<ul>
<li>
<strong><span class="math inline">\(P(M^-)\)</span></strong> è la probabilità a priori che una persona non abbia il cancro, ovvero:</li>
</ul>
<p><span class="math display">\[
  P(M^-) = 1 - P(M^+) = 1 - 0.01 = 0.99.
\]</span></p>
<ul>
<li>
<strong><span class="math inline">\(P(T^+)\)</span></strong> è la probabilità marginale che il test risulti positivo, calcolata considerando entrambe le possibilità (cioè che la persona abbia o non abbia il cancro):</li>
</ul>
<p><span class="math display">\[
  P(T^+) = P(T^+ \mid M^+) P(M^+) + P(T^+ \mid M^-) P(M^-).
\]</span></p>
<p>Sostituendo i valori numerici:</p>
<p><span class="math display">\[
  P(T^+) = (0.90 \cdot 0.01) + (0.10 \cdot 0.99) = 0.009 + 0.099 = 0.108.
\]</span></p>
<p><strong>Applicazione della Regola di Bayes.</strong><br>
Ora possiamo calcolare la probabilità a posteriori <span class="math inline">\(P(M^+ \mid T^+)\)</span>:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{0.90 \cdot 0.01}{0.108} = \frac{0.009}{0.108} = 0.0833.
\]</span></p>
<p><strong>Interpretazione del Risultato.</strong><br>
Questo risultato indica che, nonostante il test abbia una sensibilità e una specificità del 90%, la probabilità che una persona con un test positivo abbia effettivamente il cancro è solo dell’<strong>8.3%</strong>. Questo effetto è dovuto alla bassa prevalenza della malattia: anche se il test è relativamente accurato, il numero di falsi positivi è ancora alto rispetto ai veri positivi. Tale risultato conferma quanto precedentemente ottenuto nel <a href="05_conditional_prob.html" class="quarto-xref"><span>Capitolo 29</span></a>, attraverso un metodo di calcolo alternativo.</p>
<p>Questa formulazione mostra come la regola di Bayes permetta di aggiornare la probabilità di avere la malattia dopo aver osservato il risultato del test, combinando la sensibilità, la specificità e la prevalenza della malattia nella popolazione.</p>
</div>
<section id="affidabilità-di-un-test-hiv-e-aggiornamento-bayesiano" class="level3" data-number="30.5.1"><h3 data-number="30.5.1" class="anchored" data-anchor-id="affidabilità-di-un-test-hiv-e-aggiornamento-bayesiano">
<span class="header-section-number">30.5.1</span> Affidabilità di un Test HIV e Aggiornamento Bayesiano</h3>
<p>Vogliamo valutare quanto sia affidabile un test per l’HIV e come cambia la nostra conoscenza dopo due test consecutivi positivi. Per farlo, utilizzeremo la regola di Bayes per aggiornare la probabilità di infezione dopo ogni test.</p>
<div id="exm-hiv" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 30.2</strong></span> Definiamo:</p>
<ul>
<li>
<strong><span class="math inline">\(H^+\)</span></strong>: la persona ha l’HIV.</li>
<li>
<strong><span class="math inline">\(H^-\)</span></strong>: la persona non ha l’HIV.</li>
<li>
<strong><span class="math inline">\(T^+\)</span></strong>: il test risulta positivo.</li>
<li>
<strong><span class="math inline">\(T^-\)</span></strong>: il test risulta negativo.</li>
</ul>
<p>Dati noti:</p>
<ul>
<li>
<strong>Prevalenza dell’HIV nella popolazione</strong>:<br><span class="math display">\[
P(H^+) = 0.003
\]</span>
</li>
<li>
<strong>Sensibilità del test</strong> (probabilità di un test positivo dato che la persona ha l’HIV):<br><span class="math display">\[
P(T^+ \mid H^+) = 0.95
\]</span>
</li>
<li>
<strong>Specificità del test</strong> (probabilità di un test negativo dato che la persona NON ha l’HIV):<br><span class="math display">\[
P(T^- \mid H^-) = 0.9928
\]</span> Da cui il tasso di falsi positivi è:<br><span class="math display">\[
P(T^+ \mid H^-) = 1 - 0.9928 = 0.0072.
\]</span>
</li>
</ul>
<p><strong>Passo 1: Calcolo della probabilità di avere l’HIV dopo un test positivo.</strong><br>
Applichiamo la regola di Bayes:</p>
<p><span class="math display">\[
P(H^+ \mid T^+) = \frac{P(T^+ \mid H^+) P(H^+)}{P(T^+)}
\]</span></p>
<p>dove la probabilità marginale del test positivo è:</p>
<p><span class="math display">\[
P(T^+) = P(T^+ \mid H^+) P(H^+) + P(T^+ \mid H^-) P(H^-).
\]</span></p>
<p>Sostituendo i valori:</p>
<p><span class="math display">\[
P(T^+) = (0.95 \times 0.003) + (0.0072 \times 0.997)
\]</span></p>
<p><span class="math display">\[
= 0.00285 + 0.00718 = 0.01003.
\]</span></p>
<p>Ora calcoliamo la probabilità a posteriori:</p>
<p><span class="math display">\[
P(H^+ \mid T^+) = \frac{0.95 \times 0.003}{0.01003} = \frac{0.00285}{0.01003} \approx 0.2844.
\]</span></p>
<p>Dopo un <strong>primo test positivo</strong>, la probabilità di avere l’HIV sale dal 0.3% al <strong>28.44%</strong>.</p>
<p><strong>Passo 2: Calcolo della probabilità di avere l’HIV dopo due test positivi.</strong><br>
Dopo un primo test positivo, la nuova probabilità a priori diventa <span class="math inline">\(P(H^+ \mid T^+) = 0.2844\)</span>. Ora ripetiamo il calcolo per un secondo test positivo.</p>
<p>La nuova probabilità marginale di un secondo test positivo è:</p>
<p><span class="math display">\[
P(T^+ \mid T^+) = P(T^+ \mid H^+) P(H^+ \mid T^+) + P(T^+ \mid H^-) P(H^- \mid T^+) ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(P(T^+ \mid H^+) = 0.95\)</span> (la sensibilità rimane invariata).</li>
<li>
<span class="math inline">\(P(H^+ \mid T^+) = 0.2844\)</span> (aggiornato dal primo test).</li>
<li>
<span class="math inline">\(P(T^+ \mid H^-) = 0.0072\)</span> (tasso di falsi positivi).</li>
<li>
<span class="math inline">\(P(H^- \mid T^+) = 1 - P(H^+ \mid T^+) = 1 - 0.2844 = 0.7156\)</span>.</li>
</ul>
<p>Calcoliamo la probabilità di un secondo test positivo:</p>
<p><span class="math display">\[
P(T^+ \mid T^+) = (0.95 \times 0.2844) + (0.0072 \times 0.7156)
\]</span></p>
<p><span class="math display">\[
= 0.2702 + 0.00515 = 0.2753.
\]</span></p>
<p>Ora applichiamo nuovamente la regola di Bayes per ottenere la probabilità a posteriori dopo due test positivi:</p>
<p><span class="math display">\[
P(H^+ \mid T^+, T^+) = \frac{P(T^+ \mid H^+) P(H^+ \mid T^+)}{P(T^+ \mid T^+)}
\]</span></p>
<p><span class="math display">\[
= \frac{0.95 \times 0.2844}{0.2753} = \frac{0.2702}{0.2753} \approx 0.981.
\]</span></p>
<p>Dopo <strong>due test consecutivi positivi</strong>, la probabilità di avere l’HIV aumenta drasticamente al <strong>98.1%</strong>.</p>
<p><strong>Conclusioni.</strong><br>
1. <strong>Dopo un solo test positivo</strong>, la probabilità di avere l’HIV è <strong>28.44%</strong>, molto più alta rispetto alla prevalenza iniziale dello 0.3%, ma ancora lontana dal 100%. 2. <strong>Dopo due test positivi consecutivi</strong>, la probabilità sale al <strong>98.1%</strong>, rendendo la diagnosi quasi certa. 3. <strong>L’aggiornamento bayesiano mostra chiaramente l’importanza di test ripetuti</strong>: un singolo test positivo non implica automaticamente la presenza della malattia, ma il risultato combinato di più test migliora enormemente l’affidabilità della diagnosi.</p>
<p>Questa analisi evidenzia come il <strong>teorema di Bayes</strong> sia essenziale per interpretare correttamente i risultati diagnostici, specialmente quando la prevalenza della malattia è bassa.</p>
</div>
<div class="callout callout-style-simple callout-important callout-titled" title="Il Valore Predittivo di un Test di Laboratorio">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Il Valore Predittivo di un Test di Laboratorio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Per semplicità, possiamo riscrivere il teorema di Bayes in due modi distinti per calcolare ciò che viene chiamato valore predittivo del test positivo e valore predittivo del test negativo.</p>
<ul>
<li>
<strong>Valore Predittivo Positivo (VPP)</strong>: la probabilità che una persona sia malata dato un test positivo.<br>
</li>
<li>
<strong>Valore Predittivo Negativo (VPN)</strong>: la probabilità che una persona sia sana dato un test negativo.</li>
</ul>
<p>Per stimare questi valori, è essenziale comprendere tre concetti fondamentali:</p>
<ol type="1">
<li><p><strong>Prevalenza</strong> <span class="math inline">\((P(M^+))\)</span>: rappresenta la proporzione di individui malati nella popolazione. Per esempio, una prevalenza dello 0,5% implica che su 1000 persone, 5 hanno la malattia.</p></li>
<li>
<p><strong>Sensibilità</strong> <span class="math inline">\((P(T^+ \mid M^+))\)</span>: misura la capacità del test di identificare correttamente i malati. È data dal rapporto tra il numero di veri positivi e il totale dei malati:</p>
<p><span class="math display">\[
\text{Sensibilità} = \frac{TP}{TP + FN},
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(TP\)</span> sono i <strong>veri positivi</strong> (malati correttamente identificati dal test).</li>
<li>
<span class="math inline">\(FN\)</span> sono i <strong>falsi negativi</strong> (malati che il test non ha rilevato).</li>
</ul>
</li>
<li>
<p><strong>Specificità</strong> <span class="math inline">\((P(T^- \mid M^-))\)</span>: misura la capacità del test di identificare correttamente i sani. È il rapporto tra il numero di veri negativi e il totale delle persone sane:</p>
<p><span class="math display">\[
\text{Specificità} = \frac{TN}{TN + FP} ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(TN\)</span> sono i <strong>veri negativi</strong> (persone sane correttamente escluse dal test).</li>
<li>
<span class="math inline">\(FP\)</span> sono i <strong>falsi positivi</strong> (persone sane erroneamente diagnosticate come malate).</li>
</ul>
</li>
</ol>
<p><strong>Rappresentazione in tabella</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 39%">
<col style="width: 39%">
<col style="width: 13%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Positivo <span class="math inline">\((T^+)\)</span>
</th>
<th style="text-align: center;">Test Negativo <span class="math inline">\((T^-)\)</span>
</th>
<th style="text-align: center;">Totale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">
<strong>Malato</strong> <span class="math inline">\((M^+)\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(P(T^+ \cap M^+)\)</span> (<strong>Sensibilità</strong>)</td>
<td style="text-align: center;">
<span class="math inline">\(P(T^- \cap M^+)\)</span> (<span class="math inline">\(1 -\)</span> Sensibilità)</td>
<td style="text-align: center;"><span class="math inline">\(P(M^+)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">
<strong>Sano</strong> <span class="math inline">\((M^-)\)</span>
</td>
<td style="text-align: center;">
<span class="math inline">\(P(T^+ \cap M^-)\)</span> (<span class="math inline">\(1 -\)</span> Specificità)</td>
<td style="text-align: center;">
<span class="math inline">\(P(T^- \cap M^-)\)</span> (<strong>Specificità</strong>)</td>
<td style="text-align: center;"><span class="math inline">\(P(M^-)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Totale</strong></td>
<td style="text-align: center;"><span class="math inline">\(P(T^+)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(P(T^-)\)</span></td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Questa tabella descrive il legame tra i risultati del test e la condizione effettiva dell’individuo.</p>
<section id="calcolo-del-valore-predittivo-positivo-vpp" class="level4" data-number="30.5.1.1"><h4 data-number="30.5.1.1" class="anchored" data-anchor-id="calcolo-del-valore-predittivo-positivo-vpp">
<span class="header-section-number">30.5.1.1</span> Calcolo del Valore Predittivo Positivo (VPP)</h4>
<p>Il VPP esprime la probabilità che una persona con un test positivo sia effettivamente malata. Usando la regola di Bayes, otteniamo:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) \cdot P(M^+)}{P(T^+)}
\]</span></p>
<p>Sostituendo la formula di <span class="math inline">\(P(T^+)\)</span> dalla tabella:</p>
<p><span class="math display">\[
P(M^+ \mid T^+) = \frac{P(T^+ \mid M^+) \cdot P(M^+)}{P(T^+ \mid M^+) \cdot P(M^+) + P(T^+ \mid M^-) \cdot P(M^-)}
\]</span></p>
<p>Scritto in termini di sensibilità, specificità e prevalenza:</p>
<p><span class="math display">\[
VPP = \frac{(\text{Sensibilità} \times \text{Prevalenza})}{(\text{Sensibilità} \times \text{Prevalenza}) + (1 - \text{Specificità}) \times (1 - \text{Prevalenza})}.
\]</span></p>
</section><section id="calcolo-del-valore-predittivo-negativo-vpn" class="level4" data-number="30.5.1.2"><h4 data-number="30.5.1.2" class="anchored" data-anchor-id="calcolo-del-valore-predittivo-negativo-vpn">
<span class="header-section-number">30.5.1.2</span> Calcolo del Valore Predittivo Negativo (VPN)</h4>
<p>Il VPN esprime la probabilità che una persona con un test negativo sia effettivamente sana. Applicando il teorema di Bayes:</p>
<p><span class="math display">\[
P(M^- \mid T^-) = \frac{P(T^- \mid M^-) \cdot P(M^-)}{P(T^-)}
\]</span></p>
<p>Espandendo <span class="math inline">\(P(T^-)\)</span>:</p>
<p><span class="math display">\[
P(M^- \mid T^-) = \frac{P(T^- \mid M^-) \cdot (1 - P(M^+))}{P(T^- \mid M^-) \cdot (1 - P(M^+)) + P(T^- \mid M^+) \cdot P(M^+)}
\]</span></p>
<p>Sostituendo sensibilità, specificità e prevalenza:</p>
<p><span class="math display">\[
VPN = \frac{\text{Specificità} \times (1 - \text{Prevalenza})}{\text{Specificità} \times (1 - \text{Prevalenza}) + (1 - \text{Sensibilità}) \times \text{Prevalenza}}.
\]</span></p>
</section><section id="interpretazione-e-implicazioni" class="level4" data-number="30.5.1.3"><h4 data-number="30.5.1.3" class="anchored" data-anchor-id="interpretazione-e-implicazioni">
<span class="header-section-number">30.5.1.3</span> Interpretazione e Implicazioni</h4>
<ol type="1">
<li>
<p><strong>Un test con alta sensibilità e specificità non garantisce un VPP elevato</strong> se la prevalenza della malattia è bassa.</p>
<ul>
<li>Anche con un test altamente accurato, un numero elevato di falsi positivi può abbassare la probabilità effettiva di essere malati dopo un test positivo.</li>
</ul>
</li>
<li><p><strong>Il VPN è generalmente più alto quando la prevalenza è bassa</strong>, il che significa che un test negativo è molto affidabile nell’escludere la malattia quando questa è rara.</p></li>
<li><p><strong>Ripetere il test</strong> riduce l’incertezza: se un primo test positivo porta a un VPP basso, un secondo test positivo consecutivo può aumentare notevolmente la probabilità di essere effettivamente malati.</p></li>
</ol>
<p>In conclusione, il valore predittivo di un test non dipende solo dalle sue caratteristiche intrinseche, ma anche dalla prevalenza della malattia. Il <strong>teorema di Bayes</strong> ci fornisce uno strumento essenziale per aggiornare la probabilità di malattia dopo aver osservato il risultato di un test, migliorando la nostra capacità di interpretare i risultati in modo più accurato.</p>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-important callout-titled" title="Lo screening per il cancro al seno mediante mammografia">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lo screening per il cancro al seno mediante mammografia
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Implementiamo in <strong>R</strong> le formule per calcolare il <strong>valore predittivo positivo (VPP)</strong> e il <strong>valore predittivo negativo (VPN)</strong> di un test diagnostico, utilizzando gli stessi dati dell’esercizio precedente sulla mammografia.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">positive_predictive_value_of_diagnostic_test</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">sens</span>, <span class="va">spec</span>, <span class="va">prev</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">(</span><span class="va">sens</span> <span class="op">*</span> <span class="va">prev</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">sens</span> <span class="op">*</span> <span class="va">prev</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">spec</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">prev</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">negative_predictive_value_of_diagnostic_test</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">sens</span>, <span class="va">spec</span>, <span class="va">prev</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">(</span><span class="va">spec</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">prev</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">spec</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">prev</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">sens</span><span class="op">)</span> <span class="op">*</span> <span class="va">prev</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Inseriamo i dati del problema.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sens</span> <span class="op">=</span> <span class="fl">0.9</span>  <span class="co"># sensibilità</span></span>
<span><span class="va">spec</span> <span class="op">=</span> <span class="fl">0.9</span>  <span class="co"># specificità</span></span>
<span><span class="va">prev</span> <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># prevalenza</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il valore predittivo del test positivo è:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">res_pos</span> <span class="op">&lt;-</span> <span class="fu">positive_predictive_value_of_diagnostic_test</span><span class="op">(</span><span class="va">sens</span>, <span class="va">spec</span>, <span class="va">prev</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"P(M+ | T+) = %.3f\n"</span>, <span class="va">res_pos</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; P(M+ | T+) = 0.083</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il valore predittivo del test negativo è:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">res_neg</span> <span class="op">&lt;-</span> <span class="fu">negative_predictive_value_of_diagnostic_test</span><span class="op">(</span><span class="va">sens</span>, <span class="va">spec</span>, <span class="va">prev</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"P(M- | T-) = %.3f\n"</span>, <span class="va">res_neg</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; P(M- | T-) = 0.999</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section><section id="la-fallacia-del-procuratore-e-il-teorema-di-bayes" class="level2" data-number="30.6"><h2 data-number="30.6" class="anchored" data-anchor-id="la-fallacia-del-procuratore-e-il-teorema-di-bayes">
<span class="header-section-number">30.6</span> La Fallacia del Procuratore e il Teorema di Bayes</h2>
<p>Il <strong>teorema di Bayes</strong> non è solo uno strumento utile in ambito medico, ma è anche cruciale nel <strong>sistema giudiziario</strong>, dove errori nell’interpretazione delle probabilità possono avere conseguenze gravi. Un errore logico particolarmente rilevante in questo contesto è la <strong>fallacia del procuratore</strong>.</p>
<section id="cosè-la-fallacia-del-procuratore" class="level3" data-number="30.6.1"><h3 data-number="30.6.1" class="anchored" data-anchor-id="cosè-la-fallacia-del-procuratore">
<span class="header-section-number">30.6.1</span> Cos’è la fallacia del procuratore?</h3>
<p>Questa fallacia si verifica quando si confonde la probabilità di ottenere una certa evidenza se un individuo è innocente con la probabilità che un individuo sia innocente dato che quella evidenza è stata osservata. In altre parole, si scambia <strong><span class="math inline">\(P(T^+ \mid I)\)</span></strong> (la probabilità di ottenere un test positivo se la persona è innocente) con <strong><span class="math inline">\(P(I \mid T^+)\)</span></strong> (la probabilità che la persona sia innocente dato un test positivo).</p>
<p>Questo errore è particolarmente insidioso nei processi giudiziari in cui si utilizzano prove statistiche, come il <strong>test del DNA</strong>. Vediamo un esempio concreto per capire come il teorema di Bayes corregge questa errata interpretazione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 30.3</strong></span> Supponiamo che un test del DNA venga utilizzato per identificare un sospetto all’interno di una popolazione di <strong>65 milioni di persone</strong>. I parametri del test sono:</p>
<ul>
<li><p><strong>Sensibilità</strong>:<br><span class="math display">\[
P(T^+ \mid C) = 99\%
\]</span> Questa è la probabilità che il test dia un risultato positivo se la persona è effettivamente colpevole.</p></li>
<li><p><strong>Specificità</strong>:<br><span class="math display">\[
P(T^- \mid I) = 99.99997\%
\]</span> Questa è la probabilità che il test dia un risultato negativo se la persona è innocente.</p></li>
<li><p><strong>Prevalenza</strong>:<br><span class="math display">\[
P(C) = \frac{1}{65.000.000} \approx 1.54 \times 10^{-8}
\]</span> Ovvero, la probabilità a priori che una persona scelta a caso sia il vero colpevole.</p></li>
</ul>
<p>Un campione di DNA viene confrontato con quello di una persona nel database e il test dà un <strong>risultato positivo</strong>. Vogliamo calcolare la probabilità che questa persona sia effettivamente il colpevole dato il risultato positivo, ovvero:</p>
<p><span class="math display">\[
P(C \mid T^+)
\]</span></p>
<p><strong>Passo 1: Calcolo della probabilità del test positivo <span class="math inline">\(P(T^+)\)</span>.</strong><br>
La probabilità di ottenere un test positivo può derivare da due scenari:</p>
<ol type="1">
<li>Il test è positivo perché la persona è colpevole.</li>
<li>Il test è positivo perché la persona è innocente ma ha ricevuto un <strong>falso positivo</strong>.</li>
</ol>
<p>Utilizziamo la regola della probabilità totale:</p>
<p><span class="math display">\[
P(T^+) = P(T^+ \mid C) P(C) + P(T^+ \mid I) P(I) ,
\]</span></p>
<p>dove:</p>
<ul>
<li>
<strong><span class="math inline">\(P(T^+ \mid C) = 0.99\)</span></strong> (sensibilità del test)</li>
<li>
<strong><span class="math inline">\(P(I) = 1 - P(C) \approx 0.99999998\)</span></strong> (probabilità che la persona sia innocente)</li>
<li>
<strong><span class="math inline">\(P(T^+ \mid I) = 1 - P(T^- \mid I) = 1 - 0.9999997 = 0.0000003\)</span></strong> (tasso di falsi positivi)</li>
</ul>
<p>Sostituendo i valori:</p>
<p><span class="math display">\[
P(T^+) = (0.99 \times 1.54 \times 10^{-8}) + (0.0000003 \times 0.99999998)
\]</span></p>
<p><span class="math display">\[
P(T^+) = 1.5231 \times 10^{-8} + 2.9999994 \times 10^{-7}
\]</span></p>
<p><span class="math display">\[
P(T^+) = 3.1523 \times 10^{-7}
\]</span></p>
<p><strong>Passo 2: Applicazione del teorema di Bayes.</strong><br>
Ora possiamo calcolare <strong><span class="math inline">\(P(C \mid T^+)\)</span></strong>, ovvero la probabilità che la persona sia colpevole dato un test positivo, usando il teorema di Bayes:</p>
<p><span class="math display">\[
P(C \mid T^+) = \frac{P(T^+ \mid C) P(C)}{P(T^+)} .
\]</span></p>
<p>Sostituendo i valori calcolati:</p>
<p><span class="math display">\[
P(C \mid T^+) = \frac{(0.99 \times 1.54 \times 10^{-8})}{3.1523 \times 10^{-7}}
\]</span></p>
<p><span class="math display">\[
P(C \mid T^+) = \frac{1.5231 \times 10^{-8}}{3.1523 \times 10^{-7}}
\]</span></p>
<p><span class="math display">\[
P(C \mid T^+) \approx 0.0483
\]</span></p>
<p><strong>Risultato</strong>: La probabilità che la persona sia effettivamente colpevole dopo un test del DNA positivo è solo <strong>4.83%</strong>, nonostante la specificità estremamente elevata del test.</p>
<p><strong>Interpretazione: Perché la probabilità è così bassa?</strong><br>
Nonostante il test sia <strong>altamente accurato</strong>, la <strong>bassa prevalenza del colpevole nella popolazione</strong> gioca un ruolo cruciale nel ridurre il valore predittivo del test.</p>
<p>Molti innocenti vengono testati e, anche se il tasso di falsi positivi è molto basso, il numero assoluto di <strong>falsi positivi</strong> è comunque elevato a causa della dimensione della popolazione testata.</p>
<p>Questo significa che <strong>un test positivo non è sufficiente per dimostrare la colpevolezza di un individuo</strong>. Affermare, per esempio, che “c’è solo una probabilità su 3 milioni che il sospetto sia innocente” è errato, perché si confonde la specificità del test con la probabilità effettiva di colpevolezza.</p>
<p><strong>Conclusione: Il pericolo della fallacia del procuratore.</strong><br>
L’errore logico della fallacia del procuratore consiste nello scambiare:</p>
<ul>
<li>
<strong><span class="math inline">\(P(T^+ \mid I)\)</span></strong> (probabilità di ottenere un test positivo se la persona è innocente),</li>
<li>
<strong><span class="math inline">\(P(I \mid T^+)\)</span></strong> (probabilità che la persona sia innocente dato un test positivo).</li>
</ul>
<p>Questo errore porta a <strong>sovrastimare enormemente la colpevolezza</strong> basandosi su prove probabilistiche. È essenziale usare il teorema di Bayes per interpretare correttamente l’evidenza nei processi giudiziari ed evitare ingiustizie.</p>
</div>
</section><section id="principali-insegnamenti" class="level3" data-number="30.6.2"><h3 data-number="30.6.2" class="anchored" data-anchor-id="principali-insegnamenti">
<span class="header-section-number">30.6.2</span> Principali insegnamenti</h3>
<ol type="1">
<li><p><strong>Un’elevata accuratezza del test non garantisce un valore predittivo positivo elevato</strong>.<br>
Anche con una sensibilità e una specificità molto alte, l’interpretazione di un test deve considerare la <strong>prevalenza</strong> della condizione indagata. In popolazioni di grandi dimensioni con bassa prevalenza, il numero assoluto di <strong>falsi positivi</strong> può superare quello dei veri positivi, riducendo drasticamente la probabilità a posteriori che un test positivo corrisponda a un colpevole reale:<br><span class="math display">\[
P(C \mid T^+) = \frac{P(T^+ \mid C) P(C)}{P(T^+)} .
\]</span><br>
Questo spiega perché, nonostante un test altamente accurato, la probabilità di colpevolezza dopo un risultato positivo rimane bassa.</p></li>
<li><p><strong>La probabilità di colpevolezza post-test dipende dalla prevalenza iniziale</strong>.<br>
L’errore principale della fallacia del procuratore consiste nel confondere la probabilità condizionata <strong><span class="math inline">\(P(T^+ \mid I)\)</span></strong> (probabilità di un test positivo per un innocente) con <strong><span class="math inline">\(P(I \mid T^+)\)</span></strong> (probabilità che un individuo sia innocente dato un test positivo).<br>
Il valore predittivo positivo <strong><span class="math inline">\(P(C \mid T^+)\)</span></strong> dipende non solo dalla specificità del test, ma anche dalla rarità della condizione investigata. Ignorare la <strong>probabilità a priori</strong> del colpevole porta a una sovrastima della colpevolezza, generando un rischio concreto di errori giudiziari.</p></li>
<li><p><strong>L’omissione del teorema di Bayes costituisce un errore metodologico critico</strong>.<br>
Il teorema di Bayes fornisce l’unico quadro formale per aggiornare correttamente le credenze alla luce delle nuove evidenze. In ambito forense:<br><span class="math display">\[
P(C \mid T^+) \neq 1 - P(I \mid T^+),
\]</span> ma dipende dal rapporto tra <strong>prevalenza</strong> e <strong>potere diagnostico del test</strong>. L’uso improprio delle probabilità condizionate viola il principio di presunzione d’innocenza e può portare a decisioni errate basate su interpretazioni fallaci delle prove probabilistiche.</p></li>
</ol>
<section id="conclusione-epistemologica" class="level4" data-number="30.6.2.1"><h4 data-number="30.6.2.1" class="anchored" data-anchor-id="conclusione-epistemologica">
<span class="header-section-number">30.6.2.1</span> Conclusione epistemologica</h4>
<p>L’impiego di test probabilistici in ambito giudiziario richiede un’applicazione rigorosa del <strong>teorema di Bayes</strong> per evitare distorsioni interpretative. Solo un corretto aggiornamento delle credenze, integrando:</p>
<ul>
<li>
<strong>la probabilità pre-test</strong> (<span class="math inline">\(P(C)\)</span>, prevalenza del colpevole nella popolazione investigata),<br>
</li>
<li>
<strong>la potenza diagnostica del test</strong> (sensibilità e specificità),<br>
</li>
<li>
<strong>il tasso di errore strumentale</strong> (falsi positivi e falsi negativi),</li>
</ul>
<p>consente di ridurre il rischio di <strong>errori giudiziari sistematici</strong>. In assenza di questa integrazione, anche test estremamente precisi possono condurre a <strong>ingiuste condanne</strong>, trasformando strumenti scientifici affidabili in fonti di distorsione probatoria.</p>
</section></section></section><section id="probabilità-inversa-dal-problema-classico-allinferenza-bayesiana" class="level2" data-number="30.7"><h2 data-number="30.7" class="anchored" data-anchor-id="probabilità-inversa-dal-problema-classico-allinferenza-bayesiana">
<span class="header-section-number">30.7</span> Probabilità Inversa: Dal Problema Classico all’Inferenza Bayesiana</h2>
<p>Gli esempi discussi in questo capitolo mettono in evidenza una distinzione fondamentale tra due tipi di domande probabilistiche:</p>
<ol type="1">
<li>
<strong>Probabilità diretta</strong>:
<ul>
<li>
<em>“Qual è la probabilità di osservare un certo risultato, supponendo che un’ipotesi sia vera?”</em><br>
</li>
</ul>
</li>
<li>
<strong>Probabilità inversa</strong>:
<ul>
<li><em>“Qual è la probabilità che un’ipotesi sia vera, dato il risultato osservato?”</em></li>
</ul>
</li>
</ol>
<p>Questa distinzione è il cuore del teorema di Bayes e della differenza tra l’approccio frequentista e quello bayesiano alla probabilità.</p>
<section id="esempi-di-probabilità-diretta-e-inversa" class="level3" data-number="30.7.1"><h3 data-number="30.7.1" class="anchored" data-anchor-id="esempi-di-probabilità-diretta-e-inversa">
<span class="header-section-number">30.7.1</span> Esempi di Probabilità Diretta e Inversa</h3>
<p>Per comprendere meglio questa differenza, consideriamo il caso del lancio di una moneta:</p>
<ul>
<li><p><strong>Probabilità diretta</strong>: Se ipotizziamo che la moneta sia equa (<span class="math inline">\(P(Testa) = 0.5\)</span>), qual è la probabilità di ottenere <strong>zero teste</strong> in cinque lanci?<br>
Questo è un problema di probabilità diretta, in cui calcoliamo:<br><span class="math display">\[
P(D \mid H) = (0.5)^5 = 0.03125.
\]</span></p></li>
<li><p><strong>Probabilità inversa</strong>: Ora poniamo la domanda opposta. Supponiamo di aver lanciato una moneta cinque volte e di aver osservato <strong>zero teste</strong>. Quanto è probabile che la moneta sia effettivamente equa?<br>
Qui non stiamo calcolando <span class="math inline">\(P(D \mid H)\)</span>, ma piuttosto <span class="math inline">\(P(H \mid D)\)</span>, ovvero l’<strong>ipotesi a posteriori</strong> data l’evidenza osservata. Questo richiede il <strong>teorema di Bayes</strong>, perché dobbiamo combinare la probabilità del dato con un’ipotesi a priori sulla moneta.</p></li>
</ul></section><section id="dalla-probabilità-diretta-alla-probabilità-inversa-il-contributo-di-bayes" class="level3" data-number="30.7.2"><h3 data-number="30.7.2" class="anchored" data-anchor-id="dalla-probabilità-diretta-alla-probabilità-inversa-il-contributo-di-bayes">
<span class="header-section-number">30.7.2</span> Dalla Probabilità Diretta alla Probabilità Inversa: Il Contributo di Bayes</h3>
<p>Per molto tempo, lo studio della probabilità si è focalizzato esclusivamente sulla prima domanda, ossia la probabilità di osservare un certo esito sotto un’ipotesi data. Tuttavia, nel XVIII secolo, il reverendo <strong>Thomas Bayes</strong> si concentrò sulla seconda domanda, introducendo il concetto di <strong>probabilità inversa</strong>.</p>
<p>Questa intuizione ha dato origine a quello che oggi chiamiamo inferenza bayesiana, permettendoci di aggiornare la probabilità di un’ipotesi alla luce di nuove evidenze.</p>
</section><section id="limpatto-della-probabilità-inversa" class="level3" data-number="30.7.3"><h3 data-number="30.7.3" class="anchored" data-anchor-id="limpatto-della-probabilità-inversa">
<span class="header-section-number">30.7.3</span> L’Impatto della Probabilità Inversa</h3>
<p>L’idea di <strong>stimare la probabilità di un’ipotesi data l’evidenza osservata</strong> ha avuto un impatto enorme in numerosi campi:</p>
<ul>
<li>
<strong>Scienza e sperimentazione</strong>: Quanto è probabile che un’ipotesi scientifica sia vera, dato il risultato di un esperimento?<br>
</li>
<li>
<strong>Medicina</strong>: Quanto è probabile che un paziente abbia una malattia, dato un test positivo?<br>
</li>
<li>
<strong>Giustizia</strong>: Quanto è probabile che un sospettato sia colpevole, dato il risultato di un test del DNA?</li>
</ul>
<p>Tutti questi problemi richiedono non solo di calcolare la probabilità di osservare certi dati (<span class="math inline">\(P(D \mid H)\)</span>), ma anche di aggiornare le nostre convinzioni sull’ipotesi stessa (<span class="math inline">\(P(H \mid D)\)</span>).</p>
<p>In conclusione, l’inferenza bayesiana è essenziale per rispondere alla seconda domanda che abbiamo formulato sopra e rappresenta un’estensione fondamentale della teoria della probabilità. Il <strong>teorema di Bayes</strong> ci permette di passare dalla <strong>probabilità diretta</strong> alla <strong>probabilità inversa</strong>, aggiornando in modo rigoroso le nostre credenze in base alle nuove osservazioni. Senza questa prospettiva, molte domande scientifiche e decisionali rimarrebbero senza una risposta adeguata.</p>
<div class="callout callout-style-simple callout-tip callout-titled" title="Il problema di Monty Hall">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Il problema di Monty Hall
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Abbiamo già discusso in precedenza il problema di Monty Hall. Vediamo ora come possa essere risolto utilizzando il Teorema di Bayes.</p>
<p>Ci sono tre porte:</p>
<ul>
<li>dietro una porta c’è un’automobile (il premio);</li>
<li>dietro le altre due porte ci sono delle capre.</li>
</ul>
<p>Il giocatore sceglie una porta (ad esempio, la porta 1). Successivamente, il conduttore (che sa cosa c’è dietro ogni porta) apre una delle due porte rimanenti, rivelando una capra (ad esempio, apre la porta 3). A questo punto, il giocatore può decidere se mantenere la scelta iniziale o cambiare porta.</p>
<p>Vogliamo calcolare la probabilità che l’automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3.</p>
<p>Definizione degli eventi.</p>
<ul>
<li>A1, A2, A3: l’automobile si trova dietro la porta 1, 2 o 3, rispettivamente.</li>
<li>P3: il conduttore apre la porta 3, rivelando una capra.</li>
</ul>
<p>Vogliamo calcolare la probabilità condizionata <span class="math inline">\(P(A2 \mid P3)\)</span>, ovvero la probabilità che l’automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3. La formula di Bayes è:</p>
<p><span class="math display">\[
P(A2 \mid P3) = \frac{P(P3 \mid A2) \cdot P(A2)}{P(P3)}
\]</span></p>
<ol type="1">
<li>
<strong><span class="math inline">\(P(A2)\)</span></strong>: probabilità iniziale che l’automobile sia dietro la porta 2.
<ul>
<li>Poiché ci sono tre porte e solo un’automobile, <span class="math inline">\(P(A2) = \frac{1}{3}\)</span>.</li>
</ul>
</li>
<li>
<strong><span class="math inline">\(P(P3 \mid A2)\)</span></strong>: probabilità che il conduttore apra la porta 3, sapendo che l’automobile è dietro la porta 2.
<ul>
<li>Se l’automobile è dietro la porta 2, il conduttore deve aprire la porta 3 (non può aprire la porta 1, scelta dal giocatore, né la porta 2, dove c’è l’automobile). Quindi, <span class="math inline">\(P(P3 \mid A2) = 1\)</span>.</li>
</ul>
</li>
<li>
<strong><span class="math inline">\(P(P3)\)</span></strong>: probabilità che il conduttore apra la porta 3.
<ul>
<li>Il conduttore può aprire la porta 3 in due casi:
<ul>
<li>Se l’automobile è dietro la porta 2 (con probabilità <span class="math inline">\(\frac{1}{3}\)</span>).</li>
<li>Se l’automobile è dietro la porta 1 (con probabilità <span class="math inline">\(\frac{1}{3}\)</span>), e il conduttore sceglie casualmente tra la porta 2 e la porta 3 (con probabilità <span class="math inline">\(\frac{1}{2}\)</span>).</li>
</ul>
</li>
<li>Quindi: <span class="math display">\[
\begin{align}
P(P3) &amp;= P(P3 \mid A2) \cdot P(A2) + P(P3 \mid A1) \cdot P(A1) \notag\\
&amp;= 1 \cdot \frac{1}{3} + \frac{1}{2} \cdot \frac{1}{3} = \frac{1}{3} + \frac{1}{6} = \frac{1}{2}.\notag
\end{align}
\]</span>
</li>
</ul>
</li>
</ol>
<p>Sostituendo i valori calcolati:</p>
<p><span class="math display">\[
P(A2 \mid P3) = \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}} = \frac{\frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}.
\]</span></p>
<p>In conclusione, la probabilità che l’automobile sia dietro la porta 2, sapendo che il conduttore ha aperto la porta 3, è <span class="math inline">\(\frac{2}{3}\)</span> (circa 66.7%). Questo significa che cambiando porta, il giocatore ha una probabilità di vincita del 66.7%, mentre mantenendo la scelta iniziale la probabilità è solo del 33.3%.</p>
<p>Questo esempio mostra come il Teorema di Bayes permetta di aggiornare le probabilità in base a nuove informazioni. Nel contesto del problema di Monty Hall, cambiare porta raddoppia le possibilità di vincere l’automobile.</p>
</div>
</div>
</div>
</section></section><section id="riflessioni-conclusive" class="level2" data-number="30.8"><h2 data-number="30.8" class="anchored" data-anchor-id="riflessioni-conclusive">
<span class="header-section-number">30.8</span> Riflessioni Conclusive</h2>
<p>In questo capitolo abbiamo esplorato vari esempi, principalmente nel campo medico e forense, per illustrare come il teorema di Bayes permetta di combinare le informazioni derivate dalle osservazioni con le conoscenze precedenti (priori), aggiornando così il nostro grado di convinzione rispetto a un’ipotesi. Il teorema di Bayes fornisce un meccanismo razionale, noto come “aggiornamento bayesiano”, che ci consente di ricalibrare le nostre convinzioni iniziali alla luce di nuove evidenze.</p>
<p>Una lezione fondamentale che il teorema di Bayes ci insegna, sia nella ricerca scientifica che nella vita quotidiana, è che spesso non ci interessa tanto conoscere la probabilità che qualcosa accada assumendo vera un’ipotesi, quanto piuttosto la probabilità che un’ipotesi sia vera, dato che abbiamo osservato una certa evidenza. In altre parole, la forza del teorema di Bayes sta nella sua capacità di affrontare direttamente il problema inverso, cioè come dedurre la verità di un’ipotesi a partire dalle osservazioni.</p>
<p>Il framework bayesiano per l’inferenza probabilistica offre un approccio generale per comprendere come i problemi di induzione possano essere risolti in linea di principio e, forse, anche come possano essere affrontati dalla mente umana.</p>
<p>In questo capitolo ci siamo concentrati sull’applicazione del teorema di Bayes utilizzando probabilità puntuali. Tuttavia, il teorema esprime pienamente il suo potenziale quando sia l’evidenza che i gradi di certezza a priori delle ipotesi sono rappresentati attraverso distribuzioni di probabilità continue. Questo sarà l’argomento centrale nella prossima sezione della dispensa, dove approfondiremo il flusso di lavoro bayesiano e l’uso di distribuzioni continue nell’aggiornamento bayesiano.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.4.2 (2024-10-31)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.3.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] thematic_0.1.6   MetBrewer_0.2.0  ggokabeito_0.1.0 see_0.10.0      </span></span>
<span><span class="co">#&gt;  [5] gridExtra_2.3    patchwork_1.3.0  bayesplot_1.11.1 psych_2.4.12    </span></span>
<span><span class="co">#&gt;  [9] scales_1.3.0     markdown_1.13    knitr_1.49       lubridate_1.9.4 </span></span>
<span><span class="co">#&gt; [13] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.0.4     </span></span>
<span><span class="co">#&gt; [17] readr_2.1.5      tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.1   </span></span>
<span><span class="co">#&gt; [21] tidyverse_2.0.0  rio_1.2.3        here_1.0.1      </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] generics_0.1.3    stringi_1.8.4     lattice_0.22-6    hms_1.1.3        </span></span>
<span><span class="co">#&gt;  [5] digest_0.6.37     magrittr_2.0.3    evaluate_1.0.3    grid_4.4.2       </span></span>
<span><span class="co">#&gt;  [9] timechange_0.3.0  fastmap_1.2.0     rprojroot_2.0.4   jsonlite_1.9.1   </span></span>
<span><span class="co">#&gt; [13] mnormt_2.1.1      cli_3.6.4         rlang_1.1.5       munsell_0.5.1    </span></span>
<span><span class="co">#&gt; [17] withr_3.0.2       tools_4.4.2       parallel_4.4.2    tzdb_0.4.0       </span></span>
<span><span class="co">#&gt; [21] colorspace_2.1-1  pacman_0.5.1      vctrs_0.6.5       R6_2.6.1         </span></span>
<span><span class="co">#&gt; [25] lifecycle_1.0.4   htmlwidgets_1.6.4 pkgconfig_2.0.3   pillar_1.10.1    </span></span>
<span><span class="co">#&gt; [29] gtable_0.3.6      glue_1.8.0        xfun_0.51         tidyselect_1.2.1 </span></span>
<span><span class="co">#&gt; [33] rstudioapi_0.17.1 farver_2.1.2      htmltools_0.5.8.1 nlme_3.1-167     </span></span>
<span><span class="co">#&gt; [37] rmarkdown_2.29    compiler_4.4.2</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-baker2011bayesian" class="csl-entry" role="listitem">
Baker, C., Saxe, R., &amp; Tenenbaum, J. (2011). Bayesian theory of mind: Modeling joint belief-desire attribution. <em>Proceedings of the annual meeting of the cognitive science society</em>, <em>33</em>.
</div>
<div id="ref-bellhouse2004" class="csl-entry" role="listitem">
Bellhouse, D. R. (2004). <em>The Reverend Thomas Bayes, FRS: a biography to celebrate the tercentenary of his birth</em>.
</div>
<div id="ref-caudek2024fenomeni" class="csl-entry" role="listitem">
Caudek, C., &amp; Bruno, N. (2024). Fenomeni stereocinetici, teorie della percezione e sociologia della scienza. <em>Giornale italiano di psicologia</em>, <em>51</em>(3), 451–466.
</div>
<div id="ref-chivers2024everything" class="csl-entry" role="listitem">
Chivers, T. (2024). <em>Everything is Predictable: How Bayesian Statistics Explain Our World</em>. Simon; Schuster.
</div>
<div id="ref-domini20033" class="csl-entry" role="listitem">
Domini, F., &amp; Caudek, C. (2003). 3-D structure perceived from dynamic information: A new theory. <em>Trends in Cognitive Sciences</em>, <em>7</em>(10), 444–449.
</div>
<div id="ref-griffiths2024bayesian" class="csl-entry" role="listitem">
Griffiths, T. L., Chater, N., &amp; Tenenbaum, J. B. (2024). <em>Bayesian models of cognition: reverse engineering the mind</em>. MIT Press.
</div>
<div id="ref-jesseph1993berkeley" class="csl-entry" role="listitem">
Jesseph, D. M. (1993). <em>Berkeley’s philosophy of mathematics</em>. University of Chicago Press.
</div>
<div id="ref-ma2023bayesian" class="csl-entry" role="listitem">
Ma, W. J., Kording, K. P., &amp; Goldreich, D. (2023). <em>Bayesian models of perception and action: An introduction</em>. MIT press.
</div>
<div id="ref-schervish2014probability" class="csl-entry" role="listitem">
Schervish, M. J., &amp; DeGroot, M. H. (2014). <em>Probability and statistics</em> (Vol. 563). Pearson Education London, UK:
</div>
<div id="ref-spiegelhalter2019art" class="csl-entry" role="listitem">
Spiegelhalter, D. (2019). <em>The art of statistics: Learning from data</em>. Penguin UK.
</div>
<div id="ref-stigler1990history" class="csl-entry" role="listitem">
Stigler, S. M. (1990). <em>The history of statistics: The measurement of uncertainty before 1900</em>. Harvard University Press.
</div>
<div id="ref-yuille2006vision" class="csl-entry" role="listitem">
Yuille, A., &amp; Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? <em>Trends in cognitive sciences</em>, <em>10</em>(7), 301–308.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/probability/05_conditional_prob.html" class="pagination-link" aria-label="Probabilità condizionata">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/probability/07_random_var.html" class="pagination-link" aria-label="Variabili casuali">
        <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/probability/06_bayes_theorem.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>