<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Corrado Caudek">
<title>75&nbsp; Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/formal_models/introduction.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-5ce6d56fc2a85cf1942de8a9da5c14ea.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3f72b9a39ab085079172b95de82a88dc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni Generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">La crisi di replicazione e la riforma metodologica in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Campionamento, metodologia sperimentale e studi osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/04_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/05_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Dalla descrizione alla spiegazione: modelli meccanicistici e computazionali in psicologia</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/R/introduction_r_lang.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">R</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/01_r_syntax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Un approccio moderno all’analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/02_utility_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Utility functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/03_r_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/04_r_packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Pacchetti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/05_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduzione a <code>dplyr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/06_quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/07_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’ambiente di programmazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/R/08_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Utilizzo di strumenti AI</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07a_introduction_normal_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Introduzione alla distribuzione normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/11_outlier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Outlier</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_probability_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modelli probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La Probabilità come misura della certezza razionale: un’interpretazione Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_sigma-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Dal Discreto al Continuo: la <span class="math inline">\(\sigma\)</span>-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_prob_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di massa e di densità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11a_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11b_cov_cor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Covarianza e correlazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11c_joint_prob_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Caso continuo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12a_intro_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Introduzione alle distribuzioni di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Assunzione di gaussianità e trasformazioni dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/12_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/13_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori (Prior Predictive Check)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/14_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04a_stan_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Regressione lineare in Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07a_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_anova_2vie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">ANOVA ad due vie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni indipendenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Il modello di Rescorla–Wagner</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Decisioni</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/decision_analysis/01_study_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Analisi delle decisioni</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Missing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/missing/01_mnar_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Dati mancanti in psicologia: identificare e modellare i casi MNAR con un approccio Bayesiano in Stan</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01a_stime_parametri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Intervalli di fiducia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">La grandezza del campione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/05_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_piranha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Il Problema del priming: sfide e paradossi nella psicologia sociale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/09_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Indice</h2>
   
  <ul class="collapse">
<li><a href="#distribuzione-predittiva-posteriore" id="toc-distribuzione-predittiva-posteriore" class="nav-link active" data-scroll-target="#distribuzione-predittiva-posteriore"><span class="header-section-number">75.1</span> Distribuzione predittiva posteriore</a></li>
  <li><a href="#sec-logscore" id="toc-sec-logscore" class="nav-link" data-scroll-target="#sec-logscore"><span class="header-section-number">75.2</span> Il log-score: accuratezza predittiva punto per punto</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" id="toc-leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica"><span class="header-section-number">75.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</a></li>
  <li><a href="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" id="toc-criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="nav-link" data-scroll-target="#criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl"><span class="header-section-number">75.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span></a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani: Log-Score, LPPD, ELPD e LOO-CV</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Obiettivi di apprendimento
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alla fine di questo capitolo, sarai in grado di:</p>
<ul>
<li>comprendere cos’è la distribuzione predittiva posteriore e come si costruisce;</li>
<li>spiegare cosa misura il log-score e come si calcola nella pratica;</li>
<li>distinguere tra LPPD ed ELPD e comprendere il loro significato;</li>
<li>capire come LOO-CV fornisca una stima dell’ELPD;</li>
<li>collegare il confronto tra modelli alla divergenza di Kullback-Leibler.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’<em>entropia</em>, che misura l’incertezza insita in una distribuzione, e la <em>divergenza di Kullback–Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per <em>valutare e confrontare modelli statistici</em> nel contesto bayesiano.</p>
<p>Il punto di partenza è una domanda cruciale: <em>quanto bene il modello riesce a prevedere nuovi dati?</em> Un buon modello non deve solo adattarsi bene ai dati già osservati, ma anche saper <em>generalizzare</em> a situazioni future o a campioni mai visti. Questa distinzione — adattamento vs.&nbsp;generalizzazione — è il cuore della valutazione predittiva.</p>
<p>Per rendere concreta questa idea, immaginiamo di aver sviluppato un test psicologico per prevedere il livello di ansia degli studenti alla vigilia di un esame. Non basta sapere che il modello descrive bene i dati del campione che abbiamo usato per costruirlo: vogliamo anche essere ragionevolmente sicuri che le stesse previsioni funzionino per studenti che non hanno partecipato allo studio. In psicologia, scegliere tra due modelli non è diverso dal decidere quale test usare per prevedere un disturbo: entrambi mirano a capire quale strumento fornisce previsioni più affidabili sui dati futuri.</p>
<p>In questo capitolo introdurremo:</p>
<ul>
<li>la <em>distribuzione predittiva posteriore</em>, che integra l’incertezza sui parametri e ci permette di fare previsioni coerenti con il nostro stato di conoscenza;</li>
<li>il <em>log-score</em>, come misura punto per punto dell’accuratezza predittiva;</li>
<li>due sintesi fondamentali: la <em>LPPD</em> (<em>Log Pointwise Predictive Density</em>) e la <em>ELPD</em> (<em>Expected Log Predictive Density</em>);</li>
<li>la tecnica <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che fornisce una stima empirica dell’ELPD;</li>
<li>il legame concettuale tra ELPD e <em>divergenza di Kullback–Leibler</em>, che permette di interpretare il confronto tra modelli come una ricerca del modello “più vicino” alla distribuzione vera dei dati.</li>
</ul>
<p>L’obiettivo è fornire un quadro chiaro e operativo di come, in Bayes, si possa passare dai principi teorici dell’informazione a strumenti concreti per scegliere, in modo razionale, il modello più adatto.</p>
</section><section id="distribuzione-predittiva-posteriore" class="level2" data-number="75.1"><h2 data-number="75.1" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore">
<span class="header-section-number">75.1</span> Distribuzione predittiva posteriore</h2>
<p>Nel capitolo precedente abbiamo visto come la divergenza di Kullback–Leibler fornisca una misura teorica della distanza tra la realtà e il modello. In questo capitolo ci chiediamo: come possiamo stimare questa distanza quando non conosciamo la “vera” distribuzione generatrice?</p>
<p>Nel capitolo sul modello <em>beta–binomiale</em> abbiamo già incontrato il concetto di <em>distribuzione predittiva posteriore</em>: è lo strumento che, in Bayes, consente di fare previsioni su nuovi dati incorporando sia il modello che la nostra incertezza sui parametri. Qui riprendiamo quell’idea per applicarla al problema più generale della <em>valutazione e confronto tra modelli</em>.</p>
<p>In ambito bayesiano, dopo aver osservato i dati <span class="math inline">\(y\)</span> non otteniamo un singolo valore “migliore” dei parametri, ma una <em>distribuzione posteriore</em> <span class="math inline">\(p(\theta \mid y)\)</span>, che descrive tutti i valori plausibili di <span class="math inline">\(\theta\)</span> e la nostra incertezza su di essi.</p>
<p>Per esempio, uno psicologo che voglia stimare il livello medio di ansia in una popolazione, dopo aver raccolto un campione, non dirà semplicemente “la media è 4.7”, ma piuttosto “il valore più plausibile è 4.7, ma potrebbe ragionevolmente essere compreso tra 4.2 e 5.1”.</p>
<p>Quando vogliamo predire un nuovo dato <span class="math inline">\(\tilde{y}\)</span>, non fissiamo un singolo <span class="math inline">\(\theta\)</span>, ma combiniamo tutte le possibili previsioni condizionate, pesandole in base alla probabilità a posteriori di ciascun valore di <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
q(\tilde{y} \mid y) \;=\; \int p(\tilde{y} \mid \theta) \, p(\theta \mid y) \, d\theta .
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Intuizione">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuizione
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Se conoscessimo esattamente i parametri <span class="math inline">\(\theta\)</span>, useremmo semplicemente <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span> per prevedere il nuovo dato. Poiché non li conosciamo, prendiamo tutte le previsioni possibili e le combiniamo, pesando ciascuna secondo quanto <span class="math inline">\(\theta\)</span> è plausibile nella posteriore. È come consultare più esperti: il parere di ciascuno viene ponderato in base alla fiducia che riponiamo in lui.</p>
</div>
</div>
</div>
<p>In questo capitolo useremo talvolta la notazione compatta <span class="math inline">\(q(\cdot \mid y)\)</span> per riferirci alla distribuzione predittiva posteriore del modello. Più spesso, per chiarezza, scriveremo <span class="math inline">\(p(y_i \mid y)\)</span>, per evidenziare che si tratta della previsione marginale per la singola osservazione <span class="math inline">\(y_i\)</span>, ottenuta integrando la verosimiglianza <span class="math inline">\(p(y_i \mid \theta)\)</span> rispetto alla posteriore <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<section id="il-problema-della-valutazione-predittiva" class="level3" data-number="75.1.1"><h3 data-number="75.1.1" class="anchored" data-anchor-id="il-problema-della-valutazione-predittiva">
<span class="header-section-number">75.1.1</span> Il problema della valutazione predittiva</h3>
<p>Vorremmo sapere <em>quanto</em> la distribuzione predittiva posteriore <span class="math inline">\(q(\tilde{y} \mid y)\)</span> si avvicina alla <em>vera distribuzione generatrice</em> dei dati futuri, <span class="math inline">\(p(\tilde{y})\)</span>. In teoria, questa distanza si misura con la <em>divergenza di Kullback–Leibler</em>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p\left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right] .
\]</span></p>
<p>Qui incontriamo subito un ostacolo: <em>non conosciamo <span class="math inline">\(p(\tilde{y})\)</span></em>. È come voler valutare la precisione di una mappa senza avere accesso al territorio reale. Per aggirare questo problema, useremo <em>misure surrogate</em> — come il <em>log-score</em>, la <em>LPPD</em> e l’<em>ELPD</em> — che stimano indirettamente la bontà predittiva del modello sfruttando in modo ingegnoso i dati osservati.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Mappa concettuale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 39%">
<col style="width: 34%">
</colgroup>
<thead><tr class="header">
<th>Quantità</th>
<th>Significato</th>
<th>Uso principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid \theta)\)</span></td>
<td>Verosimiglianza</td>
<td>Calcolo predittivo</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(\theta \mid y)\)</span></td>
<td>Distribuzione posteriore</td>
<td>Ponderazione</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid y)\)</span></td>
<td>Predizione bayesiana media</td>
<td>Log-score, LPPD</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(y_i \mid y_{-i})\)</span></td>
<td>Predizione LOO (<em>leave-one-out</em>)</td>
<td>ELPD</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(q(\tilde{y} \mid y)\)</span></td>
<td>Distribuzione predittiva complessiva</td>
<td>Divergenza KL, confronto modelli</td>
</tr>
</tbody>
</table>
</div>
</div>
</section></section><section id="sec-logscore" class="level2" data-number="75.2"><h2 data-number="75.2" class="anchored" data-anchor-id="sec-logscore">
<span class="header-section-number">75.2</span> Il log-score: accuratezza predittiva punto per punto</h2>
<p>Una volta definita la <em>distribuzione predittiva posteriore</em>, possiamo chiederci: <em>quanto bene il modello “aveva previsto” ciascun dato osservato?</em><br>
Il <em>log-score</em> risponde a questa domanda, misurando per ogni osservazione <span class="math inline">\(y_i\)</span> il logaritmo della probabilità predittiva che il modello le assegna:</p>
<p><span id="eq-log-score-def"><span class="math display">\[
\log p(y_i \mid y) = \log \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta .
\tag{75.1}\]</span></span></p>
<p>Questa quantità assegna un punteggio alto (meno negativo) se l’osservazione era plausibile per il modello, e basso se era improbabile.</p>
<p>Il <em>log-score totale</em> è la somma su tutte le osservazioni:</p>
<p><span id="eq-log-score-sum-def"><span class="math display">\[
S = \sum_{i=1}^n \log p(y_i \mid y).
\tag{75.2}\]</span></span></p>
<p>Un log-score più alto (meno negativo) indica che il modello attribuisce maggiore probabilità ai dati effettivamente osservati. È, in pratica, un <em>voto di fiducia</em> del modello per ogni osservazione: se ciò che accade era plausibile per il modello, il punteggio sarà alto.</p>
<section id="versione-frequentista-vs.-versione-bayesiana" class="level3" data-number="75.2.1"><h3 data-number="75.2.1" class="anchored" data-anchor-id="versione-frequentista-vs.-versione-bayesiana">
<span class="header-section-number">75.2.1</span> Versione frequentista vs.&nbsp;versione bayesiana</h3>
<p>L’approccio frequentista valuta <span class="math inline">\(p(y_i \mid \hat{\theta})\)</span> usando una <em>stima puntuale</em> dei parametri (es. MLE o MAP), senza rappresentare l’incertezza parametrica:</p>
<p><span class="math display">\[
\log p(y_i \mid \hat{\theta}).
\]</span></p>
<p>L’approccio bayesiano usa la <em>densità predittiva puntuale</em> integrata sulla posteriore dei parametri:</p>
<p><span id="eq-lppd-def"><span class="math display">\[
p(y_i \mid y)= \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta ,
\tag{75.3}\]</span></span></p>
<p>dove <span class="math inline">\(\theta^{(s)}\)</span> sono campioni MCMC dalla distribuzione a posteriori.</p>
</section><section id="sec-lppd" class="level3" data-number="75.2.2"><h3 data-number="75.2.2" class="anchored" data-anchor-id="sec-lppd">
<span class="header-section-number">75.2.2</span> La LPPD: il log score “bayesiano”</h3>
<p>Quando calcoliamo il log-score <em>nella modalità bayesiana</em> e lo sommiamo su tutte le osservazioni, otteniamo la <em>Log Pointwise Predictive Density (LPPD)</em>:</p>
<p><span id="eq-lppd-def"><span class="math display">\[
\text{LPPD} = \sum_{i=1}^n \log \left[\frac{1}{S}\sum_{s=1}^S p(y_i \mid \theta^{(s)})\right].
\tag{75.4}\]</span></span></p>
<p><strong>In sintesi:</strong></p>
<ul>
<li>
<em>Log score totale (frequentista)</em> → usa una sola stima dei parametri.<br>
</li>
<li>
<em>LPPD (bayesiana)</em> → stessa logica, ma integra l’incertezza parametrica usando la distribuzione a posteriori.</li>
</ul>
<section id="come-si-calcola-in-pratica-con-mcmc" class="level4" data-number="75.2.2.1"><h4 data-number="75.2.2.1" class="anchored" data-anchor-id="come-si-calcola-in-pratica-con-mcmc">
<span class="header-section-number">75.2.2.1</span> Come si calcola in pratica con MCMC</h4>
<p>Nella formula teorica dell’<a href="#eq-lppd-def" class="quarto-xref">Equazione&nbsp;<span>75.4</span></a>, <span class="math inline">\(p(y_i \mid y)\)</span> è una <em>media predittiva</em> calcolata integrando:</p>
<ul>
<li>
<span class="math inline">\(p(y_i \mid \theta)\)</span>: la distribuzione dei dati futuri se i parametri fossero <span class="math inline">\(\theta\)</span> (cioè la verosimiglianza condizionata);</li>
<li>
<span class="math inline">\(p(\theta \mid y)\)</span>: la distribuzione posteriore dei parametri, che rappresenta l’incertezza residua dopo aver osservato i dati.</li>
</ul>
<p>Poiché l’integrale non ha quasi mai una soluzione analitica, in pratica lo <em>approssimiamo con i campioni MCMC</em> dalla posteriore:</p>
<p><span id="eq-mcmc-posterior-parameter-distr"><span class="math display">\[
p(y_i \mid y) \approx \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}),
\tag{75.5}\]</span></span></p>
<p>e quindi:</p>
<p><span id="eq-mcmc-log-score"><span class="math display">\[
\text{Log-score} \approx \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^{(s)}) \right].
\tag{75.6}\]</span></span></p>
<p>Con il log-score otteniamo dunque <em>una misura complessiva</em> di quanto bene il modello assegna una probabilità alta probabilità ai dati osservati. Questo è un indice <em>in-sample</em>.</p>
<p><strong>Limite:</strong> proprio perché valuta l’adattamento in-sample, tende a favorire modelli più complessi e può sovrastimare la capacità di generalizzare (<em>overfitting</em>).</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere tre valori posteriori di <span class="math inline">\(\theta\)</span>: 0.3 (peso 0.2), 0.5 (peso 0.5) e 0.7 (peso 0.3). Se la nuova osservazione è <span class="math inline">\(y = 3\)</span> su <span class="math inline">\(n = 5\)</span> tentativi:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">theta_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="fl">3</span>, size <span class="op">=</span> <span class="fl">5</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span><span class="va">p_y_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">likelihoods</span> <span class="op">*</span> <span class="va">posterior_weights</span><span class="op">)</span></span>
<span><span class="va">log_score</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_y_given_y</span><span class="op">)</span></span>
<span><span class="va">log_score</span></span>
<span><span class="co">#&gt; [1] -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il log-score è circa -1.29. Un valore meno negativo indica una previsione migliore.</p>
</div>
</div>
</div>
</section></section><section id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati" class="level3" data-number="75.2.3"><h3 data-number="75.2.3" class="anchored" data-anchor-id="expected-log-predictive-density-elpd-guardare-oltre-i-dati-osservati">
<span class="header-section-number">75.2.3</span> Expected Log Predictive Density (ELPD): guardare oltre i dati osservati</h3>
<p>Per valutare la <em>generalizzazione</em> dobbiamo chiederci: <em>quanto bene il modello predirebbe dati che non ha mai visto?</em> Questo porta all’<em>ELPD</em> (<em>Expected Log Predictive Density</em>), che ha la stessa struttura della LPPD, ma con una differenza cruciale: la predizione di <span class="math inline">\(y_i\)</span> è fatta <em>escludendo <span class="math inline">\(y_i\)</span> dall’adattamento</em> (<em>Leave-One-Out</em>):</p>
<p><span id="eq-elpd-def"><span class="math display">\[
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid y_{-i}),
\tag{75.7}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset privato dell’osservazione <span class="math inline">\(i\)</span>.</p>
<p>Tornando all’esempio del test per l’ansia: la LPPD valuta quanto bene il modello predice i punteggi di ansia degli studenti del campione già osservato; l’ELPD valuta quanto bene predirebbe il punteggio di un nuovo studente usando solo i dati degli altri.</p>
<p>L’ELPD è, in sostanza, una stima empirica (con segno cambiato) della KL divergence: ci dice quanto bene le previsioni del modello si avvicinano ai dati futuri, senza dover conoscere la distribuzione vera.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Interpretazione:</strong> l’ELPD è un <em>log-score out-of-sample</em>: per ogni <span class="math inline">\(y_i\)</span>, lo escludiamo, adattiamo il modello agli altri dati, e valutiamo la probabilità predittiva di <span class="math inline">\(y_i\)</span>. Più alto è l’ELPD, migliore è la capacità del modello di generalizzare a dati nuovi.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere tre osservazioni <span class="math inline">\(y_1, y_2, y_3\)</span> e che il modello stimi:</p>
<p><span class="math display">\[
p(y_1 \mid y_2,y_3)=0.6,\quad p(y_2 \mid y_1,y_3)=0.7,\quad p(y_3 \mid y_1,y_2)=0.5.
\]</span></p>
<p>L’ELPD è:</p>
<p><span class="math display">\[
\log 0.6 + \log 0.7 + \log 0.5 \; \approx\; -0.5108 -0.3567 -0.6931 = -1.5606.
\]</span></p>
<p>Un valore meno negativo indica maggiore capacità predittiva fuori campione.</p>
</div>
</div>
</div>
</section><section id="lppd-vs.-elpd-in-sintesi" class="level3" data-number="75.2.4"><h3 data-number="75.2.4" class="anchored" data-anchor-id="lppd-vs.-elpd-in-sintesi">
<span class="header-section-number">75.2.4</span> LPPD vs.&nbsp;ELPD in sintesi</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 41%">
<col style="width: 25%">
<col style="width: 22%">
</colgroup>
<thead><tr class="header">
<th>Misura</th>
<th>Dati usati per predire <span class="math inline">\(y_i\)</span>
</th>
<th>Valuta</th>
<th>Rischio principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>LPPD</strong></td>
<td>Tutti i dati, incluso <span class="math inline">\(y_i\)</span>
</td>
<td>Adattamento in-sample</td>
<td>Overfitting</td>
</tr>
<tr class="even">
<td><strong>ELPD</strong></td>
<td>Tutti i dati tranne <span class="math inline">\(y_i\)</span> (LOO)</td>
<td>Generalizzazione</td>
<td>—</td>
</tr>
</tbody>
</table>
<p><strong>Metafora.</strong> Consideriamo un esperimento sul riconoscimento di volti. Mostriamo a un partecipante 100 fotografie e lo alleniamo a riconoscerle.</p>
<ul>
<li>
<em>LPPD</em> misura quanto bene il partecipante riconosce <em>quelle stesse 100 foto</em> già viste durante l’allenamento (<em>in-sample</em>).</li>
<li>
<em>ELPD</em> misura quanto bene riconosce <em>nuove foto di persone già viste o di volti mai incontrati prima</em>, cioè immagini non incluse nell’allenamento (<em>out-of-sample</em>).</li>
</ul>
<p>Se il punteggio LPPD è alto ma l’ELPD è basso, significa che il partecipante — o il modello — ha semplicemente <em>memorizzato</em> le foto specifiche, senza aver appreso le caratteristiche generali dei volti che permettono di riconoscerne di nuovi.</p>
</section><section id="il-collegamento-con-la-divergenza-kl" class="level3" data-number="75.2.5"><h3 data-number="75.2.5" class="anchored" data-anchor-id="il-collegamento-con-la-divergenza-kl">
<span class="header-section-number">75.2.5</span> Il collegamento con la divergenza KL</h3>
<p>La <em>divergenza di Kullback–Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>) è la misura teoricamente ideale della distanza tra la distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span>, e la distribuzione predittiva posteriore di un modello, <span class="math inline">\(q(\tilde{y} \mid y)\)</span>. Nel confronto tra due modelli <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span>, la differenza tra le rispettive <span class="math inline">\(D_{\text{KL}}\)</span> equivale alla differenza tra le loro <em>accuratezze predittive medie</em> rispetto a <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, non possiamo calcolare direttamente la KL. Ma possiamo <em>stimarla indirettamente</em> con l’<em>ELPD</em> (<em>Expected Log Predictive Density</em>): un ELPD più alto significa che il modello è più “vicino” alla distribuzione vera.</p>
<p>In sintesi:</p>
<p><span class="math display">\[
\text{Massimizzare ELPD} \;\; \approx \;\; \text{Minimizzare la divergenza KL} .
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Vogliamo confrontare due modelli predittivi per il numero di “teste” in <span class="math inline">\(n=10\)</span> lanci.</p>
<ul>
<li>La <strong>distribuzione vera</strong> è <span class="math inline">\(p(y)=\text{Binom}(n=10,\;p=0.6)\)</span>.</li>
<li>Il <strong>modello candidato</strong> prevede <span class="math inline">\(q(y)=\text{Binom}(n=10,\;q=0.5)\)</span>.</li>
</ul>
<p>L’<em>ELPD</em> di un modello è l’aspettativa, rispetto alla distribuzione vera <span class="math inline">\(p\)</span>, del <em>log-score</em> del modello: <span class="math inline">\(\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]\)</span>. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori <span class="math inline">\(y=0,\dots,n\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri del problema</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># probabilità vera di "testa"</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità ipotizzata dal modello candidato</span></span>
<span></span>
<span><span class="co"># 1) Supporto dei possibili esiti</span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span></span>
<span><span class="co"># 2) Distribuzione vera p(y) su tutto il supporto</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Log-predittiva del modello candidato q su tutto il supporto</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)</span></span>
<span><span class="va">elpd_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 5) "Modello vero": usa q = p. Log-predittiva del modello vero</span></span>
<span><span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)</span></span>
<span><span class="va">elpd_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]</span></span>
<span><span class="va">kl_pq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="op">(</span><span class="va">log_p_y</span> <span class="op">-</span> <span class="va">log_q_y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello candidato (q=0.5): %.4f\n"</span>, <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello candidato (q=0.5): -2.0549</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello vero      (q=0.6): %.4f\n"</span>, <span class="va">elpd_p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello vero      (q=0.6): -1.8536</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza ELPD (vero - candidato): %.4f\n"</span>, <span class="va">elpd_p</span> <span class="op">-</span> <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza ELPD (vero - candidato): 0.2014</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"KL(p || q): %.4f\n"</span>, <span class="va">kl_pq</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; KL(p || q): 0.2014</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Cosa stiamo verificando?</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)\)</span> è <em>più basso</em> (più negativo) del valore ottenuto dal modello vero <span class="math inline">\(\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)\)</span>. → Il modello con <span class="math inline">\(q=0.6\)</span> è <em>più predittivo</em> di quello con <span class="math inline">\(q=0.5\)</span>.</p></li>
<li><p>La <em>differenza</em> tra i due ELPD è <em>uguale</em> (vicina numericamente) alla <em>divergenza di Kullback–Leibler</em>:</p></li>
</ol>
<p><span class="math display">\[
\mathrm{ELPD}(p)-\mathrm{ELPD}(q)
= \sum_y p(y)\big[\log p(y)-\log q(y)\big]
= D_{\mathrm{KL}}(p\|q)\;&gt;\;0.
\]</span></p>
<p>→ Questo mostra <em>algebricamente e numericamente</em> il legame: <em>massimizzare l’ELPD equivale a minimizzare la KL</em>.</p>
<blockquote class="blockquote">
<p>Nota sul log: nel codice usiamo il log naturale (unità in <strong>nat</strong>). Se si preferisce il log in base 2 (unità in <em>bit</em>), basta sostituire <code><a href="https://rdrr.io/r/base/Log.html">log()</a></code> con <code><a href="https://rdrr.io/r/base/Log.html">log2()</a></code>; tutte le quantità cambiano di una costante di scala, ma i <em>confronti</em> tra modelli restano identici.</p>
</blockquote>
<p><strong>In pratica.</strong></p>
<p>In questo esempio abbiamo potuto calcolare l’ELPD <em>vero</em> perché conoscevamo l’intera distribuzione generatrice <span class="math inline">\(p(y)\)</span> e potevamo integrare esattamente. Nella realtà, <span class="math inline">\(p(y)\)</span> è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD <em>empiricamente</em>, ad esempio con la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che sostituisce l’aspettativa rispetto a <span class="math inline">\(p\)</span> con una media sui dati raccolti, lasciando fuori una osservazione alla volta. Questa procedura ci consente di avvicinarci al calcolo ideale della KL, anche senza conoscere <span class="math inline">\(p(y)\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Collegamento chiave</strong><br>
L’ELPD è una stima empirica (con segno cambiato) della divergenza di Kullback–Leibler.<br>
Più alto è l’ELPD, migliore è la capacità predittiva del modello.</p>
</div>
</div>
</div>
</section></section><section id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica" class="level2" data-number="75.3"><h2 data-number="75.3" class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv-stimare-lelpd-nella-pratica">
<span class="header-section-number">75.3</span> Leave-One-Out Cross-Validation (LOO-CV): stimare l’ELPD nella pratica</h2>
<p>Poiché la distribuzione vera dei dati futuri è inaccessibile, dobbiamo usare strategie indirette per stimare quanto bene il nostro modello prevede nuove osservazioni. La validazione incrociata Leave-One-Out (LOO-CV) è una di queste strategie.</p>
<p>Abbiamo visto che l’<em>ELPD</em> è la misura ideale della capacità predittiva di un modello su dati futuri e non osservati. Il problema è che, per definizione, richiede di calcolare un’aspettativa rispetto alla <em>vera</em> distribuzione generatrice <span class="math inline">\(p(\tilde{y})\)</span>, che non conosciamo.</p>
<p><strong>Come possiamo stimarlo in pratica?</strong> La risposta è la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che simula la previsione di nuovi dati usando soltanto i dati osservati.</p>
<section id="cosè-la-loo-cv" class="level3" data-number="75.3.1"><h3 data-number="75.3.1" class="anchored" data-anchor-id="cosè-la-loo-cv">
<span class="header-section-number">75.3.1</span> Cos’è la LOO-CV</h3>
<p>La LOO-CV è un esperimento concettuale molto semplice:</p>
<ol type="1">
<li>Scegli un’osservazione <span class="math inline">\(y_i\)</span> dal dataset.</li>
<li>
<em>Escludila</em> dal set di addestramento.</li>
<li>Adatta il modello ai dati rimanenti <span class="math inline">\(y_{-i}\)</span>.</li>
<li>Calcola la densità predittiva del modello per l’osservazione esclusa: <span class="math inline">\(p(y_i \mid y_{-i})\)</span>.</li>
<li>Ripeti per ogni osservazione e somma i logaritmi ottenuti.</li>
</ol>
<p>La formula è:</p>
<p><span id="eq-loo-def"><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^{n} \log p(y_i \mid y_{-i}),
\tag{75.8}\]</span></span></p>
<p>dove <span class="math inline">\(y_{-i}\)</span> indica il dataset senza l’osservazione <span class="math inline">\(i\)</span>.</p>
<p>La struttura è identica a quella dell’ELPD “ideale”, ma ogni termine è calcolato <em>fuori campione</em>, escludendo l’osservazione corrispondente.</p>
<p>È come escludere uno studente dal gruppo di allenamento e verificare se il modello riesce a predire correttamente il suo punteggio: ripetendo questo processo per ogni studente otteniamo una misura di quanto il modello generalizza a nuovi dati.</p>
</section><section id="perché-loo-cv-funziona" class="level3" data-number="75.3.2"><h3 data-number="75.3.2" class="anchored" data-anchor-id="perché-loo-cv-funziona">
<span class="header-section-number">75.3.2</span> Perché LOO-CV funziona</h3>
<p>L’ELPD può essere scritto come:</p>
<p><span id="eq-loo-def2"><span class="math display">\[
\mathbb{E}_p[\log q(\tilde{y} \mid y)],
\tag{75.9}\]</span></span></p>
<p>dove <span class="math inline">\(q(\tilde{y} \mid y)\)</span> è la distribuzione predittiva del modello. Non possiamo calcolare l’aspettativa rispetto a <span class="math inline">\(p(\tilde{y})\)</span>, ma se trattiamo ogni osservazione <span class="math inline">\(y_i\)</span> come “nuovo dato” generato da <span class="math inline">\(p\)</span>, la <em>media sui dati osservati</em> approssima bene l’aspettativa:</p>
<p><span class="math display">\[
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>In altre parole, LOO-CV ci dice <em>quanto bene il modello predirebbe ciascun punto senza averlo mai visto</em>.</p>
</section><section id="legame-teorico-con-la-divergenza-kl" class="level3" data-number="75.3.3"><h3 data-number="75.3.3" class="anchored" data-anchor-id="legame-teorico-con-la-divergenza-kl">
<span class="header-section-number">75.3.3</span> Legame teorico con la divergenza KL</h3>
<p>La divergenza di Kullback–Leibler misura la distanza tra la distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span> e la distribuzione predittiva del modello <span class="math inline">\(q(\tilde{y} \mid y)\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p[\log p(\tilde{y})] - \mathbb{E}_p[\log q(\tilde{y} \mid y)].
\]</span></p>
<p>Come abbiamo già visto in precedenza, il primo termine, l’entropia di <span class="math inline">\(p\)</span>, è identico per tutti i modelli e quindi <em>scompare nel confronto</em>. La differenza tra due modelli <span class="math inline">\(q_1\)</span> e <span class="math inline">\(q_2\)</span> dipende solo dalle loro accuratezze predittive medie:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q_1) - D_{\text{KL}}(p \parallel q_2) =
\mathbb{E}_p[\log q_2(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_1(\tilde{y} \mid y)].
\]</span></p>
<p>Quindi <em>vince il modello con ELPD più alto</em>.</p>
</section><section id="confronto-tra-modelli" class="level3" data-number="75.3.4"><h3 data-number="75.3.4" class="anchored" data-anchor-id="confronto-tra-modelli">
<span class="header-section-number">75.3.4</span> Confronto tra modelli</h3>
<p>Poiché <span class="math inline">\(p(\tilde{y})\)</span> è sconosciuta, sostituiamo l’aspettativa con la stima empirica LOO-CV:</p>
<p><span id="eq-delta-elpd-def"><span class="math display">\[
\Delta\text{ELPD} = \text{ELPD}_{\text{LOO}}(M_1) - \text{ELPD}_{\text{LOO}}(M_2)
\tag{75.10}\]</span></span></p>
<p><span class="math inline">\(\Delta\text{ELPD}\)</span> approssima la differenza tra le divergenze KL dei modelli rispetto alla distribuzione vera. Oltre alla differenza, possiamo calcolare un <em>errore standard</em> per valutare se la superiorità di un modello è statisticamente convincente.</p>
</section><section id="sintesi" class="level3" data-number="75.3.5"><h3 data-number="75.3.5" class="anchored" data-anchor-id="sintesi">
<span class="header-section-number">75.3.5</span> Sintesi</h3>
<ul>
<li>
<strong>Problema:</strong> L’ELPD ideale richiede <span class="math inline">\(p(\tilde{y})\)</span>, che è sconosciuta.</li>
<li>
<strong>Soluzione:</strong> LOO-CV stima l’ELPD simulando previsioni out-of-sample.</li>
<li>
<strong>Teoria:</strong> L’ELPD è proporzionale (a meno di una costante) al termine di accuratezza nella KL.</li>
<li>
<strong>Pratica:</strong> Massimizzare l’ELPD stimato equivale a scegliere il modello più vicino alla distribuzione vera.</li>
</ul>
<p>Anche se non conosciamo la vera distribuzione dei dati, possiamo <em>usare l’ELPD stimata con LOO-CV come proxy della divergenza KL</em> e quindi come strumento pratico e teoricamente fondato per selezionare modelli che generalizzano bene.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Dati.</strong> Cinque prove: <span class="math inline">\(y=\{1,1,1,0,1\}\)</span> (quattro “successi”, uno “insuccesso”).</p>
<p><strong>Modello A (bayesiano).</strong> Bernoulli<span class="math inline">\((\theta)\)</span> con prior <span class="math inline">\(\theta\sim \text{Beta}(1,1)\)</span> (uninformativa). Per la LOO: per ciascun <span class="math inline">\(i\)</span>, escludi <span class="math inline">\(y_i\)</span>, calcola la posteriore <span class="math inline">\(\theta\mid y_{-i}\sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})\)</span>, dove <span class="math inline">\(s_{-i}=\sum_{j\neq i} y_j\)</span>, <span class="math inline">\(n_{-i}=n-1\)</span>. La <strong>predittiva LOO</strong> per l’osservazione esclusa è:</p>
<ul>
<li>se <span class="math inline">\(y_i=1\)</span>: <span class="math inline">\(p(y_i\mid y_{-i}) = \frac{1+s_{-i}}{(1+s_{-i})+(1+n_{-i}-s_{-i})}\)</span>;</li>
<li>se <span class="math inline">\(y_i=0\)</span>: <span class="math inline">\(p(y_i\mid y_{-i}) = \frac{1+n_{-i}-s_{-i}}{(1+s_{-i})+(1+n_{-i}-s_{-i})}\)</span>.</li>
</ul>
<p><strong>Modello B (di confronto).</strong> Moneta equa fissa: <span class="math inline">\(q=0.5\)</span>. Allora <span class="math inline">\(p(y_i\mid y_{-i})=0.5\)</span> per ogni <span class="math inline">\(i\)</span>.</p>
<p>Calcoliamo le <strong>log-densità predittive punto-per-punto</strong> e poi l’ELPD-LOO (somma delle log-densità).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funzione: log-densità predittiva LOO per il Modello A (Beta(1,1) + Bernoulli)</span></span>
<span><span class="va">loo_log_pred_beta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span>, <span class="va">y</span>, <span class="va">a0</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">b0</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">n</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">yi</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">s_minus</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">yi</span>      <span class="co"># successi escluso i</span></span>
<span>  <span class="va">n_minus</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="va">alpha</span> <span class="op">&lt;-</span> <span class="va">a0</span> <span class="op">+</span> <span class="va">s_minus</span></span>
<span>  <span class="va">beta</span>  <span class="op">&lt;-</span> <span class="va">b0</span> <span class="op">+</span> <span class="op">(</span><span class="va">n_minus</span> <span class="op">-</span> <span class="va">s_minus</span><span class="op">)</span></span>
<span>  <span class="va">p1</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="op">(</span><span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span><span class="op">)</span>  <span class="co"># pred prob di 1</span></span>
<span>  <span class="va">p</span>  <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="va">yi</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="va">p1</span> <span class="kw">else</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Log-densità LOO punto-per-punto</span></span>
<span><span class="va">lp_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="va">loo_log_pred_beta</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">lp_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span>  <span class="co"># Modello B: moneta equa</span></span>
<span></span>
<span><span class="co"># ELPD-LOO (somma delle log-densità)</span></span>
<span><span class="va">elpd_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_beta</span><span class="op">)</span></span>
<span><span class="va">elpd_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_fixed</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Differenza punto-per-punto e SE della differenza di ELPD (stima classica)</span></span>
<span><span class="va">diff_pt</span>  <span class="op">&lt;-</span> <span class="va">lp_beta</span> <span class="op">-</span> <span class="va">lp_fixed</span></span>
<span><span class="va">se_diff</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">diff_pt</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">diff_pt</span><span class="op">)</span><span class="op">)</span>  <span class="co"># SE della somma</span></span>
<span></span>
<span><span class="co"># Piccola tabella riassuntiva</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>, y <span class="op">=</span> <span class="va">y</span>,</span>
<span>  lp_beta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_beta</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  lp_fixed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_fixed</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  diff <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">diff_pt</span>, <span class="fl">6</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt;   i y lp_beta lp_fixed    diff</span></span>
<span><span class="co">#&gt; 1 1 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 2 2 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 3 3 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="co">#&gt; 4 4 0 -1.7918  -0.6931 -1.0986</span></span>
<span><span class="co">#&gt; 5 5 1 -0.4055  -0.6931  0.2877</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"\nELPD-LOO Modello A (Beta-Bernoulli): %.6f\n"</span>, <span class="va">elpd_beta</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ELPD-LOO Modello A (Beta-Bernoulli): -3.413620</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD-LOO Modello B (q=0.5 fisso)   : %.6f\n"</span>, <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD-LOO Modello B (q=0.5 fisso)   : -3.465736</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza (A - B)                 : %.6f\n"</span>, <span class="va">elpd_beta</span> <span class="op">-</span> <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza (A - B)                 : 0.052116</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"SE della differenza                : %.6f\n"</span>, <span class="va">se_diff</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; SE della differenza                : 1.386294</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Interpretazione.</strong></p>
<ul>
<li>Le <strong>righe</strong> mostrano, per ogni <span class="math inline">\(i\)</span>, la log-densità LOO del Modello A (che “vede” <span class="math inline">\(n-1\)</span> dati) e quella del Modello B (sempre <span class="math inline">\(\log 0.5\)</span>).</li>
<li>In questo dataset (4 su 5 successi) il Modello A tende a dare <strong>probabilità predittiva &gt; 0.5</strong> ai successi quando lasci fuori un successo; viceversa, penalizza di più l’unico zero lasciato fuori.</li>
<li>La <strong>differenza ELPD-LOO</strong> <span class="math inline">\(=\sum_i (\log p_A - \log p_B)\)</span> può risultare <strong>leggermente &gt; 0</strong> (cioè Modello A migliore), ma con <strong>SE ampio</strong> perché <span class="math inline">\(n\)</span> è piccolo: è normale in esempi didattici così minimali.</li>
</ul>
<blockquote class="blockquote">
<p>Regola pratica: per un confronto più “convincente”, guarda se <span class="math inline">\(|\Delta \text{ELPD}|\)</span> è almeno <em>circa 2 <span class="math inline">\(\times\)</span> SE</em>. Con <span class="math inline">\(n=5\)</span> questo raramente accade: l’obiettivo qui è capire <em>come si calcola</em> e <em>cosa significa</em>.</p>
</blockquote>
</div>
</div>
</div>
</section><section id="elpd-loo-e-il-problema-delloverfitting" class="level3" data-number="75.3.6"><h3 data-number="75.3.6" class="anchored" data-anchor-id="elpd-loo-e-il-problema-delloverfitting">
<span class="header-section-number">75.3.6</span> ELPD-LOO e il problema dell’overfitting</h3>
<p>Valutare un modello sugli stessi dati con cui è stato addestrato tende a <em>sovrastimare</em> la sua capacità predittiva, soprattutto se il modello è molto flessibile. È come giudicare le capacità di uno studente facendogli ripetere esattamente gli stessi esercizi che ha già svolto durante lo studio: otterrà sicuramente un buon punteggio, ma questo non ci dice nulla su quanto bene saprà risolvere problemi <em>nuovi</em>.</p>
<p>La <em>Leave-One-Out Cross-Validation (LOO-CV)</em> risolve questo problema:</p>
<ul>
<li>per ciascuna osservazione <span class="math inline">\(i\)</span>, il modello viene valutato su <span class="math inline">\(y_i\)</span> usando <em>solo dati che non includono <span class="math inline">\(y_i\)</span></em>;</li>
<li>il risultato è una misura <em>out-of-sample</em> della bontà predittiva, molto meno sensibile all’overfitting.</li>
</ul>
<p>Grazie a metodi efficienti come il <em>Pareto-smoothed importance sampling (PSIS)</em>, oggi è possibile stimare l’ELPD-LOO <em>senza dover riadattare il modello <span class="math inline">\(n\)</span> volte</em>. In R, questo è implementato nella funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> (pacchetto {<strong>loo</strong>}), compatibile con <code>brms</code> e <code>rstanarm</code>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: ELPD atteso vs LOO stimato">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: ELPD atteso vs LOO stimato
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Immaginiamo una moneta con probabilità vera di testa <span class="math inline">\(p=0.6\)</span>, ma un modello che assume <span class="math inline">\(q=0.5\)</span>. Calcoliamo prima l’<em>ELPD teorico</em>: il log-score atteso se il modello <span class="math inline">\(q\)</span> predice dati generati da <span class="math inline">\(p\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>  <span class="co"># probabilità vera</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># probabilità assunta dal modello</span></span>
<span></span>
<span><span class="va">y_vals</span>   <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">p_y</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>        <span class="co"># vera distribuzione</span></span>
<span><span class="va">log_q_y</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span>   <span class="co"># log-prob modello</span></span>
<span><span class="va">elpd</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD atteso (modello q = 0.5): %.4f\n"</span>, <span class="va">elpd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD atteso (modello q = 0.5): -2.0549</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ora vediamo come stimare <strong>in pratica</strong> l’ELPD-LOO con <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> partendo da un set di dati simulato:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mc-stan.org/loo/">loo</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simuliamo dati binomiali (10 lanci per 20 soggetti)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">20</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span>,</span>
<span>  n <span class="op">=</span> <span class="va">n</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Modello con q fisso = 0.5 (intercetta logit = 0)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html">brm</a></span><span class="op">(</span><span class="va">k</span> <span class="op">|</span> <span class="fu">trials</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">df</span>,</span>
<span>           family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>           prior <span class="op">=</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html">prior</a></span><span class="op">(</span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/constant.html">constant</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>, class <span class="op">=</span> <span class="st">"Intercept"</span><span class="op">)</span>,  <span class="co"># logit(0.5)=0</span></span>
<span>           iter <span class="op">=</span> <span class="fl">2000</span>, chains <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; Running MCMC with 2 chains, at most 10 in parallel...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) </span></span>
<span><span class="co">#&gt; Chain 1 finished in 0.0 seconds.</span></span>
<span><span class="co">#&gt; Chain 2 finished in 0.0 seconds.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Both chains finished successfully.</span></span>
<span><span class="co">#&gt; Mean chain execution time: 0.0 seconds.</span></span>
<span><span class="co">#&gt; Total execution time: 0.2 seconds.</span></span>
<span></span>
<span><span class="co"># Calcoliamo LOO con PSIS</span></span>
<span><span class="va">loo_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">loo_res</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Computed from 2000 by 20 log-likelihood matrix.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          Estimate  SE</span></span>
<span><span class="co">#&gt; elpd_loo    -41.8 4.3</span></span>
<span><span class="co">#&gt; p_loo         0.0 0.0</span></span>
<span><span class="co">#&gt; looic        83.5 8.6</span></span>
<span><span class="co">#&gt; ------</span></span>
<span><span class="co">#&gt; MCSE of elpd_loo is NA.</span></span>
<span><span class="co">#&gt; MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 0.5]).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pareto k diagnostic values:</span></span>
<span><span class="co">#&gt;                          Count Pct.    Min. ESS</span></span>
<span><span class="co">#&gt; (-Inf, 0.7]   (good)      0      0.0%  &lt;NA&gt;    </span></span>
<span><span class="co">#&gt;    (0.7, 1]   (bad)       0      0.0%  &lt;NA&gt;    </span></span>
<span><span class="co">#&gt;    (1, Inf)   (very bad) 20    100.0%  &lt;NA&gt;    </span></span>
<span><span class="co">#&gt; See help('pareto-k-diagnostic') for details.</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Qui:</p>
<ul>
<li>
<code>loo_res</code> fornisce <strong>ELPD-LOO</strong> (stima pratica di ELPD) e il suo errore standard;</li>
<li>è possibile confrontare due modelli con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code> usando la differenza di ELPD-LOO e il relativo SE;</li>
<li>i valori di <em>Pareto k</em> aiutano a capire se la stima PSIS è affidabile.</li>
</ul>
</div>
</div>
</div>
</section><section id="elpd-loo-e-il-problema-delloverfitting-1" class="level3" data-number="75.3.7"><h3 data-number="75.3.7" class="anchored" data-anchor-id="elpd-loo-e-il-problema-delloverfitting-1">
<span class="header-section-number">75.3.7</span> ELPD-LOO e il problema dell’overfitting</h3>
<p>Valutare un modello sugli stessi dati con cui è stato addestrato tende a <em>sovrastimare</em> la sua capacità predittiva, soprattutto se il modello è molto flessibile. È come giudicare le capacità di uno studente facendogli ripetere esattamente gli stessi esercizi che ha già svolto durante lo studio: otterrà sicuramente un buon punteggio, ma questo non ci dice nulla su quanto bene saprà risolvere problemi <em>nuovi</em>.</p>
<p>La <em>Leave-One-Out Cross-Validation (LOO-CV)</em> risolve questo problema:</p>
<ul>
<li>per ciascuna osservazione <span class="math inline">\(i\)</span>, il modello viene valutato su <span class="math inline">\(y_i\)</span> usando <em>solo dati che non includono <span class="math inline">\(y_i\)</span></em>;</li>
<li>il risultato è una misura <em>out-of-sample</em> della bontà predittiva, molto meno sensibile all’overfitting.</li>
</ul>
<p>Grazie a metodi efficienti come il <em>Pareto-smoothed importance sampling (PSIS)</em>, oggi è possibile stimare l’ELPD-LOO <em>senza dover riadattare il modello <span class="math inline">\(n\)</span> volte</em>. In R, questo è implementato nella funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> (pacchetto {<strong>loo</strong>}), compatibile con <code>brms</code> e <code>rstanarm</code>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="In pratica: stimare e confrontare l'ELPD-LOO">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In pratica: stimare e confrontare l’ELPD-LOO
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>1. Capire il concetto</strong><br>
- L’ELPD misura la capacità predittiva attesa su nuovi dati.<br>
- La LOO-CV fornisce una stima “fuori campione” di questa capacità, riducendo il rischio di overfitting.</p>
<p><strong>2. Usare PSIS-LOO</strong><br>
- Evita di riadattare il modello <span class="math inline">\(n\)</span> volte, mantenendo buona accuratezza.<br>
- È implementato in R nella funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> (pacchetto <strong>loo</strong>), compatibile con <code>brms</code> e <code>rstanarm</code>.</p>
<p><strong>3. Workflow tipico in R</strong><br>
1. Adatta ciascun modello (<code><a href="https://paulbuerkner.com/brms/reference/brm.html">brm()</a></code> o <code>stan_glm()</code>);<br>
2. Estrai le log-likelihood per osservazione (<code><a href="https://mc-stan.org/rstantools/reference/log_lik.html">log_lik()</a></code>);<br>
3. Calcola <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> → ottieni ELPD-LOO, SE, e diagnostici Pareto <em>k</em>;<br>
4. Confronta i modelli con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</p>
<p><strong>4. Decisione</strong><br>
- Preferisci il modello con ELPD-LOO più alto.<br>
- Una differenza ≥ 2×SE è un’indicazione che la differenza è sostanziale.<br>
- Considera anche semplicità e interpretabilità.</p>
</div>
</div>
</div>
</section></section><section id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl" class="level2" data-number="75.4"><h2 data-number="75.4" class="anchored" data-anchor-id="criteri-di-informazione-come-approssimazioni-della-divergenza-d_textkl">
<span class="header-section-number">75.4</span> Criteri di informazione come approssimazioni della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>
</h2>
<p>Oltre alla <em>Leave-One-Out Cross-Validation</em>, esistono altri strumenti per stimare la qualità predittiva di un modello senza dover conoscere la distribuzione vera dei dati. Molti di questi metodi derivano, in modo più o meno diretto, dalla <em>divergenza di Kullback–Leibler</em> <span class="math inline">\(D_{\text{KL}}\)</span>, che — come visto — misura la distanza tra la distribuzione reale e quella stimata dal modello.</p>
<p>L’idea di base è sempre la stessa:</p>
<ul>
<li>valutare quanto bene il modello spiega i dati (<em>bontà di adattamento</em>);</li>
<li>penalizzare la <em>complessità</em> del modello, per ridurre il rischio di <em>overfitting</em>.</li>
</ul>
<p>Questa logica si traduce in <em>criteri di informazione</em> che combinano due componenti:</p>
<ol type="1">
<li>
<em>termine di fit</em>: misura di quanto bene il modello si adatta ai dati osservati (es. log-verosimiglianza, MSE);</li>
<li>
<em>termine di penalizzazione</em>: aumenta con il numero di parametri o con la flessibilità del modello.</li>
</ol>
<p>Tra i criteri più usati troviamo:</p>
<ul>
<li>
<strong>MSE</strong> (Mean Squared Error) – semplice e intuitivo, basato sugli errori di previsione;</li>
<li>
<strong>AIC</strong> (Akaike Information Criterion) – approssima <span class="math inline">\(D_{\text{KL}}\)</span> tra il modello e la verità, penalizzando il numero di parametri;</li>
<li>
<strong>BIC</strong> (Bayesian Information Criterion) – simile all’AIC, ma con penalizzazione più forte per modelli complessi, proporzionale al numero di osservazioni;</li>
<li>
<strong>WAIC</strong> (Widely Applicable Information Criterion) – versione pienamente bayesiana, basata sulle previsioni del modello integrate sull’intera distribuzione a posteriori.</li>
</ul>
<p>Nelle sezioni seguenti vedremo come ciascun criterio si calcola, quali assunzioni richiede e in quali situazioni è preferibile rispetto agli altri.</p>
<section id="errore-quadratico-medio-mse" class="level3" data-number="75.4.1"><h3 data-number="75.4.1" class="anchored" data-anchor-id="errore-quadratico-medio-mse">
<span class="header-section-number">75.4.1</span> Errore Quadratico Medio (MSE)</h3>
<p>L’<em>Errore Quadratico Medio</em> misura la media delle differenze al quadrato tra valori osservati e previsti:</p>
<p><span id="eq-mse-def"><span class="math display">\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
\tag{75.11}\]</span></span></p>
<ul>
<li>Valori più bassi indicano previsioni più vicine ai dati osservati.</li>
<li>Non tiene conto della complessità del modello, quindi può favorire modelli eccessivamente flessibili (<em>overfitting</em>).</li>
</ul>
<p>Utile per valutare l’accuratezza, ma da solo non è adatto a scegliere tra modelli con diversa complessità.</p>
</section><section id="akaike-information-criterion-aic" class="level3" data-number="75.4.2"><h3 data-number="75.4.2" class="anchored" data-anchor-id="akaike-information-criterion-aic">
<span class="header-section-number">75.4.2</span> Akaike Information Criterion (AIC)</h3>
<p>L’<em>AIC</em> è un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> e stima quanta informazione si perde usando un modello per descrivere i dati:</p>
<p><span id="eq-aic-def"><span class="math display">\[
AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{\text{MLE}}) + 2k,
\tag{75.12}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(\hat{\theta}_{\text{MLE}}\)</span>: stima dei parametri ottenuta massimizzando la verosimiglianza;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri del modello.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine valuta l’adattamento del modello ai dati.</li>
<li>Il secondo penalizza la complessità per evitare overfitting.</li>
<li>Un AIC più basso indica un miglior equilibrio tra accuratezza e semplicità.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Basato su assunzioni asintotiche (funziona meglio con campioni grandi).</li>
<li>Usa solo stime puntuali, ignorando l’incertezza dei parametri.</li>
<li>Non è pienamente coerente con l’approccio bayesiano.</li>
</ul></section><section id="bayesian-information-criterion-bic" class="level3" data-number="75.4.3"><h3 data-number="75.4.3" class="anchored" data-anchor-id="bayesian-information-criterion-bic">
<span class="header-section-number">75.4.3</span> Bayesian Information Criterion (BIC)</h3>
<p>Il <em>BIC</em> valuta il compromesso tra <em>adattamento ai dati</em> e <em>complessità del modello</em>, applicando una penalizzazione più severa rispetto all’AIC — soprattutto quando il numero di osservazioni <span class="math inline">\(n\)</span> è grande.</p>
<p><span id="eq-bic-def"><span class="math display">\[
BIC = -2 \log p(y \mid \hat{\theta}) + \log(n) \cdot k,
\tag{75.13}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(p(y \mid \hat{\theta})\)</span>: massima verosimiglianza del modello (o MAP con prior piatti);</li>
<li>
<span class="math inline">\(n\)</span>: numero di osservazioni indipendenti;</li>
<li>
<span class="math inline">\(k\)</span>: numero di parametri stimati.</li>
</ul>
<p><strong>Interpretazione</strong></p>
<ul>
<li>Il primo termine misura l’adattamento ai dati.</li>
<li>Il secondo penalizza la complessità in modo crescente con <span class="math inline">\(n\)</span> e <span class="math inline">\(k\)</span>.</li>
<li>Un BIC più basso indica un compromesso migliore tra accuratezza e parsimonia.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Tende a favorire modelli più semplici quando <span class="math inline">\(n\)</span> è elevato.</li>
<li>Ha una giustificazione teorica bayesiana: in certe condizioni, approssima il log della <em>marginal likelihood</em>.</li>
</ul>
<p><strong>Limiti</strong></p>
<ul>
<li>Si basa su assunzioni forti (indipendenza, modelli regolari, prior deboli).</li>
<li>Può sottoselezionare modelli utili con campioni piccoli o strutture complesse.</li>
</ul></section><section id="widely-applicable-information-criterion-waic" class="level3" data-number="75.4.4"><h3 data-number="75.4.4" class="anchored" data-anchor-id="widely-applicable-information-criterion-waic">
<span class="header-section-number">75.4.4</span> Widely Applicable Information Criterion (WAIC)</h3>
<p>Il <em>WAIC</em> è una versione <em>pienamente bayesiana</em> dell’AIC:</p>
<ul>
<li>utilizza <em>tutta la distribuzione a posteriori</em> dei parametri;</li>
<li>fornisce una stima diretta della <em>capacità predittiva</em> del modello.</li>
</ul>
<p><span id="eq-waic-def"><span class="math display">\[
WAIC = -2 \left[
\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)}) \right) -
\sum_{i=1}^{n} \mathrm{Var}_{\theta^{(s)}} \big( \log p(y_i \mid \theta^{(s)}) \big)
\right],
\tag{75.14}\]</span></span></p>
<p>dove:</p>
<ul>
<li>
<span class="math inline">\(S\)</span> = numero di campioni dalla distribuzione a posteriori;</li>
<li>
<span class="math inline">\(\theta^{(s)}\)</span> = <span class="math inline">\(s\)</span>-esimo campione;</li>
<li>il secondo termine stima il <em>numero effettivo di parametri</em> basato sulla variabilità della log-verosimiglianza.</li>
</ul>
<p><strong>Vantaggi</strong></p>
<ul>
<li>Adatto anche a modelli complessi o non regolari.</li>
<li>Usa direttamente i campioni MCMC.</li>
<li>Migliore dell’AIC per modelli bayesiani, perché incorpora l’incertezza dei parametri.</li>
</ul>
<p><strong>Nota</strong> Il WAIC è strettamente collegato all’ELPD: è una sua stima approssimata ottenuta dalla posteriori, senza bisogno di eseguire la LOO-CV.</p>
<p><strong>Riepilogo comparativo.</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead><tr class="header">
<th>Criterio</th>
<th>Tipo</th>
<th>Penalizza la complessità?</th>
<th>Usa stime puntuali?</th>
<th>Supporta Bayesian MCMC?</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>MSE</td>
<td>Frequentista</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>AIC</td>
<td>Frequentista</td>
<td>✅ (modesta)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="odd">
<td>BIC</td>
<td>Frequentista/Bayesiano</td>
<td>✅ (forte)</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr class="even">
<td>WAIC</td>
<td>Bayesiano</td>
<td>✅ (effettiva)</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr class="odd">
<td>LOO-CV</td>
<td>Bayesiano</td>
<td>✅ (empirica)</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table></section></section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>La selezione del modello, in ottica bayesiana, ruota attorno a una domanda essenziale: <em>quanto bene il modello predice dati che non ha mai visto?</em></p>
<p>Il riferimento teorico è l’<em>Expected Log Predictive Density (ELPD)</em>, che misura quanto la distribuzione predittiva del modello si avvicina alla vera (e ignota) distribuzione dei dati. In termini matematici, massimizzare l’ELPD equivale a minimizzare la <em>divergenza di Kullback–Leibler</em> rispetto alla vera generatrice: due facce dello stesso obiettivo, rappresentare al meglio la realtà sottostante.</p>
<p>Poiché <span class="math inline">\(p_{\text{vera}}(y)\)</span> è sconosciuta, l’ELPD va stimato. Le principali approssimazioni sono:</p>
<ul>
<li>
<strong>LOO-CV</strong> (Leave-One-Out Cross-Validation): oggi lo strumento più affidabile, valuta ogni osservazione come “nuova” e stima la capacità di generalizzazione del modello.</li>
<li>
<strong>WAIC</strong>: alternativa completamente bayesiana, calcolata direttamente dai campioni della posteriori.</li>
<li>
<strong>AIC</strong> e <strong>BIC</strong>: criteri frequenstisti più rapidi ma basati su stime puntuali; utili in contesti semplici.</li>
<li>
<strong>MSE</strong>: misura l’accuratezza sulle osservazioni note, ma non penalizza la complessità e quindi non è adatto alla selezione del modello.</li>
</ul>
<p>Nel confronto tra modelli, la <em>differenza di ELPD</em> (stimata con LOO-CV o WAIC) andrebbe interpretata insieme al relativo <em>errore standard</em>: una regola pratica è considerare rilevante una differenza almeno doppia rispetto all’errore standard.</p>
<p><strong>In sintesi:</strong></p>
<ul>
<li>la buona statistica non si limita a spiegare il passato: sa <em>anticipare il futuro</em>;</li>
<li>la <em>divergenza KL</em> fornisce la misura teorica della distanza tra modello e realtà;</li>
<li>l’<em>ELPD</em>, stimato via LOO-CV o WAIC, traduce questa misura in una valutazione pratica della capacità predittiva;</li>
<li>la scelta del modello ottimale richiede un equilibrio tra accuratezza, generalizzazione e parsimonia.</li>
</ul>
<p>Con questi strumenti possiamo individuare modelli che colgono i veri pattern nei dati, evitando di farsi ingannare dal rumore e garantendo previsioni solide anche in contesti complessi.</p>
</section><section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’ambiente di sviluppo</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Zagreb</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] pillar_1.11.0         tinytable_0.11.0      patchwork_1.3.1      </span></span>
<span><span class="co">#&gt;  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.13.0     </span></span>
<span><span class="co">#&gt;  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.0     </span></span>
<span><span class="co">#&gt; [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         </span></span>
<span><span class="co">#&gt; [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           </span></span>
<span><span class="co">#&gt; [16] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        </span></span>
<span><span class="co">#&gt; [19] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          </span></span>
<span><span class="co">#&gt; [22] rio_1.2.3             here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] svUnit_1.0.6         tidyselect_1.2.1     farver_2.1.2        </span></span>
<span><span class="co">#&gt;  [4] fastmap_1.2.0        TH.data_1.1-3        tensorA_0.36.2.1    </span></span>
<span><span class="co">#&gt;  [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    </span></span>
<span><span class="co">#&gt; [10] lifecycle_1.0.4      processx_3.8.6       survival_3.8-3      </span></span>
<span><span class="co">#&gt; [13] magrittr_2.0.3       compiler_4.5.1       rlang_1.1.6         </span></span>
<span><span class="co">#&gt; [16] tools_4.5.1          data.table_1.17.8    knitr_1.50          </span></span>
<span><span class="co">#&gt; [19] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.8      </span></span>
<span><span class="co">#&gt; [22] curl_6.4.0           cmdstanr_0.9.0       RColorBrewer_1.1-3  </span></span>
<span><span class="co">#&gt; [25] multcomp_1.4-28      abind_1.4-8          withr_3.0.2         </span></span>
<span><span class="co">#&gt; [28] purrr_1.1.0          grid_4.5.1           stats4_4.5.1        </span></span>
<span><span class="co">#&gt; [31] xtable_1.8-4         colorspace_2.1-1     inline_0.3.21       </span></span>
<span><span class="co">#&gt; [34] emmeans_1.11.2       scales_1.4.0         MASS_7.3-65         </span></span>
<span><span class="co">#&gt; [37] cli_3.6.5            mvtnorm_1.3-3        rmarkdown_2.29      </span></span>
<span><span class="co">#&gt; [40] generics_0.1.4       RcppParallel_5.1.10  stringr_1.5.1       </span></span>
<span><span class="co">#&gt; [43] splines_4.5.1        parallel_4.5.1       vctrs_0.6.5         </span></span>
<span><span class="co">#&gt; [46] V8_6.0.5             Matrix_1.7-3         sandwich_3.1-1      </span></span>
<span><span class="co">#&gt; [49] jsonlite_2.0.0       arrayhelpers_1.1-0   glue_1.8.0          </span></span>
<span><span class="co">#&gt; [52] ps_1.9.1             codetools_0.2-20     distributional_0.5.0</span></span>
<span><span class="co">#&gt; [55] lubridate_1.9.4      stringi_1.8.7        gtable_0.3.6        </span></span>
<span><span class="co">#&gt; [58] QuickJSR_1.8.0       htmltools_0.5.8.1    Brobdingnag_1.2-9   </span></span>
<span><span class="co">#&gt; [61] R6_2.6.1             rprojroot_2.1.0      evaluate_1.0.4      </span></span>
<span><span class="co">#&gt; [64] lattice_0.22-7       backports_1.5.0      broom_1.0.9         </span></span>
<span><span class="co">#&gt; [67] snakecase_0.11.1     rstantools_2.4.0     coda_0.19-4.1       </span></span>
<span><span class="co">#&gt; [70] gridExtra_2.3        nlme_3.1-168         checkmate_2.3.2     </span></span>
<span><span class="co">#&gt; [73] xfun_0.52            zoo_1.8-14           pkgconfig_2.0.3</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/formal_models/introduction.html" class="pagination-link" aria-label="Introduzione">
        <span class="nav-page-text">Introduzione</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>