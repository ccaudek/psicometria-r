# Errori di segno e errori di grandezza

::: callout-important
## In questo capitolo imparerai a

- Relazione tra crisi della replicabilità e approccio frequentista.
- Limiti della significatività statistica.
- Errori di grandezza e errori di segno.
- Utilizzo dell'approccio bayesiano per ottenere stime più precise e affidabili.
:::

::: callout-tip
## Prerequisiti

- Leggere [Horoscopes](../../figures/horoscopes.pdf). Alla luce degli approfondimenti forniti in questo corso, questo capitolo di @McElreath_rethinking dovrebbe assumere un significato diverso rispetto a quando è stato letto all'inizio del corso.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(tidyr)
```
:::
## Introduzione

In questo capitolo analizzeremo la relazione tra la crisi della replicabilità e le procedure decisionali statistiche proprie dell'approccio frequentista. In particolare, approfondiremo gli errori di tipo *M* (*magnitude*) e di tipo *S* (*sign*), discussi da @loken2017measurement, e il loro impatto sulla validità dei risultati scientifici.

## Il Filtro della Significatività Statistica

Nel @sec-crisis abbiamo esplorato come la pratica scientifica contemporanea sia spesso compromessa da casi di frode, principalmente a causa delle significative implicazioni economiche legate alla pubblicazione su riviste scientifiche di alto prestigio. Questo fenomeno è spesso sottovalutato, poiché le riviste tendono a essere riluttanti nel riconoscere la necessità di correzioni o ritrattazioni degli articoli già pubblicati.

La frode scientifica rappresenta una minaccia evidente alla riproducibilità dei risultati, un pilastro fondamentale del metodo scientifico. Tuttavia, le difficoltà nel replicare i risultati pubblicati non sono attribuibili esclusivamente a frodi o a "pratiche di ricerca disoneste" [@nelson2018psychology]. Un problema intrinseco risiede nel metodo statistico ampiamente adottato dai ricercatori: l'approccio del test di ipotesi nulla e della significatività statistica di stampo fisheriano. Secondo questo metodo, i risultati che non raggiungono la soglia di "significatività statistica" vengono scartati, mentre quelli che la superano sono considerati credibili, basandosi esclusivamente su questo criterio [@wagenmakers2008bayesian].

Tuttavia, l'idea che la significatività statistica sia un filtro affidabile per distinguere i risultati di ricerca "validi" da quelli "non validi" è fondamentalmente errata. Numerose evidenze dimostrano i limiti di questo approccio. Per approfondire questa problematica, esamineremo lo studio di @loken2017measurement, che mette in luce la relazione tra la crisi della replicabilità e le procedure decisionali statistiche dell'approccio frequentista.

Uno dei principali problemi evidenziati da @loken2017measurement è che, in contesti di ricerca complessi, la significatività statistica fornisce prove molto deboli riguardo al segno (*sign*) o all'entità (*magnitude*) degli effetti sottostanti. In altre parole, il raggiungimento della significatività statistica non garantisce né la rilevanza né la consistenza dei risultati ottenuti. Questo solleva seri dubbi sull'affidabilità di tale criterio come unico strumento per valutare la validità delle scoperte scientifiche.

## Errori di tipo *M* e *S*

Per illustrare le implicazioni del processo decisionale basato sulla significatività statistica, @loken2017measurement hanno condotto una simulazione. In questa simulazione, hanno considerato uno scenario di ricerca ipotetico in cui era presente un effetto reale, sebbene molto debole, difficilmente rilevabile senza un ampio volume di dati. Utilizzando l'approccio frequentista, hanno cercato di identificare questo effetto valutando la significatività statistica.

I risultati della simulazione hanno mostrato che, anche in presenza di un effetto reale (seppur debole), l'approccio frequentista riusciva a rilevare un effetto statisticamente significativo solo in una piccola percentuale dei casi. Inoltre, quando un effetto significativo veniva individuato, la stima della sua grandezza risultava altamente imprecisa e instabile.

In sintesi, la significatività statistica fornisce un'indicazione generica sulla presenza o assenza di un effetto, ma non offre informazioni affidabili sulla sua entità o replicabilità. Questo problema è particolarmente rilevante in campi come la psicologia e le scienze sociali, dove gli studi spesso si basano su campioni di dimensioni ridotte e gli effetti osservati tendono a essere modesti. In tali contesti, l'approccio frequentista rischia di produrre prove deboli e instabili, compromettendo la replicabilità e l'affidabilità dei risultati.

### Simulazione semplificata

Riproduciamo qui, in forma semplificata, la simulazione condotta da @loken2017measurement. Iniziamo importando le librerie necessarie.

Consideriamo due campioni casuali indipendenti di dimensioni $n_1 = 20$ e $n_2 = 25$, estratti rispettivamente dalle distribuzioni normali $\mathcal{N}(102, 10)$ e $\mathcal{N}(100, 10)$. La dimensione effettiva dell'effetto ($d$) per la differenza tra le medie dei due campioni è calcolata utilizzando la formula:

$$
d = \frac{\bar{y}_1 - \bar{y}_2}{s_p},
$$

dove $\bar{y}_1$ e $\bar{y}_2$ rappresentano le medie campionarie dei due gruppi, e $s_p$ è la deviazione standard combinata, definita come:

$$
s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}.
$$

In questo caso specifico, la dimensione effettiva dell'effetto risulta molto piccola, indicando che la differenza osservata tra le medie dei due gruppi non ha una rilevanza pratica significativa. Ciò suggerisce che la distinzione tra i due gruppi, seppur statisticamente rilevabile, non ha un impatto sostanziale in contesti reali.

```{r}
# Parametri
mu_1 <- 102  # Media del primo gruppo
mu_2 <- 100  # Media del secondo gruppo
sigma <- 10  # Deviazione standard comune
n1 <- 20     # Numero di osservazioni nel primo gruppo
n2 <- 25     # Numero di osservazioni nel secondo gruppo

# Calcolo della differenza media
mean_difference <- abs(mu_1 - mu_2)

# Calcolo della deviazione standard pooled
pooled_sd <- sqrt(((n1 - 1) * sigma^2 + (n2 - 1) * sigma^2) / (n1 + n2 - 2))

# Calcolo di Cohen's d
cohen_d <- mean_difference / pooled_sd

# Output del risultato
cat("Dimensione dell'effetto (Cohen's d):", cohen_d, "\n")
```

Esaminiamo ora le conclusioni che emergerebbero applicando l'approccio frequentista e la sua procedura di decisione statistica in questo contesto. Supponiamo di condurre una simulazione in cui vengono estratti due campioni: il primo composto da 20 osservazioni provenienti dalla prima popolazione e il secondo da 25 osservazioni provenienti dalla seconda popolazione. Successivamente, applichiamo il test $t$ di Student per confrontare le medie dei due gruppi.

Nell'ambito dell'approccio frequentista, il valore-$p$ ottenuto dal test determina la decisione statistica. Se il valore-$p$ è superiore a 0.05, i risultati vengono considerati non significativi e, di conseguenza, scartati. Al contrario, se il valore-$p$ è inferiore a 0.05, il risultato è ritenuto "pubblicabile" e si conclude che esiste una differenza statisticamente significativa tra i due gruppi.

Per valutare in modo approfondito le conclusioni derivate da questa procedura, è necessario ripetere l'intero processo per un numero elevato di iterazioni, ad esempio 50.000 volte. Ciò significa che, in ciascuna iterazione, vengono estratti nuovi campioni, viene calcolato il test $t$ di Student e viene determinato il corrispondente valore-$p$. Ripetendo questo processo su larga scala, è possibile ottenere una distribuzione completa dei risultati, che consente di analizzare la frequenza con cui si ottengono risultati significativi e la stabilità delle stime prodotte dall'approccio frequentista in questo contesto.

```{r}
#| tags: [hide-output]
# Parametri
n_samples <- 50000
mu_1 <- 102
mu_2 <- 100
sigma <- 10
n1 <- 20
n2 <- 25

# Inizializzazione del risultato
res <- c()

# Simulazioni
set.seed(123)  # Per la riproducibilità
for (i in 1:n_samples) {
  # Generazione dei campioni casuali
  y1 <- rnorm(n1, mean = mu_1, sd = sigma)
  y2 <- rnorm(n2, mean = mu_2, sd = sigma)
  
  # Calcolo della dimensione dell'effetto
  y1bar <- mean(y1)
  y2bar <- mean(y2)
  v1 <- var(y1)
  v2 <- var(y2)
  s <- sqrt(((n1 - 1) * v1 + (n2 - 1) * v2) / (n1 + n2 - 2))
  efsize <- (y1bar - y2bar) / s
  
  # Calcolo del valore p
  t_test <- t.test(y1, y2, var.equal = TRUE)
  
  # Salvataggio della dimensione dell'effetto solo per risultati "statisticamente significativi"
  if (t_test$p.value < 0.05) {
    res <- c(res, efsize)
  }
}
```

```{r}
res_df <- data.frame(effect_size = res)

ggplot(res_df, aes(x = effect_size)) +
  geom_histogram(bins = 20, fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(
    xintercept = 0.2, color = "red", linetype = "dashed", 
    size = 1.2, label = "True Effect Size") +
  labs(
    x = "Effect Size",
    y = "Frequency",
    title = "Histogram of Effect Sizes for\n'Statistically Significant' Results"
  ) 
```

Come evidenziato da @loken2017measurement, l’applicazione dell’approccio frequentista nella procedura di decisione statistica può condurre a due tipi di errori rilevanti. Il primo, noto come errore di *magnitude* (grandezza), si manifesta quando i risultati pubblicati tendono a sovrastimare la reale entità dell’effetto. Nella simulazione condotta, nonostante la vera grandezza dell’effetto fosse modesta (0.2), la media della grandezza dell’effetto per i risultati classificati come "statisticamente significativi" era circa 0.8, suggerendo un effetto di entità "ampia". Questo indica una distorsione sistematica verso stime esagerate.

Il secondo errore, chiamato errore di *segno*, si verifica quando, a causa della variabilità campionaria, la direzione dell’effetto viene stimata in modo errato. In tali casi, il ricercatore potrebbe erroneamente concludere che $\mu_2 > \mu_1$, quando in realtà non è così. È importante sottolineare che, anche in queste situazioni, la grandezza assoluta dell’effetto risulta sovrastimata.

Un aspetto degno di nota è che queste conclusioni rimarrebbero valide anche se si considerasse l’intervallo di confidenza per la differenza tra le medie. In sintesi, l’approccio frequentista introduce un errore sistematico nella stima della grandezza dell’effetto, che rappresenta la quantità più rilevante per il ricercatore. In alcuni casi, può persino portare a errori nella determinazione della direzione dell’effetto, compromettendo ulteriormente l’affidabilità delle conclusioni scientifiche.

## Riflessioni Conclusive

In conclusione, l’approccio frequentista non rappresenta un metodo affidabile per valutare i risultati della ricerca e determinarne l’attendibilità o la necessità di scartarli [@gelman2014beyond; @loken2017measurement]. Questa mancanza di affidabilità è dovuta all’introduzione di errori sistematici nella stima della grandezza degli effetti, che in alcuni casi possono persino portare a errori nella direzione dell’effetto stesso. Alla luce di queste criticità, non sembrano esserci motivi validi per continuare a fare affidamento su questo approccio.

Al contrario, l’adozione dell’approccio bayesiano sembra offrire una soluzione più precisa e affidabile per l’analisi dei dati di ricerca. Questo metodo valuta la probabilità delle ipotesi alla luce dei dati osservati, evitando gli errori intrinseci dell’approccio frequentista e fornendo una base più solida per prendere decisioni informate sulla validità dei risultati. In questo modo, l’approccio bayesiano si presenta come un’alternativa più robusta e scientificamente rigorosa.

## Esercizi

::: {#exr-s-m-errors-1}

Esegui una simulazione con 10.000 ripetizioni (nrep = 10000) in cui vengono estratti due campioni casuali dalla stessa popolazione normale con media 0 e deviazione standard 10. Per ogni simulazione, esegui un t-test per confrontare le medie di due gruppi indipendenti.

1. *Proporzione di risultati statisticamente significativi*: Calcola la proporzione di risultati in cui si ottiene un risultato statisticamente significativo (p < 0.05) in questa condizione in cui i campioni provengono dalla stessa popolazione, e quindi non c'è una differenza reale tra i gruppi. Questo ti darà un'idea della frequenza dei falsi positivi.

2. *Grandezza dell'effetto media*: Calcola la grandezza dell'effetto (d di Cohen) media, considerando solo quei test in cui si è ottenuta una differenza statisticamente significativa. Questo valore ti mostrerà quanto grande appare l'effetto quando il risultato è significativo, nonostante la realtà sia priva di un effetto reale.

3. *Ripetizione della simulazione con diverse grandezze campionarie*: Ripeti la simulazione usando due diverse dimensioni campionarie: 20 osservazioni per gruppo e 200 osservazioni per gruppo. Confronta i risultati per capire come la dimensione del campione influenzi la proporzione di falsi positivi e la grandezza dell'effetto media.

4. *Interpretazione dei risultati*: Interpreta i risultati alla luce del concetto del "filtro della significatività statistica". Questo concetto suggerisce che tra tutti gli studi effettuati, tendono ad essere pubblicati e riportati solo quelli che ottengono risultati statisticamente significativi. Di conseguenza, i risultati significativi pubblicati possono sovrastimare la vera grandezza dell'effetto o indicare erroneamente che un effetto esiste quando in realtà non c'è. Questa simulazione dovrebbe mostrare come, anche in assenza di una differenza reale tra gruppi, si possano ottenere risultati apparentemente significativi con una certa frequenza, soprattutto quando la dimensione campionaria è piccola.

:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

