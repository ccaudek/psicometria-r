# Modello di Poisson

::: callout-important
## In questo capitolo imparerai a

- utilizzare **brms** per adattare ai dati un modello di Poisson.
:::

::: callout-tip
## Prerequisiti

- Leggere [Racial Disparities in Police Use of Deadly Force Against Unarmed Individuals Persist After Appropriately Benchmarking Shooting Data on Violent Crime Rates](https://journals.sagepub.com/doi/full/10.1177/1948550620916071) per ottenere una panoramica approfondita su questo fenomeno e sul relativo ambito di ricerca.
:::

::: callout-important
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(HDInterval, lubridate, brms, bayesplot, tidybayes)
```
:::


## Introduzione

In questo capitolo percorriamo l’intero processo che porta dall’idea di contare un evento raro – le sparatorie fatali da parte della polizia statunitense – alla stima del suo tasso medio annuo attraverso un modello di Poisson implementato con il pacchetto **brms**. Per contestualizzare il fenomeno si suggerisce di leggere l’articolo *Racial Disparities in Police Use of Deadly Force Against Unarmed Individuals Persist After Appropriately Benchmarking Shooting Data on Violent Crime Rates* [@ross2021racial]. Non è obbligatorio per seguire il capitolo, ma fornisce lo sfondo sociale e metodologico dei dati che stiamo per analizzare.

## Perché un modello di Poisson?

Quando il fenomeno di interesse è un *conteggio* – per esempio il numero di incidenti, diagnosi o, come nel nostro caso, sparatorie in un anno – la distribuzione di Poisson è spesso una scelta naturale. Questa distribuzione è definita da un solo parametro, $\lambda$, che rappresenta la media (e varianza) del conteggio. In altre parole, se conosci $\lambda$ conosci già la forma completa della distribuzione.

Per non appesantire la lettura ricordiamo qui solo l’essenziale: se $Y$ è il numero di eventi osservati in un certo intervallo di tempo, dire che

$$
Y \sim \text{Poisson}(\lambda)
$$

significa che la probabilità di osservare esattamente $k$ eventi è

$$
P(Y = k) = \frac{e^{-\lambda}\, \lambda^{k}}{k!}, \quad k = 0,1,2,\dots
$$

## Come **brms** codifica $\lambda$

Il pacchetto `brms`, così come la maggior parte dei software di regressione, non stima $\lambda$ direttamente. Per garantire che il tasso rimanga positivo adotta un *link logaritmico*: all’interno del modello viene quindi stimata la quantità

$$
\eta = \log(\lambda).
$$

Nel caso più semplice, senza predittori, `brms` utilizza un solo coefficiente, l’**intercetta** `b_Intercept`, che è proprio l’equivalente di $\eta$. Una volta ottenuti i campioni posteriori di `b_Intercept` è sufficiente applicare l’esponenziale per tornare sul piano di $\lambda$:

```r
lambda <- exp(b_Intercept)  # trasforma log-lambda in lambda
```

Ogni volta che in questo capitolo nomineremo “campioni di $\lambda$” sottintenderemo che abbiamo già eseguito questa trasformazione.

## La domanda di ricerca

Grazie all’archivio pubblico del *Washington Post* disponiamo di tutti i casi di sparatorie fatali accadute negli Stati Uniti dal 2015 in poi. L’interesse è stimare quante se ne verificano in media in un anno e descrivere l’incertezza associata a tale stima.

## Importiamo e prepariamo i dati

```{r}
url <- "https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v2/fatal-police-shootings-data.csv"
raw <- read.csv(url, stringsAsFactors = FALSE)
raw$date <- as.Date(raw$date)
raw$year <- lubridate::year(raw$date)

# Escludiamo il 2025 perché l’anno è ancora in corso e i dati sarebbero incompleti
shootings <- subset(raw, year < 2025)

df <- shootings %>%
  dplyr::count(year, name = "events")
```

```{r}
head(df)
```

A questo punto abbiamo una tabella `df` con due colonne: `year`, che va dal 2015 al 2024, ed `events`, che contiene il numero di sparatorie registrate in ciascun anno.

## Specificare la Distribuzione a Priori 

Prima di osservare i dati vogliamo dichiarare che, secondo la nostra conoscenza precedente, un intervallo plausibile per $\lambda$ va grosso modo da 400 a 900 casi l’anno, con media attorno a 600. Per ottenere una distribuzione lognormale con queste caratteristiche possiamo lavorare sulla scala logaritmica e scegliere `normal(6.4, 0.3)`. Il valore 6.4 è infatti il logaritmo naturale di 600; la deviazione 0.3 produce l’ampiezza desiderata dell’intervallo. In `brms` la specifica è molto compatta:

```{r}
prior_lambda <- prior(normal(6.4, 0.3), class = "Intercept")
```

## Adattare il modello in **brms**

Il cuore dell’analisi si riduce a poche righe, perché il linguaggio di **brms** è pensato per assomigliare alla *formula syntax* di `lm`:

```{r}
#| message: false
#| warning: false
#| output: false
m0 <- brm(
  events ~ 1,                  # ~1 indica solo l’intercetta
  family = poisson(),          # distribuzione di errore
  data   = df,
  prior  = prior_lambda,
  iter   = 3000,
  warmup = 1000,
  chains = 4,
  seed   = 123,
  backend = "cmdstanr"
)
```

Il comando produce più di quattromila campioni posteriori ($1000$ di warm‑up per ciascuna delle quattro catene + $2000$ validi) dell’intercetta logaritmica. Le diagnosi di convergenza – $\hat R$ vicino a 1, effective sample size buona – sono riportate da `summary(m0)`.

## Dalla scala log a $\lambda$

Per passare dalla stima di `b_Intercept` alla stima di $\lambda$ basta eseguire l’esponenziale sui campioni posteriori. Usando **tidybayes** l’operazione è quasi in linguaggio naturale:

```{r}
posterior_lambda <- m0 |>
  spread_draws(b_Intercept) |>
  mutate(lambda = exp(b_Intercept))

posterior_lambda |>
  median_qi(lambda, .width = 0.94)
```

L’output ci dice che il valore più credibile di $\lambda$ è intorno a 1043 casi l’anno, con un intervallo di credibilità al 94 % che va all’incirca da 1023 a 1062.

## Visualizzare la distribuzione a posteriori

Un grafico spesso vale più di mille numeri. Con **ggplot2** e il tema che abbiamo impostato all’inizio la figura è pronta in tre righe:

```{r}
posterior_lambda |>
  ggplot(aes(x = lambda)) +
  stat_halfeye(fill = "skyblue", alpha = 0.6) +
  labs(
    title = "Distribuzione a posteriori del tasso λ",
    x = "Tasso annuo di sparatorie fatali (λ)",
    y = "Densità"
  )
```

L’area più scura al centro dell’half‑eye mette in evidenza l’intervallo più denso del 50 %; l’intera “pena” laterale dell’arco rappresenta invece il 94 %.

## Un’estensione: confronto tra gruppi

Finora abbiamo trattato tutte le sparatorie fatali come se provenissero da un’unica popolazione. Una domanda molto concreta, invece, è se **il tasso di vittime disarmate** differisce tra persone classificate come bianche (codice `W` nel dataset) e tutte le altre. Per rispondere ci basta duplicare lo schema già usato: al posto di un solo tasso medio (λ) ne stimiamo due, uno per ciascun gruppo.

### Costruire il dataset

Il primo passo è filtrare le righe che ci interessano (vittime disarmate) e poi contare, per ogni anno, quante di queste vittime appartengono a ciascun gruppo.  Il codice qui sotto svolge tutto il lavoro di preparazione in un’unica catena di operazioni:

```{r}
url <- "https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v2/fatal-police-shootings-data.csv"

df_groups <- read.csv(url, stringsAsFactors = FALSE) |>
  dplyr::mutate(
    date  = as.Date(date),
    year  = lubridate::year(date),
    group = dplyr::if_else(race == "W", "White", "NonWhite")
  ) |>
  dplyr::filter(year < 2025, armed_with == "unarmed") |>
  dplyr::count(year, group, name = "events")
```

In `df_groups` ogni riga è l’abbinamento tra un anno e un gruppo, con il relativo conteggio di vittime disarmate.

### Specificare il modello

La formula `events ~ 0 + group` dice a **brms** di rinunciare a un’intercetta comune e di stimarne una diversa per ogni valore di `group`. Poiché l’intercetta è, in scala logaritmica, il nostro parametro centrale, otteniamo due log‑tassi distinti.

Per la *prior* partiamo dall’idea che, in ciascun gruppo, potremmo aspettarci circa trenta vittime disarmate l’anno ma con incertezza ampia. Lavorando nella scala log questo si traduce in una distribuzione Normale con media 3.4 e deviazione 0.3 applicata indistintamente alle due intercette:

```{r}
prior_race <- prior(normal(3.4, 0.3), class = "b")
```

Il modello completo è ora immediato:

```{r}
#| message: false
#| warning: false
#| output: false
m_groups <- brm(
  events ~ 0 + group,
  family  = poisson(),
  data    = df_groups,
  prior   = prior_race,
  iter    = 3000, warmup = 1000, chains = 4,
  backend = "cmdstanr", seed = 123
)
```

### Dal log‑tasso al tasso annuale

Dopo aver controllato che tutti gli indicatori di convergenza (in particolare R‑hat) siano a posto, trasformiamo i campioni posteriori con l’esponenziale così da tornare alla scala dei tassi veri e propri:

```{r}
post <- as_draws_df(m_groups) |>
  dplyr::transmute(
    lambda_White    = exp(b_groupWhite),
    lambda_NonWhite = exp(b_groupNonWhite),
    diff_lambda     = lambda_NonWhite - lambda_White
  )
```

Con **tidybayes** riassumiamo in una riga le quantità d’interesse, usando ad esempio un intervallo di credibilità al 94 %:

```{r}
post |>
  median_qi(lambda_White, lambda_NonWhite, diff_lambda, .width = 0.94)
```

Ne risulta che il gruppo NonWhite registra in media circa 12 vittime disarmate in più l’anno rispetto al gruppo White; l’intero intervallo di credibilità rimane sopra lo zero, perciò l’ipotesi di un tasso maggiore fra le persone non bianche è fortemente supportata dai dati.

Con due soli cambiamenti – la variabile `group` nella formula e una *prior* ragionevole per ciascun gruppo – abbiamo esteso il modello di Poisson a una comparazione fra categorie.  Tutto il resto resta identico: link logaritmico, diagnostica delle catene, trasformazione a valle dei campioni.  Una volta compreso questo meccanismo, aggiungere ulteriori gruppi o predittori diventa un esercizio di routine.

## Riflessioni Conclusive

Il modello di Poisson con prior informativa e link logaritmico è un punto di partenza potente e relativamente semplice per modellare conteggi. L’implementazione in **brms** riduce la complessità sintattica al minimo, ma non per questo dobbiamo tralasciare di capire cosa accade dietro le quinte:

* $\lambda$ è il vero protagonista, ma viene stimato indirettamente tramite la sua trasformazione logaritmica.
* La scelta della prior su `b_Intercept` va fatta *nella* scala log, pensando però a cosa significa *nella* scala originale.
* Una volta compreso il passaggio dall’intercetta logaritmica al tasso $\lambda$, è possibile arricchire il modello con più predittori, sapendo esattamente come trasformare il modello.


## Esercizi {.unnumbered} 

::: {.callout-important title="Problemi" collapse="true"}

Nella finale olimpica di calcio 2024, la Spagna ha sconfitto la Francia per 5 a 3. Supponiamo di voler calcolare la probabilità di superiorità della Spagna rispetto alla Francia utilizzando un modello coniugato Gamma-Poisson (o l’approssimazione brms con prior lognormale).  

1. Considera che il numero di gol segnati da una squadra segua una Poisson con parametro $\lambda$.  
2. Specifica un prior su $\lambda$ per entrambe le squadre, ad esempio $\alpha=1$ e $\beta=1$ nella parametrizzazione Gamma classica (oppure una Normal(0,1.4) sull’intercetta, in modo da avere una media a posteriori analoga).  
3. Aggiorna la distribuzione a posteriori conoscendo i gol segnati (5 per la Spagna e 3 per la Francia in una singola partita).  
4. Calcola la probabilità che $\lambda_{\text{Spagna}} > \lambda_{\text{Francia}}$.  

*(Ispirato a “The World Cup Problem”, [@downey2021think].)*  

**Suggerimento:** puoi risolvere il problema in modo analitico (Gamma-Poisson con un solo conteggio) oppure puoi usare **brms** costruendo un dataframe:

```{r}
df_soccer <- data.frame(
  team = c("Spain", "France"),
  goals = c(5, 3)
)
```

- Modello: `goals ~ 0 + team`, `family=poisson()`.
- Prior su `b_teamSpain` e `b_teamFrance`.
- Infine, estrai i draws e calcola la probabilità $\Pr(\exp(b_{\text{Spain}}) > \exp(b_{\text{France}}))$.

:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}


